<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>EFAVDB - Misc</title><link href="https://efavdb.com/" rel="alternate"></link><link href="https://efavdb.com/feeds/misc.atom.xml" rel="self"></link><id>https://efavdb.com/</id><updated>2020-02-14T09:00:00-08:00</updated><subtitle>Everybody's Favorite Data Blog</subtitle><entry><title>Introduction to OpenAI Scholars 2020</title><link href="https://efavdb.com/openai-scholars-intro" rel="alternate"></link><published>2020-02-14T09:00:00-08:00</published><updated>2020-02-14T09:00:00-08:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2020-02-14:/openai-scholars-intro</id><summary type="html">&lt;p&gt;Two weeks ago, I started at the &lt;a href="https://openai.com/blog/openai-scholars-spring-2020/"&gt;OpenAI Scholars&lt;/a&gt; program, which provides the opportunity to study and work full time on a project in an area of deep learning over 4 months.  I’m having a blast!  It’s been a joy focusing 100% on learning and challenging myself in …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Two weeks ago, I started at the &lt;a href="https://openai.com/blog/openai-scholars-spring-2020/"&gt;OpenAI Scholars&lt;/a&gt; program, which provides the opportunity to study and work full time on a project in an area of deep learning over 4 months.  I’m having a blast!  It’s been a joy focusing 100% on learning and challenging myself in an atmosphere full of friendly intellectual energy and&amp;nbsp;drive.&lt;/p&gt;
&lt;p&gt;My mentor is Jerry Tworek, an OpenAI research scientist who works on reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) in robotics, and I’ve also chosen to focus on &lt;span class="caps"&gt;RL&lt;/span&gt; during the program.  I constructed a &lt;a href="https://docs.google.com/document/d/1MlM5bxMqqiUIig5I6Y28fegvbqokjuvS2llVd2dIIRE/edit?usp=sharing"&gt;syllabus&lt;/a&gt; that will definitely evolve over time, but I’ll try to keep it up-to-date to serve as a useful record for myself and a guide for others who might be interested in a similar course of&amp;nbsp;study.&lt;/p&gt;
&lt;p&gt;Some casual notes from the last two&amp;nbsp;weeks:&lt;/p&gt;
&lt;p&gt;(1) There are manifold benefits to working on a topic that is in my mentor’s area of expertise.  For example, I’ve already benefited from Jerry’s intuition around hyperparameter tuning and debugging &lt;span class="caps"&gt;RL&lt;/span&gt;-specific problems, as well as his guidance on major concepts I should focus on in my first month, namely, model-free &lt;span class="caps"&gt;RL&lt;/span&gt; divided broadly into Q-Learning and Policy&amp;nbsp;Gradients.&lt;/p&gt;
&lt;p&gt;(2) &lt;strong&gt;Weights &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Biases&lt;/strong&gt; at &lt;a href="wandb.com"&gt;wandb.com&lt;/a&gt; is a fantastic free tool for tracking machine learning experiments that many people use at OpenAI.  It was staggeringly simple to integrate wandb with my training script &amp;#8212; both for local runs and in the cloud!  Just ~4 extra lines of code, and logged metrics automagically appear in my wandb dashboard, with auto-generated plots grouped by experiment name, saved artifacts,&amp;nbsp;etc.&lt;/p&gt;
&lt;p&gt;Here&amp;#8217;s an example of a &lt;a href="https://app.wandb.ai/frangipane/dqn?workspace=user-frangipane"&gt;dashboard&lt;/a&gt; tracking experiments for my first attempt at implementing a deep &lt;span class="caps"&gt;RL&lt;/span&gt; algorithm from scratch (&lt;span class="caps"&gt;DQN&lt;/span&gt;, or Deep Q learning).  The script that is generating the experiments is still a work in progress, but you can see how few lines were required to integrate with wandb &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/DQN/dqn.py"&gt;here&lt;/a&gt;.  Stay tuned for a blog post about &lt;span class="caps"&gt;DQN&lt;/span&gt; itself in the&amp;nbsp;future!&lt;/p&gt;
&lt;p&gt;(3) I&amp;#8217;ve found it very helpful to parallelize reading Sutton and Barto&amp;#8217;s &lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction&lt;/a&gt;, &lt;em&gt;the&lt;/em&gt; classic text on &lt;span class="caps"&gt;RL&lt;/span&gt;, with watching David Silver&amp;#8217;s pedagogical online &lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;lectures&lt;/a&gt;.  Silver&amp;#8217;s lectures follow the book closely for the first few chapters, then start condensing several chapters per lecture beginning around lecture 4 or 5 &amp;#8212; helpful since I&amp;#8217;m aiming to ramp up on &lt;span class="caps"&gt;RL&lt;/span&gt; over a short period of time!  Silver also supplements with insightful explanations and material that aren&amp;#8217;t covered in the book, e.g. insights about the convergence properties of some &lt;span class="caps"&gt;RL&lt;/span&gt;&amp;nbsp;algorithms.&lt;/p&gt;
&lt;p&gt;Note, Silver contributed to the work on Deep Q Learning applied to Atari that generated a lot of interest in deep &lt;span class="caps"&gt;RL&lt;/span&gt; beginning in 2013, leading to a &lt;a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf"&gt;publication&lt;/a&gt; in Nature in 2015, so his lecture 6 on Value Function Approximation (&lt;a href="https://www.youtube.com/watch?v=UoPei5o4fps"&gt;video&lt;/a&gt;, &lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/FA.pdf"&gt;slides&lt;/a&gt;) is a perfect accompaniment to reading the&amp;nbsp;paper.&lt;/p&gt;</content><category term="Misc"></category><category term="OpenAI"></category><category term="reinforcement learning"></category><category term="machine learning"></category><category term="deep learning"></category></entry><entry><title>Making AI Interpretable with Generative Adversarial Networks</title><link href="https://efavdb.com/gans" rel="alternate"></link><published>2018-04-05T09:43:00-07:00</published><updated>2018-04-05T09:43:00-07:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2018-04-05:/gans</id><summary type="html">&lt;p&gt;It has been quite awhile since I have posted, largely because soon after I started my job at Square I had a child! I hope to have some newer blog post soon. But along those lines I want to share a &lt;a href="https://medium.com/square-corner-blog/making-ai-interpretable-with-generative-adversarial-networks-766abc953edf"&gt;blog post&lt;/a&gt; I did with a coworker (&lt;a href="https://www.linkedin.com/in/juan-hernandez-025a5532/"&gt;Juan Hernandez …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;It has been quite awhile since I have posted, largely because soon after I started my job at Square I had a child! I hope to have some newer blog post soon. But along those lines I want to share a &lt;a href="https://medium.com/square-corner-blog/making-ai-interpretable-with-generative-adversarial-networks-766abc953edf"&gt;blog post&lt;/a&gt; I did with a coworker (&lt;a href="https://www.linkedin.com/in/juan-hernandez-025a5532/"&gt;Juan Hernandez&lt;/a&gt;) for Square that gives a taste of some of the cool data science work we have been up to. This post covers work we did to create a framework for making models interpretable.&lt;br&gt;
&lt;a href="https://medium.com/square-corner-blog/making-ai-interpretable-with-generative-adversarial-networks-766abc953edf"&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/800/1*lhYEmrsW9kqgB8nfIb9GJQ.png"&gt;&lt;/a&gt;&lt;/p&gt;</content><category term="Misc"></category></entry></feed>