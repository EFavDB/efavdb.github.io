<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>EFAVDB - Machine learning</title><link href="https://efavdb.com/" rel="alternate"></link><link href="https://efavdb.com/feeds/machine-learning.atom.xml" rel="self"></link><id>https://efavdb.com/</id><updated>2020-06-29T12:00:00-07:00</updated><subtitle>Everybody's Favorite Data Blog</subtitle><entry><title>Long term credit assignment with temporal reward transport</title><link href="https://efavdb.com/ltca" rel="alternate"></link><published>2020-06-29T12:00:00-07:00</published><updated>2020-06-29T12:00:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2020-06-29:/ltca</id><summary type="html">&lt;p&gt;[&lt;span class="caps"&gt;TOC&lt;/span&gt;]&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Standard reinforcement learning algorithms struggle with poor sample efficiency in the presence of sparse rewards with long temporal delays between action and effect. To address the long term credit assignment problem, we build on the work of [1] to use “temporal reward transport” (&lt;span class="caps"&gt;TRT&lt;/span&gt;) to augment the immediate …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[&lt;span class="caps"&gt;TOC&lt;/span&gt;]&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Standard reinforcement learning algorithms struggle with poor sample efficiency in the presence of sparse rewards with long temporal delays between action and effect. To address the long term credit assignment problem, we build on the work of [1] to use “temporal reward transport” (&lt;span class="caps"&gt;TRT&lt;/span&gt;) to augment the immediate rewards of significant state-action pairs with rewards from the distant future using an attention mechanism to identify candidates for &lt;span class="caps"&gt;TRT&lt;/span&gt;. A series of gridworld experiments show clear improvements in learning when &lt;span class="caps"&gt;TRT&lt;/span&gt; is used in conjunction with a standard advantage actor critic&amp;nbsp;algorithm.&lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Episodic reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) models the interaction of an agent with an environment as a Markov Decision Process with a finite number of time steps &lt;span class="math"&gt;\(T\)&lt;/span&gt;.  The environment dynamics &lt;span class="math"&gt;\(p(s’,r|s, a)\)&lt;/span&gt; are modeled as a joint probability distribution over the next state &lt;span class="math"&gt;\(s'\)&lt;/span&gt; and reward &lt;span class="math"&gt;\(r\)&lt;/span&gt; picked up along the way given the previous state &lt;span class="math"&gt;\(s\)&lt;/span&gt; and action &lt;span class="math"&gt;\(a\)&lt;/span&gt;.  In general, the agent does not have access to an exact model of the&amp;nbsp;environment.&lt;/p&gt;
&lt;p&gt;The agent&amp;#8217;s goal is to maximize its cumulative rewards, the discounted returns &lt;span class="math"&gt;\(G_t\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{return} \tag{1}
G_t := R_{t+1} + \gamma R_{t+2} + … = \sum_{k=0}^T \gamma^k R_{t+k+1}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(0 \leq \gamma \leq 1\)&lt;/span&gt;, and &lt;span class="math"&gt;\(R_{t}\)&lt;/span&gt; is the reward at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;.  In episodic &lt;span class="caps"&gt;RL&lt;/span&gt;, the discount factor &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; is often used to account for uncertainty in the future, to favor rewards now vs. later, and as a variance reduction technique, e.g. in policy gradient methods [2,&amp;nbsp;3].&lt;/p&gt;
&lt;p&gt;Using a discount factor &lt;span class="math"&gt;\(\gamma &amp;lt; 1\)&lt;/span&gt; introduces a timescale by exponentially suppressing rewards in the future by &lt;span class="math"&gt;\(\exp(-n/\tau_{\gamma})\)&lt;/span&gt;.  The number of timesteps it takes for a reward to decay by &lt;span class="math"&gt;\(1/e\)&lt;/span&gt; is &lt;span class="math"&gt;\(\tau_{\gamma} = 1/(1-\gamma)\)&lt;/span&gt;, in units of timesteps, which follows from solving for &lt;span class="math"&gt;\(n\)&lt;/span&gt; after setting the left and right sides of (\ref{discount-timescale}) to be&amp;nbsp;equal&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\label{discount-timescale}\tag{2}
\gamma ^ n \approx \frac{1}{e} = \lim_{n \rightarrow \infty} \left(1 - \frac{1}{n} \right)^n
\end{align}&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;The state value function &lt;span class="math"&gt;\(v_{\pi}(s)\)&lt;/span&gt; is the expected return when starting in state &lt;span class="math"&gt;\(s\)&lt;/span&gt;, following policy &lt;span class="math"&gt;\(\pi(a|s) := p(a|s)\)&lt;/span&gt;, a function of the current&amp;nbsp;state.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{state-value} \tag{3}
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Policy gradient algorithms improve the policy by using gradient ascent along the gradient of the value&amp;nbsp;function.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{policy-gradient} \tag{4} 
\nabla_{\theta} v_\pi(s_0) = \mathbb{E}_{\tau \sim \pi}\left[\sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(A_t | S_t) \mathcal{R}(\tau)\right],
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\tau \sim \pi\)&lt;/span&gt; describes the agent&amp;#8217;s trajectory following policy &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; beginning from state &lt;span class="math"&gt;\(s_0\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\mathcal{R}(\tau)\)&lt;/span&gt; is a function of the rewards obtained along the trajectory.  In practice, policy gradients approximate the expected value in (\ref{policy-gradient}) by sampling, which results in very high variance estimates of the&amp;nbsp;gradient.&lt;/p&gt;
&lt;p&gt;Common techniques to reduce the variance of the estimated policy gradient include&amp;nbsp;[2]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;only assigning credit for rewards (the &amp;#8220;rewards-to-go&amp;#8221;) accumulated after a particular action was taken instead of crediting the action for all rewards from the&amp;nbsp;trajectory.&lt;/li&gt;
&lt;li&gt;subtracting a baseline from the rewards weight that is independent of action.  Oftentimes, this baseline is the value function in&amp;nbsp;(\ref{state-value}).&lt;/li&gt;
&lt;li&gt;using a large batch&amp;nbsp;size.&lt;/li&gt;
&lt;li&gt;using the value function (\ref{state-value}) to bootstrap the returns some number of steps into the future instead of using the full raw discounted return, giving rise to a class of algorithms called actor critics that learn a policy and value function in parallel.  For example, one-step bootstrapping would approximate the discounted returns in (\ref{return})&amp;nbsp;as&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{bootstrap} \tag{5}
G_t = R_{t+1} + \gamma R_{t+2} +  \gamma^2 R_{t+3} + \ldots \approx R_{t+1} + \gamma V(S_{t+1}),
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(V(S_{t+1})\)&lt;/span&gt; is the estimate of the value of state &lt;span class="math"&gt;\(S_{t+1}\)&lt;/span&gt;&amp;nbsp;(\ref{state-value}).&lt;/p&gt;
&lt;p&gt;All of these techniques typically make use of discounting, so an action receives little credit for rewards that happen more than &lt;span class="math"&gt;\(\tau_{\gamma}\)&lt;/span&gt; timesteps in the future, making it challenging for standard reinforcement learning algorithms to learn effective policies in situations where action and effect are separated by long temporal&amp;nbsp;delays.&lt;/p&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;h2&gt;Temporal reward&amp;nbsp;transport&lt;/h2&gt;
&lt;p&gt;We use temporal reward transport (or &lt;span class="caps"&gt;TRT&lt;/span&gt;), inspired directly by the Temporal Value Transport algorithm from [1], to mitigate the loss of signal from discounting by splicing temporally delayed future rewards to the immediate rewards following an action that the &lt;span class="caps"&gt;TRT&lt;/span&gt; algorithm determines should receive&amp;nbsp;credit.&lt;/p&gt;
&lt;p&gt;To assign credit to a specific observation-action pair, we use an attention layer in a neural network binary classifier.  The classifier predicts whether the undiscounted returns for an episode are below or above a certain threshold.  If a particular observation and its associated action are highly attended to for the classification problem, then that triggers the splicing of future rewards in the episode to that particular observation-action&amp;nbsp;pair.&lt;/p&gt;
&lt;p&gt;Model training is divided into two&amp;nbsp;parts:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Experience collection using the current policy in an advantage actor critic (&lt;span class="caps"&gt;A2C&lt;/span&gt;)&amp;nbsp;model.&lt;/li&gt;
&lt;li&gt;Parameter updates for the &lt;span class="caps"&gt;A2C&lt;/span&gt; model and binary&amp;nbsp;classifier.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class="caps"&gt;TRT&lt;/span&gt; happens between step 1 and 2; it plays no role in experience collection, but modifies the collected rewards through the splicing mechanism, thereby affecting the advantage and, consequently, the policy gradient in&amp;nbsp;(\ref{policy-gradient}).&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \notag
R_t \rightarrow R_t + [\text{distal rewards} (t' &amp;gt; t + \tau_\gamma)]
\end{eqnarray}&lt;/div&gt;
&lt;h2&gt;Environment for&amp;nbsp;experiments&lt;/h2&gt;
&lt;p&gt;We created a &lt;a href="https://github.com/openai/gym"&gt;gym&lt;/a&gt; &lt;a href="https://github.com/maximecb/gym-minigrid"&gt;gridworld&lt;/a&gt; environment to specifically study long term credit assignment.  The environment is a simplified version of the 3-d DeepMind Lab experiments laid out in [1].  As in [1], we structure the environment to comprise three phases.  In the first phase, the agent must take an action that yields no immediate reward.  In the second phase, the agent engages with distractions that yield immediate rewards.  In the final phase, the agent can acquire a distal reward, depending on the action it took in phase&amp;nbsp;1.&lt;/p&gt;
&lt;p&gt;Concretely, the gridworld environment consists&amp;nbsp;of:&lt;/p&gt;
&lt;p&gt;(1) Empty grid with key: agent can pick up the key but receives no immediate reward for picking it&amp;nbsp;up.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_gridworld_p1.png" alt="Phase 1" style="width:190px;"&gt;
&lt;/p&gt;

&lt;p&gt;(2) Distractor phase: Agent engages with distractors, gifts that yield immediate&amp;nbsp;rewards.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_gridworld_p2.png" alt="Phase 1" style="width:190px;"&gt;
&lt;/p&gt;

&lt;p&gt;(3) Delayed reward phase: Agent should move to a green goal grid cell.  If the agent is carrying the key when it reaches the goal, it is rewarded extra&amp;nbsp;points.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_gridworld_p3.png" alt="Phase 1" style="width:190px;"&gt;
&lt;/p&gt;

&lt;p&gt;The agent remains in each phase for a fixed period of time, regardless of how quickly it finishes the intermediate task, and then teleports to the next phase.  At the end of each episode, the environment resets with a different random seed that randomizes the placement of the key in phase 1 and distractor objects in phase&amp;nbsp;2.&lt;/p&gt;
&lt;h2&gt;Experiments&lt;/h2&gt;
&lt;p&gt;In all experiments, we fix the time spent in phase 1 and phase 3, the number of distractor gifts in phase 2, as well as the distal reward in phase 3.  In phase 3, the agent receives 5 points for reaching the goal without a key and 20 points for reaching the goal carrying a key (with a small penalty proportional to step count to encourage moving quickly to the&amp;nbsp;goal).&lt;/p&gt;
&lt;p&gt;Our evaluation metric for each experiment is the the distal reward obtained in phase 3, which focuses on whether the agent learns to pick up the key in phase 1 in order to acquire the distal reward, although we verify that the agent is also learning to open the gifts in phase 2 by plotting the overall returns (see &amp;#8220;Data and code availability&amp;#8221;&amp;nbsp;section).&lt;/p&gt;
&lt;p&gt;Each experiment varies a particular parameter in the second phase, namely, the time delay, distractor reward size, and distractor reward variance, and compares the performance of the baseline &lt;span class="caps"&gt;A2C&lt;/span&gt; algorithm with &lt;span class="caps"&gt;A2C&lt;/span&gt; supplemented with &lt;span class="caps"&gt;TRT&lt;/span&gt; (&lt;span class="caps"&gt;A2C&lt;/span&gt;+&lt;span class="caps"&gt;TRT&lt;/span&gt;).&lt;/p&gt;
&lt;h3&gt;Time delay in distractor&amp;nbsp;phase&lt;/h3&gt;
&lt;p&gt;We vary the time spent in the distractor phase, &lt;span class="math"&gt;\(T_2\)&lt;/span&gt;, as a multiple of the discount factor timescale.  We used a discount factor of &lt;span class="math"&gt;\(\gamma=0.99\)&lt;/span&gt;, which corresponds to a timescale of ~100 steps according to (\ref{discount-timescale}).  We ran experiments for &lt;span class="math"&gt;\(T_2 = (0, 0.5, 1, 2) * \tau_{\gamma}\)&lt;/span&gt;.  The distractor reward is 3 points per&amp;nbsp;gift.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_time_delay_expt_plots.png" alt="P2 time delay expt" style="width:800px;"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Fig 1. Returns in phase 3 for time delays in phase 2 of 0.5&lt;span class="math"&gt;\(\tau_{\gamma}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\tau_{\gamma}\)&lt;/span&gt;, and 2&lt;span class="math"&gt;\(\tau_{\gamma}\)&lt;/span&gt;.
&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;As the environment becomes more challenging from left to right with increasing time delay, we see that &lt;span class="caps"&gt;A2C&lt;/span&gt; plateaus around 5 points in phase 3, corresponding to reaching the goal without the key, whereas &lt;span class="caps"&gt;A2C&lt;/span&gt;+&lt;span class="caps"&gt;TRT&lt;/span&gt; increasingly learns to pick up the key over the training&amp;nbsp;period.&lt;/p&gt;
&lt;h3&gt;Distractor reward&amp;nbsp;size&lt;/h3&gt;
&lt;p&gt;We vary the size of the distractor rewards, 4 gifts for the agent to toggle open, in phase 2.  We run experiments for a reward of 0, 1, 5, and 8, resulting in maximum possible rewards in phase 2 of 0, 4, 20, and&amp;nbsp;32.&lt;/p&gt;
&lt;p&gt;In comparison, the maximum possible reward in phase 3 is&amp;nbsp;20.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_reward_expt_plots.png" alt="P2 reward size expt" style="width:800px;"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Fig 2. Returns in phase 3 for distractor rewards of size 0, 5, and 8.
&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;Like the time delay experiments, we see that &lt;span class="caps"&gt;A2C&lt;/span&gt;+&lt;span class="caps"&gt;TRT&lt;/span&gt; shows progress learning to pick up the key, whereas &lt;span class="caps"&gt;A2C&lt;/span&gt; does not over the training period with increasing distractor&amp;nbsp;sizes.&lt;/p&gt;
&lt;h3&gt;Distractor reward&amp;nbsp;variance&lt;/h3&gt;
&lt;p&gt;We fix the mean reward size of the gifts in phase 2 at 5, but change the variance of the rewards by drawing each reward from a uniform distribution centered at 5, with minimum and maximum ranges of [5, 5], [3, 7], and [0, 10], corresponding to variances of 0, 1.33, and 8.33,&amp;nbsp;respectively.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_reward_var_expt_plots.png" alt="P2 reward variance expt" style="width:800px;"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Fig 3. Returns in phase 3 for distractor reward variance of size 0, 1.33, and 8.33.
&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;The signal-to-noise ratio of the policy gradient, defined as the ratio of the squared magnitude of the expected gradient to the variance of the gradient estimate, was shown to be approximately inversely proportional to the variance of the distractor rewards in phase 2 in [1] for &lt;span class="math"&gt;\(\gamma = 1\)&lt;/span&gt;.  The poor performance of &lt;span class="caps"&gt;A2C&lt;/span&gt; in the highest variance (low signal-to-noise ratio) case is consistent with this observation, with a small standard deviation in performance around the plateau value of 5 compared to the experiments on time delay and distractor reward&amp;nbsp;size.&lt;/p&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;Like temporal value transport introduced in [1], &lt;span class="caps"&gt;TRT&lt;/span&gt; is a heuristic.  Nevertheless, coupling this heuristic with &lt;span class="caps"&gt;A2C&lt;/span&gt; has been shown to improve performance on several tasks characterized by delayed rewards that are a challenge for standard deep &lt;span class="caps"&gt;RL&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Our contribution is a simplified, modular implementation of core ideas in [1], namely, splicing additional rewards from the distant future to state-action pairs identified as significant through a self-attention mechanism.  Unlike [1], we implement the self-attention mechanism in a completely separate model and splice the rewards-to-go instead of an estimated value.  In addition to the modularity that comes splitting out the attention mechanism for &lt;span class="caps"&gt;TRT&lt;/span&gt; into a separate model, another advantage of decoupling the models is that we can increase the learning rate of the classifier without destabilizing the learning of the main actor critic model if the classification problem is comparatively&amp;nbsp;easy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related&amp;nbsp;work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Other works also draw on the idea of using hindsight to reduce the variance estimates of the policy gradient, and hence increase sample efficiency.  &amp;#8220;Hindsight credit assignment&amp;#8221; proposed in [7] similarly learns discriminative models in hindsight that give rise to a modified form of the value function, evaluated using tabular models in a few toy environments (not focused specifically on the long term credit assignment problem).  &lt;span class="caps"&gt;RUDDER&lt;/span&gt; [8] is more similar in spirit to [1] and &lt;span class="caps"&gt;TRT&lt;/span&gt; in the focus on redistributing rewards to significant state-action pairs, but identified using saliency analysis on an &lt;span class="caps"&gt;LSTM&lt;/span&gt; instead of an attention&amp;nbsp;mechanism.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Future&amp;nbsp;work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The robustness of the &lt;span class="caps"&gt;TRT&lt;/span&gt; algorithm should be further assessed on a wider variety of environments, including e.g. Atari Bowling, which is another environment with a delayed reward task used for evaluations by [1] and [8].  It remains to be seen whether the attention mechanism and &lt;span class="caps"&gt;TRT&lt;/span&gt; can handle more complex scenarios, in particular scenarios where a sequence of actions must be taken.  Just as it is difficult to extract interpretable features from a linear model in the presence of multicollinearity, it is possible that the attention-based classifier may encounter similar problems identifying important state-action pairs when a sequence of actions is required, as our model has no mechanism for causal&amp;nbsp;reasoning.&lt;/p&gt;
&lt;p&gt;Although our experiments only evaluated &lt;span class="caps"&gt;TRT&lt;/span&gt; on &lt;span class="caps"&gt;A2C&lt;/span&gt;, coupling it with any policy gradient method based on sampling action space should yield similar benefits, which could be straightforwardly tested with our modular&amp;nbsp;implementation.&lt;/p&gt;
&lt;p&gt;A benefit of using self-attention is the temporal granularity over an &lt;span class="caps"&gt;LSTM&lt;/span&gt;.  However, a downside is that our approach relies on having the full context of the episode for the attention mechanism ([1] similarly relies on full episodes), in contrast to other methods that can handle commonly used truncation windows with a bootstrapped final value for non-terminal states.  Holding full episodes in memory can become untenable for very long episodes, but we have not yet worked out a way to handle this situation in the current&amp;nbsp;setup.&lt;/p&gt;
&lt;p&gt;Our first pass implementation transported the raw rewards-to-go instead of the value estimate used in [1], but it is unclear whether transporting the rewards-to-go (essentially re-introducing a portion of the undiscounted Monte Carlo returns) for a subset of important state-action pairs provides a strong enough signal to outweigh the advantages of using a boostrapped estimate intended for variance reduction; the answer may depend on the particular task/environment and is of course contingent on the quality of the value&amp;nbsp;estimate.&lt;/p&gt;
&lt;p&gt;The classifier model itself has a lot of room for experimentation.  The idea of using a classifier was motivated by a wish to easily extract state-action pairs with high returns from the attention layer, although we have yet to explore whether this provides a clear benefit over a regression model like&amp;nbsp;[1].&lt;/p&gt;
&lt;p&gt;The binary classifier is trained to predict whether the rewards-to-go of each subsequence of an episode exceeds a moving average of maximum returns.  On the one hand, this is less intuitive than only making the prediction for undiscounted returns of the full episode and introduces highly non-iid inputs for the classifier, which can make make training less stable.  On the other hand, one can interpret the current format as a form of data augmentation that results in more instances of the positive class (high return episodes) that benefits the&amp;nbsp;classifier.&lt;/p&gt;
&lt;p&gt;If the classifier were modified to only make a single prediction per episode, it may be necessary to create a buffer of recent experiences to shift the distribution of data towards more positive samples for the classifier to draw from in addition to episodes generated from the most recent policy (with the untested assumption that the classifier would be less sensitive to training on such off-policy data than the main actor critic model while benefiting from the higher incidence of the positive&amp;nbsp;class).&lt;/p&gt;
&lt;p&gt;Finally, the &lt;span class="caps"&gt;TRT&lt;/span&gt; algorithm introduces additional hyperparameters that could benefit from additional tuning, including the constant factor multiplying the transported rewards and the attention score threshold to trigger &lt;span class="caps"&gt;TRT&lt;/span&gt;.&lt;/p&gt;
&lt;h1&gt;Methods&lt;/h1&gt;
&lt;h2&gt;Environment&lt;/h2&gt;
&lt;p&gt;The agent receives a partial observation of the environment, the 7x7 grid in front of it, with each grid cell encoding 3 input values, resulting in 7x7x3 values total (not&amp;nbsp;pixels).&lt;/p&gt;
&lt;p&gt;The gridworld environment supports 7 actions: left, right, forward, pickup, drop, toggle,&amp;nbsp;done.&lt;/p&gt;
&lt;p&gt;The environment consists of three&amp;nbsp;phases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Phase 1 &amp;#8220;key&amp;#8221;: 6x6 grid cells, time spent = 30&amp;nbsp;steps&lt;/li&gt;
&lt;li&gt;Phase 2 &amp;#8220;gifts&amp;#8221;: 10x10 grid cells, time spent = 50 steps (except for the time delay experiment, which varies the time&amp;nbsp;spent)&lt;/li&gt;
&lt;li&gt;Phase 3 &amp;#8220;goal&amp;#8221;: 7x7 grid cells, time spent =&amp;nbsp;70.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the agent picks up the key in phase 1, it is initialized carrying the key in phase 3, but not phase 2.  The carrying state is visible to the agent in phase 1 and 3.  Except for the time delay experiment, each episode is 150&amp;nbsp;timesteps.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distractor rewards in phase&amp;nbsp;2:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 distractor objects, gifts that the agent can toggle open, that yield immediate&amp;nbsp;rewards&lt;/li&gt;
&lt;li&gt;Each opened gift yields a mean reward of 3 points (except in the reward size&amp;nbsp;experiment)&lt;/li&gt;
&lt;li&gt;Gift rewards have a variance of 0 (except in the reward variance&amp;nbsp;experiment)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Distal rewards in phase&amp;nbsp;3:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;5 points for reaching the goal without a key and 20 points for reaching the goal carrying a key.  There is a small penalty of &lt;code&gt;-0.9 * step_count / max_steps=70&lt;/code&gt; to encourage moving quickly to the goal.  For convenience of parallelizing experience collection of complete episodes, the time in the final phase is fixed, even if the agent finishes the task of navigating to the green goal earlier.  Furthermore, for convenience of tracking rewards acquired in the final phase, the agent only receives the reward for task completion in the last step of the final phase, even though this last reward reflects the time and state in which the agent initially reached the&amp;nbsp;goal.&lt;/p&gt;
&lt;p&gt;Note, unlike the Reconstructive Memory Agent in [1], our agent does not have the ability to encode and reconstruct memories, and our environment is not set up to test for that&amp;nbsp;ability.&lt;/p&gt;
&lt;h2&gt;Agent&amp;nbsp;model&lt;/h2&gt;
&lt;p&gt;The agent&amp;#8217;s model is an actor critic consisting of 3 components: an image encoder convolutional net (&lt;span class="caps"&gt;CNN&lt;/span&gt;), an recurrent neural net layer providing memory, and dual heads outputting the policy and value.  We used an open-sourced model that has been extensively tested for gym-minigrid environments from&amp;nbsp;[4].&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_a2c_model.png" alt="A2C model" style="width:300px;"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Fig 4. &lt;span class="caps"&gt;A2C&lt;/span&gt; model with three convolutional layers, &lt;span class="caps"&gt;LSTM&lt;/span&gt;, and dual policy and value function heads.
&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;The image encoder consists of three convolutional layers interleaved with rectified linear (ReLU) activation functions.  A max pooling layer also immediately precedes the second convolutional&amp;nbsp;layer.&lt;/p&gt;
&lt;p&gt;The encoded image is followed by a single Long Short Term Memory (&lt;span class="caps"&gt;LSTM&lt;/span&gt;)&amp;nbsp;layer.&lt;/p&gt;
&lt;p&gt;The &lt;span class="caps"&gt;LSTM&lt;/span&gt; outputs a hidden state which feeds into the dual heads of the actor critic.  Both heads consist of two fully connected linear layers sandwiching a tanh activation layer.  The output of the actor, the policy, is the same size as the action space in the environment.  The output of the critic is a scalar corresponding to the estimated&amp;nbsp;value.&lt;/p&gt;
&lt;h2&gt;Binary classifier with&amp;nbsp;self-attention&lt;/h2&gt;
&lt;p&gt;The inputs to the binary classifier are the sequence of image embeddings output by the actor critic model&amp;#8217;s &lt;span class="caps"&gt;CNN&lt;/span&gt; (not the hidden state of the &lt;span class="caps"&gt;LSTM&lt;/span&gt;) and one-hot encoded actions taken in that&amp;nbsp;state.&lt;/p&gt;
&lt;p&gt;The action passes through a linear layer with 32 hidden units before concatenation with the image&amp;nbsp;embedding.&lt;/p&gt;
&lt;p&gt;Next, the concatenated vector &lt;span class="math"&gt;\(\mathbf{x}_i\)&lt;/span&gt; undergoes three separate linear transformations, playing the role of &amp;#8220;query&amp;#8221;, &amp;#8220;key&amp;#8221; and &amp;#8220;value&amp;#8221; (see [6] for an excellent explanation upon which we based our implementation of attention).  Each transformation projects the vector to a space of size equal to the length of the&amp;nbsp;episode.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{key-query} \tag{6}
\mathbf{q}_i &amp;amp;=&amp;amp; \mathbf{W}_q \mathbf{x}_i \\
\mathbf{k}_i &amp;amp;=&amp;amp; \mathbf{W}_k \mathbf{x}_i \\
\mathbf{v}_i &amp;amp;=&amp;amp; \mathbf{W}_v \mathbf{x}_i \\
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;The self-attention layer outputs a weighted average over the value vectors, where the weight is not a parameter of the neural net, but the dot product of the query and key&amp;nbsp;vectors.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{self-attention} \tag{7}
w'_{ij} &amp;amp;=&amp;amp; \mathbf{q}_i^\top \mathbf{k}_j \\
w_{ij} &amp;amp;=&amp;amp; \text{softmax}(w'_{ij}) \\
\mathbf{y_i} &amp;amp;=&amp;amp; \sum_j w_{ij} \mathbf{v_j}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;The dot product in (\ref{self-attention}) is between embeddings of different frames in an episode.  We apply masking to the weight matrix before the softmax in (\ref{self-attention}) to ensure that observations from different episodes do not pay attention to each other, in addition to future masking (observations can only attend past observations in the same&amp;nbsp;episode).&lt;/p&gt;
&lt;p&gt;The output of the attention layer then passes through a fully connected layer with 64 hidden units, followed by a ReLU activation, and the final output is a scalar, the logit predicting whether the rewards-to-go from a given observation are below or above a&amp;nbsp;threshold.&lt;/p&gt;
&lt;p&gt;The threshold itself is a moving average over the maximum undiscounted returns seen across network updates, where the averaging window is a hyperparameter that should balance updating the threshold in response to higher returns due to an improving policy (in general, increasing, although monotonicity is not enforced) with not increasing so quickly such that there are too few episodes in the positive (high returns) class in a given batch of collected&amp;nbsp;experiences.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_classifier_model.png" alt="Classifier model" style="width:300px;"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Fig 5. Binary classifier model with attention, accepting sequences as input.
&lt;/small&gt;&lt;/p&gt;
&lt;h2&gt;Temporal reward&amp;nbsp;transport&lt;/h2&gt;
&lt;p&gt;After collecting a batch of experiences by following the &lt;span class="caps"&gt;A2C&lt;/span&gt; model&amp;#8217;s policy, we calculate the attention scores &lt;span class="math"&gt;\(w_{ij}\)&lt;/span&gt; from (\ref{self-attention}) using observations from the full episode as&amp;nbsp;context.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_attention_single_episode.png" alt="Attention scores single episode" style="width:400px;"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Fig 6. Attention scores for a single episode with future masking (of the upper right triangle).  The bright vertical stripes correspond to two highly attended state-action pairs.
&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;We calculate the importance, defined as the average weight of observation &lt;span class="math"&gt;\(i\)&lt;/span&gt;, ignoring masked regions in the attention matrix&amp;nbsp;as&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{importance} \tag{8}
\text{importance}_i = \frac{1}{T - i} \sum_{j \geq i}^{T} w_{ij}
\end{eqnarray}&lt;/div&gt;
&lt;p align="center"&gt;
&lt;img src="images/ltca_importances.png" alt="Importances for a batch of frames" style="width:600px;"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Fig 7. Importances for a batch of collected experiences (16 processes x 600 frames = 9600 frames), with frame or step number on the horizontal axis and process number on the vertical axis.
&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;Observations with an importance score above a threshold (between 0 and 1) hyperparameter are eligible for &lt;span class="caps"&gt;TRT&lt;/span&gt;.  After identifying the candidates for &lt;span class="caps"&gt;TRT&lt;/span&gt;, we add the distal rewards-to-go, weighted by the importance and another hyperparameter for tuning the impact of the &lt;span class="caps"&gt;TRT&lt;/span&gt; rewards &lt;span class="math"&gt;\(\alpha_{TRT}\)&lt;/span&gt; to the original reward &lt;span class="math"&gt;\(r_i\)&lt;/span&gt; obtained during experience&amp;nbsp;collection:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{trt} \tag{9}
r_i &amp;amp;\rightarrow&amp;amp; r_i + \text{TRT-reward}_i \\
\text{TRT-reward}_i &amp;amp;\equiv&amp;amp; \alpha_{TRT} * \text{importance}_i * \text{rewards-to-go}_i
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;We define the distal rewards-to-go in (\ref{trt}) as the total undiscounted returns from observation &lt;span class="math"&gt;\(i\)&lt;/span&gt;, excluding rewards accumulated in an immediate time window of size equal to the discount factor timescale &lt;span class="math"&gt;\(\tau_\gamma\)&lt;/span&gt; defined in (\ref{discount-timescale}).  This temporal exclusion zone helps prevent overcounting&amp;nbsp;rewards.&lt;/p&gt;
&lt;p&gt;We calculate the advantage after &lt;span class="caps"&gt;TRT&lt;/span&gt; using the generalized advantage estimation algorithm &lt;span class="caps"&gt;GAE&lt;/span&gt;-&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; [3] with &lt;span class="math"&gt;\(\lambda=0.95\)&lt;/span&gt;, which, analogous to &lt;span class="caps"&gt;TD&lt;/span&gt;-&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; [2], calculates the advantage from an exponentially weighted average over 1- to n-step bootstrapped estimates of the &lt;span class="math"&gt;\(Q\)&lt;/span&gt; value.  One of the benefits of using &lt;span class="caps"&gt;GAE&lt;/span&gt;-&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; is the spillover effect that enables the &lt;span class="caps"&gt;TRT&lt;/span&gt;-reinforced rewards to directly affect neighboring state-action pairs in addition to the triggering state-action&amp;nbsp;pair.&lt;/p&gt;
&lt;h2&gt;Training&lt;/h2&gt;
&lt;p&gt;For experience collection, we used 16 parallel processes, with 600 frames collected per process for a batch size of 9600 frames between parameter&amp;nbsp;updates.&lt;/p&gt;
&lt;p&gt;The &lt;span class="caps"&gt;A2C&lt;/span&gt; model loss per time step&amp;nbsp;is&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \label{a2c-loss} \tag{10}
\mathcal{L}_{A2C} \equiv \mathcal{L}_{policy} + \alpha_{value} \mathcal{L}_{value} - \alpha_{entropy} \text{entropy},
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \notag
\mathcal{L}_{policy} &amp;amp;=&amp;amp; - \log p(a_t | o_t, h_t), \\
\mathcal{L}_{value} &amp;amp;=&amp;amp; \left\Vert \hat{V}(o_t, h_t) - R_t \right\Vert^2,
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;and &lt;span class="math"&gt;\(o_t\)&lt;/span&gt; and &lt;span class="math"&gt;\(h_t\)&lt;/span&gt; are the observation and hidden state from the &lt;span class="caps"&gt;LSTM&lt;/span&gt; at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;, respectively.  We accumulate the losses defined in (\ref{a2c-loss}) by iterating over batches of consecutive time steps equal to the size of the &lt;span class="caps"&gt;LSTM&lt;/span&gt; memory of 10, i.e. truncated backpropagation in time for 10&amp;nbsp;timesteps.&lt;/p&gt;
&lt;p&gt;The classifier with attention has a &lt;a href="https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss"&gt;binary cross entropy loss&lt;/a&gt;, where the contribution to the loss from positive examples is weighted by a factor of&amp;nbsp;2.&lt;/p&gt;
&lt;p&gt;We clip both gradient norms according to a hyperparameter &lt;span class="math"&gt;\(\text{max_grad_norm}=0.5\)&lt;/span&gt;, and we optimize both models using RMSprop with learning rates of 0.01, RMSprop &lt;span class="math"&gt;\(\alpha=0.99\)&lt;/span&gt;, and RMSprop &lt;span class="math"&gt;\(\epsilon=1e^{-8}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h1&gt;Data and code&amp;nbsp;availability&lt;/h1&gt;
&lt;h2&gt;Environment&lt;/h2&gt;
&lt;p&gt;Code for the components of the 3 phase environment is in our &lt;a href="https://github.com/frangipane/gym-minigrid"&gt;fork&lt;/a&gt; of &lt;a href="https://github.com/maximecb/gym-minigrid"&gt;gym-minigrid&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The base environment for running the experiments is defined in &lt;a href="https://github.com/frangipane/rl-credit/blob/master/rl_credit/examples/environment.py"&gt;https://github.com/frangipane/rl-credit/&lt;/a&gt;.  Each experiment script subclasses that base environment, varying some parameter in the distractor&amp;nbsp;phase.&lt;/p&gt;
&lt;h2&gt;Experiments&lt;/h2&gt;
&lt;p&gt;The parameters and results of the experiments are documented in the following publicly available reports on Weights and&amp;nbsp;Biases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://app.wandb.ai/frangipane/distractor_time_delays/reports/Distractor-Gift-time-delay--VmlldzoxMjYyNzY"&gt;Distractor phase time&amp;nbsp;delays&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://app.wandb.ai/frangipane/distractor_reward_size/reports/Distractor-gift-reward-size--VmlldzoxMjcxMTI"&gt;Distractor phase reward&amp;nbsp;size&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://app.wandb.ai/frangipane/distractor_reward_variance/reports/Distractor-gift-variance--VmlldzoxMjgzNTc"&gt;Distractor phase variance of&amp;nbsp;rewards&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Code for running the experiments is at &lt;a href="https://github.com/frangipane/rl-credit"&gt;https://github.com/frangipane/rl-credit&lt;/a&gt; in the examples/&amp;nbsp;submodule.&lt;/p&gt;
&lt;h1&gt;Acknowledgements&lt;/h1&gt;
&lt;p&gt;Thank you to OpenAI, my OpenAI mentor J. Tworek, Microsoft for the cloud computing credits, Square for supporting my participation in the program, and my 2020 cohort of Scholars: A. Carrera, P. Mishkin, K. Ndousse, J. Orbay, A. Power (especially for the tip about future masking in transformers), and K.&amp;nbsp;Slama.&lt;/p&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Hung C, Lillicrap T, Abramson J, et al. 2019. &lt;a href="https://www.nature.com/articles/s41467-019-13073-w"&gt;Optimizing agent behavior over long time scales by transporting value&lt;/a&gt;. Nat Commun 10,&amp;nbsp;5223.&lt;/p&gt;
&lt;p&gt;[2] Sutton R, Barto A.  2018.  &lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction (2nd Edition)&lt;/a&gt;.  Cambridge (&lt;span class="caps"&gt;MA&lt;/span&gt;): &lt;span class="caps"&gt;MIT&lt;/span&gt;&amp;nbsp;Press.&lt;/p&gt;
&lt;p&gt;[3] Schulman J, Moritz P, Levine S, et al. 2016. &lt;a href="https://arxiv.org/abs/1506.02438"&gt;High-Dimensional Continuous Control Using Generalized Advantage Estimation&lt;/a&gt;. &lt;span class="caps"&gt;ICLR&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;[4] Willems L.  &lt;a href="https://github.com/lcswillems/rl-starter-files"&gt;&lt;span class="caps"&gt;RL&lt;/span&gt; Starter Files&lt;/a&gt; and &lt;a href="https://github.com/lcswillems/torch-ac"&gt;Torch &lt;span class="caps"&gt;AC&lt;/span&gt;&lt;/a&gt;.&amp;nbsp;GitHub.&lt;/p&gt;
&lt;p&gt;[5] Chevalier-Boisvert M, Willems L, Pal S.  2018.  &lt;a href="https://github.com/maximecb/gym-minigrid"&gt;Minimalistic Gridworld Environment for OpenAI Gym&lt;/a&gt;.&amp;nbsp;GitHub.&lt;/p&gt;
&lt;p&gt;[6] Bloem P.  2019.  &lt;a href="http://www.peterbloem.nl/blog/transformers"&gt;Transformers from Scratch&lt;/a&gt; [blog].  [accessed 2020 May 1].&amp;nbsp;http://www.peterbloem.nl/blog/transformers.&lt;/p&gt;
&lt;p&gt;[7] Harutyunyan A, Dabney W, Mesnard T. 2019.  &lt;a href="http://papers.nips.cc/paper/9413-hindsight-credit-assignment.pdf"&gt;Hindsight Credit Assignment&lt;/a&gt;. Advances in Neural Information Processing Systems 32:&amp;nbsp;12488&amp;#8212;12497.&lt;/p&gt;
&lt;p&gt;[8] Arjona-Medina J, Gillhofer M, Widrich M, et al. 2019.  &lt;a href="https://papers.nips.cc/paper/9509-rudder-return-decomposition-for-delayed-rewards.pdf"&gt;&lt;span class="caps"&gt;RUDDER&lt;/span&gt;: Return Decomposition for Delayed Rewards&lt;/a&gt;.  Advances in Neural Information Processing Systems 32:&amp;nbsp;13566&amp;#8212;13577.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine learning"></category><category term="machine learning"></category><category term="reinforcement learning"></category><category term="OpenAI"></category></entry><entry><title>Q-learning and DQN</title><link href="https://efavdb.com/dqn" rel="alternate"></link><published>2020-04-06T00:00:00-07:00</published><updated>2020-04-06T00:00:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2020-04-06:/dqn</id><summary type="html">&lt;p&gt;[&lt;span class="caps"&gt;TOC&lt;/span&gt;]&lt;/p&gt;
&lt;p&gt;Q-learning is a reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) algorithm that is the basis for deep Q networks (&lt;span class="caps"&gt;DQN&lt;/span&gt;), the algorithm by Google DeepMind that achieved human-level performance for a range of Atari games and kicked off the deep &lt;span class="caps"&gt;RL&lt;/span&gt; revolution starting in&amp;nbsp;2013-2015.&lt;/p&gt;
&lt;p&gt;We begin with some historical context, then provide …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[&lt;span class="caps"&gt;TOC&lt;/span&gt;]&lt;/p&gt;
&lt;p&gt;Q-learning is a reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) algorithm that is the basis for deep Q networks (&lt;span class="caps"&gt;DQN&lt;/span&gt;), the algorithm by Google DeepMind that achieved human-level performance for a range of Atari games and kicked off the deep &lt;span class="caps"&gt;RL&lt;/span&gt; revolution starting in&amp;nbsp;2013-2015.&lt;/p&gt;
&lt;p&gt;We begin with some historical context, then provide an overview of value function methods / Q-learning, and conclude with a discussion of &lt;span class="caps"&gt;DQN&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If you want to skip straight to code, the implementation of &lt;span class="caps"&gt;DQN&lt;/span&gt; that we used to train the agent playing Atari Breakout below is available &lt;a href="https://github.com/frangipane/reinforcement-learning/tree/master/DQN"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/atari_breakout.gif" alt="Atari Breakout" style="width:250px;"&gt;
&lt;/p&gt;

&lt;p&gt;If you watch the video long enough, you&amp;#8217;ll see the agent has learned a strategy that favors breaking bricks at the edges so the ball &amp;#8220;breaks out&amp;#8221; to the upper side, resulting in a cascade of&amp;nbsp;points.&lt;/p&gt;
&lt;h2&gt;Historical&amp;nbsp;context&lt;/h2&gt;
&lt;p&gt;The theories that underpin today’s reinforcement learning algorithms were developed decades ago.  For example, Watkins developed Q-learning, a value function method, in &lt;a href="http://www.cs.rhul.ac.uk/~chrisw/thesis.html"&gt;1989&lt;/a&gt;, and Williams proposed the &lt;span class="caps"&gt;REINFORCE&lt;/span&gt; policy gradient method in &lt;a href="https://link.springer.com/content/pdf/10.1007%2FBF00992696.pdf"&gt;1987&lt;/a&gt;. So why the recent surge of interest in deep &lt;span class="caps"&gt;RL&lt;/span&gt;?&lt;/p&gt;
&lt;h3&gt;Representational power from Neural&amp;nbsp;Networks&lt;/h3&gt;
&lt;p&gt;Until 2013, most applications of &lt;span class="caps"&gt;RL&lt;/span&gt; relied on hand engineered inputs for value function and policy representations, which drastically limited the scope of applicability to the real world.  Mnih et. al [1] made use of advances in computational power and neural network (&lt;span class="caps"&gt;NN&lt;/span&gt;) architectures to use a deep &lt;span class="caps"&gt;NN&lt;/span&gt; for &lt;em&gt;value function approximation&lt;/em&gt;, showing that NNs can learn a useful representation from raw pixel inputs in Atari&amp;nbsp;games.&lt;/p&gt;
&lt;h3&gt;Variations on a theme: vanilla &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms don’t work well&amp;nbsp;out-of-the-box&lt;/h3&gt;
&lt;p&gt;The basic &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms that were developed decades ago do not work well in practice without modifications.  For example, &lt;span class="caps"&gt;REINFORCE&lt;/span&gt; relies on Monte Carlo estimates of the performance gradient; such estimates of the performance gradient are high variance, resulting in unstable or impractically slow learning (poor sample efficiency).  The original Q-learning algorithm also suffers from instability due to correlated sequential training data and parameter updates affecting both the estimator and target, creating a “moving target” and hence&amp;nbsp;divergences.&lt;/p&gt;
&lt;p&gt;We can think of these original &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms as the Wright Brothers plane.
&lt;p align="center"&gt;
&lt;img src="images/wright_brothers_plane.png" alt="Wright brothers plane" style="width:500px;"&gt;
&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;The foundational shape is there and recognizable in newer models.  However, the enhancements of newer algorithms aren&amp;#8217;t just bells and whistles &amp;#8212; they have enabled the move from toy problems into more functional&amp;nbsp;territory.&lt;/p&gt;
&lt;h2&gt;Q-learning&lt;/h2&gt;
&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;&lt;span class="caps"&gt;RL&lt;/span&gt; models the sequential decision-making problem as a Markov Decision Process (&lt;span class="caps"&gt;MDP&lt;/span&gt;): transitions from state to state involve both environment dynamics and an agent whose actions affect both the probability of transitioning to the next state and the reward&amp;nbsp;received.&lt;/p&gt;
&lt;p&gt;The goal is to find a policy, a mapping from state to actions, that will maximize the agent’s expected returns, i.e. their cumulative future&amp;nbsp;rewards.&lt;/p&gt;
&lt;p&gt;Q-learning is an algorithm for learning the eponymous &lt;span class="math"&gt;\(Q(s,a)\)&lt;/span&gt; action-value function, defined as the expected returns for each state-action &lt;span class="math"&gt;\((s,a)\)&lt;/span&gt; pair, corresponding to following the optimal&amp;nbsp;policy.&lt;/p&gt;
&lt;h3&gt;Goal: solve the Bellman optimality&amp;nbsp;equation&lt;/h3&gt;
&lt;p&gt;Recall that &lt;span class="math"&gt;\(q_*\)&lt;/span&gt; is described by a self-consistent, recursive relation, the Bellman optimality equation, that falls out from the Markov property [6, 7] of&amp;nbsp;MDPs&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{action-value-bellman-optimality} \tag{1}
q_*(s, a) &amp;amp;=&amp;amp; \mathbb{E}_{\pi*} [R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}', a') | S_t = s, A_t = a] \\
          &amp;amp;=&amp;amp; \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a') ]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(0 \leq \gamma \leq 1\)&lt;/span&gt; is the &lt;em&gt;discount rate&lt;/em&gt; which characterizes how much we weight rewards now vs. later, &lt;span class="math"&gt;\(R_{t+1}\)&lt;/span&gt; is the reward at timestep &lt;span class="math"&gt;\(t+1\)&lt;/span&gt;, and &lt;span class="math"&gt;\(p(s', r | s, a)\)&lt;/span&gt; is the environment transition&amp;nbsp;dynamics.&lt;/p&gt;
&lt;p&gt;Our &lt;a href="https://efavdb.com/intro-rl-toy-example.html"&gt;introduction to &lt;span class="caps"&gt;RL&lt;/span&gt;&lt;/a&gt; provides more background on the Bellman equations in case (\ref{action-value-bellman-optimality}) looks&amp;nbsp;unfamiliar.&lt;/p&gt;
&lt;h3&gt;The Q-learning approach to solving the Bellman&amp;nbsp;equation&lt;/h3&gt;
&lt;p&gt;We use capitalized &lt;span class="math"&gt;\(Q\)&lt;/span&gt; to denote an estimate and lowercase &lt;span class="math"&gt;\(q\)&lt;/span&gt; to denote the real action-value function.  The Q-learning algorithm makes the following&amp;nbsp;update:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{q-learning} \tag{2}
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;The quantity in square brackets in (\ref{q-learning}) is exactly 0 for the optimal action-value, &lt;span class="math"&gt;\(q*\)&lt;/span&gt;, based on (\ref{action-value-bellman-optimality}).  We can think of it as an error term, “the Bellman error”, that describes how far off the target quantity &lt;span class="math"&gt;\(R_{t+1} + \gamma \max_a Q(S_{t+1}, a)\)&lt;/span&gt; is from our current estimate &lt;span class="math"&gt;\(Q(S_t, A_t)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The goal with Q-learning is to iteratively calculate (\ref{q-learning}), updating our estimate of &lt;span class="math"&gt;\(Q\)&lt;/span&gt; to reduce the Bellman error, until we have converged on a&amp;nbsp;solution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q-learning makes two&amp;nbsp;approximations:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I. It replaces the expectation value in (\ref{action-value-bellman-optimality}) with sampled estimates, similar to Monte Carlo estimates.  Unlike the dynamic programming approach we described in an earlier &lt;a href="https://efavdb.com/dp-in-rl.html"&gt;post&lt;/a&gt;, sampling is necessary since we don&amp;#8217;t have access to the model of the environment, i.e. the environment transition&amp;nbsp;dynamics.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;II&lt;/span&gt;. It replaces the target &lt;span class="math"&gt;\(R_{t+1} + \max_a \gamma q_*(s’,a’)\)&lt;/span&gt; in (\ref{action-value-bellman-optimality}), which contains the true action-value function &lt;span class="math"&gt;\(q_*\)&lt;/span&gt;, with the one-step temporal difference, &lt;span class="caps"&gt;TD&lt;/span&gt;(0), target &lt;span class="math"&gt;\(R_{t+1} + \gamma \max_a Q(S_{t+1}, a)\)&lt;/span&gt;.  The &lt;span class="caps"&gt;TD&lt;/span&gt;(0) target is an example of &lt;em&gt;bootstrapping&lt;/em&gt; because it makes use of the current estimate of the action-value function, instead of, say the cumulative rewards from an entire episode, which would be a Monte Carlo target.  Temporal difference methods reduce variance that comes from sampling a single trajectory like Monte Carlo at the cost of introducing bias from using an approximate function in the target for&amp;nbsp;updates.&lt;/p&gt;
&lt;p&gt;Figure 8.11 of [7] nicely summarizes the types of approximations and their limits in the following&amp;nbsp;diagram:&lt;/p&gt;
&lt;p&gt;&lt;img alt="backup approximations" src="https://efavdb.com/images/backup_limits_diagram_sutton_barto.png"&gt;&lt;/p&gt;
&lt;h2&gt;Deep Q-Networks (&lt;span class="caps"&gt;DQN&lt;/span&gt;)&lt;/h2&gt;
&lt;h3&gt;Key contributions to&amp;nbsp;Q-learning&lt;/h3&gt;
&lt;p&gt;The &lt;span class="caps"&gt;DQN&lt;/span&gt; authors made two key enhancements to the original Q-learning algorithm to actually make it&amp;nbsp;work:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Experience replay buffer&lt;/strong&gt;: to reduce the instability caused by training on highly correlated sequential data, store samples (transition tuples &lt;span class="math"&gt;\((s, a, s’, r)\)&lt;/span&gt;) in an “experience replay buffer”.  Cut down correlations by randomly sampling the buffer for minibatches of training data.  The idea of experience replay was introduced by &lt;a href="http://www.incompleteideas.net/lin-92.pdf"&gt;Lin in 1992&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Freeze the target network&lt;/strong&gt;: to address the instability caused by chasing a moving target, freeze the target network and only update it periodically with the latest parameters from the trained&amp;nbsp;estimator.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These modifications enabled [1] to successfully train a deep Q-network, an action-value function approximated by a convolutional neural net, on the high dimensional visual inputs of a variety of Atari&amp;nbsp;games.&lt;/p&gt;
&lt;p&gt;The authors also employed a number of tweaks / data preprocessing on top of the aforementioned key enhancements.  One preprocessing trick of note was the concatenation of the four most recent frames as input into the Q-network in order to provide some sense of velocity or trajectory, e.g. the trajectory of a ball in games such as Pong or Breakout.  This preprocessing decision helps uphold the assumption that the problem is a Markov Decision Process, which underlies the Bellman optimality equations and Q-learning algorithms; otherwise, the assumption is violated if the agent only observes some fraction of the state of the environment, turning the problem into a &lt;a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process"&gt;partially observable &lt;span class="caps"&gt;MDP&lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;span class="caps"&gt;DQN&lt;/span&gt; implementation in&amp;nbsp;code&lt;/h3&gt;
&lt;p&gt;We’ve implemented &lt;span class="caps"&gt;DQN&lt;/span&gt; &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/DQN/dqn.py"&gt;here&lt;/a&gt;, tested for (1) the &lt;a href="https://gym.openai.com/envs/CartPole-v1/"&gt;Cartpole&lt;/a&gt; toy problem, which uses a multilayer perceptron &lt;code&gt;MLPCritic&lt;/code&gt; as the Q-function approximator for non-visual input data, and (2) Atari Breakout, which uses a convolutional neural network &lt;code&gt;CNNCritic&lt;/code&gt; as the Q-function approximator for the (visual) Atari pixel&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;The Cartpole problem is trainable on the average modern laptop &lt;span class="caps"&gt;CPU&lt;/span&gt;, but we recommend using a beefier setup with GPUs and lots of memory to do Q-learning on Atari.  Thanks to the OpenAI Scholars program and Microsoft, we were able to train &lt;span class="caps"&gt;DQN&lt;/span&gt; on Breakout using an Azure &lt;a href="https://docs.microsoft.com/en-us/azure/virtual-machines/nc-series"&gt;Standard_NC24&lt;/a&gt; consisting of 224 GiB &lt;span class="caps"&gt;RAM&lt;/span&gt; and 2 K80&amp;nbsp;GPUs.&lt;/p&gt;
&lt;p&gt;The values from the &lt;span class="math"&gt;\(Q\)&lt;/span&gt; estimator and frozen target network are fed into the Huber loss that is used to update the parameters of the Q-function in this code&amp;nbsp;snippet:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_loss_q&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;o2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;obs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;act&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rew&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;obs2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;done&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="c1"&gt;# Pick out q-values associated with / indexed by the action that was taken&lt;/span&gt;
        &lt;span class="c1"&gt;# for that observation&lt;/span&gt;
        &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gather&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ac&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;long&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

        &lt;span class="c1"&gt;# Bellman backup for Q function&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;no_grad&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
                &lt;span class="c1"&gt;# Targets come from frozen target Q-network&lt;/span&gt;
                &lt;span class="n"&gt;q_target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target_q_network&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
                &lt;span class="n"&gt;backup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;gamma&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;q_target&lt;/span&gt;

        &lt;span class="n"&gt;loss_q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;smooth_l1_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;backup&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss_q&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The experience replay buffer was taken from OpenAI’s Spinning Up in &lt;span class="caps"&gt;RL&lt;/span&gt; [6] code tutorials for the&amp;nbsp;problem:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ReplayBuffer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    A simple FIFO experience replay buffer for DDPG agents.&lt;/span&gt;

&lt;span class="sd"&gt;    Copied from: https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py#L11,&lt;/span&gt;
&lt;span class="sd"&gt;    modified action buffer for discrete action space.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;obs_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;store&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rew&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_obs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;obs_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;obs&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;obs2_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next_obs&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;act_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;act&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rew_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rew&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;done_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;idxs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;obs_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idxs&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                     &lt;span class="n"&gt;obs2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;obs2_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idxs&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                     &lt;span class="n"&gt;act&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;act_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idxs&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                     &lt;span class="n"&gt;rew&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rew_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idxs&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                     &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;done_buf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idxs&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;act&amp;#39;&lt;/span&gt;
                &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
                &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally, we used OpenAI’s baselines &lt;a href="https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py"&gt;Atari wrappers&lt;/a&gt; to handle the rather involved data preprocessing&amp;nbsp;steps.&lt;/p&gt;
&lt;p&gt;You can see logs and plots like this plot of the mean raw returns per step in the environment for the Atari &lt;span class="caps"&gt;DQN&lt;/span&gt; training run in our &lt;a href="https://app.wandb.ai/frangipane/dqn/runs/30fhfv6y?workspace=user-frangipane"&gt;wandb dashboard&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="training curve" src="https://efavdb.com/images/atari_training_returns.png"&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;From a pedagogical point of view, Q-learning is a good study for someone getting off the ground with &lt;span class="caps"&gt;RL&lt;/span&gt; since it pulls together many core &lt;span class="caps"&gt;RL&lt;/span&gt; concepts,&amp;nbsp;namely:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Model the sequential decision making process as an &lt;strong&gt;&lt;span class="caps"&gt;MDP&lt;/span&gt;&lt;/strong&gt; where environment dynamics are&amp;nbsp;unknown.&lt;/li&gt;
&lt;li&gt;Frame the problem as finding &lt;strong&gt;action-value functions&lt;/strong&gt; that satisfy the Bellman&amp;nbsp;equations.&lt;/li&gt;
&lt;li&gt;Iteratively solve the Bellman equations using &lt;strong&gt;bootstrapped  estimates&lt;/strong&gt; from samples of an agent’s interactions with an&amp;nbsp;environment.&lt;/li&gt;
&lt;li&gt;Use neural networks to &lt;strong&gt;approximate value functions&lt;/strong&gt; to handle the more realistic situation of an observation space being too high-dimensional to be stored in&amp;nbsp;table.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class="caps"&gt;DQN&lt;/span&gt; on top of vanilla Q-learning itself is noteworthy because the modifications &amp;#8212; experience replay and frozen target networks &amp;#8212; are what make Q-learning actually work, demonstrating that the devil is in the&amp;nbsp;details.&lt;/p&gt;
&lt;p&gt;Furthermore, the &lt;span class="caps"&gt;DQN&lt;/span&gt; tricks have been incorporated in many other &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms, e.g. see [6] for more examples.  The tricks aren’t necessarily “pretty”, but they come from understanding/intuition about shortcomings of the basic&amp;nbsp;algorithms.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Papers&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[1] Mnih et al 2015 - &lt;a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#the-optimal-q-function-and-the-optimal-action"&gt;Human-level control through deep reinforcement&amp;nbsp;learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Video&amp;nbsp;lectures&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[2] David Silver - &lt;span class="caps"&gt;RL&lt;/span&gt; lecture 6 Value Function Approximation (&lt;a href="https://www.youtube.com/watch?v=UoPei5o4fps"&gt;video&lt;/a&gt;, &lt;a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf"&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;[3] Sergey Levine’s lecture (&lt;span class="caps"&gt;CS285&lt;/span&gt;) on value function methods (&lt;a href="https://www.youtube.com/watch?v=doR5bMe-Wic&amp;amp;list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&amp;amp;index=8&amp;amp;t=129s"&gt;video&lt;/a&gt;, &lt;a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-7.pdf"&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;[4] Sergey Levine’s lecture (&lt;span class="caps"&gt;CS285&lt;/span&gt;) on deep &lt;span class="caps"&gt;RL&lt;/span&gt; with Q-functions (&lt;a href="https://www.youtube.com/watch?v=7Lwf-BoIu3M&amp;amp;list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&amp;amp;index=9&amp;amp;t=0s"&gt;video&lt;/a&gt;, &lt;a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf"&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;[5] Vlad Mnih - Berkeley Deep &lt;span class="caps"&gt;RL&lt;/span&gt; Bootcamp 2017 - Core Lecture 3 &lt;span class="caps"&gt;DQN&lt;/span&gt; + Variants (&lt;a href="https://www.youtube.com/watch?v=fevMOp5TDQs"&gt;video&lt;/a&gt;, &lt;a href="https://drive.google.com/open?id=0BxXI_RttTZAhVUhpbDhiSUFFNjg"&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Books /&amp;nbsp;tutorials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[6] OpenAI - Spinning Up: &lt;a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#the-optimal-q-function-and-the-optimal-action"&gt;The Optimal Q-Function and the Optimal&amp;nbsp;Action&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[7] Sutton and Barto - &lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction (2nd Edition)&lt;/a&gt;, section 6.5 “Q-learning: Off-policy &lt;span class="caps"&gt;TD&lt;/span&gt; Control”, section 16.5 “Human-level Video Game&amp;nbsp;Play”&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine learning"></category><category term="machine learning"></category><category term="reinforcement learning"></category><category term="OpenAI"></category></entry><entry><title>Dynamic programming in reinforcement learning</title><link href="https://efavdb.com/reinforcement-learning-dynamic-programming" rel="alternate"></link><published>2020-03-28T12:00:00-07:00</published><updated>2020-03-28T12:00:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2020-03-28:/reinforcement-learning-dynamic-programming</id><summary type="html">&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;We discuss how to use dynamic programming (&lt;span class="caps"&gt;DP&lt;/span&gt;) to solve reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) problems where we have a perfect model of the environment.  &lt;span class="caps"&gt;DP&lt;/span&gt; is a general approach to solving problems by breaking them into subproblems that can be solved separately, cached, then combined to solve the overall&amp;nbsp;problem …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;We discuss how to use dynamic programming (&lt;span class="caps"&gt;DP&lt;/span&gt;) to solve reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) problems where we have a perfect model of the environment.  &lt;span class="caps"&gt;DP&lt;/span&gt; is a general approach to solving problems by breaking them into subproblems that can be solved separately, cached, then combined to solve the overall&amp;nbsp;problem.&lt;/p&gt;
&lt;p&gt;We’ll use a toy model, taken from [1], of a student transitioning between five states in college, which we also used in our &lt;a href="https://efavdb.com/intro-rl-toy-example.html"&gt;introduction&lt;/a&gt; to &lt;span class="caps"&gt;RL&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt="student MDP" src="https://efavdb.com/images/student_mdp.png"&gt;&lt;/p&gt;
&lt;p&gt;The model (dynamics) of the environment describe the probabilities of receiving a reward &lt;span class="math"&gt;\(r\)&lt;/span&gt; in the next state &lt;span class="math"&gt;\(s'\)&lt;/span&gt; given the current state &lt;span class="math"&gt;\(s\)&lt;/span&gt; and action &lt;span class="math"&gt;\(a\)&lt;/span&gt; taken, &lt;span class="math"&gt;\(p(s’, r | s, a)\)&lt;/span&gt;.  We can read these dynamics off the diagram of the student Markov Decision Process (&lt;span class="caps"&gt;MDP&lt;/span&gt;), for&amp;nbsp;example:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p(s'=\text{CLASS2}, r=-2 | s=\text{CLASS1}, a=\text{study}) =&amp;nbsp;1.0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p(s'=\text{CLASS2}, r=1 | s=\text{CLASS3}, a=\text{pub}) =&amp;nbsp;0.4\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If you&amp;#8217;d like to jump straight to code, see this &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb"&gt;jupyter notebook&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;The role of value functions in &lt;span class="caps"&gt;RL&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The agent’s (student’s) policy maps states to actions, &lt;span class="math"&gt;\(\pi(a|s) := p(a|s)\)&lt;/span&gt;. 
The goal is to find the optimal policy &lt;span class="math"&gt;\(\pi_*\)&lt;/span&gt; that will maximize the expected cumulative rewards, the discounted return &lt;span class="math"&gt;\(G_t\)&lt;/span&gt;, in each state &lt;span class="math"&gt;\(s\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The value functions, &lt;span class="math"&gt;\(v_{\pi}(s)\)&lt;/span&gt; and &lt;span class="math"&gt;\(q_{\pi}(s, a)\)&lt;/span&gt;, in MDPs formalize this&amp;nbsp;goal.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
v_{\pi}(s) &amp;amp;=&amp;amp; \mathbb{E}_{\pi}[G_t | S_t = s] \\
q_{\pi}(s, a) &amp;amp;=&amp;amp; \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;We want to be able to calculate the value function for an arbitrary policy, i.e. &lt;em&gt;prediction&lt;/em&gt;, as well as use the value functions to find an optimal policy, i.e. the &lt;em&gt;control&lt;/em&gt;&amp;nbsp;problem.&lt;/p&gt;
&lt;h2&gt;Policy&amp;nbsp;evaluation&lt;/h2&gt;
&lt;p&gt;Policy evaluation deals with the problem of calculating the value function for some arbitrary policy.  In our introduction to &lt;span class="caps"&gt;RL&lt;/span&gt; &lt;a href="https://efavdb.com/intro-rl-toy-example.html"&gt;post&lt;/a&gt;, we showed that the value functions obey self-consistent, recursive relations, that make them amenable to &lt;span class="caps"&gt;DP&lt;/span&gt; approaches given a model of the&amp;nbsp;environment.&lt;/p&gt;
&lt;p&gt;These recursive relations are the Bellman expectation equations, which write the value of each state in terms of an average over the values of its successor / neighboring states, along with the expected reward along the&amp;nbsp;way.&lt;/p&gt;
&lt;p&gt;The Bellman expectation equation for &lt;span class="math"&gt;\(v_{\pi}(s)\)&lt;/span&gt;&amp;nbsp;is&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{state-value-bellman} \tag{1}
v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_{\pi}(s’) ],
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; is the discount factor &lt;span class="math"&gt;\(0 \leq \gamma \leq 1\)&lt;/span&gt; that weights the importance of future vs. current returns. &lt;strong&gt;&lt;span class="caps"&gt;DP&lt;/span&gt; turns (\ref{state-value-bellman}) into an update rule&lt;/strong&gt; (\ref{policy-evaluation}), &lt;span class="math"&gt;\(\{v_k(s’)\} \rightarrow v_{k+1}(s)\)&lt;/span&gt;, which iteratively converges towards the solution, &lt;span class="math"&gt;\(v_\pi(s)\)&lt;/span&gt;, for&amp;nbsp;(\ref{state-value-bellman}):&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{policy-evaluation} \tag{2}
v_{k+1}(s) = \sum_{a} \pi(a|s) \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_k(s’) ]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Applying policy evaluation to our student model for an agent with a random policy, we arrive at the following state value function (see &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb"&gt;jupyter notebook&lt;/a&gt; for&amp;nbsp;implementation):&lt;/p&gt;
&lt;p&gt;&lt;img alt="student MDP value function random policy" src="https://efavdb.com/images/student_mdp_values_random_policy.png"&gt;&lt;/p&gt;
&lt;h2&gt;Finding the optimal value functions and&amp;nbsp;policy&lt;/h2&gt;
&lt;h3&gt;Policy&amp;nbsp;iteration&lt;/h3&gt;
&lt;p&gt;We can evaluate the value functions for a given policy by turning the Bellman expectation equation (\ref{state-value-bellman}) into an update equation with the iterative policy evaluation&amp;nbsp;algorithm.&lt;/p&gt;
&lt;p&gt;But how do we use value functions to achieve our end goal of finding an optimal policy that corresponds to the optimal value&amp;nbsp;functions?&lt;/p&gt;
&lt;p&gt;Imagine we know the value function for a policy.  If taking the greedy action, corresponding to taking &lt;span class="math"&gt;\(\text{arg} \max_a q_{\pi}(s,a)\)&lt;/span&gt;, from any state in that policy is not consistent with that policy, or, equivalently, &lt;span class="math"&gt;\(\max_a q_{\pi}(s,a) &amp;gt; v_\pi(s)\)&lt;/span&gt;, then the policy is not optimal since we can improve the policy by taking the greedy action in that state and then onwards following the original&amp;nbsp;policy.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;policy iteration&lt;/em&gt; algorithm involves taking turns calculating the value function for a policy (policy evaluation) and improving on the policy (policy improvement) by taking the greedy action in each state for that value function until converging to &lt;span class="math"&gt;\(\pi_*\)&lt;/span&gt; and &lt;span class="math"&gt;\(v_*\)&lt;/span&gt; (see [2] for pseudocode for policy&amp;nbsp;iteration).&lt;/p&gt;
&lt;h3&gt;Value&amp;nbsp;iteration&lt;/h3&gt;
&lt;p&gt;Unlike policy iteration, the value iteration algorithm does not require complete convergence of policy evaluation before policy improvement, and, in fact, makes use of just a single iteration of policy evaluation.  Just as policy evaluation could be viewed as turning the Bellman expectation equation into an update, value iteration turns the Bellman optimality equation into an&amp;nbsp;update.&lt;/p&gt;
&lt;p&gt;In our previous &lt;a href="https://efavdb.com/intro-rl-toy-example.html"&gt;post&lt;/a&gt; introducing &lt;span class="caps"&gt;RL&lt;/span&gt; using the student example, we saw that the optimal value functions are the solutions to the Bellman optimality equation, e.g. for the optimal state-value&amp;nbsp;function:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{state-value-bellman-optimality} \tag{3}
v_*(s) &amp;amp;=&amp;amp; \max_a q_{\pi*}(s, a) \\
    &amp;amp;=&amp;amp; \max_a \mathbb{E} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] \\
    &amp;amp;=&amp;amp; \max_a \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_*(s’) ]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;As a &lt;span class="caps"&gt;DP&lt;/span&gt; update equation, (\ref{state-value-bellman-optimality})&amp;nbsp;becomes:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{value-iteration} \tag{4}
v_{k+1}(s) = \max_a \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_k(s’) ]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Value iteration combines (truncated) policy evaluation with policy improvement in a single step; the state-value functions are updated with the averages of the value functions of the neighbor states that can occur from a greedy action, i.e. the action that maximizes the right hand side of&amp;nbsp;(\ref{value-iteration}).&lt;/p&gt;
&lt;p&gt;Applying value iteration to our student model, we arrive at the following optimal state value function, with the optimal policy delineated by red arrows (see &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb"&gt;jupyter notebook&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img alt="student MDP optimal policy and value function" src="https://efavdb.com/images/student_mdp_optimal_policy.png"&gt;&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;We’ve discussed how to solve for (a) the value functions of an arbitrary policy, (b) the optimal value functions and optimal policy.  Solving for (a) involves turning the Bellman expectation equations into an update, whereas (b) involves turning the Bellman optimality equations into an update.  These algorithms are guaranteed to converge (see [1] for notes on how the contraction mapping theorem guarantees&amp;nbsp;convergence).&lt;/p&gt;
&lt;p&gt;You can see the application of both policy evaluation and value iteration to the student model problem in this &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb"&gt;jupyter notebook&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a name="References"&gt;References&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;[1] David Silver&amp;#8217;s &lt;span class="caps"&gt;RL&lt;/span&gt; Course Lecture 3 - Planning by Dynamic Programming (&lt;a href="https://www.youtube.com/watch?v=Nd1-UUMVfz4"&gt;video&lt;/a&gt;,
  &lt;a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf"&gt;slides&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[2] Sutton and Barto -
  &lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction&lt;/a&gt; - Chapter 4: Dynamic&amp;nbsp;Programming&lt;/p&gt;
&lt;p&gt;[3] Denny Britz’s &lt;a href="https://github.com/dennybritz/reinforcement-learning/tree/master/DP"&gt;notes&lt;/a&gt; on &lt;span class="caps"&gt;RL&lt;/span&gt; and &lt;span class="caps"&gt;DP&lt;/span&gt;, including crisp implementations in code of policy evaluation, policy iteration, and value iteration for the gridworld example discussed in&amp;nbsp;[2].&lt;/p&gt;
&lt;p&gt;[4] Deep &lt;span class="caps"&gt;RL&lt;/span&gt; Bootcamp Lecture 1: Motivation + Overview + Exact Solution Methods, by Pieter Abbeel (&lt;a href="https://www.youtube.com/watch?v=qaMdN6LS9rA"&gt;video&lt;/a&gt;, &lt;a href="https://drive.google.com/open?id=0BxXI_RttTZAhVXBlMUVkQ1BVVDQ"&gt;slides&lt;/a&gt;) - a very compressed&amp;nbsp;intro.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine learning"></category><category term="reinforcement learning"></category><category term="machine learning"></category></entry><entry><title>Introduction to reinforcement learning by example</title><link href="https://efavdb.com/intro-rl-toy-example" rel="alternate"></link><published>2020-03-11T12:00:00-07:00</published><updated>2020-03-11T12:00:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2020-03-11:/intro-rl-toy-example</id><summary type="html">&lt;p&gt;We take a top-down approach to introducing reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) by starting with a toy example: a student going through college.  In order to frame the problem from the &lt;span class="caps"&gt;RL&lt;/span&gt; point-of-view, we&amp;#8217;ll walk through the following&amp;nbsp;steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Setting up a model of the problem&lt;/strong&gt; as a Markov Decision Process …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;We take a top-down approach to introducing reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) by starting with a toy example: a student going through college.  In order to frame the problem from the &lt;span class="caps"&gt;RL&lt;/span&gt; point-of-view, we&amp;#8217;ll walk through the following&amp;nbsp;steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Setting up a model of the problem&lt;/strong&gt; as a Markov Decision Process, the framework that underpins the &lt;span class="caps"&gt;RL&lt;/span&gt; approach to sequential decision-making&amp;nbsp;problems&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deciding on an objective&lt;/strong&gt;: maximize&amp;nbsp;rewards&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Writing down an equation whose solution is our objective&lt;/strong&gt;: Bellman&amp;nbsp;equations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;David Silver walks through this example in his &lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;lecture notes&lt;/a&gt; on &lt;span class="caps"&gt;RL&lt;/span&gt;, but as far as we can tell, does not provide code, so we&amp;#8217;re sharing our implementation,&amp;nbsp;comprising:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the student&amp;#8217;s college &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/discrete_limit_env.py"&gt;environment&lt;/a&gt; using the OpenAI gym&amp;nbsp;package.&lt;/li&gt;
&lt;li&gt;a &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP.ipynb"&gt;jupyter notebook&lt;/a&gt; sampling from the&amp;nbsp;model&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Student in toy&amp;nbsp;college&lt;/h2&gt;
&lt;p&gt;We model the student as an agent in a college environment who can move between five states: &lt;span class="caps"&gt;CLASS&lt;/span&gt; 1, 2, 3, the &lt;span class="caps"&gt;FACEBOOK&lt;/span&gt; state, and &lt;span class="caps"&gt;SLEEP&lt;/span&gt; state.  The states are represented by the four circles and square.  The &lt;span class="caps"&gt;SLEEP&lt;/span&gt; state &amp;#8212; the square with no outward bound arrows &amp;#8212; is a terminal state, i.e. once a student reaches that state, her journey is&amp;nbsp;finished.&lt;/p&gt;
&lt;p&gt;&lt;img alt="student MDP" src="https://efavdb.com/images/student_mdp.png"&gt;&lt;/p&gt;
&lt;p&gt;Actions that a student can take in her current state are labeled in red (facebook/quit/study/sleep/pub) and influence which state she’ll find herself in&amp;nbsp;next.&lt;/p&gt;
&lt;p&gt;In this model, most state transitions are deterministic functions of the action in the current state, e.g. if she decides to study in &lt;span class="caps"&gt;CLASS&lt;/span&gt; 1, then she’ll definitely advance to &lt;span class="caps"&gt;CLASS&lt;/span&gt; 2.  The single non-deterministic state transition is if she goes pubbing while in &lt;span class="caps"&gt;CLASS&lt;/span&gt; 3, where the pubbing action is indicated by a solid dot; she can end up in &lt;span class="caps"&gt;CLASS&lt;/span&gt; 1, 2 or back in 3 with probability 0.2, 0.4, or 0.4, respectively, depending on how reckless the pubbing&amp;nbsp;was.&lt;/p&gt;
&lt;p&gt;The model also specifies the reward &lt;span class="math"&gt;\(R\)&lt;/span&gt; associated with acting in one state and ending up in the next.  In this example, the dynamics &lt;span class="math"&gt;\(p(s’,r|s,a)\)&lt;/span&gt;, are given to us, i.e. we have a full model of the environment, and, hopefully, the rewards have been designed to capture the actual end goal of the&amp;nbsp;student.&lt;/p&gt;
&lt;h2&gt;Markov Decision&amp;nbsp;Process&lt;/h2&gt;
&lt;p&gt;Formally, we’ve modeled the student’s college experience as a finite Markov Decision Process (&lt;span class="caps"&gt;MDP&lt;/span&gt;).  The dynamics are Markov because the probability of ending up in the next state depends only on the current state and action, not on any history leading up to the current state.  The Markov property is integral to the simplification of the equations that describe the model, which we&amp;#8217;ll see in a&amp;nbsp;bit.&lt;/p&gt;
&lt;p&gt;The components of an &lt;span class="caps"&gt;MDP&lt;/span&gt;&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(S\)&lt;/span&gt; - the set of possible&amp;nbsp;states&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(R\)&lt;/span&gt; - the set of (scalar)&amp;nbsp;rewards&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(A\)&lt;/span&gt; - the set of possible actions in each&amp;nbsp;state&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The dynamics of the system are described by the probabilities of receiving a reward in the next state given the current state and action taken, &lt;span class="math"&gt;\(p(s’,r|s,a)\)&lt;/span&gt;.  In this example, the &lt;span class="caps"&gt;MDP&lt;/span&gt; is finite because there are a finite number of states, rewards, and&amp;nbsp;actions.&lt;/p&gt;
&lt;p&gt;The student’s agency in this environment comes from how she decides to act in each state.  The mapping of a state to actions is the &lt;strong&gt;policy&lt;/strong&gt;, &lt;span class="math"&gt;\(\pi(a|s) := p(a|s)\)&lt;/span&gt;, and can be a deterministic or stochastic function of her&amp;nbsp;state.&lt;/p&gt;
&lt;p&gt;Suppose we have an indifferent student who always chooses actions randomly.  We can sample from the &lt;span class="caps"&gt;MDP&lt;/span&gt; to get some example trajectories the student might experience with this policy.  In the sample trajectories below, the states are enclosed in parentheses &lt;code&gt;(STATE)&lt;/code&gt;, and actions enclosed in square brackets &lt;code&gt;[action]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample trajectories&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CLASS1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="c1"&gt;--[facebook]--&amp;gt;(FACEBOOK)--[facebook]--&amp;gt;(FACEBOOK)--[facebook]--&amp;gt;(FACEBOOK)--[facebook]--&amp;gt;(FACEBOOK)--[quit]--&amp;gt;(CLASS1)--[facebook]--&amp;gt;(FACEBOOK)--[quit]--&amp;gt;(CLASS1)--[study]--&amp;gt;(CLASS2)--[sleep]--&amp;gt;(SLEEP)&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FACEBOOK&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="c1"&gt;--[quit]--&amp;gt;(CLASS1)--[study]--&amp;gt;(CLASS2)--[study]--&amp;gt;(CLASS3)--[study]--&amp;gt;(SLEEP)&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SLEEP&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CLASS1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="c1"&gt;--[facebook]--&amp;gt;(FACEBOOK)--[quit]--&amp;gt;(CLASS1)--[study]--&amp;gt;(CLASS2)--[sleep]--&amp;gt;(SLEEP)&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FACEBOOK&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="c1"&gt;--[facebook]--&amp;gt;(FACEBOOK)--[facebook]--&amp;gt;(FACEBOOK)--[facebook]--&amp;gt;(FACEBOOK)--[facebook]--&amp;gt;(FACEBOOK)--[quit]--&amp;gt;(CLASS1)--[facebook]--&amp;gt;(FACEBOOK)--[quit]--&amp;gt;(CLASS1)--[study]--&amp;gt;(CLASS2)--[study]--&amp;gt;(CLASS3)--[pub]--&amp;gt;(CLASS2)--[study]--&amp;gt;(CLASS3)--[study]--&amp;gt;(SLEEP)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Rewards following a random policy&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;Under this random policy, what total reward would the student expect when starting from any of the states?  We can estimate the expected rewards by summing up the rewards per trajectory and plotting the distributions of total rewards per starting&amp;nbsp;state:&lt;/p&gt;
&lt;p&gt;&lt;img alt="histogram of sampled returns" src="https://efavdb.com/images/intro_rl_histogram_sampled_returns.png"&gt;&lt;/p&gt;
&lt;h2&gt;Maximizing rewards: discounted return and value&amp;nbsp;functions&lt;/h2&gt;
&lt;p&gt;We’ve just seen how we can estimate rewards starting from each state given a random policy.  Next, we’ll formalize our goal in terms of maximizing&amp;nbsp;returns.&lt;/p&gt;
&lt;h3&gt;Returns&lt;/h3&gt;
&lt;p&gt;We simply summed the rewards from the sample trajectories above, but the quantity we often want to maximize in practice is the &lt;strong&gt;discounted return &lt;span class="math"&gt;\(G_t\)&lt;/span&gt;&lt;/strong&gt;, which is a sum of the weighted&amp;nbsp;rewards:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{return} \tag{1}
G_t := R_{t+1} + \gamma R_{t+2} + … = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(0 \leq \gamma \leq 1\)&lt;/span&gt;.  &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; is the &lt;em&gt;discount rate&lt;/em&gt; which characterizes how much we weight rewards now vs. later.  Discounting is mathematically useful for avoiding infinite returns in MDPs without a terminal state and allows us to account for uncertainty in the future when we don’t have a perfect model of the&amp;nbsp;environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aside&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The discount factor introduces a time scale since it says that we don&amp;#8217;t care about rewards that are far in the future.  The half-life (actually, the &lt;span class="math"&gt;\(1/e\)&lt;/span&gt; life) of a reward in units of time steps is &lt;span class="math"&gt;\(1/(1-\gamma)\)&lt;/span&gt;, which comes from a definition of &lt;span class="math"&gt;\(1/e\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\frac{1}{e} = \lim_{n \rightarrow \infty} \left(1 - \frac{1}{n} \right)^n
\end{align}&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\gamma = 0.99\)&lt;/span&gt; is often used in practice, which corresponds to a half-life of 100 timesteps since &lt;span class="math"&gt;\(0.99^{100} = (1 - 1/100)^{100} \approx 1/e\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Value&amp;nbsp;functions&lt;/h3&gt;
&lt;p&gt;Earlier, we were able to estimate the expected undiscounted returns starting from each state by sampling from the &lt;span class="caps"&gt;MDP&lt;/span&gt; under a random policy.  Value functions formalize this notion of the &amp;#8220;goodness&amp;#8221; of being in a&amp;nbsp;state.&lt;/p&gt;
&lt;h4&gt;State value function &lt;span class="math"&gt;\(v\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;state value function&lt;/strong&gt; &lt;span class="math"&gt;\(v_{\pi}(s)\)&lt;/span&gt; is the expected return when starting in state &lt;span class="math"&gt;\(s\)&lt;/span&gt;, following policy &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{state-value} \tag{2}
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;The state value function can be written as a recursive relationship, the Bellman expectation equation, expressing the value of a state in terms of the values of its neighors by making use of the Markov&amp;nbsp;property.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{state-value-bellman} \tag{3}
v_{\pi}(s) &amp;amp;=&amp;amp; \mathbb{E}_{\pi}[G_t | S_t = s] \\
       &amp;amp;=&amp;amp; \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+2} | S_t = s] \\
       &amp;amp;=&amp;amp; \sum_{a} \pi(a|s) \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_{\pi}(s’) ]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;This equation expresses the value of a state as an average over the discounted value of its neighbor / successor states, plus the expected reward transitioning from &lt;span class="math"&gt;\(s\)&lt;/span&gt; to &lt;span class="math"&gt;\(s’\)&lt;/span&gt;, and &lt;span class="math"&gt;\(v_{\pi}\)&lt;/span&gt; is the unique&lt;a href="#unique"&gt;*&lt;/a&gt; solution.  The distribution of rewards depends on the student’s policy since her actions influence her future&amp;nbsp;rewards.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note on terminology&lt;/em&gt;:
Policy &lt;em&gt;evaluation&lt;/em&gt; uses the Bellman expectation equation to solve for the value function given a policy &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; and environment dynamics &lt;span class="math"&gt;\(p(s’, r | s, a)\)&lt;/span&gt;.  This is different from policy iteration and value iteration, which are concerned with finding an optimal&amp;nbsp;policy.&lt;/p&gt;
&lt;p&gt;We can solve the Bellman equation for the value function as an alternative to the sampling we did earlier for the student toy example.  Since the problem has a small number of states and actions, and we have full knowledge of the environment, an exact solution is feasible by directly solving the system of linear equations or iteratively using dynamic programming.  Here is the solution to (\ref{state-value-bellman}) for &lt;span class="math"&gt;\(v\)&lt;/span&gt; under a random policy in the student example (compare to the sample means in the histogram of&amp;nbsp;returns):&lt;/p&gt;
&lt;p&gt;&lt;img alt="student MDP value function random policy" src="https://efavdb.com/images/student_mdp_values_random_policy.png"&gt;&lt;/p&gt;
&lt;p&gt;We can verify that the solution is self-consistent by spot checking the value of a state in terms of the values of its neighboring states according to the Bellman equation, e.g. the &lt;span class="caps"&gt;CLASS1&lt;/span&gt; state with &lt;span class="math"&gt;\(v_{\pi}(\text{CLASS1}) = -1.3\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
v_{\pi}(\text{CLASS1}) = 0.5 [-2 + 2.7] + 0.5 [-1 + -2.3] = -1.3
$$&lt;/div&gt;
&lt;h4&gt;Action value function &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;Another value function is the action value function &lt;span class="math"&gt;\(q_{\pi}(s, a)\)&lt;/span&gt;, which is the expected return from a state &lt;span class="math"&gt;\(s\)&lt;/span&gt; if we follow a policy &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; after taking an action &lt;span class="math"&gt;\(a\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{action-value} \tag{4}
q_{\pi}(s, a) := \mathbb{E}_{\pi} [ G_t | S_t = s, A = a ]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;We can also write &lt;span class="math"&gt;\(v\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt; in terms of each other.  For example, the state value function can be viewed as an average over the action value functions for that state, weighted by the probability of taking each action, &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;, from that&amp;nbsp;state:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{state-value-one-step-backup} \tag{5}
v_{\pi}(s) = \sum_{a} \pi(a|s) q_{\pi}(s, a)
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Rewriting &lt;span class="math"&gt;\(v\)&lt;/span&gt; in terms of &lt;span class="math"&gt;\(q\)&lt;/span&gt; in (\ref{state-value-one-step-backup}) is useful later for thinking about the &amp;#8220;advantage&amp;#8221;, &lt;span class="math"&gt;\(A(s,a)\)&lt;/span&gt;, of taking an action in a state, namely how much better is an action in that state than the&amp;nbsp;average?&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
A(s,a) \equiv q(s,a) - v(s)
\end{align}&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Why &lt;span class="math"&gt;\(q\)&lt;/span&gt; in addition to &lt;span class="math"&gt;\(v\)&lt;/span&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Looking ahead, we almost never have access to the environment dynamics in real world problems, but solving for &lt;span class="math"&gt;\(q\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(v\)&lt;/span&gt; lets us get around this problem; we can figure out the best action to take in a state solely using &lt;span class="math"&gt;\(q\)&lt;/span&gt; (we further expand on this in our &lt;a href="#optimalq"&gt;discussion&lt;/a&gt; below on the Bellman optimality equation for &lt;span class="math"&gt;\(q_*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A concrete example of using &lt;span class="math"&gt;\(q\)&lt;/span&gt; is provided in our &lt;a href="https://efavdb.com/multiarmed-bandits"&gt;post&lt;/a&gt; on multiarmed bandits (an example of a simple single-state &lt;span class="caps"&gt;MDP&lt;/span&gt;), which discusses agents/algorithms that don&amp;#8217;t have access to the true environment dynamics.  The strategy amounts to estimating the action value function of the slot machine and using those estimates to inform which slot machine arms to pull in order to maximize&amp;nbsp;rewards.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Optimal value and&amp;nbsp;policy&lt;/h2&gt;
&lt;p&gt;The crux of the &lt;span class="caps"&gt;RL&lt;/span&gt; problem is finding a policy that maximizes the expected return.  A policy &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; is defined to be better than another policy &lt;span class="math"&gt;\(\pi’\)&lt;/span&gt; if &lt;span class="math"&gt;\(v_{\pi}(s) &amp;gt; v_{\pi’}(s)\)&lt;/span&gt; for all states.  We are guaranteed&lt;a href="#unique"&gt;*&lt;/a&gt; an optimal state value function &lt;span class="math"&gt;\(v_*\)&lt;/span&gt; which corresponds to one or more optimal policies &lt;span class="math"&gt;\(\pi*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Recall that the value function for an arbitrary policy can be written in terms of an average over the action values for that state (\ref{state-value-one-step-backup}).  In contrast, the optimal value function &lt;span class="math"&gt;\(v_*\)&lt;/span&gt; must be consistent with following a policy that selects the action that maximizes the action value functions from a state, i.e. taking a &lt;span class="math"&gt;\(\max\)&lt;/span&gt; (\ref{state-value-bellman-optimality}) instead of an average (\ref{state-value-one-step-backup}) over &lt;span class="math"&gt;\(q\)&lt;/span&gt;, leading to the &lt;strong&gt;Bellman optimality equation&lt;/strong&gt; for &lt;span class="math"&gt;\(v_*\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{state-value-bellman-optimality} \tag{6}
v_*(s) &amp;amp;=&amp;amp; \max_a q_{\pi*}(s, a) \\
    &amp;amp;=&amp;amp; \max_a \mathbb{E}_{\pi*} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] \\
    &amp;amp;=&amp;amp; \max_a \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_*(s’) ]
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;The optimal policy immediately follows: take the action in a state that maximizes the right hand side of (\ref{state-value-bellman-optimality}).  The &lt;a href="https://en.wikipedia.org/wiki/Bellman_equation#Bellman's_Principle_of_Optimality"&gt;principle of optimality&lt;/a&gt;, which applies to the Bellman optimality equation, means that this greedy policy actually corresponds to the optimal policy!  Note: Unlike the Bellman expectation equations, the Bellman optimality equations are a nonlinear system of equations due to taking the&amp;nbsp;max.&lt;/p&gt;
&lt;p&gt;The Bellman optimality equation for the action value function &lt;span class="math"&gt;\(q_*(s,a)\)&lt;/span&gt;&lt;a name="optimalq"&gt;&lt;/a&gt;&amp;nbsp;is:&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{action-value-bellman-optimality} \tag{7}
q_*(s, a) &amp;amp;=&amp;amp; \mathbb{E}_{\pi*} [R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}', a') | S_t = s, A_t = a] \\
          &amp;amp;=&amp;amp; \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a') ]
\end{eqnarray}&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;Looking ahead: In practice, without a knowledge of the environment dynamics, &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms based on solving value functions can approximate the expectation in (\ref{action-value-bellman-optimality}) by sampling, i.e. interacting with the environment, and iteratively selecting the action that corresponds to maximizing &lt;span class="math"&gt;\(q\)&lt;/span&gt; in each state that the agent lands in along its trajectory, which is possible since the maximum occurs &lt;strong&gt;inside&lt;/strong&gt; the summation in (\ref{action-value-bellman-optimality}).   In contrast, this sampling approach doesn&amp;#8217;t work for (\ref{state-value-bellman-optimality}) because of the maximum &lt;strong&gt;outside&lt;/strong&gt; the summation in&amp;#8230;that&amp;#8217;s why action value functions are so useful when we lack a model of the&amp;nbsp;environment!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Here is the optimal state value function and policy for the student example, which we solve for in a later&amp;nbsp;post:&lt;/p&gt;
&lt;p&gt;&lt;img alt="student MDP optimal value function" src="https://efavdb.com/images/student_mdp_optimal_values.png"&gt;&lt;/p&gt;
&lt;p&gt;Comparing the values per state under the optimal policy vs the random policy, the value in every state under the optimal policy exceeds the value under the random&amp;nbsp;policy.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;We’ve discussed how the problem of sequential decision making can be framed as an &lt;span class="caps"&gt;MDP&lt;/span&gt; using the student toy &lt;span class="caps"&gt;MDP&lt;/span&gt; as an example.  The goal in &lt;span class="caps"&gt;RL&lt;/span&gt; is to figure out a policy &amp;#8212; what actions to take in each state &amp;#8212; that maximizes our&amp;nbsp;returns.&lt;/p&gt;
&lt;p&gt;MDPs provide a framework for approaching the problem by defining the value of each state, the value functions, and using the value functions to define what a “best policy” means.  The value functions are unique solutions to the Bellman equations, and the &lt;span class="caps"&gt;MDP&lt;/span&gt; is “solved” when we know the optimal value&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;Much of reinforcement learning centers around trying to solve these equations under different conditions, e.g. unknown environment dynamics and large &amp;#8212; possibly continuous &amp;#8212; states and/or action spaces that require approximations to the value&amp;nbsp;functions.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ll discuss how we arrived at the solutions for this toy problem in a future&amp;nbsp;post!&lt;/p&gt;
&lt;h3&gt;Example&amp;nbsp;code&lt;/h3&gt;
&lt;p&gt;Code for sampling from the student environment under a random policy in order to generate the trajectories and histograms of returns is available in this &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP.ipynb"&gt;jupyter notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/discrete_limit_env.py"&gt;code&lt;/a&gt; for the student environment creates an environment with an &lt;span class="caps"&gt;API&lt;/span&gt; that is compatible with OpenAI gym &amp;#8212; specifically, it is derived from the &lt;code&gt;gym.envs.toy_text.DiscreteEnv&lt;/code&gt; environment.&lt;/p&gt;
&lt;p&gt;&lt;a name="unique"&gt;&lt;em&gt;&lt;/a&gt;The uniqueness of the solution to the Bellman equations for finite MDPs is stated without proof in Ref [2], but Ref [1] motivates it briefly via the &lt;/em&gt;contraction mapping&amp;nbsp;theorem*.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] David Silver&amp;#8217;s &lt;span class="caps"&gt;RL&lt;/span&gt; Course Lecture 2 - (&lt;a href="https://www.youtube.com/watch?v=lfHX2hHRMVQ"&gt;video&lt;/a&gt;,
  &lt;a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf"&gt;slides&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[2] Sutton and Barto -
  &lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction&lt;/a&gt; - Chapter 3: Finite Markov Decision&amp;nbsp;Processes&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine learning"></category><category term="reinforcement learning"></category><category term="machine learning"></category><category term="OpenAI"></category></entry><entry><title>Multiarmed bandits in the context of reinforcement learning</title><link href="https://efavdb.com/multiarmed-bandits" rel="alternate"></link><published>2020-02-25T12:00:00-08:00</published><updated>2020-02-25T12:00:00-08:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2020-02-25:/multiarmed-bandits</id><summary type="html">&lt;p&gt;&lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction&lt;/a&gt; by Sutton and Barto[1] is a book that is universally recommended to beginners in their &lt;span class="caps"&gt;RL&lt;/span&gt; studies.  The first chapter is an extended text-heavy introduction. The second chapter deals with multiarmed bandits, i.e. slot machines with multiple arms, and is the subject of today …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction&lt;/a&gt; by Sutton and Barto[1] is a book that is universally recommended to beginners in their &lt;span class="caps"&gt;RL&lt;/span&gt; studies.  The first chapter is an extended text-heavy introduction. The second chapter deals with multiarmed bandits, i.e. slot machines with multiple arms, and is the subject of today&amp;#8217;s&amp;nbsp;post.&lt;/p&gt;
&lt;p&gt;Before getting into the &lt;em&gt;what&lt;/em&gt; and &lt;em&gt;how&lt;/em&gt; of bandits, I&amp;#8217;d like to address the &lt;strong&gt;why&lt;/strong&gt;, since the &amp;#8220;why&amp;#8221; can guard against getting lost in the details / not seeing the forest for the&amp;nbsp;trees.&lt;/p&gt;
&lt;h1&gt;Why discuss multiarmed&amp;nbsp;bandits?&lt;/h1&gt;
&lt;p&gt;&lt;span class="caps"&gt;RL&lt;/span&gt; treats the problem of trying to achieve a goal in an environment where an agent is &lt;em&gt;not&lt;/em&gt; instructed about which actions to take to achieve that goal, in contrast to supervised learning problems.  Learning the best actions to take is a complicated problem, since the best actions depend on what state an agent is in, e.g. an agent trying to get to a goalpost east of its current location as quickly as possible may find that moving east is a generally good policy, but not if there is a fire-breathing dragon in the way, in which case, it might make sense to move up or down to navigate around the&amp;nbsp;obstacle.&lt;/p&gt;
&lt;p&gt;Multiarmed bandits are simpler problem: a single state system.  No matter which action an agent takes, i.e. which slot machine arm the agent pulls, the agent ends up back in the same state; the distribution of rewards as a consequence of the agent&amp;#8217;s action remains the same, assuming a stationary distribution of rewards, and actions have no effect on subsequent states or rewards.  This simple case study is useful for building intuition and introducing &lt;span class="caps"&gt;RL&lt;/span&gt; concepts that will be expanded on in later chapters of&amp;nbsp;[1].&lt;/p&gt;
&lt;h1&gt;Key &lt;span class="caps"&gt;RL&lt;/span&gt; concepts introduced by the multiarmed bandit&amp;nbsp;problem&lt;/h1&gt;
&lt;h2&gt;The nature of the&amp;nbsp;problem&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Agent has a goal&lt;/strong&gt;: In &lt;span class="caps"&gt;RL&lt;/span&gt; and multiarmed bandit problems, we want to figure out the strategy, or &amp;#8220;policy&amp;#8221; in &lt;span class="caps"&gt;RL&lt;/span&gt; lingo, that will maximize our rewards.  For the simple bandit problem, this goal is equivalent to maximizing the reward &amp;#8212; literally, money! &amp;#8212; for each arm&amp;nbsp;pull.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unlike supervised learning, no ground truth is supplied&lt;/strong&gt;: Each slot has a different distribution of rewards, but the agent playing the machine does not know that distribution.  Instead, the agent has to try different actions and evaluate how good the actions are.  The goodness of an action is straightforwardly determined by its immediate reward in the bandit&amp;nbsp;case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exploration vs. exploitation&lt;/strong&gt;:  Based on a few trials, one arm may appear to yield the highest rewards, but the agent may decide to try others occasionally to improve its estimates of the rewards, an example of balancing exploration and exploitation.  The various algorithms handle exploration vs. exploitation differently, but this example introduces one method that is simple but widely-used in practice: the epsilon-greedy algorithm, which takes greedy actions most of the time (exploits) but takes random actions (explores) a fraction epsilon of the&amp;nbsp;time.&lt;/p&gt;
&lt;h3&gt;Different approaches to learning a&amp;nbsp;policy&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;model-free&lt;/strong&gt;:  All the strategies discussed in [1] for solving the bandit problem are &amp;#8220;model-free&amp;#8221; strategies.  In real world applications, a model of the world is rarely available, and the agent has to figure out how to act based on sampled experience, and the same applies to the bandit case; even though bandits are a simpler single state system (we don&amp;#8217;t have to model transitions from state to state), an agent still does not know the model that generates the probability of a reward &lt;span class="math"&gt;\(r\)&lt;/span&gt; given an action &lt;span class="math"&gt;\(a\)&lt;/span&gt;, &lt;span class="math"&gt;\(P(r|a)\)&lt;/span&gt; and has to figure that out from trial and&amp;nbsp;error.&lt;/p&gt;
&lt;p&gt;There &lt;em&gt;are&lt;/em&gt; model-based algorithms that attempt to model the environment&amp;#8217;s transition dynamics from data, but many popular algorithms today are model-free because of the difficulty of modeling those&amp;nbsp;dynamics.&lt;/p&gt;
&lt;h4&gt;Learning&amp;nbsp;action-values&lt;/h4&gt;
&lt;p&gt;The bandit problem introduces the idea of estimating the expected value associated with each action, namely the &lt;em&gt;action-value function&lt;/em&gt; in &lt;span class="caps"&gt;RL&lt;/span&gt; terms.  The concept is very intuitive &amp;#8212; as an agent pulls on different bandit arms, it will accumulate rewards associated with each arm.  A simple way to estimate the expected value per arm is just to average the rewards generated by pulling on each slot.  The policy that follows is then implicit, namely, take the action / pull on the arm with the highest estimated&amp;nbsp;action-value!&lt;/p&gt;
&lt;p&gt;Historically, &lt;span class="caps"&gt;RL&lt;/span&gt; formalism has dealt with estimating value functions and using them to figure out a policy, which includes the Q-Learning (&amp;#8220;Q&amp;#8221; stands for action-value!) approach we mentioned in our earlier &lt;a href="https://efavdb.com/openai-scholars-intro"&gt;post&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Learning policies&amp;nbsp;directly&lt;/h4&gt;
&lt;p&gt;[1] also use the bandit problem to introduce a type of algorithm that approaches the problem, not indirectly by learning a value function and deriving the policy from those value functions, but by parameterizing the policy directly and learning the parameters that optimize the rewards.  This class of algorithm is a &amp;#8220;policy gradient method&amp;#8221; and is very popular today for its nice convergence properties.  After the foreshadowing in the bandit problem, policy gradients only reappear very late in [1] &amp;#8212; chapter&amp;nbsp;13!&lt;/p&gt;
&lt;p&gt;We now provide code for&amp;nbsp;concreteness.&lt;/p&gt;
&lt;h1&gt;Ground truth is hidden in our multiarmed&amp;nbsp;bandit&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;Bandit&lt;/code&gt; class initializes a multiarmed bandit. The distribution of rewards per arm follows a Gaussian distribution with some mean dollar&amp;nbsp;amount.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Bandit&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;N-armed bandit with stationary distribution of rewards per arm.&lt;/span&gt;
&lt;span class="sd"&gt;    Each arm (action) is identified by an integer.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_arms&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_arms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_arms&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;
        &lt;span class="c1"&gt;# a dict of the mean action_value per arm, w/ each action_value sampled from a Gaussian&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_arms&lt;/span&gt;&lt;span class="p"&gt;))}&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;  &lt;span class="c1"&gt;# arms of the bandit&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Get reward from bandit for action&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Implementation detail: the means per arm, stored in &lt;code&gt;self.action_values&lt;/code&gt;, are drawn from a Gaussian distribution upon&amp;nbsp;initialization).&lt;/p&gt;
&lt;p&gt;The agent doesn&amp;#8217;t know the true mean rewards per arm &amp;#8212; it only sees a sample reward when he takes the action of pulling on a particular bandit arm (&lt;code&gt;__call__&lt;/code&gt;).&lt;/p&gt;
&lt;h1&gt;Action, reward, update&amp;nbsp;strategy&lt;/h1&gt;
&lt;p&gt;For every action the agent takes, it gets a reward.  With each additional interaction with the bandit, the agent has a new data point it can use to update its strategy (whether indirectly, via an updated action-value estimate, or directly in the policy&amp;nbsp;gradient).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;BaseBanditAlgo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ABC&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Base class for algorithms to maximize the rewards &lt;/span&gt;
&lt;span class="sd"&gt;    for the multiarmed bandit problem&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Bandit&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bandit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bandit&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timestep&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

    &lt;span class="nd"&gt;@abstractmethod&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_select_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

    &lt;span class="nd"&gt;@abstractmethod&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_update_for_action_and_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
         &lt;span class="k"&gt;pass&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_select_action&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_update_for_action_and_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_timesteps&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_timesteps&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timestep&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Two types of strategies: value based and policy&amp;nbsp;based&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;value based - agents try to directly estimate the value of
   each action (and whose policies, i.e. probability of selecting an
   action, are therefore implicit, since the agent will want to choose
   the action that has the highest&amp;nbsp;value)&lt;/li&gt;
&lt;li&gt;policy based - agents don&amp;#8217;t try to directly estimate the value
   of an action and instead directly store the policy, i.e. the
   probability of taking each&amp;nbsp;action.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;An example of a &lt;strong&gt;value based&lt;/strong&gt; strategy / action-value method for the
bandit problem is the &lt;code&gt;EpsilonGreedy&lt;/code&gt; approach, which selects the
action associated with the highest estimated action-value with probability &lt;span class="math"&gt;\(1-\epsilon\)&lt;/span&gt;, but chooses a random arm
a fraction &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; of the time as part of its exploration&amp;nbsp;strategy.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;EpsilonGreedy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEstimateActionValueAlgo&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Greedy algorithm that explores/samples from the non-greedy action some fraction, &lt;/span&gt;
&lt;span class="sd"&gt;    epsilon, of the time.&lt;/span&gt;

&lt;span class="sd"&gt;    - For a basic greedy algorithm, set epsilon = 0.&lt;/span&gt;
&lt;span class="sd"&gt;    - For optimistic intialization, set q_init &amp;gt; mu, the mean of the Gaussian from&lt;/span&gt;
&lt;span class="sd"&gt;      which the real values per bandit arm are sampled (default is 0).&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Bandit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_select_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# take random action&lt;/span&gt;
            &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# take greedy action&lt;/span&gt;
            &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;est_action_values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;est_action_values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(See end of post for additional action-value&amp;nbsp;methods.)&lt;/p&gt;
&lt;p&gt;An example of a &lt;strong&gt;policy based&lt;/strong&gt; strategy is the &lt;code&gt;GradientBandit&lt;/code&gt;
method, which stores its policy, the probability per action in
&lt;code&gt;self.preferences&lt;/code&gt;.  It learns these preferences by doing stochastic
gradient ascent along the preferences in the gradient of the expected
reward in &lt;code&gt;_update_for_action_and_reward&lt;/code&gt; (see [1] for&amp;nbsp;derivation).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;GradientBandit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseBanditAlgo&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Algorithm that does not try to estimate action values directly and, instead, tries to learn&lt;/span&gt;
&lt;span class="sd"&gt;    a preference for each action (equivalent to stochastic gradient ascent along gradient in expected&lt;/span&gt;
&lt;span class="sd"&gt;    reward over preferences).&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Bandit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;  &lt;span class="c1"&gt;# step-size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reward_baseline_avg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;preferences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_calc_probs_from_preferences&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_calc_probs_from_preferences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Probabilities per action follow a Boltzmann distribution over the preferences &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;exp_preferences_for_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;preferences&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()}&lt;/span&gt;
        &lt;span class="n"&gt;partition_fxn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;exp_preferences_for_action&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probabilities_for_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OrderedDict&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;partition_fxn&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; 
                                                     &lt;span class="n"&gt;exp_preferences_for_action&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_select_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probabilities_for_action&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt; 
                                &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probabilities_for_action&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_update_for_action_and_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Update preferences&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;reward_diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reward_baseline_avg&lt;/span&gt;

        &lt;span class="c1"&gt;# can we combine these updates into single expression using kronecker delta?&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;preferences&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;reward_diff&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probabilities_for_action&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bandit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;continue&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;preferences&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;reward_diff&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probabilities_for_action&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reward_baseline_avg&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timestep&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;reward_diff&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_calc_probs_from_preferences&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Extra: Total rewards for different bandit&amp;nbsp;algorithms&lt;/h1&gt;
&lt;p&gt;We have discussed a bunch of different bandit algorithms, but haven&amp;#8217;t seen what rewards they yield in&amp;nbsp;practice!&lt;/p&gt;
&lt;p&gt;In this
&lt;a href="https://github.com/frangipane/reinforcement-learning/blob/master/00-Introduction/multiarmed_bandits.ipynb"&gt;Jupyter notebook&lt;/a&gt;,
we run the algorithms through a range of values for their parameters
to compare their cumulative rewards across 1000 timesteps (also
averaged across many trials of different bandits to smooth things
out).  In the end, we arrive at a plot of the parameter study, that
reproduces Figure 2.6 in&amp;nbsp;[1].&lt;/p&gt;
&lt;p&gt;&lt;img alt="![parameter study]({static}/images/reproduce_multiarmed_bandit_parameter_study.png)" src="https://efavdb.com/images/reproduce_multiarmed_bandit_parameter_study.png"&gt;&lt;/p&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Sutton and Barto - &lt;a href="http://incompleteideas.net/book/RLbook2018.pdf"&gt;Reinforcement Learning: An Introduction (2nd&amp;nbsp;Edition)&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine learning"></category><category term="reinforcement learning"></category><category term="machine learning"></category></entry></feed>