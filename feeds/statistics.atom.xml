<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>EFAVDB - Statistics</title><link href="http/" rel="alternate"></link><link href="http/feeds/statistics.atom.xml" rel="self"></link><id>http/</id><updated>2016-08-21T00:00:00-07:00</updated><subtitle>Everybody's Favorite Data Blog</subtitle><entry><title>Hyperparameter sample-size dependence</title><link href="http/model-selection.html" rel="alternate"></link><published>2016-08-21T00:00:00-07:00</published><updated>2016-08-21T00:00:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:None,2016-08-21:http/model-selection.html</id><summary type="html">&lt;p&gt;Here, we briefly review a subtlety associated with machine-learning model selection: the fact that the optimal hyperparameters for a model can vary with training set size, &lt;span class="math"&gt;\(N.\)&lt;/span&gt; To illustrate this point, we derive expressions for the optimal strength for both &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; regularization in single-variable models. We find that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here, we briefly review a subtlety associated with machine-learning model selection: the fact that the optimal hyperparameters for a model can vary with training set size, &lt;span class="math"&gt;\(N.\)&lt;/span&gt; To illustrate this point, we derive expressions for the optimal strength for both &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; regularization in single-variable models. We find that the optimal &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; approaches a finite constant as &lt;span class="math"&gt;\(N\)&lt;/span&gt; increases, but that the optimal &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; decays exponentially fast with &lt;span class="math"&gt;\(N.\)&lt;/span&gt; Sensitive dependence on &lt;span class="math"&gt;\(N\)&lt;/span&gt; such as this should be carefully extrapolated out when optimizing mission-critical&amp;nbsp;models.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;There are two steps one must carry out to fit a machine-learning model. First, a specific model form and cost function must be selected, and second the model must be fit to the data. The first of these steps is often treated by making use of a training-test data split: One trains a set of candidate models to a fraction of the available data and then validates their performance using a hold-out, test set. The model that performs best on the latter is then selected for&amp;nbsp;production.&lt;/p&gt;
&lt;p&gt;Our purpose here is to highlight a subtlety to watch out for when carrying out an optimization as above: the fact that the optimal model can depend sensitively on training set size &lt;span class="math"&gt;\(N\)&lt;/span&gt;. This observation suggests that the training-test split paradigm must sometimes be applied with care: Because a subsample is used for training in the first, selection step, the model identified as optimal there may not be best when training on the full data&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;To illustrate the above points, our main effort here is to present some toy examples where the optimal hyperparameters can be characterized exactly: We derive the optimal &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; regularization strength for models having only a single variable. These examples illustrate two opposite limits: The latter approaches a finite constant as &lt;span class="math"&gt;\(N\)&lt;/span&gt; increases, but the former varies exponentially with &lt;span class="math"&gt;\(N\)&lt;/span&gt;. This shows that strong &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dependence can sometimes occur, but is not necessarily always an issue. In practice, a simple way to check for sensitivity is to vary the size of your training set during model selection: If a strong dependence is observed, care should be taken during the final&amp;nbsp;extrapolation.&lt;/p&gt;
&lt;p&gt;We now walk through our two&amp;nbsp;examples.&lt;/p&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(L_2\)&lt;/span&gt;&amp;nbsp;optimization&lt;/h3&gt;
&lt;p&gt;We start off by positing that we have a method for generating a Bayesian posterior for a parameter &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that is a function of a vector of &lt;span class="math"&gt;\(N\)&lt;/span&gt; random samples &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;. To simplify our discussion, we assume that &amp;#8212; given a flat prior &amp;#8212; this is unbiased and normal with variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;. We write &lt;span class="math"&gt;\(\theta_0 \equiv \theta_0(\textbf{x})\)&lt;/span&gt; for the maximum a posteriori (&lt;span class="caps"&gt;MAP&lt;/span&gt;) value under the flat prior. With the introduction of an &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; prior, the posterior for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is then&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{1}  
P\left(\theta \vert \theta_0(\textbf{x})\right) \propto \exp\left( - \frac{(\theta - \theta_0)^2}{2 \sigma^2} - \Lambda \theta^2 \right).  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Setting the derivative of the above to zero, the point-estimate, &lt;span class="caps"&gt;MAP&lt;/span&gt; is given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{2}  
\hat{\theta} = \frac{\theta_0}{1 + 2 \Lambda \sigma^2}.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The average squared error of this estimate is obtained by averaging over the possible &lt;span class="math"&gt;\(\theta_0\)&lt;/span&gt; values. Our assumptions above imply that &lt;span class="math"&gt;\(\theta_0\)&lt;/span&gt; is normal about the true parameter value, &lt;span class="math"&gt;\(\theta_*\)&lt;/span&gt;, so we have&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  
\langle (\hat{\theta} - \theta_*)^2 \rangle &amp;amp;\equiv&amp;amp; \int_{\infty}^{\infty} \frac{1}{\sqrt{2 \pi \sigma^2}}  
\exp\left( - \frac{(\theta_0 - \theta_*)^2}{2 \sigma^2}\right) \left ( \frac{\theta_0}{1 + 2 \Lambda \sigma^2} - \theta_* \right)^2 d \theta_0 \\  
&amp;amp;=&amp;amp; \frac{ 4 \Lambda^2 \sigma^4 \theta_*^2 }{(1 + 2 \Lambda \sigma^2 )^2} + \frac{\sigma^2}{\left( 1 + 2 \Lambda \sigma^2 \right)^2}. \tag{3} \label{error}  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The optimal &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; is readily obtained by minimizing this average error. This gives,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \label{result}  
\Lambda = \frac{1}{2 \theta_*^2}, \tag{4}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
a constant, independent of sample size. The mean squared error with this choice is obtained by plugging (\ref{result}) into (\ref{error}). This gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\langle (\hat{\theta} - \theta_*)^2 \rangle = \frac{\sigma^2}{1 + \sigma^2 / \theta_*^2}. \tag{5}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Notice that this is strictly less than &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; &amp;#8212; the variance one would get without regularization &amp;#8212; and that the benefit is largest when &lt;span class="math"&gt;\(\sigma^2 \gg \theta_*^2\)&lt;/span&gt;. That is, &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; regularization is most effective when &lt;span class="math"&gt;\(\theta_*\)&lt;/span&gt; is hard to differentiate from zero &amp;#8212; an intuitive&amp;nbsp;result!&lt;/p&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(L_1\)&lt;/span&gt;&amp;nbsp;optimization&lt;/h3&gt;
&lt;p&gt;The analysis for &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; optimization is similar to the above, but slightly more involved. We go through it quickly. The posterior with an &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; prior is given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{6}  
P\left(\theta \vert \theta_0(\textbf{x})\right) \propto \exp\left( - \frac{(\theta - \theta_0)^2}{2 \sigma^2} - \Lambda \vert \theta \vert \right).  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Assuming for simplicity that &lt;span class="math"&gt;\(\hat{\theta} &amp;gt; 0\)&lt;/span&gt;, the &lt;span class="caps"&gt;MAP&lt;/span&gt; value is now&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{7}  
\hat{\theta} = \begin{cases}  
\theta_0 - \Lambda \sigma^2 &amp;amp; \text{if } \theta_0 - \Lambda \sigma^2 &amp;gt; 0 \\  
0 &amp;amp; \text{else}.  
\end{cases}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The mean squared error of the estimator is&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{8}  
\langle (\hat{\theta} - \theta_*)^2 \rangle \equiv \int \frac{1}{\sqrt{2 \pi \sigma^2}}  
\exp\left( - \frac{(\theta_0 - \theta_*)^2}{2 \sigma^2}\right) \left ( \hat{\theta} - \theta_* \right)^2 d \theta_0.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This can be evaluated in terms of error functions. The optimal value of &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; is obtained by differentiating the above. Doing this, one finds that it satisfies the equation&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{9}  
\exp\left( - \frac{(\tilde{\Lambda}- \tilde{\theta_*})^2}{2} \right ) + \sqrt{\frac{\pi}{2}} \tilde{\Lambda} \ \text{erfc}\left( \frac{\tilde{\Lambda} - \tilde{\theta_*}}{\sqrt{2}} \right ) = 0,  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where &lt;span class="math"&gt;\(\tilde{\Lambda} \equiv \sigma \Lambda\)&lt;/span&gt; and &lt;span class="math"&gt;\(\tilde{\theta_*} \equiv \theta_* / \sigma\)&lt;/span&gt;. In general, the equation above must be solved numerically. However, in the case where &lt;span class="math"&gt;\(\theta_* \gg \sigma\)&lt;/span&gt; &amp;#8212; relevant when &lt;span class="math"&gt;\(N\)&lt;/span&gt; is large &amp;#8212; we can obtain a clean asymptotic solution. In this case, we have &lt;span class="math"&gt;\(\tilde{\theta_*} \gg 1\)&lt;/span&gt; and we expect &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; small. This implies that the above equation can be approximated as&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{10}  
\exp\left( - \frac{\tilde{\theta_*}^2}{2} \right ) - \sqrt{2 \pi} \tilde{\Lambda} \sim 0.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Solving gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{11}  
\Lambda \sim \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left( - \frac{\theta_*^2}{2 \sigma^2} \right ) \sim \frac{\sqrt{N}}{\sqrt{2 \pi \sigma_1^2}} \exp\left( - \frac{N \theta_*^2}{2 \sigma_1^2} \right ).  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, in the last line we have made the &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dependence explicit, writing &lt;span class="math"&gt;\(\sigma^2 = \sigma_1^2 / N\)&lt;/span&gt; &amp;#8212; a form that follows when our samples &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; are independent. Whereas the optimal &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; regularization strength approaches a constant, our result here shows that the optimal &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; strength decays exponentially to zero as &lt;span class="math"&gt;\(N\)&lt;/span&gt;&amp;nbsp;increases.&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;The subtlety that we have discussed here is likely already familiar to those with significant applied modeling experience: optimal model hyperparameters can vary with training set size. However, the two toy examples we have presented are interesting in that they allow for this &lt;span class="math"&gt;\(N\)&lt;/span&gt; dependence to be derived explicitly. Interestingly, we have found that the &lt;span class="caps"&gt;MSE&lt;/span&gt;-minimizing &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; regularization remains finite, even at large training set size, but the optimal &lt;span class="math"&gt;\(L_1\)&lt;/span&gt; regularization goes to zero in this same limit. For small and medium &lt;span class="math"&gt;\(N\)&lt;/span&gt;, this exponential dependence represents a strong sensitivity to &lt;span class="math"&gt;\(N\)&lt;/span&gt; &amp;#8212; one that must be carefully taken into account when extrapolating to the full training&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;One can imagine many other situations where hyperparameters vary strongly with &lt;span class="math"&gt;\(N\)&lt;/span&gt;. For example, very complex systems may allow for ever-increasing model complexity as more data becomes available. Again, in practice, the most straightforward method to check for this is to vary the size of the training set during model selection. If strong dependence is observed, this should be extrapolated out to obtain the truly optimal model for&amp;nbsp;production.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>Average queue wait times with random arrivals</title><link href="http/average-queue-wait-times-with-random-arrivals.html" rel="alternate"></link><published>2016-04-23T09:51:00-07:00</published><updated>2016-04-23T09:51:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:None,2016-04-23:http/average-queue-wait-times-with-random-arrivals.html</id><summary type="html">&lt;p&gt;Queries ping a certain computer server at random times, on average &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; arriving per second. The server can respond to one per second and those that can&amp;#8217;t be serviced immediately are queued up. What is the average wait time per query? Clearly if &lt;span class="math"&gt;\(\lambda \ll 1\)&lt;/span&gt;, the average wait …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Queries ping a certain computer server at random times, on average &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; arriving per second. The server can respond to one per second and those that can&amp;#8217;t be serviced immediately are queued up. What is the average wait time per query? Clearly if &lt;span class="math"&gt;\(\lambda \ll 1\)&lt;/span&gt;, the average wait time is zero. But if &lt;span class="math"&gt;\(\lambda &amp;gt; 1\)&lt;/span&gt;, the queue grows indefinitely and the answer is infinity! Here, we give a simple derivation of the general result &amp;#8212; (9)&amp;nbsp;below.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The mathematics of queue waiting times &amp;#8212; first worked out by &lt;a href="https://en.wikipedia.org/wiki/Erlang_(unit)"&gt;Agner Krarup Erlang&lt;/a&gt; &amp;#8212; is interesting for two reasons. First, as noted above, queues can exhibit phase-transition like behaviors: If the average arrival time is shorter than the average time it takes to serve a customer, the line will grow indefinitely, causing the average wait time to diverge. Second, when the average arrival time is less than the service time, waiting times are governed entirely by fluctuations &amp;#8212; and so can&amp;#8217;t be estimated well using mean-field arguments. For example, in the very low arrival rate limit, the only situation where anyone would ever have to wait at all is that where someone else happens to arrive just before them &amp;#8212; an unlucky, rare&amp;nbsp;event.&lt;/p&gt;
&lt;p&gt;Besides being interesting from a theoretical perspective, an understanding of queue formation phenomena is also critical for many practical applications &amp;#8212; both in computer science and in wider industry settings. Optimal staffing of a queue requires a careful estimate of the expected customer arrival rate. If too many workers are staffed, the average wait time will be nicely low, but workers will largely be idle. Staff too few, and the business could enter into the divergent queue length regime &amp;#8212; certainly resulting in unhappy customers and lost business (or dropped queries). Staffing just the right amount requires a sensitive touch &amp;#8212; and in complex cases, a good understanding of the&amp;nbsp;theory.&lt;/p&gt;
&lt;p&gt;In order to derive the average wait time for queues of different sorts, one often works within the framework of Markov processes. This approach is very general and elementary, but requires a bit of effort to develop the machinery needed get to the end results. Here, we demonstrate an alternative, sometimes faster approach that is based on writing down an integral equation for the wait time distribution. We consider only a simple case &amp;#8212; that where the queue is serviced by only one staff member, the customers arrive at random times via a Poisson process, and each customer requires the same time to service, one&amp;nbsp;second.&lt;/p&gt;
&lt;h3&gt;Integral equation&amp;nbsp;formulation&lt;/h3&gt;
&lt;p&gt;Suppose the &lt;span class="math"&gt;\(N\)&lt;/span&gt;-th customer arrives at time &lt;span class="math"&gt;\(0\)&lt;/span&gt;, and let &lt;span class="math"&gt;\(P(t)\)&lt;/span&gt; be the probability that this customer has to wait a time &lt;span class="math"&gt;\(t\geq 0\)&lt;/span&gt; before being served. This wait time can be written in terms of the arrival and wait times of the previous customer: If this previous customer arrived at time &lt;span class="math"&gt;\(t^{\prime}\)&lt;/span&gt; and has to wait a time &lt;span class="math"&gt;\(w\)&lt;/span&gt; before being served, his service will conclude at time &lt;span class="math"&gt;\(t = t^{\prime} + w + 1\)&lt;/span&gt;. If this is greater than &lt;span class="math"&gt;\(0\)&lt;/span&gt;, the &lt;span class="math"&gt;\(N\)&lt;/span&gt;-th customer will have to wait before being served. In particular, he will wait &lt;span class="math"&gt;\(t\)&lt;/span&gt; if the previous customer waited &lt;span class="math"&gt;\(w = t - t^{\prime} - 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The above considerations allow us to write down an equation satisfied by the wait time distribution. If we let the probability that the previous customer arrived at &lt;span class="math"&gt;\(t^{\prime}\)&lt;/span&gt; be &lt;span class="math"&gt;\(A(t^{\prime})\)&lt;/span&gt;, we have (for &lt;span class="math"&gt;\(t &amp;gt; 0\)&lt;/span&gt;)&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\tag{1} \label{int_eqn}  
P(t) = \int_{-\infty}^{0^-} A(t^{\prime}) P(t - t^{\prime} - 1) d t^{\prime}  
= \int_{-\infty}^{0^-} \lambda e^{\lambda t^{\prime}} P(t - t^{\prime} - 1) d t^{\prime}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, in the first equality we&amp;#8217;re simply averaging over the possible arrival times of the previous customer (which had to occur before the &lt;span class="math"&gt;\(N\)&lt;/span&gt;-th, at &lt;span class="math"&gt;\(0\)&lt;/span&gt;), multiplying by the probability &lt;span class="math"&gt;\(P(t - t^{\prime} - 1)\)&lt;/span&gt; that this customer had to wait the amount of time &lt;span class="math"&gt;\(w\)&lt;/span&gt; needed so that the &lt;span class="math"&gt;\(N\)&lt;/span&gt;-th customer will wait &lt;span class="math"&gt;\(t\)&lt;/span&gt;. We also use the symmetry that each customer has the same wait time distribution at steady state. In the second equality, we have plugged in the arrival time distribution appropriate for our Poisson&amp;nbsp;model.&lt;/p&gt;
&lt;p&gt;To proceed, we differentiate both sides of (\ref{int_eqn}) with respect to &lt;span class="math"&gt;\(t\)&lt;/span&gt;,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{2} \label{int2}  
P^{\prime}(t) = \int_{-\infty}^{0^-} \lambda e^{\lambda t^{\prime}} \frac{d}{dt}P(t - t^{\prime} - 1) d t^{\prime} = - \int_{-\infty}^{0^-} \lambda e^{\lambda t^{\prime}} \frac{d}{dt^{\prime}}P(t - t^{\prime} - 1) d t^{\prime}.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The second equality follows after noticing that we can switch the parameter being differentiated in the first. Integrating by parts, we obtain&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
P^{\prime}(t) = \lambda \left [P(t) - P(t-1) \right], \tag{3} \label{sol}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
a delay differential equation for the wait time distribution. This could be integrated numerically to get the full solution. However, our interest here is primarily the mean waiting time &amp;#8212; as we show next, it&amp;#8217;s easy to extract this part of the solution&amp;nbsp;analytically.&lt;/p&gt;
&lt;h3&gt;Probability of no wait and the mean wait&amp;nbsp;time&lt;/h3&gt;
&lt;p&gt;We can obtain a series of useful relations by multiplying (\ref{sol}) by powers of &lt;span class="math"&gt;\(t\)&lt;/span&gt; and integrating. The first such expression is obtained by multiplying by &lt;span class="math"&gt;\(t^1\)&lt;/span&gt;. Doing this and integrating its left side, we obtain&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{4} \label{int3}  
\int_{0^{+}}^{\infty} P^{\prime}(t) t dt = \left . P(t) t \right |_{0^{+}}^{\infty} - \int_{0^+}^{\infty} P(t) dt = 1 - P(0).  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Similarly integrating its right side, we&amp;nbsp;obtain&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{5} \label{int4}  
\lambda \int_{0^{+}}^{\infty} t \left [P(t) - P(t-1) \right] = \lambda [ \overline{t} - \overline{(t + 1)} ] = - \lambda.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Equating the last two lines, we obtain the probability of no wait,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{6} \label{int5}  
P(0) = 1 - \lambda.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This shows that when the arrival rate is low, the probability of no wait goes to one &amp;#8212; an intuitively reasonable result. On the other hand, as &lt;span class="math"&gt;\(\lambda \to 1\)&lt;/span&gt;, the probability of no wait approaches zero. In between, the idle time fraction of our staffer (which is equal to the probability of no wait, given a random arrival time) grows linearly, connecting these two&amp;nbsp;limits.&lt;/p&gt;
&lt;p&gt;To obtain an expression for the average wait time, we carry out a similar analysis to that above, but multiply (\ref{sol}) by &lt;span class="math"&gt;\(t^2\)&lt;/span&gt; instead. The integral on left is then&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{7} \label{int6}  
\int_{0^{+}}^{\infty} P^{\prime}(t) t^2 dt = \left . P(t) t^2 \right |_{0^{+}}^{\infty} - 2\int_{0^+}^{\infty} P(t) t dt = - 2 \overline{t}.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Similarly, the integral at right is&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{8} \label{fin_int}  
\lambda \int_{0^{+}}^{\infty} t^2 \left [P(t) - P(t-1) \right] = \lambda \overline{ t^2} - \overline{ (t + 1)^2} = - \lambda (2 \overline{t} +1).  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Equating the last two lines and rearranging gives our solution for the average wait,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{9} \label{fin}  
\overline{t} = \frac{\lambda}{2 (1 - \lambda)}.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
As advertised, this diverges as &lt;span class="math"&gt;\(\lambda \to 1\)&lt;/span&gt;, see illustration in the plot below. It&amp;#8217;s very interesting that even as &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; approaches this extreme limit, the line is still empty a finite fraction of the time &amp;#8212; see (\ref{int5}). Evidently a finite idle time fraction can&amp;#8217;t be avoided, even as one approaches the divergent &lt;span class="math"&gt;\(\lambda = 1\)&lt;/span&gt;&amp;nbsp;limit.&lt;/p&gt;
&lt;p&gt;&lt;img alt="average wait time" src="http/wp-content/uploads/2016/04/Screen-Shot-2016-04-23-at-5.02.38-PM.png"&gt;&lt;/p&gt;
&lt;h3&gt;Conclusions and&amp;nbsp;extensions&lt;/h3&gt;
&lt;p&gt;To carry this approach further, one could consider the case where the queue feeds &lt;span class="math"&gt;\(k\)&lt;/span&gt; staff, rather than just one. I&amp;#8217;ve made progress on this effort in certain cases, but have been stumped on the general problem. One interesting thing you can intuit about this &lt;span class="math"&gt;\(k\)&lt;/span&gt;-staff version is that one approaches the mean-field analysis as &lt;span class="math"&gt;\(k\to \infty\)&lt;/span&gt; (adding more staff tends to smooth things over, resulting in a diminishing of the importance of the randomness of the arrival times). This means that as &lt;span class="math"&gt;\(k\)&lt;/span&gt; grows, we&amp;#8217;ll have very little average wait time for any &lt;span class="math"&gt;\(\lambda&amp;lt;1\)&lt;/span&gt;, but again divergent wait times for any &lt;span class="math"&gt;\(\lambda \geq 1\)&lt;/span&gt; &amp;#8212; like an infinite step function. Another direction one could pursue is to allow the service times to follow a distribution. Both cases can also be worked out using the Markov approach &amp;#8212; references to such work can be found in the link provided in the&amp;nbsp;introduction.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>Improved Bonferroni correction factors for multiple pairwise comparisons</title><link href="http/bonferroni-correction-for-multiple-pairwise-comparison-tests.html" rel="alternate"></link><published>2016-04-10T07:58:00-07:00</published><updated>2016-04-10T07:58:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:None,2016-04-10:http/bonferroni-correction-for-multiple-pairwise-comparison-tests.html</id><summary type="html">&lt;p&gt;A common task in applied statistics is the pairwise comparison of the responses of &lt;span class="math"&gt;\(N\)&lt;/span&gt; treatment groups in some statistical test &amp;#8212; the goal being to decide which pairs exhibit differences that are statistically significant. Now, because there is one comparison being made for each pairing, a naive application of the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A common task in applied statistics is the pairwise comparison of the responses of &lt;span class="math"&gt;\(N\)&lt;/span&gt; treatment groups in some statistical test &amp;#8212; the goal being to decide which pairs exhibit differences that are statistically significant. Now, because there is one comparison being made for each pairing, a naive application of the Bonferroni correction analysis suggests that one should set the individual pairwise test sizes to &lt;span class="math"&gt;\(\alpha_i \to \alpha_f/{N \choose 2}\)&lt;/span&gt; in order to obtain a desired family-wise type 1 error rate of &lt;span class="math"&gt;\(\alpha_f\)&lt;/span&gt;. Indeed, this solution is suggested by many texts. However, implicit in the Bonferroni analysis is the assumption that the comparisons being made are each mutually independent. This is not the case here, and we show that as a consequence the naive approach often returns type 1 error rates far from those desired. We provide adjusted formulas that allow for error-free Bonferroni-like corrections to be&amp;nbsp;made.&lt;/p&gt;
&lt;p&gt;[edit (7/4/2016): After posting this article, I&amp;#8217;ve since found that the method we suggest here is related to / is a generalization of Tukey&amp;#8217;s range test &amp;#8212; see &lt;a href="https://en.wikipedia.org/wiki/Tukey%27s_range_test"&gt;here&lt;/a&gt;.]&lt;/p&gt;
&lt;p&gt;[edit (6/11/2018): I&amp;#8217;ve added the notebook used below to our Github, &lt;a href="https://github.com/EFavDB/improved_bonferroni"&gt;here&lt;/a&gt;]&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;In this post, we consider a particular kind of statistical test where one examines &lt;span class="math"&gt;\(N\)&lt;/span&gt; different treatment groups, measures some particular response within each, and then decides which of the &lt;span class="math"&gt;\({N \choose 2}\)&lt;/span&gt; pairs appear to exhibit responses that differ significantly. This is called the pairwise comparison problem (or sometimes &amp;#8220;posthoc analysis&amp;#8221;). It comes up in many contexts, and in general it will be of interest whenever one is carrying out a multiple-treatment&amp;nbsp;test.&lt;/p&gt;
&lt;p&gt;Our specific interest here is in identifying the appropriate individual measurement error bars needed to guarantee a given family-wise type 1 error rate, &lt;span class="math"&gt;\(\alpha_f\)&lt;/span&gt;. Briefly, &lt;span class="math"&gt;\(\alpha_f\)&lt;/span&gt; is the probability that we incorrectly make any assertion that two measurements differ significantly when the true effect sizes we&amp;#8217;re trying to measure are actually all the same. This can happen due to the nature of statistical fluctuations. For example, when measuring the heights of &lt;span class="math"&gt;\(N\)&lt;/span&gt; identical objects, measurement error can cause us to incorrectly think that some pair have slightly different heights, even though that&amp;#8217;s not the case. A classical approach to addressing this problem is given by the Bonferroni approximation: If we consider &lt;span class="math"&gt;\(\mathcal{N}\)&lt;/span&gt; independent comparisons, and each has an individual type 1 error rate of &lt;span class="math"&gt;\(\alpha_i,\)&lt;/span&gt; then the family-wise probability of not making any type 1 errors is simply the product of the probabilities that we don&amp;#8217;t make any individual type 1 errors,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{1} \label{bon1}  
p_f = (1 - \alpha_f) = p_i^{\mathcal{N}} \equiv \left ( 1 - \alpha_i \right)^{\mathcal{N}} \approx 1 - \mathcal{N} \alpha_i.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The last equality here is an expansion that holds when &lt;span class="math"&gt;\(p_f\)&lt;/span&gt; is close to &lt;span class="math"&gt;\(1\)&lt;/span&gt;, the limit we usually work in. Rearranging (\ref{bon1}) gives a simple expression,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{2} \label{bon2}  
\alpha_i = \frac{\alpha_f}{\mathcal{N}}.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This is the (naive) Bonferroni approximation &amp;#8212; it states that one should use individual tests of size &lt;span class="math"&gt;\(\alpha_f / \mathcal{N}\)&lt;/span&gt; in order to obtain a family-wise error rate of &lt;span class="math"&gt;\(\alpha_f\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The reason why we refer to (\ref{bon2}) as the naive Bonferroni approximation is that it doesn&amp;#8217;t actually apply to the problem we consider here. The reason why is that &lt;span class="math"&gt;\(p_f \not = p_i^{\mathcal{N}}\)&lt;/span&gt; in (\ref{bon1}) if the &lt;span class="math"&gt;\(\mathcal{N}\)&lt;/span&gt; comparisons considered are not independent: This is generally the case for our system of &lt;span class="math"&gt;\(\mathcal{N} = {N \choose 2}\)&lt;/span&gt; comparisons, since they are based on an underlying set of measurements having only &lt;span class="math"&gt;\(N\)&lt;/span&gt; degrees of freedom (the object heights, in our example). Despite this obvious issue, the naive approximation is often applied in this context. Here, we explore the nature of the error incurred in such applications, and we find that it is sometimes very significant. We also show that it&amp;#8217;s actually quite simple to apply the principle behind the Bonferroni approximation without error: One need only find a way to evaluate the true &lt;span class="math"&gt;\(p_f\)&lt;/span&gt; for any particular choice of error bars. Inverting this then allows one to identify the error bars needed to obtain the desired &lt;span class="math"&gt;\(p_f\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;General&amp;nbsp;treatment&lt;/h3&gt;
&lt;p&gt;In this section, we derive a formal expression for the type 1 error rate in the pairwise comparison problem. For simplicity, we will assume 1) that the uncertainty in each of our &lt;span class="math"&gt;\(N\)&lt;/span&gt; individual measurements is the same (e.g., the variance in the case of Normal variables), and 2) that our pairwise tests assert that two measurements differ statistically if and only if they are more than &lt;span class="math"&gt;\(k\)&lt;/span&gt; units&amp;nbsp;apart.&lt;/p&gt;
&lt;p&gt;To proceed, we consider the probability that a type 1 error does not occur, &lt;span class="math"&gt;\(p_f\)&lt;/span&gt;. This requires that all &lt;span class="math"&gt;\(N\)&lt;/span&gt; measurements sit within &lt;span class="math"&gt;\(k\)&lt;/span&gt; units of each other. For any set of values satisfying this condition, let the smallest of the set be &lt;span class="math"&gt;\(x\)&lt;/span&gt;. We have &lt;span class="math"&gt;\(N\)&lt;/span&gt; choices for which of the treatments sit as this position. The remaining &lt;span class="math"&gt;\((N-1)\)&lt;/span&gt; values must all be within the region &lt;span class="math"&gt;\((x, x+k)\)&lt;/span&gt;. Because we&amp;#8217;re considering the type 1 error rate, we can assume that each of the independent measurements takes on the same distribution &lt;span class="math"&gt;\(P(x)\)&lt;/span&gt;. These considerations imply&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{3} \label{gen}  
p_{f} \equiv 1 - \alpha_{f} = N \int_{-\infty}^{\infty} P(x) \left \{\int_x^{x+k} P(y) dy \right \}^{N-1} dx.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Equation (\ref{gen}) is our main result. It is nice for a couple of reasons. First, its form implies that when &lt;span class="math"&gt;\(N\)&lt;/span&gt; is large it will scale like &lt;span class="math"&gt;\(a \times p_{1,eff}^N\)&lt;/span&gt;, for some &lt;span class="math"&gt;\(k\)&lt;/span&gt;-dependent numbers &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(p_{1,eff}\)&lt;/span&gt;. This is reminiscent of the expression (\ref{bon1}), where &lt;span class="math"&gt;\(p_f\)&lt;/span&gt; took the form &lt;span class="math"&gt;\(p_i^{\mathcal{N}}\)&lt;/span&gt;. Here, we see that the correct value actually scales like some number to the &lt;span class="math"&gt;\(N\)&lt;/span&gt;-th power, not the &lt;span class="math"&gt;\(\mathcal{N}\)&lt;/span&gt;-th. This reflects the fact that we actually only have &lt;span class="math"&gt;\(N\)&lt;/span&gt; independent degrees of freedom here, not &lt;span class="math"&gt;\({N \choose 2}\)&lt;/span&gt;. Second, when the inner integral above can be carried out formally, (\ref{gen}) can be expressed as a single one-dimensional integral. In such cases, the integral can be evaluated numerically for any &lt;span class="math"&gt;\(k\)&lt;/span&gt;, allowing one to conveniently identify the &lt;span class="math"&gt;\(k\)&lt;/span&gt; that returns any specific, desired &lt;span class="math"&gt;\(p_f\)&lt;/span&gt;. We illustrate both points in the next two sections, where we consider Normal and Cauchy variables,&amp;nbsp;respectively.&lt;/p&gt;
&lt;h3&gt;Normally-distributed&amp;nbsp;responses&lt;/h3&gt;
&lt;p&gt;We now consider the case where the individual statistics are each Normally-distributed about zero, and we reject any pair if they are more than &lt;span class="math"&gt;\(k \times \sqrt{2} \sigma\)&lt;/span&gt; apart, with &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; the variance of the individual statistics. In this case, the inner integral of (\ref{gen}) goes to&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{4} \label{inner_g}  
\frac{1}{\sqrt{2 \pi \sigma^2}} \int_x^{x+k \sqrt{2} \sigma} \exp\left [ -\frac{y^2}{2 \sigma^2} \right] dy = \frac{1}{2} \left [\text{erf}(k + \frac{x}{\sqrt{2} \sigma}) - \text{erf}(\frac{x}{\sqrt{2} \sigma})\right].  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Plugging this into (\ref{gen}), we obtain&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{5} \label{exact_g}  
p_f = \int \frac{N e^{-x^2 / 2 \sigma^2}}{\sqrt{2 \pi \sigma^2}} \exp \left ((N-1) \log \frac{1}{2} \left [\text{erf}(k + \frac{x}{\sqrt{2} \sigma}) - \text{erf}(\frac{x}{\sqrt{2} \sigma})\right]\right)dx.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This exact expression (\ref{exact_g}) can be used to obtain the &lt;span class="math"&gt;\(k\)&lt;/span&gt; value needed to achieve any desired family-wise type 1error rate. Example solutions obtained in this way are compared to the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-values returned by the naive Bonferroni approach in the table below. The last column &lt;span class="math"&gt;\(p_{f,Bon}\)&lt;/span&gt; shown is the family-wise success rate that you get when you plug in &lt;span class="math"&gt;\(k_{Bon},\)&lt;/span&gt; the naive Bonferroni &lt;span class="math"&gt;\(k\)&lt;/span&gt; value targeting &lt;span class="math"&gt;\(p_{f,exact}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(N\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p_{f,exact}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(k_{exact}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(k_{Bon}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p_{f,&amp;nbsp;Bon}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(4\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.90\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(2.29\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(2.39\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.921\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(8\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.90\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(2.78\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(2.91\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.929\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(4\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.95\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(2.57\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(2.64\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.959\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(8\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.95\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(3.03\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(3.10\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.959\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Examining the table shown, you can see that the naive approach is consistently overestimating the &lt;span class="math"&gt;\(k\)&lt;/span&gt; values (error bars) needed to obtain the desired family-wise rates &amp;#8212; but not dramatically so. The reason for the near-accuracy is that two solutions basically scale the same way with &lt;span class="math"&gt;\(N\)&lt;/span&gt;. To see this, one can carry out an asymptotic analysis of (\ref{exact_g}). We skip the details and note only that at large &lt;span class="math"&gt;\(N\)&lt;/span&gt; we have&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{6} \label{asy_g}  
p_f \sim \text{erf} \left ( \frac{k}{2}\right)^N  
\sim \left (1 - \frac{e^{-k^2 / 4}}{k \sqrt{\pi}/2} \right)^N.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This is interesting because the individual pairwise tests have p-values given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{7} \label{asy_i}  
p_i = \int_{-k\sqrt{2}\sigma}^{k\sqrt{2}\sigma} \frac{e^{-x^2 / (4 \sigma^2)}}{\sqrt{4 \pi \sigma^2 }} = \text{erf}(k /\sqrt{2}) \sim 1 - \frac{e^{-k^2/2}}{k \sqrt{\pi/2}}.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
At large &lt;span class="math"&gt;\(k\)&lt;/span&gt;, this is dominated by the exponential. Comparing with (\ref{asy_g}), this implies&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{8} \label{fin_g}  
p_f \sim \left (1 - \alpha_i^{1/2} \right)^N \sim 1 - N \alpha_i^{1/2} \equiv 1 - \alpha_f.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Fixing &lt;span class="math"&gt;\(\alpha_f\)&lt;/span&gt;, this requires that &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt; scale like &lt;span class="math"&gt;\(N^{-2}\)&lt;/span&gt;, the same scaling with &lt;span class="math"&gt;\(N\)&lt;/span&gt; as the naive Bonferroni solution. Thus, in the case of Normal variables, the Bonferroni approximation provides an inexact, but reasonable approximation (nevertheless, we suggest going with the exact approach using (\ref{exact_g}), since it&amp;#8217;s just as easy!). We show in the next section that this is not the case for Cauchy&amp;nbsp;variables.&lt;/p&gt;
&lt;h3&gt;Cauchy-distributed&amp;nbsp;variables&lt;/h3&gt;
&lt;p&gt;We&amp;#8217;ll now consider the case of &lt;span class="math"&gt;\(N\)&lt;/span&gt; independent, identically-distributed Cauchy variables having half widths &lt;span class="math"&gt;\(a\)&lt;/span&gt;,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{9} \label{c_dist}  
P(x) = \frac{a}{\pi} \frac{1}{a^2 + x^2}.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
When we compare any two, we will reject the null if they are more than &lt;span class="math"&gt;\(ka\)&lt;/span&gt; apart. With this choice, the inner integral of (\ref{gen}) is now&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\tag{10} \label{inner_c}  
\frac{a}{\pi} \int_x^{x+ k a} \frac{1}{a^2 + y^2} dy = \frac{1}{\pi} \left [\tan^{-1}(k + x/a) - \tan^{-1}(x/a) \right].  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Plugging into into (\ref{gen}) now&amp;nbsp;gives&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{11} \label{exact_c}  
p_f = \int \frac{N a/\pi}{a^2 + x^2} \exp \left ((N-1) \log  
\frac{1}{\pi} \left [\tan^{-1}(k + x/a) - \tan^{-1}(x/a) \right]  
\right).  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This is the analog of (\ref{exact_g}) for Cauchy variables &amp;#8212; it can be used to find the exact &lt;span class="math"&gt;\(k\)&lt;/span&gt; value needed to obtain a given family-wise type 1 error rate. The table below compares the exact values to those returned by the naive Bonferroni analysis [obtained using the fact that the difference between two independent Cauchy variables of width &lt;span class="math"&gt;\(a\)&lt;/span&gt; is itself a Cauchy distributed variable, but with width &lt;span class="math"&gt;\(2a\)&lt;/span&gt;].&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(N\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p_{f,exact}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(k_{exact}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(k_{Bon}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p_{f,&amp;nbsp;Bon}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(4\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.90\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(27\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(76\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.965\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(8\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.90\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(55\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(350\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.985\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(4\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.95\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(53\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(153\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.983\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(8\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.95\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(107\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(700\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.993\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case, you can see that the naive Bonferroni approximation performs badly. For example, in the last line, it suggests using error bars that are seven times too large for each point estimate. The error gets even worse as &lt;span class="math"&gt;\(N\)&lt;/span&gt; grows: Again, skipping the details, we note that in this limit, (\ref{exact_c}) scales like&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{12} \label{asym_c}  
p_f \sim \left [\frac{2}{\pi} \tan^{-1}(k/2) \right]^N.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This can be related to the individual &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; values, which are given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{13} \label{asym2_c}  
p_i = \int_{-ka}^{ka} \frac{2 a / \pi}{4 a^2 + x^2}dx = \frac{2}{\pi}\tan^{-1}(k/2).  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Comparing the last two lines, we obtain&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{14} \label{asym3_c}  
p_f \equiv 1 - \alpha_f \sim p_i^N \sim 1 - N \alpha_i.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Although we&amp;#8217;ve been a bit sloppy with coefficients here, (\ref{asym3_c}) gives the correct leading &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dependence: &lt;span class="math"&gt;\(k_{exact} \sim 1/\alpha_i \propto N\)&lt;/span&gt;. We can see this linear scaling in the table above. This explains why &lt;span class="math"&gt;\(k_{exact}\)&lt;/span&gt; and &lt;span class="math"&gt;\(k_{Bon}\)&lt;/span&gt; &amp;#8212; which scales like &lt;span class="math"&gt;\({N \choose 2} \sim N^2\)&lt;/span&gt; &amp;#8212; differ more and more as &lt;span class="math"&gt;\(N\)&lt;/span&gt; grows. In this case, you should definitely never use the naive approximation, but instead stick to the exact analysis based on&amp;nbsp;(\ref{exact_c}).&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Some people criticize the Bonferroni correction factor as being too conservative. However, our analysis here suggests that this feeling may be due in part to its occasional improper application. The naive approximation simply does not apply in the case of pairwise comparisons because the &lt;span class="math"&gt;\({N \choose 2}\)&lt;/span&gt; pairs considered are not independent &amp;#8212; there are only &lt;span class="math"&gt;\(N\)&lt;/span&gt; independent degrees of freedom in this problem. Although the naive correction does not apply to the problem of pairwise comparisons, we&amp;#8217;ve shown here that it remains a simple matter to correctly apply the principle behind it: One can easily select any desired family-wise type 1 error rate through an appropriate selection of the individual test sizes &amp;#8212; just use&amp;nbsp;(\ref{gen})!&lt;/p&gt;
&lt;p&gt;We hope you enjoyed this post &amp;#8212; we anticipate writing a bit more on hypothesis testing in the near&amp;nbsp;future.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>Maximum-likelihood asymptotics</title><link href="http/maximum-likelihood-asymptotics.html" rel="alternate"></link><published>2015-12-30T00:01:00-08:00</published><updated>2015-12-30T00:01:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:None,2015-12-30:http/maximum-likelihood-asymptotics.html</id><summary type="html">&lt;p&gt;In this post, we review two facts about maximum-likelihood estimators: 1) They are consistent, meaning that they converge to the correct values given a large number of samples, &lt;span class="math"&gt;\(N\)&lt;/span&gt;, and 2) They satisfy the &lt;a href="http://efavdb.com/multivariate-cramer-rao-bound/"&gt;Cramer-Rao&lt;/a&gt; lower bound for unbiased parameter estimates in this same limit &amp;#8212; that is, they have the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post, we review two facts about maximum-likelihood estimators: 1) They are consistent, meaning that they converge to the correct values given a large number of samples, &lt;span class="math"&gt;\(N\)&lt;/span&gt;, and 2) They satisfy the &lt;a href="http://efavdb.com/multivariate-cramer-rao-bound/"&gt;Cramer-Rao&lt;/a&gt; lower bound for unbiased parameter estimates in this same limit &amp;#8212; that is, they have the lowest possible variance of any unbiased estimator, in the &lt;span class="math"&gt;\(N\gg 1\)&lt;/span&gt;&amp;nbsp;limit.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;We begin with a simple example maximum-likelihood inference problem: Suppose one has obtained &lt;span class="math"&gt;\(N\)&lt;/span&gt; independent samples &lt;span class="math"&gt;\(\{x_1, x_2, \ldots, x_N\}\)&lt;/span&gt; from a Gaussian distribution of unknown mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;. In order to obtain a maximum-likelihood estimate for these parameters, one asks which &lt;span class="math"&gt;\(\hat{\mu}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\hat{\sigma}^2\)&lt;/span&gt; would be most likely to generate the samples observed. To find these, we first write down the probability of observing the samples, given our model. This is simply&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
P(\{x_1, x_2, \ldots, x_N\} \vert \mu, \sigma^2) = \exp\left [ \sum_{i=1}^N \left (-\frac{1}{2} \log (2 \pi \sigma^2) -\frac{1}{2 \sigma^2} (x_i - \mu)^2\right ) \right ]. \tag{1} \label{1}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
To obtain the maximum-likelihood estimates, we maximize (\ref{1}): Setting its derivatives with respect to &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; to zero and solving gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\label{mean}  
\hat{\mu} &amp;amp;= \frac{1}{N} \sum_i x_i \tag{2} \\  
\hat{\sigma}^2 &amp;amp;= \frac{1}{N} \sum_i (x_i - \hat{\mu})^2. \tag{3} \label{varhat}  
\end{align}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
These are mean and variance values that would be most likely to generate our observation set &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;. Our solutions show that they are both functions of the random observation set. Because of this, &lt;span class="math"&gt;\(\hat{\mu}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\hat{\sigma}^2\)&lt;/span&gt; are themselves random variables, changing with each sample set that happens to be observed. Their distributions can be characterized by their mean values, variances,&amp;nbsp;etc.&lt;/p&gt;
&lt;p&gt;The average squared error of a parameter estimator is determined entirely by its bias and variance &amp;#8212; see eq (2) of &lt;a href="http://efavdb.com/bayesian-linear-regression/"&gt;prior post&lt;/a&gt;. Now, one can show that the &lt;span class="math"&gt;\(\hat{\mu}\)&lt;/span&gt; estimate of (\ref{mean}) is unbiased, but this is not the case for the variance estimator (\ref{varhat}) &amp;#8212; one should (famously) divide by &lt;span class="math"&gt;\(N-1\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(N\)&lt;/span&gt; here to obtain an unbiased estimator&lt;span class="math"&gt;\(^1\)&lt;/span&gt;. This shows that maximum-likelihood estimators need not be unbiased. Why then are they so popular? One reason is that these estimators are guaranteed to be unbiased when &lt;span class="math"&gt;\(N\)&lt;/span&gt;, the sample size, is large. Further, in this same limit, these estimators achieve the minimum possible variance for any unbiased parameter estimate &amp;#8212; as set by the fundamental &lt;a href="http://efavdb.com/multivariate-cramer-rao-bound/"&gt;Cramer-Rao&lt;/a&gt; bound. The purpose of this post is to review simple proofs of these latter two facts about maximum-likelihood estimators&lt;span class="math"&gt;\(^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Consistency&lt;/h3&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(P(x \vert \theta^*)\)&lt;/span&gt; be some distribution characterized by a parameter &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; that is unknown. We will show that the maximum-likelihood estimator converges to &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; when &lt;span class="math"&gt;\(N\)&lt;/span&gt; is large: As in (\ref{1}), the maximum-likelihood solution is that &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that maximizes&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{4} \label{4}  
J \equiv \frac{1}{N}\sum_{i=1}^N \log P(x_i \vert \theta),  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where the &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt; are the independent samples taken from &lt;span class="math"&gt;\(P(x \vert \theta^*)\)&lt;/span&gt;. By the law of large numbers, when &lt;span class="math"&gt;\(N\)&lt;/span&gt; is large, this average over the samples converges to its population mean. In other words,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{5}  
\lim_{N \to \infty}J \rightarrow \int_x P(x \vert \theta^*) \log P(x \vert \theta) dx.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
We will show that &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; is the &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; value that maximizes the above. We can do this directly, writing&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\begin{align}  
J(\theta) - J(\theta^*) &amp;amp; = \int_x P(x \vert \theta^*) \log \left ( \frac{P(x \vert \theta) }{P(x \vert \theta^*)}\right) \\  
&amp;amp; \leq \int_x P(x \vert \theta^*) \left ( \frac{P(x \vert \theta) }{P(x \vert \theta^*)} - 1 \right) \\  
&amp;amp; = \int_x P(x \vert \theta) - P(x \vert \theta^*) = 1 - 1 = 0. \tag{6} \label{6}  
\end{align}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, we have used &lt;span class="math"&gt;\(\log t \leq t-1\)&lt;/span&gt; in the second line. Rearranging the above shows that &lt;span class="math"&gt;\(J(\theta^*) \geq J(\theta)\)&lt;/span&gt; for all &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; &amp;#8212; when &lt;span class="math"&gt;\(N \gg 1\)&lt;/span&gt;, meaning that &lt;span class="math"&gt;\(J\)&lt;/span&gt; is maximized at &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt;. That is, the maximum-likelihood estimator &lt;span class="math"&gt;\(\hat{\theta} \to \theta^*\)&lt;/span&gt; in this limit&lt;span class="math"&gt;\(^3\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Optimal&amp;nbsp;variance&lt;/h3&gt;
&lt;p&gt;To derive the variance of a general maximum-likelihood estimator, we will see how its average value changes upon introduction of a small Bayesian prior, &lt;span class="math"&gt;\(P(\theta) \sim \exp(\Lambda \theta)\)&lt;/span&gt;. The trick will be to evaluate the change in two separate ways &amp;#8212; this takes a few lines, but is quite straightforward. In the first approach, we do a direct maximization: The quantity to be maximized is now&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \label{7}  
J = \sum_{i=1}^N \log P(x_i \vert \theta) + \Lambda \theta. \tag{7}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Because we take &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; small, we can use a Taylor expansion to find the new solution, writing&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \label{8}  
\hat{\theta} = \theta^* + \theta_1 \Lambda + O(\Lambda^2). \tag{8}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Setting the derivative of (\ref{7}) to zero, with &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; given by its value in (\ref{8}), we obtain&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\sum_{i=1}^N \partial_{\theta} \left . \log P(x_i \vert \theta) \right \vert_{\theta^*} + \sum_{i=1}^N \partial_{\theta}^2 \left . \log P(x_i \vert \theta) \right \vert_{\theta^*} \times \theta_1 \Lambda + \Lambda + O(\Lambda^2) = 0. \tag{9} \label{9}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The first term here goes to zero at large &lt;span class="math"&gt;\(N\)&lt;/span&gt;, as above. Setting the terms at &lt;span class="math"&gt;\(O(\Lambda^1)\)&lt;/span&gt; to zero gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\theta_1 = - \frac{1}{ \sum_{i=1}^N \partial_{\theta}^2 \left . \log P(x_i \vert \theta) \right \vert_{\theta^*} }. \tag{10} \label{10}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Plugging this back into (\ref{8}) gives the first order correction to &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; due to the perturbation. Next, as an alternative approach, we evaluate the change in &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; by maximizing the &lt;span class="math"&gt;\(P(\theta)\)&lt;/span&gt; distribution, expanding about its unperturbed global maximum, &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt;: We write, formally,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{11} \label{11}  
P(\theta) = \exp\left [ - a_0 - a_2 (\theta - \theta^*)^2 - a_3 (\theta - \theta^*)^3 + \ldots + \Lambda \theta \right].  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Differentiating to maximize (\ref{11}), and again assuming a solution of form (\ref{8}), we obtain&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\label{12} \tag{12}  
-2 a_2 \times \theta_1 \Lambda + \Lambda + O(\Lambda^2) = 0 \ \ \to \ \ \theta_1 = \frac{1}{2 a_2}.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
We now require consistency between our two approaches, equating (\ref{10}) and (\ref{12}). This gives an expression for &lt;span class="math"&gt;\(a_2\)&lt;/span&gt;. Plugging this back into (\ref{11}) then gives (for the unperturbed distribution)&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{13} \label{13}  
P(\theta) = \mathcal{N} \exp \left [ N \frac{ \langle \partial_{\theta}^2 \left . \log P(x, \theta) \right \vert_{\theta^*} \rangle }{2} (\theta - \theta^*)^2 + \ldots \right].  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Using this Gaussian approximation&lt;span class="math"&gt;\(^4\)&lt;/span&gt;, we can now read off the large &lt;span class="math"&gt;\(N\)&lt;/span&gt; variance of &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; as&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{14} \label{14}  
var(\hat{\theta}) = - \frac{1}{N} \times \frac{1}{\langle \partial_{\theta}^2 \left . \log P(x, \theta) \right \vert_{\theta^*} \rangle }.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This is the lowest possible value for any unbiased estimator, as set by the Cramer-Rao bound. The proof shows that maximum-likelihood estimators always saturate this bound, in the large &lt;span class="math"&gt;\(N\)&lt;/span&gt; limit &amp;#8212; a remarkable result. We discuss the intuitive meaning of the Cramer-Rao bound in a &lt;a href="http://efavdb.com/multivariate-cramer-rao-bound/"&gt;prior post&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Footnotes&lt;/h3&gt;
&lt;p&gt;[1] To see that (\ref{varhat}) is biased, we just need to evaluate the average of &lt;span class="math"&gt;\(\sum_i (x_i - \hat{\mu})^2\)&lt;/span&gt;. This&amp;nbsp;is&lt;/p&gt;
&lt;div class="math"&gt;$$  
\overline{\sum_i x_i^2 - 2 \sum_{i,j} \frac{x_i x_j}{N} + \sum_{i,j,k} \frac{x_j x_k}{N^2}} = N \overline{x^2} - (N-1) \overline{x}^2 - \overline{x^2} \\  
= (N-1) \left ( \overline{x^2} - \overline{x}^2 \right) \equiv (N-1) \sigma^2.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Dividing through by &lt;span class="math"&gt;\(N\)&lt;/span&gt;, we see that &lt;span class="math"&gt;\(\overline{\hat{\sigma}^2} = \left(\frac{N-1}{N}\right)\sigma^2\)&lt;/span&gt;. The deviation from the true variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; goes to zero at large &lt;span class="math"&gt;\(N\)&lt;/span&gt;, but is non-zero for any finite &lt;span class="math"&gt;\(N\)&lt;/span&gt;: The estimator is biased, but the bias goes to zero at large &lt;span class="math"&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;[2] The consistency proof is taken from lecture notes by D. Panchenko, see &lt;a href="http://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/lecture3.pdf"&gt;here&lt;/a&gt;. Professor Panchenko is quite famous for having proven the correctness of the Parisi ansatz in replica theory. Our variance proof is original &amp;#8212; please let us know if you have seen it elsewhere. Note that it can also be easily extended to derive the covariance matrix of a set of maximum-likelihood estimators that are jointly distributed &amp;#8212; we cover only the scalar case here, for&amp;nbsp;simplicity.&lt;/p&gt;
&lt;p&gt;[3] The proof here actually only shows that there is no &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that gives larger likelihood than &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; in the large &lt;span class="math"&gt;\(N\)&lt;/span&gt; limit. However, for some problems, it is possible that more than one &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; maximizes the likelihood. A trivial example is given by the case where the distribution is actually only a function of &lt;span class="math"&gt;\((\theta - \theta_0)^2\)&lt;/span&gt;. In this case, both values &lt;span class="math"&gt;\(\theta_0 \pm (\theta^* - \theta_0)\)&lt;/span&gt; will necessarily maximize the&amp;nbsp;likelihood.&lt;/p&gt;
&lt;p&gt;[4] It&amp;#8217;s a simple matter to carry this analysis further, including the cubic and higher order terms in the expansion (\ref{11}). These lead to correction terms for (\ref{14}), smaller in magnitude than that given there. These terms become important when &lt;span class="math"&gt;\(N\)&lt;/span&gt; decreases in&amp;nbsp;magnitude.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>A review of parameter regularization and Bayesian regression</title><link href="http/bayesian-linear-regression.html" rel="alternate"></link><published>2015-10-11T00:01:00-07:00</published><updated>2015-10-11T00:01:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:None,2015-10-11:http/bayesian-linear-regression.html</id><summary type="html">&lt;p&gt;Here, we review parameter regularization, which is a method for improving regression models through the penalization of non-zero parameter estimates. Why is this effective? Biasing parameters towards zero will (of course!) unfavorably bias a model, but it will also reduce its variance. At times the latter effect can win out …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here, we review parameter regularization, which is a method for improving regression models through the penalization of non-zero parameter estimates. Why is this effective? Biasing parameters towards zero will (of course!) unfavorably bias a model, but it will also reduce its variance. At times the latter effect can win out, resulting in a net reduction in generalization error. We also review Bayesian regressions &amp;#8212; in effect, these generalize the regularization approach, biasing model parameters to any specified prior estimates, not necessarily&amp;nbsp;zero.&lt;/p&gt;
&lt;p&gt;This is the second of a series of posts expounding on topics discussed in the text, &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;&amp;#8220;An Introduction to Statistical Learning&amp;#8221;&lt;/a&gt;. Here, we cover material from its Chapters 2 and 6. See prior post &lt;a href="http://efavdb.com/leave-one-out-cross-validation/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Introduction and&amp;nbsp;overview&lt;/h3&gt;
&lt;p&gt;In this post, we will be concerned with the problem of fitting a function of the form&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\label{function}  
y(\vec{x}_i) = f(\vec{x}_i) + \epsilon_i \tag{1},  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where &lt;span class="math"&gt;\(f\)&lt;/span&gt; is the function&amp;#8217;s systematic part and &lt;span class="math"&gt;\(\epsilon_i\)&lt;/span&gt; is a random error. These errors have mean zero and are iid &amp;#8212; their presence is meant to take into account dependences in &lt;span class="math"&gt;\(y\)&lt;/span&gt; on features that we don&amp;#8217;t have access to. To &amp;#8220;fit&amp;#8221; such a function, we will suppose that one has chosen some appropriate regression algorithm (perhaps a linear model, a random forest, etc.) that can be used to generate an approximation &lt;span class="math"&gt;\(\hat{f}\)&lt;/span&gt; to &lt;span class="math"&gt;\(y\)&lt;/span&gt;, given a training set of example &lt;span class="math"&gt;\((\vec{x}_i, y_i)\)&lt;/span&gt;&amp;nbsp;pairs.&lt;/p&gt;
&lt;p&gt;The primary concern when carrying out a regression is often to find a fit that will be accurate when applied to points not included in the training set. There are two sources of error that one has to grapple with: Bias in the algorithm &amp;#8212; sometimes the result of using an algorithm that has insufficient flexibility to capture the nature of the function being fit, and variance &amp;#8212; this relates to how sensitive the resulting fit is to the samples chosen for the training set. The latter issue is closely related to the concept of &lt;a href="https://en.wikipedia.org/wiki/Overfitting"&gt;overfitting&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To mitigate overfitting, &lt;a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)"&gt;parameter regularization&lt;/a&gt; is often applied. As we detail below, this entails penalizing non-zero parameter estimates. Although this can favorably reduce the variance of the resulting model, it will also introduce bias. The optimal amount of regularization is therefore determined by appropriately balancing these two&amp;nbsp;effects.&lt;/p&gt;
&lt;p&gt;In the following, we carefully review the mathematical definitions of model bias and variance, as well as how these effects contribute to the error of an algorithm. We then show that regularization is equivalent to assuming a particular form of Bayesian prior that causes the parameters to be somewhat &amp;#8220;sticky&amp;#8221; around zero &amp;#8212; this stickiness is what results in model variance reduction. Because standard regularization techniques bias towards zero, they work best when the underlying true feature dependences are sparse. When this is not true, one should attempt an analogous variance reduction through application of the more general Bayesian regression&amp;nbsp;framework.&lt;/p&gt;
&lt;h3&gt;Squared error&amp;nbsp;decomposition&lt;/h3&gt;
&lt;p&gt;The first step to understanding regression error is the following identity: Given any fixed &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt;, we have&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\begin{align}  
\overline{\left (\hat{f}(\vec{x}) - y(\vec{x}) \right)^2} &amp;amp;= \overline{\left (\hat{f}(\vec{x}) - \overline{\hat{f}(\vec{x})} \right)^2} + \left (\overline{\hat{f}(\vec{x})} - f(\vec{x}) \right)^2 + \overline{ \epsilon^2} \\  
&amp;amp; \equiv var\left(\hat{f}(\vec{x})\right) + bias\left(\hat{f}(\vec{x})\right)^2 + \overline{\epsilon^2}. \tag{2}\label{error_decomp}  
\end{align}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, overlines represent averages over two things: The first is the random error &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; values, and the second is the training set used to construct &lt;span class="math"&gt;\(\hat{f}\)&lt;/span&gt;. The left side of (\ref{error_decomp}) gives the average squared error of our algorithm, at point &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; &amp;#8212; i.e., the average squared error we can expect to get, given a typical training set and &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; value. The right side of the equation decomposes this error into separate, independent components. The first term at right &amp;#8212; the variance of &lt;span class="math"&gt;\(\hat{f}(\vec{x})\)&lt;/span&gt; &amp;#8212; relates to how widely the estimate at &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; changes as one randomly samples from the space of possible training sets. Similarly, the second term &amp;#8212; the algorithm&amp;#8217;s squared bias &amp;#8212; relates to the systematic error of the algorithm at &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt;. The third and final term above gives the average squared random error &amp;#8212; this provides a fundamental lower bound on the accuracy of any estimator of &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We turn now to the proof of (\ref{error_decomp}). We write the left side of this equation as&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\label{detail}  
\begin{align} \tag{3}  
\overline{\left (\hat{f}(\vec{x}) - y(\vec{x}) \right)^2} &amp;amp;= \overline{\left ( \left \{\hat{f}(\vec{x}) - f(\vec{x}) \right \} - \left \{ y(\vec{x}) - f(\vec{x}) \right \} \right)^2}\\  
&amp;amp;=  
\overline{\left ( \hat{f}(\vec{x}) - f(\vec{x}) \right)^2}  
- 2 \overline{ \left (\hat{f}(\vec{x}) - f(\vec{x}) \right ) \left (y(\vec{x}) - f(\vec{x}) \right ) }  
+ \overline{ \left (y(\vec{x}) - f(\vec{x}) \right)^2}.  
\end{align}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The middle term here is zero. To see this, note that it is the average of the product of two independent quantities: The first factor, &lt;span class="math"&gt;\(\hat{f}(\vec{x}) - f(\vec{x})\)&lt;/span&gt;, varies only with the training set, while the second factor, &lt;span class="math"&gt;\(y(\vec{x}) - f(\vec{x})\)&lt;/span&gt;, varies only with &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;. Because these two factors are independent, their average product is the product of their individual averages, the second of which is zero, by definition. Now, the third term in (\ref{detail}) is simply &lt;span class="math"&gt;\(\overline{\epsilon^2}\)&lt;/span&gt;. To complete the proof, we need only evaluate the first term above. To do that, we write&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align} \tag{4} \label{detail2}  
\overline{\left ( \hat{f}(\vec{x}) - f(\vec{x}) \right)^2} &amp;amp;=  
\overline{\left ( \left \{ \hat{f}(\vec{x}) - \overline{\hat{f}(\vec{x})} \right \}- \left \{f(\vec{x}) -\overline{\hat{f}(\vec{x})} \right \}\right)^2} \\  
&amp;amp;=  
\overline{\left ( \hat{f}(\vec{x}) - \overline{\hat{f}(\vec{x})} \right)^2}  
-2  
\overline{ \left \{ \hat{f}(\vec{x}) - \overline{\hat{f}(\vec{x})} \right \} \left \{f(\vec{x}) -\overline{\hat{f}(\vec{x})} \right \} }  
+  
\left ( f(\vec{x}) -\overline{\hat{f}(\vec{x})} \right)^2.  
\end{align}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The middle term here is again zero. This is because its second factor is a constant, while the first averages to zero, by definition. The first and third terms above are the algorithm&amp;#8217;s variance and squared bias, respectively. Combining these observations with (\ref{detail}), we obtain&amp;nbsp;(\ref{error_decomp}).&lt;/p&gt;
&lt;h3&gt;Bayesian&amp;nbsp;regression&lt;/h3&gt;
&lt;p&gt;In order to introduce Bayesian regression, we focus on the special case of least-squares regressions. In this context, one posits that the samples generated take the form (\ref{function}), with the error &lt;span class="math"&gt;\(\epsilon_i\)&lt;/span&gt; terms now iid, Gaussian distributed with mean zero and standard deviation &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;. Under this assumption, the probability of observing values &lt;span class="math"&gt;\((y_1, y_2,\ldots, y_N)\)&lt;/span&gt; at &lt;span class="math"&gt;\((\vec{x}_1, \vec{x}_2,\ldots,\vec{x}_N)\)&lt;/span&gt; is given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\begin{align}  
\tag{5} \label{5}  
P(\vec{y} \vert f) &amp;amp;= \prod_{i=1}^N \frac{1}{(2 \pi \sigma)^{1/2}} \exp \left [-\frac{1}{2 \sigma^2} (y_i - f(\vec{x}_i))^2 \right]\\  
&amp;amp;= \frac{1}{(2 \pi \sigma)^{N/2}} \exp \left [-\frac{1}{2 \sigma^2} (\vec{y} - \vec{f})^2 \right],  
\end{align}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where &lt;span class="math"&gt;\(\vec{y} \equiv (y_1, y_2,\ldots, y_N)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{f} \equiv (f_1, f_2,\ldots, f_N)\)&lt;/span&gt;. In order to carry out a maximum-likelihood analysis, one posits a parameterization for &lt;span class="math"&gt;\(f(\vec{x})\)&lt;/span&gt;. For example, one could posit the linear form,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{6}  
f(\vec{x}) = \vec{\theta} \cdot \vec{x}.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Once a parameterization is selected, its optimal &lt;span class="math"&gt;\(\vec{\theta}\)&lt;/span&gt; values are selected by maximizing (\ref{5}), which gives the least-squares&amp;nbsp;fit.&lt;/p&gt;
&lt;p&gt;One sometimes would like to nudge (or bias) the parameters away from those that maximize (\ref{5}), towards some values considered reasonable ahead of time. A simple way to do this is to introduce a Bayesian prior for the parameters &lt;span class="math"&gt;\(\vec{\theta}\)&lt;/span&gt;. For example, one might posit a prior of the form&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{7} \label{7}  
P(f) \equiv P(\vec{\theta}) \propto \exp \left [- \frac{1}{2\sigma^2} (\vec{\theta} - \vec{\theta}_0)  
\Lambda (\vec{\theta} - \vec{\theta}_0)\right].  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, &lt;span class="math"&gt;\(\vec{\theta}_0\)&lt;/span&gt; represents a best guess for what &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; should be before any data is taken, and the matrix &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; determines how strongly we wish to bias &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; to this value: If the components of &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; are large (small), then we strongly (weakly) constrain &lt;span class="math"&gt;\(\vec{\theta}\)&lt;/span&gt; to sit near &lt;span class="math"&gt;\(\vec{\theta}_0\)&lt;/span&gt;. To carry out the regression, we combine (\ref{5}-\ref{7}) with Bayes&amp;#8217; rule, giving&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\tag{8}  
P(\vec{\theta} \vert \vec{y}) = \frac{P(\vec{y}\vert \vec{\theta}) P(\vec{\theta})}{P(\vec{y})}  
\propto \exp \left [-\frac{1}{2 \sigma^2} (\vec{y} - \vec{\theta} \cdot \vec{x})^2 - \frac{1}{2\sigma^2} (\vec{\theta} - \vec{\theta}_0)  
\Lambda (\vec{\theta} - \vec{\theta}_0)\right].  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The most likely &lt;span class="math"&gt;\(\vec{\theta}\)&lt;/span&gt; now minimizes the quadratic &amp;#8220;cost function&amp;#8221;,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{9} \label{9}  
F(\theta) \equiv (\vec{y} - \vec{\theta} \cdot \vec{x})^2 +(\vec{\theta} - \vec{\theta}_0)  
\Lambda (\vec{\theta} - \vec{\theta}_0),  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
a Bayesian generalization of the usual squared error. With this, our heavy-lifting is at an end. We now move to a quick review of regularization, which will appear as a simple application of the Bayesian&amp;nbsp;method.&lt;/p&gt;
&lt;h3&gt;Parameter regularization as special&amp;nbsp;cases&lt;/h3&gt;
&lt;p&gt;The most common forms of regularization are the so-called &amp;#8220;ridge&amp;#8221; and &amp;#8220;lasso&amp;#8221;. In the context of least-squares fits, the former involves minimization of the quadratic form&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\tag{10} \label{ridge}  
F_{ridge}(\theta) \equiv (\vec{y} - \hat{f}(\vec{x}; \vec{\theta}))^2 + \Lambda \sum_i \theta_i^2,  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
while in the latter, one minimizes&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\tag{11} \label{lasso}  
F_{lasso}(\theta) \equiv (\vec{y} - \hat{f}(\vec{x}; \vec{\theta}))^2 + \Lambda \sum_i \vert\theta_i \vert.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The terms proportional to &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; above are the so-called regularization terms. In elementary courses, these are generally introduced to least-squares fits in an ad-hoc manner: Conceptually, it is suggested that these terms serve to penalize the inclusion of too many parameters in the model, with individual parameters now taking on large values only if they are really essential to the&amp;nbsp;fit.&lt;/p&gt;
&lt;p&gt;While the conceptual argument above may be correct, the framework we&amp;#8217;ve reviewed here allows for a more sophisticated understanding of regularization: (\ref{ridge}) is a special case of (\ref{9}), with &lt;span class="math"&gt;\(\vec{\theta}_0\)&lt;/span&gt; set to &lt;span class="math"&gt;\((0,0,\ldots, 0)\)&lt;/span&gt;. Further, the lasso form (\ref{lasso}) is also a special-case form of Bayesian regression, with the prior set to &lt;span class="math"&gt;\(P(\vec{\theta}) \propto \exp \left (- \frac{\Lambda}{2 \sigma^2} \sum_i \vert \theta_i \vert \right)\)&lt;/span&gt;. As advertised, regularization is a form of Bayesian&amp;nbsp;regression.&lt;/p&gt;
&lt;p&gt;Why then does regularization &amp;#8220;work&amp;#8221;? For the same reason any other Bayesian approach does: Introduction of a prior will bias a model (if chosen well, hopefully not by much), but will also effect a reduction in its variance. The appropriate amount of regularization balances these two effects. Sometimes &amp;#8212; but not always &amp;#8212; a non-zero amount of bias is&amp;nbsp;required.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;In summary, our main points here were three-fold: (i) We carefully reviewed the mathematical definitions of model bias and variance, deriving (\ref{error_decomp}). (ii) We reviewed how one can inject Bayesian priors to regressions: The key is to use the random error terms to write down the probability of seeing a particular observational data point. (iii) We reviewed the fact that the ridge and lasso &amp;#8212; (\ref{ridge}) and (\ref{lasso}) &amp;#8212; can be considered Bayesian&amp;nbsp;priors.&lt;/p&gt;
&lt;p&gt;Intuitively, one might think introduction of a prior serves to reduce the bias in a model: Outside information is injected into a model, nudging its parameters towards values considered reasonable ahead of time. In fact, this nudging introduces bias! Bayesian methods work through reduction in variance, not bias &amp;#8212; A good prior is one that does not introduce too much&amp;nbsp;bias.&lt;/p&gt;
&lt;p&gt;When, then, should one use regularization? Only when one expects the optimal model to be largely sparse. This is often the case when working on machine learning algorithms, as one has the freedom there to throw a great many feature variables into a model, expecting only a small (a prior, unknown) minority of them to really prove informative. However, when not working in high-dimensional feature spaces, sparseness should not be expected. In this scenario, one should reason some other form of prior, and attempt a variance reduction through the more general Bayesian&amp;nbsp;framework.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>How not to sort by average rating, revisited</title><link href="http/ranking-revisited.html" rel="alternate"></link><published>2015-07-11T20:30:00-07:00</published><updated>2015-07-11T20:30:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:None,2015-07-11:http/ranking-revisited.html</id><summary type="html">&lt;p&gt;What is the best method for ranking items that have positive and negative reviews? Some sites, including reddit, have adopted an algorithm suggested by &lt;a href="http://www.evanmiller.org/"&gt;Evan Miller&lt;/a&gt; to generate their item rankings. However, this algorithm can sometimes be unfairly pessimistic about new, good items. This is especially true of items whose …&lt;/p&gt;</summary><content type="html">&lt;p&gt;What is the best method for ranking items that have positive and negative reviews? Some sites, including reddit, have adopted an algorithm suggested by &lt;a href="http://www.evanmiller.org/"&gt;Evan Miller&lt;/a&gt; to generate their item rankings. However, this algorithm can sometimes be unfairly pessimistic about new, good items. This is especially true of items whose first few votes are negative &amp;#8212; an issue that can be &amp;#8220;gamed&amp;#8221; by adversaries. In this post, we consider three alternative ranking methods that can enable high-quality items to more-easily bubble-up. The last is the simplest, but continues to give good results: One simply seeds each item&amp;#8217;s vote count with a suitable fixed number of hidden &amp;#8220;starter&amp;#8221;&amp;nbsp;votes.  &lt;/p&gt;
&lt;h3&gt;Introduction &amp;#8212; a review of Evan Miller&amp;#8217;s&amp;nbsp;post&lt;/h3&gt;
&lt;p&gt;In an &lt;a href="http://www.evanmiller.org/how-not-to-sort-by-average-rating.html"&gt;insightful prior post&lt;/a&gt;, Evan Miller (&lt;span class="caps"&gt;EM&lt;/span&gt;) considered the problem of ranking items that had been reviewed as positive or negative (up-voted or down-voted, represented by a 1 or a 0, respectively) by a sample of users. He began by illustrating that two of the more readily-arrived at solutions to this problem are highly flawed. To&amp;nbsp;review:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bad method 1:&lt;/strong&gt; Rank item &lt;span class="math"&gt;\(i\)&lt;/span&gt; by &lt;span class="math"&gt;\(n_i(1) - n_i(0)\)&lt;/span&gt;, its up-vote count minus its down-vote&amp;nbsp;count.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Issue:&lt;/em&gt; If one item has garnered 60 up-votes and 40 down-votes, it will get the same score as an item with only 20 votes, all positive. Yet, the latter has a 100% up-vote rate (20 for 20), suggesting that it is of very high quality. Despite this, the algorithm ranks the two&amp;nbsp;equally.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bad method 2:&lt;/strong&gt; Rank item &lt;span class="math"&gt;\(i\)&lt;/span&gt; by &lt;span class="math"&gt;\(\hat{p} \equiv n_i(1)/[n_i(0) + n_i(1)]\)&lt;/span&gt;, its sample up-vote rate (average&amp;nbsp;rating).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Issue:&lt;/em&gt; If any one item has only one vote, an up-vote, it will be given a perfect score by this algorithm. This means that it will be ranked above all other items, despite the fact that a single vote is not particularly informative/convincing. In general, this method can work well, but only once each item has a significant number of&amp;nbsp;votes.&lt;/p&gt;
&lt;p&gt;To avoid the issues of these two bad methods (BMs), &lt;span class="caps"&gt;EM&lt;/span&gt; suggests scoring and ranking each item by the &lt;em&gt;lower limit of its up-vote-rate confidence interval&lt;/em&gt;. This is (&lt;a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval"&gt;&lt;span class="caps"&gt;E.B.&lt;/span&gt; Wilson, 1927&lt;/a&gt;),&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{1} \label{emsol}  
p_{W} = \frac{\hat{p} + \frac{z_{\alpha/2}^2}{2n} - z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p}) + \frac{z_{\alpha/2}^2}{4n} }{n}}}{1 + \frac{z_{\alpha/2}^2}{n}},  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where &lt;span class="math"&gt;\(\hat{p}\)&lt;/span&gt; is again the sample up-vote rate, &lt;span class="math"&gt;\(z_{\alpha/2}\)&lt;/span&gt; is a positive constant that sets the size of the confidence interval used, and &lt;span class="math"&gt;\(n\)&lt;/span&gt; is the total number of votes that have so far been recorded. The score &lt;span class="math"&gt;\(p_{W}\)&lt;/span&gt; approaches &lt;span class="math"&gt;\(\hat{p}\)&lt;/span&gt; once an item has a significant number of votes &amp;#8212; it consequently avoids the pitfall of &lt;span class="caps"&gt;BM1&lt;/span&gt; above. By construction, it also avoids the pitfall of &lt;span class="caps"&gt;BM2&lt;/span&gt;. With both of these pitfalls avoided, the &lt;span class="caps"&gt;EM&lt;/span&gt; method can sometimes provide a reasonable, practical ranking&amp;nbsp;system.&lt;/p&gt;
&lt;h3&gt;Potential issue with&amp;nbsp;(\ref{emsol})&lt;/h3&gt;
&lt;p&gt;Although (\ref{emsol}) does a good job of avoiding the pitfall associated with &lt;span class="caps"&gt;BM2&lt;/span&gt;, it can do a poor job of handling a related pitfall: If any new item has only a few votes, and these each happen to be down-votes, its sample up-vote rate will be &lt;span class="math"&gt;\(\hat{p} = 0\)&lt;/span&gt;. In this case, (\ref{emsol}) gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\label{problem} \tag{2}  
p_{W} = \left .\frac{\hat{p} + \frac{z_{\alpha/2}^2}{2n} - z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p}) + \frac{z_{\alpha/2}^2}{4n} }{n}}}{1 + \frac{z_{\alpha/2}^2}{n}}\right \vert_{\hat{p} = 0} = 0.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Now, &lt;span class="math"&gt;\(p_W\)&lt;/span&gt; is always between &lt;span class="math"&gt;\(0\)&lt;/span&gt; and &lt;span class="math"&gt;\(1\)&lt;/span&gt;, so (\ref{problem}) implies that any new, quickly-down-voted item will immediately be ranked below all others. This is extremely harsh and potentially unfair. For example, consider the case of a newly-opened restaurant: If an adversary were to quickly down-vote this restaurant on some ranking site &amp;#8212; the day of its opening &amp;#8212; the new restaurant would be ranked below all others, including the adversary. This would occur even if the new restaurant were of very high true quality. This could have potentially-damaging consequences, for both the restaurant and the ranking site &amp;#8212; whose lists should provide only the best&amp;nbsp;recommendations!&lt;/p&gt;
&lt;p&gt;An ideal ranking system should explicitly take into account the large uncertainty present when only a small number of votes have been recorded. The score (\ref{emsol}) does a good job of this on the high &lt;span class="math"&gt;\(\hat{p}\)&lt;/span&gt; end, but a poor job on the low &lt;span class="math"&gt;\(\hat{p}\)&lt;/span&gt; end. This approach may be appropriate for cases where one is risk-averse on the high end only, but in general one should protect against both sorts of quick, strong judgements. Below we consider some alternative, &lt;a href="https://en.wikipedia.org/wiki/Bayesian_statistics"&gt;Bayesian&lt;/a&gt; ranking solutions. The last is easy to understand and implement: One simply gives each item a hidden number of up- and down-votes to start with. These hidden &amp;#8220;starter&amp;#8221; votes can be chosen in various ways &amp;#8212; they serve to simply bias new items towards an intermediate value early on, with the bias becoming less important as more votes come in. This approach avoids each of the pitfalls we have&amp;nbsp;discussed.&lt;/p&gt;
&lt;h3&gt;Bayesian&amp;nbsp;formulation&lt;/h3&gt;
&lt;p&gt;Note: This section and the next are both fairly mathematical. They can be skipped for those wishing to focus on application method&amp;nbsp;only.&lt;/p&gt;
&lt;p&gt;To start our Bayesian analysis, we begin by positing a general beta distribution for the up-vote rate prior distribution,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{3}\label{beta}  
P(p) = \tilde{\mathcal{N}} p^a (1-p)^b.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, &lt;span class="math"&gt;\(\tilde{\mathcal{N}}\)&lt;/span&gt; is a normalization factor and &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; are some constants (we suggest methods for choosing their values in the discussion section). The function &lt;span class="math"&gt;\(P(p)\)&lt;/span&gt; specifies an initial guess &amp;#8212; in the absence of any reviews for an item &amp;#8212; for what we think the probability is that it will have up-vote rate &lt;span class="math"&gt;\(p\)&lt;/span&gt;. If item &lt;span class="math"&gt;\(i\)&lt;/span&gt; actually has been reviewed, we can update our guess for its distribution using &lt;a href="https://en.wikipedia.org/wiki/Bayes'_theorem"&gt;Bayes&amp;#8217; rule&lt;/a&gt;:&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align} \tag{4} \label{BR}  
P(p \vert n_i(1), n_i(0)) =\frac{ P( n_i(1), n_i(0) \vert p ) P(p)}{P(n_i(1), n_i(0))} = \mathcal{N} p^{n_i(1)+a}(1-p)^{n_i(0)+b}.  
\end{align}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, we have evaluated &lt;span class="math"&gt;\( P( n(1), n(0) \vert p )\)&lt;/span&gt; using the &lt;a href="https://en.wikipedia.org/wiki/Binomial_distribution"&gt;binomial distribution&lt;/a&gt;, we&amp;#8217;ve plugged in (\ref{beta}) for &lt;span class="math"&gt;\(P(p)\)&lt;/span&gt;, and we&amp;#8217;ve collected all &lt;span class="math"&gt;\(p\)&lt;/span&gt;-independent factors into the new normalization factor &lt;span class="math"&gt;\(\mathcal{N}\)&lt;/span&gt;. The formula (\ref{&lt;span class="caps"&gt;BR&lt;/span&gt;}) provides the basis for the three ranking methods discussed&amp;nbsp;below.&lt;/p&gt;
&lt;h3&gt;Three Bayesian ranking&amp;nbsp;systems&lt;/h3&gt;
&lt;p&gt;Let&amp;#8217;s&amp;nbsp;rank!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayesian method 1:&lt;/strong&gt; Choose the ordering that is most&amp;nbsp;likely.&lt;/p&gt;
&lt;p&gt;It is a simple matter to write down a formal expression for the probability of any ranking. For example, given two items we have&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
P(p_1 &amp;gt; p_2) = \int_0^1 dp_1 \int_0^{p_1} dp_2 P(p_1) P(p_2). \tag{5} \label{int}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Plugging in (\ref{&lt;span class="caps"&gt;BR&lt;/span&gt;}) for the &lt;span class="math"&gt;\(P(p_i)\)&lt;/span&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s, this can be evaluated numerically. Evaluating the probability for the opposite ordering, we can then choose that which is most likely to be&amp;nbsp;correct.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Pros:&lt;/em&gt; Approach directly optimizes for the object we&amp;#8217;re interested in, the ranking &amp;#8212; very&amp;nbsp;appealing!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Cons:&lt;/em&gt; Given &lt;span class="math"&gt;\(N\)&lt;/span&gt; items, one has &lt;span class="math"&gt;\(N!\)&lt;/span&gt; integrals to carry out &amp;#8212; untenable for large &lt;span class="math"&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Note:&lt;/em&gt; See posssiblywrong&amp;#8217;s post &lt;a href="https://possiblywrong.wordpress.com/2014/05/31/reddits-comment-ranking-algorithm-revisited/"&gt;here&lt;/a&gt; for some related, interesting&amp;nbsp;points.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayesian method 2:&lt;/strong&gt; Rank item &lt;span class="math"&gt;\(i\)&lt;/span&gt; by its median &lt;span class="math"&gt;\(p\)&lt;/span&gt;-value.&lt;/p&gt;
&lt;p&gt;Sorting by an item score provides an approach that will scale well even at large &lt;span class="math"&gt;\(N\)&lt;/span&gt;. A natural score to consider is an item&amp;#8217;s median &lt;span class="math"&gt;\(p\)&lt;/span&gt;-value: that which it has a &lt;span class="math"&gt;\(50/50\)&lt;/span&gt; shot of being larger (or smaller) than. Using (\ref{&lt;span class="caps"&gt;BR&lt;/span&gt;}), this satisfies&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{6}\label{m2}  
\frac{\int_0^{p_{med}} p^{n_i(1)+a}(1-p)^{n_i(0)+b} dp}{\int_0^{1} p^{n_i(1)+a}(1-p)^{n_i(0)+b} dp} = 1/2.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The integral at left actually has a name &amp;#8212; it&amp;#8217;s called the &lt;a href="http://mathworld.wolfram.com/IncompleteBetaFunction.html"&gt;incomplete beta function&lt;/a&gt;. Using a statistics package, it can be inverted to give &lt;span class="math"&gt;\(p_{med}\)&lt;/span&gt;. For example, if we set &lt;span class="math"&gt;\(a = b = 1\)&lt;/span&gt;, an item with a single up-vote and no down-votes would get a score of &lt;span class="math"&gt;\(0.614\)&lt;/span&gt;. In other words, we&amp;#8217;d guess there&amp;#8217;s a 50/50 shot that the item&amp;#8217;s up-vote rate falls above this value, so we&amp;#8217;d rank it higher than any other item whose &lt;span class="math"&gt;\(p\)&lt;/span&gt; value is known to be smaller than&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Pros:&lt;/em&gt; Sorting is fast. Gives intuitive, meaningful score for each&amp;nbsp;item.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Cons:&lt;/em&gt; Inverting (\ref{m2}) can be somewhat slow, e.g. &lt;span class="math"&gt;\(\sim 10^{-3}\)&lt;/span&gt; seconds in&amp;nbsp;Mathematica.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Note&lt;/em&gt;: &lt;span class="caps"&gt;EM&lt;/span&gt; also derived this score function, in a follow-up to his original post. However, he motivated it in a slightly different way &amp;#8212; see &lt;a href="http://www.evanmiller.org/bayesian-average-ratings.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayesian method 3:&lt;/strong&gt; Rank item &lt;span class="math"&gt;\(i\)&lt;/span&gt; by its most likely (aka &lt;a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation"&gt;&lt;span class="caps"&gt;MAP&lt;/span&gt;&lt;/a&gt;) &lt;span class="math"&gt;\(p\)&lt;/span&gt;-value.&lt;/p&gt;
&lt;p&gt;The most likely &lt;span class="math"&gt;\(p\)&lt;/span&gt;-value for each item provides another natural score function. To find this, we simply set the derivative of (\ref{&lt;span class="caps"&gt;BR&lt;/span&gt;}) to zero,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\begin{align}  
\partial_p p^{n_i(1)+a}(1-p)^{n_i(0)+b} &amp;amp;= \left (\frac{n_i(1)+a}{p} + \frac{n_i(0)+b}{1-p} \right ) p^{n_i(1)+a}(1-p)^{n_i(0)+b} = 0 \\  
\to p = \tilde{p} &amp;amp;\equiv \frac{n_i(1)+a}{(n_i(1)+a) + (n_i(0)+b)}. \tag{7} \label{final}  
\end{align}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This form &lt;span class="math"&gt;\(\tilde{p}\)&lt;/span&gt; is interesting because it resembles the sample mean &lt;span class="math"&gt;\(\hat{p}\)&lt;/span&gt; considered above. However, the actual number of up- and down-votes, &lt;span class="math"&gt;\(n_i(1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(n_i(0)\)&lt;/span&gt;, are supplemented in (\ref{final}) by &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt;, respectively. We can thus interpret these values as effective &amp;#8220;starter votes&amp;#8221;, given to each item before any real reviews are recorded. Their effect is to bias our guess for &lt;span class="math"&gt;\(p\)&lt;/span&gt; towards the prior&amp;#8217;s peak value, with the bias being most strong when &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; are chosen large and/or when we have few actual votes present. For any non-zero choices, (\ref{final}) avoids each of the pitfalls discussed above. Further, it approaches the true up-vote rate in the limit of large review sample sizes, as&amp;nbsp;required.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Pros:&lt;/em&gt; Sorting is fast. Simple method for avoiding the common&amp;nbsp;pitfalls.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\bullet\)&lt;/span&gt; Cons:&lt;/em&gt; Have to pick &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; &amp;#8212; see below for suggested&amp;nbsp;methods.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;We consider each of the four ranking methods we&amp;#8217;ve discussed here to be interesting and useful &amp;#8212; the three Bayesian ranking systems, as well as &lt;a href="http://www.evanmiller.org/how-not-to-sort-by-average-rating.html"&gt;&lt;span class="caps"&gt;EM&lt;/span&gt;&amp;#8217;s original system&lt;/a&gt;, which works well when one only needs to protect against false positives (again, we note that Bayesian method 2 was also considered by &lt;span class="caps"&gt;EM&lt;/span&gt; in a &lt;a href="http://www.evanmiller.org/bayesian-average-ratings.html"&gt;follow-up&lt;/a&gt; to his original post). In practice, the three Bayesian approaches will each tend to return similar, but sometimes slightly different rankings. With regards to &amp;#8220;correctness&amp;#8221;, the essential point is that each method is well-motivated and avoids the common pitfalls. However, the final method is the easiest to apply, so it might be the most&amp;nbsp;practical.&lt;/p&gt;
&lt;p&gt;To apply the Bayesian methods, one must specify the &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; values defining the prior, (\ref{&lt;span class="caps"&gt;BR&lt;/span&gt;}). We suggest three methods for choosing these: 1) Choose these values to provide a good approximation to your actual distribution, fitting only to items for which you have good statistics. 2) A/B test to get the ranking that optimizes some quantity you are interested in, e.g. clicks. 3) Heuristics: For example, if simplicity is key, choose &lt;span class="math"&gt;\(a= b =1\)&lt;/span&gt;, which biases towards an up-vote rate of &lt;span class="math"&gt;\(0.5\)&lt;/span&gt;. If a conservative estimate is desired for new items, one can set &lt;span class="math"&gt;\(b\)&lt;/span&gt; larger than &lt;span class="math"&gt;\(a\)&lt;/span&gt;. Finally, if you want to raise the number of actual votes required before the sample rates dominate, simply increase the values of &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt;&amp;nbsp;accordingly.&lt;/p&gt;
&lt;p&gt;To conclude, we present some example output in the table below. We show values for the Wilson score &lt;span class="math"&gt;\(p_W\)&lt;/span&gt;, with &lt;span class="math"&gt;\(z_{\alpha/2}\)&lt;/span&gt; set to &lt;span class="math"&gt;\(1.281\)&lt;/span&gt; in (\ref{emsol}) (the value &lt;a href="https://github.com/reddit/reddit/blob/62db2373f2555df17ebeb13968e243fccfbeff5f/r2/r2/lib/db/_sorts.pyx"&gt;reddit uses&lt;/a&gt;), and the seed score &lt;span class="math"&gt;\(\tilde{p}\)&lt;/span&gt;, with &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; set to &lt;span class="math"&gt;\(1\)&lt;/span&gt; in (\ref{final}). Notice that the two scores are in near-agreement for the last item shown, which has already accumulated a fair number of votes. However, &lt;span class="math"&gt;\(p_W\)&lt;/span&gt; is significantly lower than &lt;span class="math"&gt;\(\tilde{p}\)&lt;/span&gt; for each of the first three items. For example, the third has an up-vote rate of &lt;span class="math"&gt;\(66%\)&lt;/span&gt;, but is only given a Wilson score of &lt;span class="math"&gt;\(0.32\)&lt;/span&gt;: This means that it would be ranked below any mature item having an up-vote rate at least this high &amp;#8212; including fairly unpopular items liked by only one in three! This observation explains why it is nearly impossible to have new comments noticed on a reddit thread that has already hit the front page. Were reddit to move to a ranking system that were less pessimistic of new comments, its mature threads might remain&amp;nbsp;dynamic.&lt;/p&gt;
&lt;p&gt;up-votes&lt;/p&gt;
&lt;p&gt;down-votes&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p_W\)&lt;/span&gt;, &lt;span class="math"&gt;\(z_{\alpha/2}=&amp;nbsp;1.281\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\tilde{p}\)&lt;/span&gt;, &lt;span class="math"&gt;\(a=b=1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;1&lt;/p&gt;
&lt;p&gt;0&lt;/p&gt;
&lt;p&gt;0.38&lt;/p&gt;
&lt;p&gt;0.67&lt;/p&gt;
&lt;p&gt;1&lt;/p&gt;
&lt;p&gt;1&lt;/p&gt;
&lt;p&gt;0.16&lt;/p&gt;
&lt;p&gt;0.5&lt;/p&gt;
&lt;p&gt;2&lt;/p&gt;
&lt;p&gt;1&lt;/p&gt;
&lt;p&gt;0.32&lt;/p&gt;
&lt;p&gt;0.6&lt;/p&gt;
&lt;p&gt;40&lt;/p&gt;
&lt;p&gt;10&lt;/p&gt;
&lt;p&gt;0.72&lt;/p&gt;
&lt;p&gt;0.79&lt;/p&gt;
&lt;p&gt;Cover image from &lt;a href="https://commons.wikimedia.org/wiki/File:Pseudotsuga_seed_seedling.png"&gt;&lt;span class="caps"&gt;USDA&lt;/span&gt; Forest Service&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>Multivariate Cramer-Rao inequality</title><link href="http/multivariate-cramer-rao-bound.html" rel="alternate"></link><published>2015-06-20T09:00:00-07:00</published><updated>2015-06-20T09:00:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:None,2015-06-20:http/multivariate-cramer-rao-bound.html</id><summary type="html">&lt;p&gt;The Cramer-Rao inequality addresses the question of how accurately one can estimate a set of parameters &lt;span class="math"&gt;\(\vec{\theta} = \{\theta_1, \theta_2, \ldots, \theta_m \}\)&lt;/span&gt; characterizing a probability distribution &lt;span class="math"&gt;\(P(x) \equiv P(x; \vec{\theta})\)&lt;/span&gt;, given only some samples &lt;span class="math"&gt;\(\{x_1, \ldots, x_n\}\)&lt;/span&gt; taken from &lt;span class="math"&gt;\(P\)&lt;/span&gt;. Specifically, the inequality provides a rigorous lower …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The Cramer-Rao inequality addresses the question of how accurately one can estimate a set of parameters &lt;span class="math"&gt;\(\vec{\theta} = \{\theta_1, \theta_2, \ldots, \theta_m \}\)&lt;/span&gt; characterizing a probability distribution &lt;span class="math"&gt;\(P(x) \equiv P(x; \vec{\theta})\)&lt;/span&gt;, given only some samples &lt;span class="math"&gt;\(\{x_1, \ldots, x_n\}\)&lt;/span&gt; taken from &lt;span class="math"&gt;\(P\)&lt;/span&gt;. Specifically, the inequality provides a rigorous lower bound on the covariance matrix of any unbiased set of estimators to these &lt;span class="math"&gt;\(\{\theta_i\}\)&lt;/span&gt; values. In this post, we review the general, multivariate form of the inequality, including its significance and&amp;nbsp;proof.  &lt;/p&gt;
&lt;h3&gt;Introduction and theorem&amp;nbsp;statement&lt;/h3&gt;
&lt;p&gt;The analysis of data very frequently requires one to attempt to characterize a probability distribution. For instance, given some random, stationary process that generates samples &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;, one might wish to estimate the mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; of the probability distribution &lt;span class="math"&gt;\(P\)&lt;/span&gt; characterizing this process. To do this, one could construct an estimator function &lt;span class="math"&gt;\(\hat{\mu}(\{x_i\})\)&lt;/span&gt; &amp;#8212; a function of some samples taken from &lt;span class="math"&gt;\(P\)&lt;/span&gt; &amp;#8212; that is intended to provide an approximation to &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;. Given &lt;span class="math"&gt;\(n\)&lt;/span&gt; samples, a natural choice is provided by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\hat{\mu}(\{x_i\}) = \frac{1}{n}\sum_{i = 1}^n x_i, \tag{1}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
the mean of the samples. This particular choice of estimator will always be unbiased given a stationary &lt;span class="math"&gt;\(P\)&lt;/span&gt; &amp;#8212; meaning that it will return the correct result, on average. However, each particular sample set realization will return a slightly different mean estimate. This means that &lt;span class="math"&gt;\(\hat{\mu}\)&lt;/span&gt; is itself a random variable having its own distribution and&amp;nbsp;width.&lt;/p&gt;
&lt;p&gt;More generally, one might be interested in a distribution characterized by a set of &lt;span class="math"&gt;\(m\)&lt;/span&gt; parameters &lt;span class="math"&gt;\(\{\theta_i\}\)&lt;/span&gt;. Consistently good estimates to these values require estimators with distributions that are tightly centered around the true &lt;span class="math"&gt;\(\{\theta_i\}\)&lt;/span&gt; values. The Cramer-Rao inequality tells us that there is a fundamental limit to how tightly centered such estimators can be, given only &lt;span class="math"&gt;\(n\)&lt;/span&gt; samples. We state the result&amp;nbsp;below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt; &lt;em&gt;The multivariate Cramer-Rao inequality&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(P\)&lt;/span&gt; be a distribution characterized by a set of &lt;span class="math"&gt;\(m\)&lt;/span&gt; parameters &lt;span class="math"&gt;\(\{\theta_i\}\)&lt;/span&gt;, and let &lt;span class="math"&gt;\(\{\hat{\theta_i}\equiv \hat{\theta_i}(\{x_i\})\}\)&lt;/span&gt; be an unbiased set of estimator functions for these parameters. Then, the covariance matrix (see definition below) for the &lt;span class="math"&gt;\(\hat{\{\theta_i\}}\)&lt;/span&gt;&amp;nbsp;satisfies,&lt;/p&gt;
&lt;div class="math"&gt;$$ cov(\hat{\theta}, \hat{\theta}) \geq \frac{1}{n} \times \frac{1}{ cov(\nabla_{\vec{\theta}} \log P(x),\nabla_{\vec{\theta}} \log P(x) )}. \tag{2} \label{CR} $$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, the inequality holds in the sense that left side of the above equation, minus the right, is positive semi-definite. We discuss the meaning and significance of this equation in the next&amp;nbsp;section.&lt;/p&gt;
&lt;h3&gt;Interpretation of the&amp;nbsp;result&lt;/h3&gt;
&lt;p&gt;To understand (\ref{&lt;span class="caps"&gt;CR&lt;/span&gt;}), we must first review a couple of definitions. These&amp;nbsp;follow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 1&lt;/strong&gt;. Let &lt;span class="math"&gt;\(\vec{u}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{v}\)&lt;/span&gt; be two jointly-distributed vectors of stationary random variables. The covariance matrix of &lt;span class="math"&gt;\(\vec{u}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{v}\)&lt;/span&gt; is defined by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
cov(\vec{u}, \vec{v})_{ij} = \overline{(u_{i}- \overline{u_i})(v_{j}- \overline{v_j})} \equiv \overline{\delta u_{i} \delta v_{j}}\tag{3} \label{cov},  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where we use overlines for averages. In words, (\ref{cov}) states that &lt;span class="math"&gt;\(cov(\vec{u}, \vec{v})_{ij}\)&lt;/span&gt; is the correlation function of the fluctuations of &lt;span class="math"&gt;\(u_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(v_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 2&lt;/strong&gt;. A real, square matrix &lt;span class="math"&gt;\(M\)&lt;/span&gt; is said to be positive semi-definite if&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\vec{a}^T\cdot M \cdot \vec{a} \geq 0 \tag{4} \label{pd}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
for all real vectors &lt;span class="math"&gt;\(\vec{a}\)&lt;/span&gt;. It is positive definite if the &amp;#8220;&lt;span class="math"&gt;\(\geq\)&lt;/span&gt;&amp;#8221; above can be replaced by a &amp;#8220;&lt;span class="math"&gt;\(&amp;gt;\)&lt;/span&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The interesting consequences of (\ref{&lt;span class="caps"&gt;CR&lt;/span&gt;}) follow from the following&amp;nbsp;observation:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Observation&lt;/strong&gt;. For any constant vectors &lt;span class="math"&gt;\(\vec{a}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{b}\)&lt;/span&gt;, we have&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
cov(\vec{a}^T\cdot\vec{u}, \vec{b}^T \cdot \vec{v}) = \vec{a}^T \cdot cov(\vec{u}, \vec{v}) \cdot \vec{b}. \tag{5} \label{fact}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This follows from the definition&amp;nbsp;(\ref{cov}).&lt;/p&gt;
&lt;p&gt;Taking &lt;span class="math"&gt;\(\vec{a}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{b}\)&lt;/span&gt; to both be along &lt;span class="math"&gt;\(\hat{i}\)&lt;/span&gt; in (\ref{fact}), and combining with (\ref{pd}), we see that (\ref{&lt;span class="caps"&gt;CR&lt;/span&gt;}) implies that&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\sigma^2(\hat{\theta}_i^2) \geq \frac{1}{n} \times \left (\frac{1}{ cov(\nabla_{\vec{\theta}} \log P(x),\nabla_{\vec{\theta}} \log P(x) )} \right)_{ii},\tag{6}\label{CRsimple}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where we use &lt;span class="math"&gt;\(\sigma^2(x)\)&lt;/span&gt; to represent the variance of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. The left side of (\ref{CRsimple}) is the variance of the estimator function &lt;span class="math"&gt;\(\hat{\theta}_i\)&lt;/span&gt;, whereas the right side is a function of &lt;span class="math"&gt;\(P\)&lt;/span&gt; only. This tells us that there is fundamental &amp;#8212; distribution-dependent &amp;#8212; lower limit on the uncertainty one can achieve when attempting to estimate &lt;em&gt;any parameter characterizing a distribution&lt;/em&gt;. In particular, (\ref{CRsimple}) states that the best variance one can achieve scales like &lt;span class="math"&gt;\(O(1/n)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(n\)&lt;/span&gt; is the number of samples available&lt;span class="math"&gt;\(^1\)&lt;/span&gt; &amp;#8212; very&amp;nbsp;interesting!&lt;/p&gt;
&lt;p&gt;Why is there a relationship between the left and right matrices in (\ref{&lt;span class="caps"&gt;CR&lt;/span&gt;})? Basically, the right side relates to the inverse rate at which the probability of a given &lt;span class="math"&gt;\(x\)&lt;/span&gt; changes with &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;: If &lt;span class="math"&gt;\(P(x \vert \theta)\)&lt;/span&gt; is highly peaked, the gradient of &lt;span class="math"&gt;\(P(x \vert \theta)\)&lt;/span&gt; will take on large values. In this case, a typical observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; will provide significant information relating to the true &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; value, allowing for unbiased &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; estimates that have low variance. In the opposite limit, where typical observations are not very &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;-informative, unbiased &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; estimates must have large variance&lt;span class="math"&gt;\(^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We now turn to the proof of (\ref{&lt;span class="caps"&gt;CR&lt;/span&gt;}).&lt;/p&gt;
&lt;h3&gt;Theorem&amp;nbsp;proof&lt;/h3&gt;
&lt;p&gt;Our discussion here expounds on that in the &lt;a href="http://sfb649.wiwi.hu-berlin.de/fedc_homepage/xplore/tutorials/mvahtmlframe74.html"&gt;online text&lt;/a&gt; of Cízek, Härdle, and Weron. We start by deriving a few simple lemmas. We state and derive these sequentially&amp;nbsp;below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt; Let &lt;span class="math"&gt;\(T_j(\{x_i\}) \equiv \partial_{\theta_j} \log P(\{x_i\}; \vec{\theta})\)&lt;/span&gt; be a function of a set of independent sample values &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;. Then, the average of &lt;span class="math"&gt;\(T_j(\{x_i\})\)&lt;/span&gt; is&amp;nbsp;zero.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; We obtain the average of &lt;span class="math"&gt;\(T_j(\{x_i\})\)&lt;/span&gt; through integration over the &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;, weighted by &lt;span class="math"&gt;\(P\)&lt;/span&gt;,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\int P(\{x_i\};\vec{\theta}) \partial_{\theta_j} \log P(\{x_i\}; \vec{\theta}) d\vec{x} = \int P \frac{\partial_{\theta_j} P}{P} d\vec{x} = \partial_{\theta_j} \int P d\vec{x} = \partial_{\theta_j} 1 = 0. \tag{7}  
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Lemma 2&lt;/strong&gt;. The covariance matrix of an unbiased &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{T}\)&lt;/span&gt; is the identity&amp;nbsp;matrix.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Using (\ref{cov}), the assumed fact that &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; is unbiased, and Lemma 1, we have&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}  
cov \left (\hat{\theta}(\{x_i\}), \vec{T}(\{x_i\}) \right)_{jk} &amp;amp;= \int P(\{x_i\}) (\hat{\theta}_j - \theta_j ) \partial_{\theta_k} \log P(\{x_i\}) d\vec{x}\\ &amp;amp; = \int (\hat{\theta}_j - \theta_j ) \partial_{\theta_k} P d\vec{x} \\  
&amp;amp;= -\int P \partial_{\theta_k} (\hat{\theta}_j - \theta_j ) d \vec{x} \tag{8}  
\end{align}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, we have integrated by parts in the last line. Now, &lt;span class="math"&gt;\(\partial_{\theta_k} \theta_j = \delta_{jk}\)&lt;/span&gt;. Further, &lt;span class="math"&gt;\(\partial_{\theta_k} \hat{\theta}_j = 0\)&lt;/span&gt;, since &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; is a function of the samples &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt; only. Plugging these results into the last line, we obtain&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
cov \left (\hat{\theta}, \vec{T} \right)_{jk} = \delta_{jk} \int P d\vec{x} = \delta_{jk}. \tag{9}  
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Lemma 3&lt;/strong&gt;. The covariance matrix of &lt;span class="math"&gt;\(\vec{T}\)&lt;/span&gt; is &lt;span class="math"&gt;\(n\)&lt;/span&gt; times the covariance matrix of &lt;span class="math"&gt;\(\nabla_{\vec{\theta}} \log P(x_1 ; \vec{\theta})\)&lt;/span&gt; &amp;#8212; a single-sample version of &lt;span class="math"&gt;\(\vec{T}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; From the definition of &lt;span class="math"&gt;\(\vec{T}\)&lt;/span&gt;, we have&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
T_j = \partial_{\theta_j} \log P(\{x_i\}, \vec{\theta}) = \sum_{i=1}^n \partial_{\theta_j} \log P(x_i, \vec{\theta}), \tag{10}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where the last line follows from the fact that the &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt; are independent, so that &lt;span class="math"&gt;\(P(\{x_i\}, \vec{\theta}) = \prod P(x_i; \vec{\theta})\)&lt;/span&gt;. The sum on the right side of the above equation is a sum of &lt;span class="math"&gt;\(n\)&lt;/span&gt; independent, identically-distributed random variables. If follows that their covariance matrix is &lt;span class="math"&gt;\(n\)&lt;/span&gt; times that for any&amp;nbsp;individual.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 4&lt;/strong&gt;. Let &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; be two scalar stationary random variables. Then, their correlation coefficient is defined to be &lt;span class="math"&gt;\(\rho \equiv \frac{cov(x,y)}{\sigma(x) \sigma(y)}\)&lt;/span&gt;. This satisfies&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
-1 \leq \rho \leq 1 \label{CC} \tag{11}  
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Consider the variance of &lt;span class="math"&gt;\(\frac{x}{\sigma(x)}+\frac{y}{\sigma(y)}\)&lt;/span&gt;. This is&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\begin{align}  
var \left( \frac{x}{\sigma(x)}+\frac{y}{\sigma(y)} \right) &amp;amp;= \frac{\sigma^2(x)}{\sigma^2(x)} + 2\frac{ cov(x,y)}{\sigma(x) \sigma(y)} + \frac{\sigma^2(y)}{\sigma^2(y)} \\  
&amp;amp;= 2 + 2 \frac{ cov(x,y)}{\sigma(x) \sigma(y)} \geq 0. \tag{12}  
\end{align}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This gives the left side of (\ref{&lt;span class="caps"&gt;CC&lt;/span&gt;}). Similarly, considering the variance of &lt;span class="math"&gt;\(\frac{x}{\sigma(x)}-\frac{y}{\sigma(y)}\)&lt;/span&gt; gives the right&amp;nbsp;side.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;re now ready to prove the Cramer-Rao&amp;nbsp;result.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof of Cramer-Rao inequality&lt;/strong&gt;. Consider the correlation coefficient of the two scalars &lt;span class="math"&gt;\(\vec{a} \cdot \hat{\theta}\)&lt;/span&gt; and &lt;span class="math"&gt;\( \vec{b} \cdot \vec{T}\)&lt;/span&gt;, with &lt;span class="math"&gt;\(\vec{a}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{b}\)&lt;/span&gt; some constant vectors. Using (\ref{fact}) and Lemma 2, this can be written as&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}  
\rho &amp;amp; \equiv \frac{cov(\vec{a} \cdot \hat{\theta} ,\vec{b} \cdot \vec{T})}{\sqrt{var(\vec{a} \cdot \hat{\theta})var(\vec{b} \cdot \vec{T})}} \\  
&amp;amp;= \frac{\vec{a}^T \cdot \vec{b}}{\left(\vec{a}^T \cdot cov(\hat{\theta}, \hat{\theta}) \cdot \vec{a} \right)^{1/2} \left( \vec{b}^T \cdot cov(\vec{T},\vec{T}) \cdot \vec{b} \right)^{1/2}}\leq 1. \tag{13}  
\end{align}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The last inequality here follows from Lemma 4. We can find the direction &lt;span class="math"&gt;\(\hat{b}\)&lt;/span&gt; where the bound above is most tight &amp;#8212; at fixed &lt;span class="math"&gt;\(\vec{a}\)&lt;/span&gt; &amp;#8212; by maximizing the numerator while holding the denominator fixed in value. Using a Lagrange multiplier to hold &lt;span class="math"&gt;\(\left( \vec{b}^T \cdot cov(\vec{T},\vec{T}) \cdot \vec{b} \right) \equiv 1\)&lt;/span&gt;, the numerator&amp;#8217;s extremum occurs where&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\vec{a}^T + 2 \lambda \vec{b}^T \cdot cov(\vec{T},\vec{T}) = 0 \ \ \to \ \ \vec{b}^T = - \frac{1}{2 \lambda} \vec{a}^T \cdot cov(\vec{T}, \vec{T})^{-1}. \tag{14}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Plugging this form into the prior line, we obtain&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
- \frac{\vec{a}^T \cdot cov(\vec{T},\vec{T})^{-1} \cdot \vec{a}}{\left(\vec{a}^T \cdot cov(\hat{\theta}, \hat{\theta}) \cdot \vec{a} \right)^{1/2} \left(\vec{a}^T \cdot cov(\vec{T},\vec{T})^{-1} \cdot \vec{a} \right)^{1/2}}\leq 1. \tag{15}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Squaring and rearranging terms, we obtain&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$  
\vec{a}^T \cdot \left (cov(\hat{\theta},\hat{\theta}) - cov(\vec{T},\vec{T})^{-1} \right ) \cdot \vec{a} \geq 0. \tag{16}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This holds for any &lt;span class="math"&gt;\(\vec{a}\)&lt;/span&gt;, implying that &lt;span class="math"&gt;\(cov(\hat{\theta}, \hat{\theta}) - cov(\vec{T},\vec{T})^{-1} $ is positive semi-definite -- see (\ref{pd}). Applying Lemma 3, we obtain the result\)&lt;/span&gt;^3&lt;span class="math"&gt;\(.&amp;nbsp;$\blacksquare\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thank you for reading &amp;#8212; we hope you&amp;nbsp;enjoyed.&lt;/p&gt;
&lt;p&gt;[1] More generally, (\ref{fact}) tells us that an observation similar to (\ref{CRsimple}) holds for any linear combination of the &lt;span class="math"&gt;\(\{\theta_i\}\)&lt;/span&gt;. Notice also that the proof we provide here could also be applied to any individual &lt;span class="math"&gt;\(\theta_i\)&lt;/span&gt;, giving &lt;span class="math"&gt;\(\sigma^2(\hat{\theta}_i) \geq 1/n \times 1/\langle(\partial_{\theta_i} \log P)^2\rangle\)&lt;/span&gt;. This is easier to apply than (\ref{&lt;span class="caps"&gt;CR&lt;/span&gt;}), but is less&amp;nbsp;stringent.&lt;/p&gt;
&lt;p&gt;[2] It might be challenging to intuit the exact function that appears on the right side of &lt;span class="math"&gt;\((\ref{CR})\)&lt;/span&gt;. However, the appearance of &lt;span class="math"&gt;\(\log P\)&lt;/span&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s does make some intuitive sense, as it allows the derivatives involved to measure rates of change relative to typical values, &lt;span class="math"&gt;\(\nabla_{\theta} P / P\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;[3] The discussion here covers the &amp;#8220;standard proof&amp;#8221; of the Cramer-Rao result. Its brilliance is that it allows one to work with scalars. In contrast, when attempting to find my own proof, I began with the fact that all covariance matrices are positive definite. Applying this result to the covariance matrix of a linear combination of &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{T}\)&lt;/span&gt;, one can quickly get to results similar in form to the Cramer-Rao bound, but not quite identical. After significant work, I was eventually able to show that &lt;span class="math"&gt;\(\sqrt{cov(\hat{\theta},\hat{\theta})} - 1/\sqrt{cov(\vec{T},\vec{T}) } \geq 0\)&lt;/span&gt;. However, I have yet to massage my way to the final result using this approach &amp;#8212; the difficulty being that the matrices involved don&amp;#8217;t commute. By working with scalars from the start, the proof here cleanly avoids all such&amp;nbsp;issues.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry><entry><title>Mathematics of measles</title><link href="http/math-of-measles.html" rel="alternate"></link><published>2015-02-25T13:37:00-08:00</published><updated>2015-02-25T13:37:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:None,2015-02-25:http/math-of-measles.html</id><summary type="html">&lt;p&gt;Here, we introduce &amp;#8212; and outline a solution to &amp;#8212; a generalized &lt;span class="caps"&gt;SIR&lt;/span&gt; model for infectious disease. This is referenced in our &lt;a href="http://efavdb.com/vaccination-rates/"&gt;following post&lt;/a&gt; on measles and vaccination rates. Our generalized &lt;span class="caps"&gt;SIR&lt;/span&gt; model differs from the &lt;a href="http://en.wikipedia.org/wiki/Epidemic_model#The_SIR_model"&gt;original &lt;span class="caps"&gt;SIR&lt;/span&gt; model&lt;/a&gt; of Kermack and McKendrick in that we allow for two susceptible sub-populations, one …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here, we introduce &amp;#8212; and outline a solution to &amp;#8212; a generalized &lt;span class="caps"&gt;SIR&lt;/span&gt; model for infectious disease. This is referenced in our &lt;a href="http://efavdb.com/vaccination-rates/"&gt;following post&lt;/a&gt; on measles and vaccination rates. Our generalized &lt;span class="caps"&gt;SIR&lt;/span&gt; model differs from the &lt;a href="http://en.wikipedia.org/wiki/Epidemic_model#The_SIR_model"&gt;original &lt;span class="caps"&gt;SIR&lt;/span&gt; model&lt;/a&gt; of Kermack and McKendrick in that we allow for two susceptible sub-populations, one vaccinated against disease and one not. We conclude by presenting some python code that integrates the equations numerically. An example solution obtained using this code is given&amp;nbsp;below.  &lt;/p&gt;
&lt;p&gt;[caption width=&amp;#8221;800&amp;#8221; caption=&amp;#8221;Solution shown corresponds to a 20% unvaccinated population, a condition supporting outbreak.&amp;#8221;][iframe src=&amp;#8221;https://plot.ly/~Jonathan Landy/58&amp;#8221; width=&amp;#8221;90%&amp;#8221;&amp;nbsp;height=&amp;#8221;450&amp;#8221;][/caption]&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;The&amp;nbsp;model&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The equations describing our generalized &lt;span class="caps"&gt;SIR&lt;/span&gt; model are&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{eq1}  
\dot{S}_{U} &amp;amp;=&amp;amp; - b_{U} S_{U} I\\ \label{eq2}  
\dot{S}_{V} &amp;amp;=&amp;amp; - b_{V} S_{V} I\\ \label{eq3}  
\dot{R} &amp;amp;=&amp;amp; k I\\  
1 &amp;amp;=&amp;amp; I + R + S_U + S_V \label{Ieq}  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, &lt;span class="math"&gt;\(S_{U}\)&lt;/span&gt;, &lt;span class="math"&gt;\(S_{V}\)&lt;/span&gt;, &lt;span class="math"&gt;\(I\)&lt;/span&gt;, and &lt;span class="math"&gt;\(R\)&lt;/span&gt; are population fractions corresponding to those unvaccinated and as yet uninfected, vaccinated and as yet uninfected, currently infected and contagious, and once contagious but no longer (recovered, perhaps), respectively. The first two equations above are instances of the &lt;a href="http://en.wikipedia.org/wiki/Law_of_mass_action"&gt;law of mass action&lt;/a&gt;. They approximate the infection rates as being proportional to the rates of susceptible-infected individual encounters. We refer to &lt;span class="math"&gt;\(b_{U}\)&lt;/span&gt; and &lt;span class="math"&gt;\(b_{V}\)&lt;/span&gt; here as the &lt;em&gt;infection rate parameters&lt;/em&gt; of the two subpopulations. The third equation above approximates the dynamics of recovery: The form chosen supposes that an infected individual has a fixed probability of returning to health each day. We will refer to &lt;span class="math"&gt;\(k\)&lt;/span&gt; as the &lt;em&gt;recovery rate parameter&lt;/em&gt;. The final equation above simply states that the subpopulation fractions have to always sum to&amp;nbsp;one.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Parameter&amp;nbsp;estimation&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;We can estimate the values &lt;span class="math"&gt;\(b_{U}\)&lt;/span&gt; and &lt;span class="math"&gt;\(b_{V}\)&lt;/span&gt; by introducing a close contact number (&lt;span class="math"&gt;\(ccn\)&lt;/span&gt;) variable, which is the average number of close contacts that individual infected, contagious people make per day. As a rough ball park, let us suppose that &lt;span class="math"&gt;\(ccn \approx 3\)&lt;/span&gt;. According to the &lt;span class="caps"&gt;CDC&lt;/span&gt;, an un-vaccinated person making close contact with someone with measles has a 90&lt;span class="math"&gt;\(%\)&lt;/span&gt; chance of contracting the illness. On the other hand, those who have been vaccinated a single time have a 95&lt;span class="math"&gt;\(%\)&lt;/span&gt; chance of being immune to the disease. Let&amp;#8217;s estimate that the combined population of individuals who have been vaccinated have a 1&lt;span class="math"&gt;\(%\)&lt;/span&gt; chance of contracting the illness upon close contact. These considerations suggest&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  
b_{U} \approx 3 \times 0.9 &amp;amp;=&amp;amp; 0.27, \ \  
b_{V} \approx 3 \times 0.01 &amp;amp;=&amp;amp; 0.03  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The value of &lt;span class="math"&gt;\(k\)&lt;/span&gt; can be simply estimated using the fact that infected individuals are only contagious for about &lt;span class="math"&gt;\(8\)&lt;/span&gt; days, only four of which occur before rash appears. Assuming those who are showing symptoms quickly stop circulating, this suggests about five &amp;#8220;effectively contagious&amp;#8221; days, or&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  
k \approx 1/5 = 0.2.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Note that here and elsewhere, we measure time in units of&amp;nbsp;days.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s important to note that, although the qualitative properties of the solutions to our model are insensitive to parameter value variations, this is not true for the numerical values that it predicts. We have chosen parameter values that seem reasonable to us. Further, with these choices, many of the model&amp;#8217;s key quantitative values line up with corresponding &lt;span class="caps"&gt;CDC&lt;/span&gt; estimates. Those interested can experiment to see what sort of flexibility is allowed through modest parameter&amp;nbsp;variation.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Solution by&amp;nbsp;quadrature&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Equations (\ref{eq1}-\ref{eq3}) give&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{Svals}  
S_{U} = S_{U0} e^{ - \frac{b_{U} R}{k}}, \ \ \  
S_{V} = S_{V0} e^{- \frac{b_{V} R}{k}}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Combining with (\ref{Ieq}) and integrating gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  
\frac{\dot{R}}{k} =I_0 -S_{U0} \left [ e^{ - \frac{b_{U} R}{k}}- 1 \right ] - S_{20} \left [e^{ - \frac{b_{V} R}{k}}- 1 \right ] - R  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Integrating again,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  
kt = \int_{0}^R \frac{d R^{\prime}}{I_0 -S_{U0} \left [ e^{ - \frac{b_{U} R^{\prime}}{k}}- 1 \right ] - S_{V0} \left [e^{ - \frac{b_{V} R^{\prime}}{k}}- 1 \right ] - R^{\prime}} \label{solution}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This implicitly defines &lt;span class="math"&gt;\(R\)&lt;/span&gt; as function of&amp;nbsp;time.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Small time&amp;nbsp;behavior&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;At small &lt;span class="math"&gt;\(t\)&lt;/span&gt;, &lt;span class="math"&gt;\(R\)&lt;/span&gt; is also small, so (\ref{solution}) can be approximated as&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  
k t = \int_{0}^R \frac{d R^{\prime}}{I_0 + \left [ \frac{ b_{U} S_{U0}}{k} +\frac{ b_{V} S_{V0}}{k} - 1 \right ]R^{\prime}}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This form can be integrated analytically. Doing so, and solving for &lt;span class="math"&gt;\(R\)&lt;/span&gt;, we obtain&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  
R = \frac{k}{b_{U} S_{U0} + b_{V} S_{V0} - k} \left \{e^{ (b_{U} S_{U0} + b_{V} S_{V0} - k )t } -1 \right \}, \ \ \  
I = I_0 e^{ (b_{U} S_{U0} + b_{V} S_{V0} - k )t}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Early disease spread is characterized by either exponential growth or decay, governed by the sign of the parameter combination &lt;span class="math"&gt;\(b_{U} S_{U0} + b_{V} S_{V0} - k\)&lt;/span&gt;: a phase&amp;nbsp;transition!&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Total&amp;nbsp;contractions&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The total number of people infected in an outbreak can be obtained by evaluating &lt;span class="math"&gt;\(R\)&lt;/span&gt; at long times, where &lt;span class="math"&gt;\(I = 0\)&lt;/span&gt;. In this limit, using (\ref{Ieq}) and (\ref{Svals}), we have&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  
S_{U0} e^{- \frac{b_{U} R}{k}}+ S_{V0} e^{ - \frac{b_{V} R}{k}}+ R = 1.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This equation can be solved numerically to obtain the total contraction count as a function of the model parameters and initial conditions. A plot against &lt;span class="math"&gt;\(S_{U0}\)&lt;/span&gt; of such a solution for our measles-appropriate parameter estimates is given in our &lt;a href="http://efavdb.com/vaccination-rates/"&gt;next post&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Numerical integration in&amp;nbsp;python&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Below, we provide code that can be used to integrate (\ref{eq1}-\ref{Ieq}). The plot shown in our introduction provides one example solution. It&amp;#8217;s quite interesting to see how the solutions vary with parameter values, and we suggest that those interested try it&amp;nbsp;out.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#Solving the SIR model for infectious disease. JSL 2/18/2015  &lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;

&lt;span class="n"&gt;ccn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="c1"&gt;#\`\`close contact number&amp;quot; = people per day  &lt;/span&gt;
&lt;span class="c1"&gt;#interacting closely with typical infected person&lt;/span&gt;

&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="c1"&gt;#Rate of &amp;#39;recovery&amp;#39; [1].  &lt;/span&gt;
&lt;span class="n"&gt;b1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ccn&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt; &lt;span class="c1"&gt;#Approximate infection rate un-vaccinated [3].  &lt;/span&gt;
&lt;span class="n"&gt;b2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ccn&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt; &lt;span class="c1"&gt;#Approximate infection rate un-vaccinated [4].&lt;/span&gt;

&lt;span class="c1"&gt;#Initial conditions (fraction of people in each category)  &lt;/span&gt;
&lt;span class="n"&gt;I0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt; &lt;span class="c1"&gt;#initial population fraction infected.  &lt;/span&gt;
&lt;span class="n"&gt;S10&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt; &lt;span class="c1"&gt;#population fraction unvaccinated.  &lt;/span&gt;
&lt;span class="n"&gt;S20&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;I0&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;S10&lt;/span&gt; &lt;span class="c1"&gt;#population fraction vacccinated.  &lt;/span&gt;
&lt;span class="n"&gt;R0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt; &lt;span class="c1"&gt;#intial recovered fraction.&lt;/span&gt;

&lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt; &lt;span class="c1"&gt;#integration time step  &lt;/span&gt;
&lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="c1"&gt;#total days considered&lt;/span&gt;

&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;I0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;  
&lt;span class="n"&gt;S1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;S10&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;  
&lt;span class="n"&gt;S2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;S20&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;  
&lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;R0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;

&lt;span class="n"&gt;S1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;b1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;S1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;  
&lt;span class="n"&gt;S2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;b2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;S2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;  
&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;S1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; \  
&lt;span class="n"&gt;b2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;S2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;  
&lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;

&lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;

&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;pylab&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;S1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;S2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;green&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;1400&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="n"&gt;I0&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;b1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;S10&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;S20&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; \  
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;1400&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;purple&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yscale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;time [days]&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;population %&lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;s&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;#[1] Measles patients are contagious for eight days  &lt;/span&gt;
&lt;span class="c1"&gt;# four of which are before symptoms appear. [2]  &lt;/span&gt;
&lt;span class="c1"&gt;#[2] http://www.cdc.gov/measles/about/transmission.html  &lt;/span&gt;
&lt;span class="c1"&gt;#[3] Assume infected have close contact with five people/day.  &lt;/span&gt;
&lt;span class="c1"&gt;# 90% of the un-vaccinated get sick in such situations.  &lt;/span&gt;
&lt;span class="c1"&gt;#[4] Single vaccination gives ~95% immunity rate [5]. Many  &lt;/span&gt;
&lt;span class="c1"&gt;# have two doses, which drops rate to very low.  &lt;/span&gt;
&lt;span class="c1"&gt;#[5] http://www.cdc.gov/mmwr/preview/mmwrhtml/00053391.htm&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Statistics"></category></entry></feed>