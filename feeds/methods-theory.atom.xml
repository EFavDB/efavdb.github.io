<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>EFAVDB - Methods, Theory</title><link href="https://efavdb.com/" rel="alternate"></link><link href="https://efavdb.com/feeds/methods-theory.atom.xml" rel="self"></link><id>https://efavdb.com/</id><updated>2017-03-18T22:36:00-07:00</updated><subtitle>Everybody's Favorite Data Blog</subtitle><entry><title>Model AUC depends on test set difficulty</title><link href="https://efavdb.com/model-auc-depends-on-test-set-difficulty" rel="alternate"></link><published>2017-03-18T22:36:00-07:00</published><updated>2017-03-18T22:36:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2017-03-18:/model-auc-depends-on-test-set-difficulty</id><summary type="html">&lt;p&gt;The &lt;span class="caps"&gt;AUC&lt;/span&gt; score is a popular summary statistic that is often used to communicate the performance of a classifier. However, we illustrate here that this score depends not only on the quality of the model in question, but also on the difficulty of the test set considered: If samples are …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;span class="caps"&gt;AUC&lt;/span&gt; score is a popular summary statistic that is often used to communicate the performance of a classifier. However, we illustrate here that this score depends not only on the quality of the model in question, but also on the difficulty of the test set considered: If samples are added to a test set that are easily classified, the &lt;span class="caps"&gt;AUC&lt;/span&gt; will go up &amp;#8212; even if the model studied has not improved. In general, this behavior implies that isolated, single &lt;span class="caps"&gt;AUC&lt;/span&gt; scores cannot be used to meaningfully qualify a model&amp;#8217;s performance. Instead, the &lt;span class="caps"&gt;AUC&lt;/span&gt; should be considered a score that is primarily useful for comparing and ranking multiple models &amp;#8212; each at a common test set&amp;nbsp;difficulty.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;An important challenge associated with building good classification algorithms centers around their optimization: If an adjustment is made to an algorithm, we need a score that will enable us to decide whether or not the change made was an improvement. Many scores are available for this purpose. A sort-of all-purpose score that is quite popular for characterizing binary classifiers is the model &lt;span class="caps"&gt;AUC&lt;/span&gt; score (defined&amp;nbsp;below).&lt;/p&gt;
&lt;p&gt;The purpose of this post is to illustrate a subtlety associated with the &lt;span class="caps"&gt;AUC&lt;/span&gt; that is not always appreciated: The score depends strongly on the difficulty of the test set used to measure model performance. In particular, if any soft-balls are added to a test set that are easily classified (i.e., are far from any decision boundary), the &lt;span class="caps"&gt;AUC&lt;/span&gt; will increase. This increase does not imply a model improvement. Two key take-aways&amp;nbsp;follow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;span class="caps"&gt;AUC&lt;/span&gt; is an inappropriate score for comparing models validated on test sets having differing sampling distributions. Therefore, comparing the AUCs of models trained on samples having differing distributions requires care: The training sets can have different distributions, but the test sets must&amp;nbsp;not.&lt;/li&gt;
&lt;li&gt;A single &lt;span class="caps"&gt;AUC&lt;/span&gt; measure cannot typically be used to meaningfully communicate the quality of a single model (though single model &lt;span class="caps"&gt;AUC&lt;/span&gt; scores are often&amp;nbsp;reported!)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The primary utility of the &lt;span class="caps"&gt;AUC&lt;/span&gt; is that it allows one to compare multiple models at fixed test set difficulty: If a model change results in an increase in the &lt;span class="caps"&gt;AUC&lt;/span&gt; at fixed test set distribution, it can often be considered an&amp;nbsp;improvement.&lt;/p&gt;
&lt;p&gt;We review the definition of the &lt;span class="caps"&gt;AUC&lt;/span&gt; below and then demonstrate the issues alluded to&amp;nbsp;above.&lt;/p&gt;
&lt;h3&gt;The &lt;span class="caps"&gt;AUC&lt;/span&gt; score,&amp;nbsp;reviewed&lt;/h3&gt;
&lt;p&gt;Here, we quickly review the definition of the &lt;span class="caps"&gt;AUC&lt;/span&gt;. This is a score that can be used to quantify the accuracy of a binary classification algorithm on a given test set &lt;span class="math"&gt;\(\mathcal{S}\)&lt;/span&gt;. The test set consists of a set of feature vector-label pairs of the&amp;nbsp;form
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{1}
\mathcal{S} = \{(\textbf{x}_i, y_i) \}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Here, &lt;span class="math"&gt;\(\textbf{x}_i\)&lt;/span&gt; is the set of features, or predictor variables, for example &lt;span class="math"&gt;\(i\)&lt;/span&gt; and &lt;span class="math"&gt;\(y_i \in \{0,1 \}\)&lt;/span&gt; is the label for example &lt;span class="math"&gt;\(i\)&lt;/span&gt;. A classifier function &lt;span class="math"&gt;\(\hat{p}_1(\textbf{x})\)&lt;/span&gt; is one that attempts to guess the value of &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; given only the feature vector &lt;span class="math"&gt;\(\textbf{x}_i\)&lt;/span&gt;. In particular, the output of the function &lt;span class="math"&gt;\(\hat{p}_1(\textbf{x}_i)\)&lt;/span&gt; is an estimate for the probability that the label &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; is equal to &lt;span class="math"&gt;\(1\)&lt;/span&gt;. If the algorithm is confident that the class is &lt;span class="math"&gt;\(1\)&lt;/span&gt; (&lt;span class="math"&gt;\(0\)&lt;/span&gt;), the probability returned will be large&amp;nbsp;(small).&lt;/p&gt;
&lt;p&gt;To characterize model performance, we can set a threshold value of &lt;span class="math"&gt;\(p^*\)&lt;/span&gt; and mark all examples in the test set with &lt;span class="math"&gt;\(\hat{p}(\textbf{x}_i) &amp;gt; p^*\)&lt;/span&gt; as being candidates for class one. The fraction of the truly positive examples in &lt;span class="math"&gt;\(\mathcal{S}\)&lt;/span&gt; marked in this way is referred to as the true-positive rate (&lt;span class="caps"&gt;TPR&lt;/span&gt;) at threshold &lt;span class="math"&gt;\(p^*\)&lt;/span&gt;. Similarly, the fraction of negative examples in &lt;span class="math"&gt;\(\mathcal{S}\)&lt;/span&gt; marked is referred to as the false-positive rate (&lt;span class="caps"&gt;FPR&lt;/span&gt;) at threshold &lt;span class="math"&gt;\(p^*\)&lt;/span&gt;. Plotting the &lt;span class="caps"&gt;TPR&lt;/span&gt; against the &lt;span class="caps"&gt;FPR&lt;/span&gt; across all thresholds gives the model&amp;#8217;s so-called receiver operating characteristic (&lt;span class="caps"&gt;ROC&lt;/span&gt;) curve. A hypothetical example is shown at right in blue. The dashed line is just the &lt;span class="math"&gt;\(y=x\)&lt;/span&gt; line, which corresponds to the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve of a random classifier (one returning a uniform random &lt;span class="math"&gt;\(p\)&lt;/span&gt; value each&amp;nbsp;time).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/03/example.png"&gt;&lt;img alt="example" src="https://efavdb.com/wp-content/uploads/2017/03/example.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Notice that if the threshold is set to &lt;span class="math"&gt;\(p^* = 1\)&lt;/span&gt;, no positive or negative examples will typically be marked as candidates, as this would require one-hundred percent confidence of class &lt;span class="math"&gt;\(1\)&lt;/span&gt;. This means that we can expect an &lt;span class="caps"&gt;ROC&lt;/span&gt; curve to always go through the point &lt;span class="math"&gt;\((0,0)\)&lt;/span&gt;. Similarly, with &lt;span class="math"&gt;\(p^*\)&lt;/span&gt; set to &lt;span class="math"&gt;\(0\)&lt;/span&gt;, all examples should be marked as candidates for class &lt;span class="math"&gt;\(1\)&lt;/span&gt; &amp;#8212; and so an &lt;span class="caps"&gt;ROC&lt;/span&gt; curve should also always go through the point &lt;span class="math"&gt;\((1,1)\)&lt;/span&gt;. In between, we hope to see a curve that increases in the &lt;span class="caps"&gt;TPR&lt;/span&gt; direction more quickly than in the &lt;span class="caps"&gt;FPR&lt;/span&gt; direction &amp;#8212; since this would imply that the examples the model is most confident about tend to actually be class &lt;span class="math"&gt;\(1\)&lt;/span&gt; examples. In general, the larger the Area Under the (&lt;span class="caps"&gt;ROC&lt;/span&gt;) Curve &amp;#8212; again, blue at right &amp;#8212; the better. We call this area the &amp;#8220;&lt;span class="caps"&gt;AUC&lt;/span&gt; score for the model&amp;#8221; &amp;#8212; the topic of this&amp;nbsp;post.&lt;/p&gt;
&lt;h3&gt;&lt;span class="caps"&gt;AUC&lt;/span&gt; sensitivity to test set&amp;nbsp;difficulty&lt;/h3&gt;
&lt;p&gt;To illustrate the sensitivity of the &lt;span class="caps"&gt;AUC&lt;/span&gt; score to test set difficulty, we now consider a toy classification problem: In particular, we consider a set of unit-variance normal distributions, each having a different mean &lt;span class="math"&gt;\(\mu_i\)&lt;/span&gt;. From each distribution, we will take a single sample &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;. From this, we will attempt to estimate whether or not the corresponding mean satisfies &lt;span class="math"&gt;\(\mu_i &amp;gt; 0\)&lt;/span&gt;. That is, our training set will take the form &lt;span class="math"&gt;\(\mathcal{S} = \{(x_i, \mu_i)\}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(x_i \sim N(\mu_i, 1)\)&lt;/span&gt;. For different &lt;span class="math"&gt;\(\mathcal{S}\)&lt;/span&gt;, we will study the &lt;span class="caps"&gt;AUC&lt;/span&gt; of the classifier&amp;nbsp;function,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \label{classifier} \tag{2}
\hat{p}(x) = \frac{1}{2} (1 + \text{tanh}(x))
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
A plot of this function is shown below. You can see that if any test sample &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; is far to the right (left) of &lt;span class="math"&gt;\(x=0\)&lt;/span&gt;, the model will classify the sample as positive (negative) with high certainty. At intermediate values near the boundary, the estimated probability of being in the positive class lifts in a reasonable&amp;nbsp;way.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/03/classifier-2.png"&gt;&lt;img alt="classifier" src="https://efavdb.com/wp-content/uploads/2017/03/classifier-2.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Notice that if a test example has a mean very close to zero, it will be difficult to classify that example as positive or negative. This is because both positive and negative &lt;span class="math"&gt;\(x\)&lt;/span&gt; samples are equally likely in this case. This means that the model cannot do much better than a random guess for such &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;. On the other hand, if an example &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; is selected that is very far from the origin, a single sample &lt;span class="math"&gt;\(x\)&lt;/span&gt; from &lt;span class="math"&gt;\(N(\mu, 1)\)&lt;/span&gt; will be sufficient to make a very good guess as to whether &lt;span class="math"&gt;\(\mu &amp;gt; 0\)&lt;/span&gt;. Such examples are hard to get wrong,&amp;nbsp;soft-balls.&lt;/p&gt;
&lt;p&gt;The impact of adding soft-balls to the test set on the &lt;span class="caps"&gt;AUC&lt;/span&gt; for model (\ref{classifier}) can be studied by changing the sampling distribution of &lt;span class="math"&gt;\(\mathcal{S}\)&lt;/span&gt;. The following python snippet takes samples &lt;span class="math"&gt;\(\mu_i\)&lt;/span&gt; from three distributions &amp;#8212; one tight about &lt;span class="math"&gt;\(0\)&lt;/span&gt; (resulting in a very difficult test set), one that is very wide containing many soft-balls that are easily classified, and one that is intermediate. The &lt;span class="caps"&gt;ROC&lt;/span&gt; curves that result from these three cases are shown following the code. The three curves are very different, with the &lt;span class="caps"&gt;AUC&lt;/span&gt; of the soft-ball set very large and that of the tight set close to that of the random classifier. Yet, in each case the model considered was the same &amp;#8212; (\ref{classifier}). How could the &lt;span class="caps"&gt;AUC&lt;/span&gt; have&amp;nbsp;improved?!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;SAMPLES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
&lt;span class="n"&gt;means_std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;means_std&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;001&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;means&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;means_std&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SAMPLES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;
    &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x_set&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;thresholds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;roc_curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;means_std&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;k--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;*&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;means_std&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;lower right&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shadow&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;lower right&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shadow&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;TPR versus FPR -- The ROC curve&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Means sampled for each case&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/03/Examples.png"&gt;&lt;img alt="Examples" src="https://efavdb.com/wp-content/uploads/2017/03/Examples.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The explanation for the differing &lt;span class="caps"&gt;AUC&lt;/span&gt; values above is clear: Consider, for example, the effect of adding soft-ball negatives to &lt;span class="math"&gt;\(\mathcal{S}\)&lt;/span&gt;. In this case, the model (\ref{classifier}) will be able to correctly identify almost all true positive examples at a much higher threshold than that where it begins to mis-classify the introduced negative softballs. This means that the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve will now hit a &lt;span class="caps"&gt;TPR&lt;/span&gt; value of &lt;span class="math"&gt;\(1\)&lt;/span&gt; well-before the &lt;span class="caps"&gt;FPR&lt;/span&gt; does (which requires all negatives &amp;#8212; including the soft-balls to be mis-classified). Similarly, if many soft-ball positives are added in, these will be easily identified as such well-before any negative examples are mis-classified. This again results in a raising of the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve, and an increase in &lt;span class="caps"&gt;AUC&lt;/span&gt; &amp;#8212; all without any improvement in the actual model quality, which we have held&amp;nbsp;fixed.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;The toy example considered above illustrates the general point the &lt;span class="caps"&gt;AUC&lt;/span&gt; of a model is really a function of both the model and the test set it is being applied to. Keeping this in mind will help to prevent incorrect interpretations of the &lt;span class="caps"&gt;AUC&lt;/span&gt;. A special case to watch out for in practice is the situation where the &lt;span class="caps"&gt;AUC&lt;/span&gt; changes upon adjustment of the training and testing protocol applied (which can result, for example, from changes to how training examples are collected for the model). If you see such a change occur in your work, be careful to consider whether or not it is possible that the difficulty of the test set has changed in the process. If so, the change in the &lt;span class="caps"&gt;AUC&lt;/span&gt; may not indicate a change in model&amp;nbsp;quality.&lt;/p&gt;
&lt;p&gt;Because the &lt;span class="caps"&gt;AUC&lt;/span&gt; score of a model can depend highly on the difficulty of the test set, reporting this score alone will generally not provide much insight into the accuracy of the model &amp;#8212; which really depends only on performance near the true decision boundary and not on soft-ball performance. Because of this, it may be a good practice to always report &lt;span class="caps"&gt;AUC&lt;/span&gt; scores for optimized models next to those of some fixed baseline model. Comparing the differences of the two &lt;span class="caps"&gt;AUC&lt;/span&gt; scores provides an approximate method for removing the effect of test set difficulty. If you come across an isolated, high &lt;span class="caps"&gt;AUC&lt;/span&gt; score in the wild, remember that this does not imply a good&amp;nbsp;model!&lt;/p&gt;
&lt;p&gt;A special situation exists where reporting an isolated &lt;span class="caps"&gt;AUC&lt;/span&gt; score for a single model can provide value: The case where the test set employed shares the same distribution as that of the application set (the space where the model will be employed). In this case, performance within the test set directly relates to expected performance during application. However, applying the &lt;span class="caps"&gt;AUC&lt;/span&gt; to situations such as this is not always useful. For example, if the positive class sits within only a small subset of feature space, samples taken from much of the rest of the space will be &amp;#8220;soft-balls&amp;#8221; &amp;#8212; examples easily classified as not being in the positive class. Measuring the &lt;span class="caps"&gt;AUC&lt;/span&gt; on test sets over the full feature space in this context will always result in &lt;span class="caps"&gt;AUC&lt;/span&gt; values near one &amp;#8212; leaving it difficult to register improvements in the model near the decision boundary through measurement of the &lt;span class="caps"&gt;AUC&lt;/span&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>Deep reinforcement learning, battleship</title><link href="https://efavdb.com/battleship" rel="alternate"></link><published>2016-10-15T13:52:00-07:00</published><updated>2016-10-15T13:52:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2016-10-15:/battleship</id><summary type="html">&lt;p&gt;Here, we provide a brief introduction to reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) &amp;#8212; a general technique for training programs to play games efficiently. Our aim is to explain its practical implementation: We cover some basic theory and then walk through a minimal python program that trains a neural network to play the game …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here, we provide a brief introduction to reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) &amp;#8212; a general technique for training programs to play games efficiently. Our aim is to explain its practical implementation: We cover some basic theory and then walk through a minimal python program that trains a neural network to play the game&amp;nbsp;battleship.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Reinforcement learning (&lt;span class="caps"&gt;RL&lt;/span&gt;) techniques are methods that can be used to teach algorithms to play games efficiently. Like supervised machine-learning (&lt;span class="caps"&gt;ML&lt;/span&gt;) methods, &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms learn from data &amp;#8212; in this case, past game play data. However, whereas supervised-learning algorithms train only on data that is already available, &lt;span class="caps"&gt;RL&lt;/span&gt; addresses the challenge of performing well while still in the process of collecting data. In particular, we seek design principles&amp;nbsp;that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Allow programs to identify good strategies from past&amp;nbsp;examples,&lt;/li&gt;
&lt;li&gt;Enable fast learning of new strategies through continued game&amp;nbsp;play.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The reason we particularly want our algorithms to learn fast here is that &lt;span class="caps"&gt;RL&lt;/span&gt; is most fruitfully applied in contexts where training data is limited &amp;#8212; or where the space of strategies is so large that it would be difficult to explore exhaustively. It is in these regimes that supervised techniques have trouble and &lt;span class="caps"&gt;RL&lt;/span&gt; methods&amp;nbsp;shine.&lt;/p&gt;
&lt;p&gt;In this post, we review one general &lt;span class="caps"&gt;RL&lt;/span&gt; training procedure: The policy-gradient, deep-learning scheme. We review the theory behind this approach in the next section. Following that, we walk through a simple python implementation that trains a neural network to play the game&amp;nbsp;battleship.&lt;/p&gt;
&lt;p&gt;Our python code can be downloaded from our github page, &lt;a href="https://github.com/EFavDB/battleship"&gt;here&lt;/a&gt;. It requires the jupyter, tensorflow, numpy, and matplotlib&amp;nbsp;packages.&lt;/p&gt;
&lt;h3&gt;Policy-gradient, deep &lt;span class="caps"&gt;RL&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Policy-gradient, deep &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms consist of two main components: A policy network and a rewards function. We detail these two below and then describe how they work together to train good&amp;nbsp;models.&lt;/p&gt;
&lt;h4&gt;The policy&amp;nbsp;network&lt;/h4&gt;
&lt;p&gt;The policy for a given deep &lt;span class="caps"&gt;RL&lt;/span&gt; algorithm is a neural network that maps state values &lt;span class="math"&gt;\(s\)&lt;/span&gt; to probabilities for given game actions &lt;span class="math"&gt;\(a\)&lt;/span&gt;. In other words, the input layer of the network accepts a numerical encoding of the environment &amp;#8212; the state of the game at a particular moment. When this input is fed through the network, the values at the output layer correspond to the log probabilities that each of the actions available to us is optimal &amp;#8212; one output node is present for each possible action that we can choose. Note that if we knew with certainty which move we should take, only one output node would have a finite probability. However, if our network is uncertain which action is optimal, more than one output node will have finite&amp;nbsp;weight.&lt;/p&gt;
&lt;p&gt;To illustrate the above, we present a diagram of the network used in our battleship program below. (For a review of the rules of battleship, see footnote [1].) For simplicity, we work with a 1-d battleship grid. We then encode our current knowledge of the environment using one input neuron for each of our opponent&amp;#8217;s grid positions. In particular, we use the following encoding for each neuron /&amp;nbsp;index:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \label{input} \tag{1}
x_{0,i} = \begin{cases}
-1 &amp;amp; \text{Have not yet bombed $i$} \\
\ 0 &amp;amp; \text{Have bombed $i$, no ship} \\
+1 &amp;amp; \text{Have bombed $i$, ship present}.
\end{cases}
\end{align}&lt;/div&gt;
&lt;p&gt;
In our example figure below, we have five input neurons, so the board is of size five. The first three neurons have value &lt;span class="math"&gt;\(-1\)&lt;/span&gt; implying we have not yet bombed those grid points. Finally, the last two are &lt;span class="math"&gt;\(+1\)&lt;/span&gt; and &lt;span class="math"&gt;\(0\)&lt;/span&gt;, respectively, implying that a ship does sit at the fourth site, but not at the&amp;nbsp;fifth.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2016/10/nn.jpg"&gt;&lt;img alt="network" src="https://efavdb.com/wp-content/uploads/2016/10/nn.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note that in the output layer of the policy network shown, the first three values are labeled with log probabilities. These values correspond to the probabilities that we should next bomb each of these indices, respectively. We cannot re-bomb the fourth and fifth grid points, so although the network may output some values to these neurons, we&amp;#8217;ll ignore&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;Before moving on, we note that the reason we use a neural network for our policy is to allow for efficient generalization: For games like Go that have a very large number of states, it is not feasible to collect data on every possible board position. This is exactly the context where &lt;span class="caps"&gt;ML&lt;/span&gt; algorithms excel &amp;#8212; generalizing from past observations to make good predictions for new situations. In order to keep our focus on &lt;span class="caps"&gt;RL&lt;/span&gt;, we won&amp;#8217;t review how &lt;span class="caps"&gt;ML&lt;/span&gt; algorithms work in this post (however, you can check out our &lt;a href="http://efavdb.github.io/archives"&gt;archives&lt;/a&gt; section for relevant primers). Instead we simply note that &amp;#8212; utilizing these tools &amp;#8212; we can get good performance by training only on a &lt;em&gt;representative subset&lt;/em&gt; of games &amp;#8212; allowing us to avoid study of the full set, which can be much&amp;nbsp;larger.&lt;/p&gt;
&lt;h4&gt;The rewards&amp;nbsp;function&lt;/h4&gt;
&lt;p&gt;To train an &lt;span class="caps"&gt;RL&lt;/span&gt; algorithm, we must carry out an iterative game play / scoring process: We play games according to our current policy, selecting moves with frequencies proportional to the probabilities output by the network. If the actions taken resulted in good outcomes, we want to strengthen the probability of those actions going&amp;nbsp;forward.&lt;/p&gt;
&lt;p&gt;The rewards function is the tool we use to formally score our outcomes in past games &amp;#8212; we will encourage our algorithm to try to maximize this quantity during game play. In effect, it is a hyper-parameter for the &lt;span class="caps"&gt;RL&lt;/span&gt; algorithm: many different functions could be used, each resulting in different learning characteristics. For our battleship program, we have used the&amp;nbsp;function
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \label{rewards} \tag{2}
r(a;t_0) = \sum_{t \geq t_0} \left ( h(t) - \overline{h(t)} \right) (0.5)^{t-t0}
\end{align}&lt;/div&gt;
&lt;p&gt;
Given a completed game log, this function looks at the action &lt;span class="math"&gt;\(a\)&lt;/span&gt; taken at time &lt;span class="math"&gt;\(t_0\)&lt;/span&gt; and returns a weighted sum of hit values &lt;span class="math"&gt;\(h(t)\)&lt;/span&gt; for this and all future steps in the game. Here, &lt;span class="math"&gt;\(h(t)\)&lt;/span&gt; is &lt;span class="math"&gt;\(1\)&lt;/span&gt; if we had a hit at step &lt;span class="math"&gt;\(t\)&lt;/span&gt; and is &lt;span class="math"&gt;\(0\)&lt;/span&gt;&amp;nbsp;otherwise.&lt;/p&gt;
&lt;p&gt;In arriving at (\ref{rewards}), we admit that we did not carry out a careful search over the set of all possible rewards functions. However, we have confirmed that this choice results in good game play, and it is well-motivated: In particular, we note that the weighting term &lt;span class="math"&gt;\((0.5)^{t-t0}\)&lt;/span&gt; serves to strongly incentivize a hit on the current move (we get a reward of &lt;span class="math"&gt;\(1\)&lt;/span&gt; for a hit at &lt;span class="math"&gt;\(t_0\)&lt;/span&gt;), but a hit at &lt;span class="math"&gt;\((t_0 + 1)\)&lt;/span&gt; also rewards the action at &lt;span class="math"&gt;\(t_0\)&lt;/span&gt; &amp;#8212; with value &lt;span class="math"&gt;\(0.5\)&lt;/span&gt;. Similarly, a hit at &lt;span class="math"&gt;\((t_0 + 2)\)&lt;/span&gt; rewards &lt;span class="math"&gt;\(0.25\)&lt;/span&gt;, etc. This weighted look-ahead aspect of (\ref{rewards}) serves to encourage efficient exploration of the board: It forces the program to care about moves that will enable future hits. The other ingredient of note present in (\ref{rewards}) is the subtraction of &lt;span class="math"&gt;\(\overline{h(t)}\)&lt;/span&gt;. This is the expected rewards that a random network would obtain. By pulling this out, we only reward our network if it is outperforming random choices &amp;#8212; this results in a net speed-up of the learning&amp;nbsp;process.&lt;/p&gt;
&lt;h4&gt;Stochastic gradient&amp;nbsp;descent&lt;/h4&gt;
&lt;p&gt;In order to train our algorithm to maximize captured rewards during game play, we apply gradient descent. To carry this out, we imagine allowing our network parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; to vary at some particular step in the game. Averaging over all possible actions, the gradient of the expected rewards is then&amp;nbsp;formally,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \nonumber
\partial_{\theta} \langle r(a \vert s) \rangle &amp;amp;\equiv &amp;amp; \partial_{\theta} \int p(a \vert \theta, s) r(a \vert s) da \\ \nonumber
&amp;amp;=&amp;amp; \int p(a \vert \theta, s) r(a \vert s) \partial_{\theta} \log \left ( p(a \vert \theta, s) \right) da \\
&amp;amp;\equiv &amp;amp; \langle r(a \vert s) \partial_{\theta} \log \left ( p(a \vert \theta, s) \right) \rangle. \tag{3} \label{formal_ev}
\end{align}&lt;/div&gt;
&lt;p&gt;
Here, the &lt;span class="math"&gt;\(p(a)\)&lt;/span&gt; values are the action probability outputs of our&amp;nbsp;network.&lt;/p&gt;
&lt;p&gt;Unfortunately, we usually can&amp;#8217;t evaluate the last line above. However, what we can do is approximate it using a sampled value: We simply play a game with our current network, then replace the expected value above by the reward actually captured on the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th&amp;nbsp;move,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\hat{g}_i = r(a_i) \nabla_{\theta} \log p(a_i \vert s_i, \theta). \tag{4} \label{estimator}
\end{align}&lt;/div&gt;
&lt;p&gt;
Here, &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; is the action that was taken, &lt;span class="math"&gt;\(r(a_i)\)&lt;/span&gt; is reward that was captured, and the derivative of the logarithm shown can be evaluated via back-propagation (aside for those experienced with neural networks: this is the derivative of the cross-entropy loss function that would apply if you treated the event like a supervised-learning training example &amp;#8212; with the selected action &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; taken as the label). The function &lt;span class="math"&gt;\(\hat{g}_i\)&lt;/span&gt; provides a noisy estimate of the desired gradient, but taking many steps will result in a &amp;#8220;stochastic&amp;#8221; gradient descent, on average pushing us towards correct rewards&amp;nbsp;maximization.&lt;/p&gt;
&lt;h4&gt;Summary of the training&amp;nbsp;process&lt;/h4&gt;
&lt;p&gt;In summary, then, &lt;span class="caps"&gt;RL&lt;/span&gt; training proceeds iteratively: To initialize an iterative step, we first play a game with our current policy network, selecting moves stochastically according to the network&amp;#8217;s output. After the game is complete, we then score our outcome by evaluating the rewards captured on each move &amp;#8212; for example, in the battleship game we use (\ref{rewards}). Once this is done, we then estimate the gradient of the rewards function using (\ref{estimator}). Finally, we update the network parameters, moving &lt;span class="math"&gt;\(\theta \to \theta + \alpha \sum \hat{g}_i\)&lt;/span&gt;, with &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; a small step size parameter. To continue, we then play a new game with the updated network,&amp;nbsp;etc.&lt;/p&gt;
&lt;p&gt;To see that this process does, in fact, encourage actions that have resulted in good outcomes during training, note that (\ref{estimator}) is proportional to the rewards captured at the step &lt;span class="math"&gt;\(i\)&lt;/span&gt;. Consequently, when we adjust our parameters in the direction of (\ref{estimator}), we will strongly encourage those actions that have resulted in large rewards outcomes. Further, those moves with negative rewards are actually suppressed. In this way, over time, the network will learn to examine the system and suggest those moves that will likely produce the best&amp;nbsp;outcomes.&lt;/p&gt;
&lt;p&gt;That&amp;#8217;s it for the basics of deep, policy-gradient &lt;span class="caps"&gt;RL&lt;/span&gt;. We now turn to our python example,&amp;nbsp;battleship.&lt;/p&gt;
&lt;h3&gt;Python code walkthrough &amp;#8212; battleship &lt;span class="caps"&gt;RL&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Load the needed&amp;nbsp;packages.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pylab&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Define our network &amp;#8212; a fully connected, three layer system. The code below is mostly tensorflow boilerplate that can be picked up by going through their first tutorials. The one unusual thing is that we have our learning rate in (26) set to the placeholder value (9). This will allow us to vary our step sizes with observed rewards captured&amp;nbsp;below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;SHIP_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

&lt;span class="n"&gt;hidden_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt;
&lt;span class="n"&gt;output_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt;

&lt;span class="n"&gt;input_positions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[])&lt;/span&gt;
&lt;span class="c1"&gt;# Generate hidden layer&lt;/span&gt;
&lt;span class="n"&gt;W1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
&lt;span class="n"&gt;b1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;h1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_positions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Second layer -- linear classifier for action logits&lt;/span&gt;
&lt;span class="n"&gt;W2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_units&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
&lt;span class="n"&gt;b2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_units&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b2&lt;/span&gt;
&lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;init&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize_all_variables&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;cross_entropy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sparse_softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;xentropy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;train_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cross_entropy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Start TF session&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next, we define a method that will allow us to play a game using our network. The &lt;span class="caps"&gt;TRAINING&lt;/span&gt; variable specifies whether or not to take the optimal moves or to select moves stochastically. Note that the method returns a set of logs that record the game proceedings. These are needed for&amp;nbsp;training.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;TRAINING&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;play_game&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRAINING&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; Play game of battleship using network.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# Select random location for ship&lt;/span&gt;
    &lt;span class="n"&gt;ship_left&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;SHIP_SIZE&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ship_positions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ship_left&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ship_left&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;SHIP_SIZE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="c1"&gt;# Initialize logs for game&lt;/span&gt;
    &lt;span class="n"&gt;board_position_log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;action_log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;hit_log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="c1"&gt;# Play through game&lt;/span&gt;
    &lt;span class="n"&gt;current_board&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt;&lt;span class="p"&gt;)]]&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hit_log&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;SHIP_SIZE&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;board_position_log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;current_board&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]])&lt;/span&gt;
        &lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;input_positions&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;current_board&lt;/span&gt;&lt;span class="p"&gt;})[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;action_log&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
        &lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;bomb_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;bomb_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# update board, logs&lt;/span&gt;
        &lt;span class="n"&gt;hit_log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bomb_index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ship_positions&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;current_board&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;bomb_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bomb_index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ship_positions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;action_log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bomb_index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;board_position_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hit_log&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our implementation of the rewards function&amp;nbsp;(\ref{rewards}):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rewards_calculator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hit_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gamma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; Discounted sum of future hits over trajectory&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;hit_log_weighted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;
    &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SHIP_SIZE&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hit_log&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BOARD_SIZE&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;gamma&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hit_log&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[((&lt;/span&gt;&lt;span class="n"&gt;gamma&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hit_log_weighted&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hit_log&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally, our training loop. Here, we iteratively play through many games, scoring after each game, then adjusting parameters &amp;#8212; setting the placeholder learning rate equal to &lt;span class="caps"&gt;ALPHA&lt;/span&gt; times the rewards&amp;nbsp;captured.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;game_lengths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;TRAINING&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt; &lt;span class="c1"&gt;# Boolean specifies training mode&lt;/span&gt;
&lt;span class="n"&gt;ALPHA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.06&lt;/span&gt; &lt;span class="c1"&gt;# step size&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;game&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;board_position_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hit_log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;play_game&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRAINING&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;game_lengths&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action_log&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;rewards_log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rewards_calculator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hit_log&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;current_board&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rewards_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;board_position_log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_log&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Take step along gradient&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;TRAINING&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;train_step&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;input_positions&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;current_board&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;ALPHA&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Running this last cell, we see that the training works! The following is an example trace from the play_game() method, with the variable &lt;span class="caps"&gt;TRAINING&lt;/span&gt; set to False. This illustrates an intelligent move selection&amp;nbsp;process.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Example game trace output&lt;/span&gt;
&lt;span class="p"&gt;([[[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]],&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here, the first five lines are the board encodings that the network was fed each step &amp;#8212; using (\ref{input}). The second to last row presents the sequential grid selections that were chosen. Finally, the last row is the hit log. Notice that the first two moves nicely sample different regions of the board. After this, a hit was recorded at &lt;span class="math"&gt;\(6\)&lt;/span&gt;. The algorithm then intelligently selects &lt;span class="math"&gt;\(7\)&lt;/span&gt; and &lt;span class="math"&gt;\(8\)&lt;/span&gt;, which it can infer must be the final locations of the&amp;nbsp;ship.&lt;/p&gt;
&lt;p&gt;The plot below provides further characterization of the learning process. This shows the running average game length (steps required to fully bomb ship) versus training epoch. The program learns the basics quite quickly, then continues to gradually improve over time&amp;nbsp;[2].&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2016/10/trace.jpg"&gt;&lt;img alt="trace" src="https://efavdb.com/wp-content/uploads/2016/10/trace.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;In this post, we have covered a variant of &lt;span class="caps"&gt;RL&lt;/span&gt; &amp;#8212; namely, the policy-gradient, deep &lt;span class="caps"&gt;RL&lt;/span&gt; scheme. This is a method that typically defaults to the currently best-known strategy, but occasionally samples from other approaches, ultimately resulting in an iterative improvement in policy. The two main ingredients here are the policy network and the rewards function. Although network architecture design is usually the place where most of the thinking is involved in supervised learning, it is the rewards function that typically requires the most thought in the &lt;span class="caps"&gt;RL&lt;/span&gt; context. A good choice should be as local in time as possible, so as to facilitate training (distant forecast dependence will result in a slow learning process). However, the rewards function should also directly attack the ultimate end of the process (&amp;#8220;winning&amp;#8221; the game &amp;#8212; encouragement of side quests that aren&amp;#8217;t necessary can often occur if care is not taken). Balancing these two competing demands can be a challenge, and rewards function design is therefore something of an art&amp;nbsp;form.&lt;/p&gt;
&lt;p&gt;Our brief introduction here was intended only to illustrate the gist of how &lt;span class="caps"&gt;RL&lt;/span&gt; is carried out in practice. For further details, we can recommend two resources: the text book by Sutton and Barto [3] and a recent talk by John Schulman&amp;nbsp;[4].&lt;/p&gt;
&lt;h3&gt;Footnotes and&amp;nbsp;references&lt;/h3&gt;
&lt;p&gt;[1] Game rules: Battleship is a two-player game. Both players begin with a finite regular grid of positions &amp;#8212; hidden from their opponent &amp;#8212; and a set of &amp;#8220;ships&amp;#8221;. Each player receives the same quantity of each type of ship. At the start of the game, each player places the ships on their grid in whatever locations they like, subject to some constraints: A ship of length 2, say, must occupy two contiguous indices on the board, and no two ships can occupy the same grid location. Once placed, the ships are fixed in position for the remainder of the game. At this point, game play begins, with the goal being to sink the opponent ships. The locations of the enemy ships are initially unknown because we cannot see the opponent&amp;#8217;s grid. To find the ships, one &amp;#8220;bombs&amp;#8221; indices on the enemy grid &amp;#8212; with bombing occurs in turns. When an opponent index is bombed, the opponent must truthfully state whether or not a ship was located at the index bombed. Whoever succeeds in bombing all their opponent&amp;#8217;s occupied indices first wins the game. Therefore, the problem reduces to finding the enemy ship indices as quickly as&amp;nbsp;possible.&lt;/p&gt;
&lt;p&gt;[2] One of my colleagues (&lt;span class="caps"&gt;HC&lt;/span&gt;) has suggested that the program likely begins to overfit at some point. However, the 1-d version of the game has so few possible ship locations that characterization of this effect via a training and test set split does not seem appropriate. However, this approach could work were we to move to higher dimensions and introduce multiple&amp;nbsp;ships.&lt;/p&gt;
&lt;p&gt;[3] Sutton and Barto, (2016). &amp;#8220;Reinforcement Learning: An Introduction&amp;#8221;. Text site, &lt;a href="https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[4] John Schulman, (2016). &amp;#8220;Bay Area Deep Learning School&amp;#8221;. Youtube recording of talk available &lt;a href="https://www.youtube.com/watch?v=9dXiAecyJrY"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>Bayesian Statistics: MCMC</title><link href="https://efavdb.com/metropolis" rel="alternate"></link><published>2016-08-07T18:37:00-07:00</published><updated>2016-08-07T18:37:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2016-08-07:/metropolis</id><summary type="html">&lt;p&gt;We review the Metropolis algorithm &amp;#8212; a simple Markov Chain Monte Carlo (&lt;span class="caps"&gt;MCMC&lt;/span&gt;) sampling method &amp;#8212; and its application to estimating posteriors in Bayesian statistics. A simple python example is&amp;nbsp;provided.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;One of the central aims of statistics is to identify good methods for fitting models to data. One way to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We review the Metropolis algorithm &amp;#8212; a simple Markov Chain Monte Carlo (&lt;span class="caps"&gt;MCMC&lt;/span&gt;) sampling method &amp;#8212; and its application to estimating posteriors in Bayesian statistics. A simple python example is&amp;nbsp;provided.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;One of the central aims of statistics is to identify good methods for fitting models to data. One way to do this is through the use of Bayes&amp;#8217; rule: If &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; is a vector of &lt;span class="math"&gt;\(k\)&lt;/span&gt; samples from a distribution and &lt;span class="math"&gt;\(\textbf{z}\)&lt;/span&gt; is a vector of model parameters, Bayes&amp;#8217; rule&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{1} \label{Bayes}
p(\textbf{z} \vert \textbf{x}) = \frac{p(\textbf{x} \vert \textbf{z}) p(\textbf{z})}{p(\textbf{x})}.
\end{align}&lt;/div&gt;
&lt;p&gt;
Here, the probability at left, &lt;span class="math"&gt;\(p(\textbf{z} \vert \textbf{x})\)&lt;/span&gt; &amp;#8212; the &amp;#8220;posterior&amp;#8221; &amp;#8212; is a function that tells us how likely it is that the underlying true parameter values are &lt;span class="math"&gt;\(\textbf{z}\)&lt;/span&gt;, given the information provided by our observations &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;. Notice that if we could solve for this function, we would be able to identify which parameter values are most likely &amp;#8212; those that are good candidates for a fit. We could also use the posterior&amp;#8217;s variance to quantify how uncertain we are about the true, underlying parameter&amp;nbsp;values.&lt;/p&gt;
&lt;p&gt;Bayes&amp;#8217; rule gives us a method for evaluating the posterior &amp;#8212; now our goal: We need only evaluate the right side of (\ref{Bayes}). The quantities shown there&amp;nbsp;are&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p(\textbf{x} \vert \textbf{z})\)&lt;/span&gt; &amp;#8212; This is the probability of seeing &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; at fixed parameter values &lt;span class="math"&gt;\(\textbf{z}\)&lt;/span&gt;. Note that if the model is specified, we can often immediately write this part down. For example, if we have a Normal distribution model, specifying &lt;span class="math"&gt;\(\textbf{z}\)&lt;/span&gt; means that we have specified the Normal&amp;#8217;s mean and variance. Given these, we can say how likely it is to observe any &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p(\textbf{z})\)&lt;/span&gt; &amp;#8212; the &amp;#8220;prior&amp;#8221;. This is something we insert by hand before taking any data. We choose its form so that it covers the values we expect are reasonable for the parameters in&amp;nbsp;question.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p(\textbf{x})\)&lt;/span&gt; &amp;#8212; the denominator. Notice that this doesn&amp;#8217;t depend on &lt;span class="math"&gt;\(\textbf{z}\)&lt;/span&gt;, and so represents a normalization constant for the&amp;nbsp;posterior.&lt;/p&gt;
&lt;p&gt;It turns out that the last term above can sometimes be difficult to evaluate analytically, and so we must often resort to numerical methods for estimating the posterior. Monte Carlo sampling is one of the most common approaches taken for doing this. The idea behind Monte Carlo is to take many samples &lt;span class="math"&gt;\(\{\textbf{z}_i\}\)&lt;/span&gt; from the posterior (\ref{Bayes}). Once these are obtained, we can approximate population averages by averages over the samples. For example, the true posterior average &lt;span class="math"&gt;\(\langle\textbf{z} \rangle \equiv \int \textbf{z} p(\textbf{z} \vert \textbf{x}) d \textbf{z}\)&lt;/span&gt; can be approximated by &lt;span class="math"&gt;\(\overline{\textbf{z}} \equiv \frac{1}{N}\sum_i \textbf{z}_i\)&lt;/span&gt;, the sample average. By the law of large numbers, the sample averages are guaranteed to approach the distribution averages as &lt;span class="math"&gt;\(N \to \infty\)&lt;/span&gt;. This means that Monte Carlo can always be used to obtain very accurate parameter estimates, provided we take &lt;span class="math"&gt;\(N\)&lt;/span&gt; sufficiently large &amp;#8212; and that we can find a convenient way to sample from the posterior. In this post, we review one simple variant of Monte Carlo that allows for posterior sampling: the Metropolis&amp;nbsp;algorithm.&lt;/p&gt;
&lt;h2&gt;Metropolis&amp;nbsp;Algorithm&lt;/h2&gt;
&lt;h3&gt;Iterative&amp;nbsp;Procedure&lt;/h3&gt;
&lt;p&gt;Metropolis is an iterative, try-accept algorithm. We initialize the algorithm by selecting a parameter vector &lt;span class="math"&gt;\(\textbf{z}\)&lt;/span&gt; at random. Following this, we repeatedly carry out the following two steps to obtain additional posterior&amp;nbsp;samples:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Identify a next candidate sample &lt;span class="math"&gt;\(\textbf{z}_j\)&lt;/span&gt; via some random process. This candidate selection step can be informed by the current sample&amp;#8217;s position, &lt;span class="math"&gt;\(\textbf{z}_i\)&lt;/span&gt;. For example, one could require that the next candidate be selected from those parameter vectors a given step-size distance from the current sample, &lt;span class="math"&gt;\(\textbf{z}_j \in \{\textbf{z}_k: \vert \textbf{z}_i - \textbf{z}_k \vert = \delta \}\)&lt;/span&gt;. However, while the candidate selected can depend on the current sample, it must not depend on any prior history of the sampling process. Whatever the process chosen (there&amp;#8217;s some flexibility here), we write &lt;span class="math"&gt;\(t_{i,j}\)&lt;/span&gt; for the rate of selecting &lt;span class="math"&gt;\(\textbf{z}_j\)&lt;/span&gt; as the next candidate given the current sample is &lt;span class="math"&gt;\(\textbf{z}_i\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Once a candidate is identified, we either accept or reject it via a second random process. If it is accepted, we mark it down as the next sample, then go back to step one, using the current sample to inform the next candidate selection. Otherwise, we mark the current sample down again, taking it as a repeat sample, and then use it to return to candidate search step, as above. Here, we write &lt;span class="math"&gt;\(A_{i,j}\)&lt;/span&gt; for the rate of accepting &lt;span class="math"&gt;\(\textbf{z}_j\)&lt;/span&gt;, given that it was selected as the next candidate, starting from &lt;span class="math"&gt;\(\textbf{z}_i\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Selecting the trial and acceptance&amp;nbsp;rates&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2016/08/Untitled-1.jpg"&gt;&lt;img alt="Untitled-1" src="https://efavdb.com/wp-content/uploads/2016/08/Untitled-1.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In order to ensure that our above process selects samples according to the distribution (\ref{Bayes}), we need to appropriately set the &lt;span class="math"&gt;\(\{t_{i,j}\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\{A_{i,j}\}\)&lt;/span&gt; values. To do that, note that at equilibrium one must see the same number of hops from &lt;span class="math"&gt;\(\textbf{z}_i\)&lt;/span&gt; to &lt;span class="math"&gt;\(\textbf{z}_j\)&lt;/span&gt; as hops from &lt;span class="math"&gt;\(\textbf{z}_j\)&lt;/span&gt; from &lt;span class="math"&gt;\(\textbf{z}_i\)&lt;/span&gt; (if this did not hold, one would see a net shifting of weight from one to the other over time, contradicting the assumption of equilibrium). If &lt;span class="math"&gt;\(\rho_i\)&lt;/span&gt; is the fraction of samples the process takes from state &lt;span class="math"&gt;\(i\)&lt;/span&gt;, this condition can be written&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \label{inter}
\rho_i t_{i,j} A_{i,j} = \rho_j t_{j,i} A_{j,i} \tag{3}
\end{align}&lt;/div&gt;
&lt;p&gt;
To select a process that returns the desired sampling weight, we solve for &lt;span class="math"&gt;\(\rho_i\)&lt;/span&gt; over &lt;span class="math"&gt;\(\rho_j\)&lt;/span&gt; in (\ref{inter}) and then equate this to the ratio required by (\ref{Bayes}). This&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{4} \label{cond}
\frac{\rho_i}{\rho_j} = \frac{t_{j,i} A_{j,i}}{t_{i,j} A_{i,j}}
\equiv \frac{p(\textbf{x} \vert \textbf{z}_i)p(\textbf{z}_i)}{p(\textbf{x} \vert \textbf{z}_j)p(\textbf{z}_j)}.
\end{align}&lt;/div&gt;
&lt;p&gt;
Now, the single constraint above is not sufficient to pin down all of our degrees of freedom. In the Metropolis case, we choose the following working balance: The trial rates between states are set equal, &lt;span class="math"&gt;\(t_{i,j} = t_{j,i}\)&lt;/span&gt; (but remain unspecified &amp;#8212; left to the discretion of the coder on a case-by-case basis), and we&amp;nbsp;set
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{5}
A_{i,j} = \begin{cases}
1, &amp;amp; \text{if } p(\textbf{z}_j \vert \textbf{x}) &amp;gt; p(\textbf{z}_i \vert \textbf{x}) \\
\frac{p(\textbf{x} \vert \textbf{z}_j)p(\textbf{z}_j)}{p(\textbf{x} \vert \textbf{z}_i)p(\textbf{z}_i)} \equiv \frac{p(\textbf{z}_j \vert \textbf{x})}{p(\textbf{z}_i \vert \textbf{x})}, &amp;amp; \text{else}.
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;
This last equation says that we choose to always accept a candidate sample if it is more likely than the current one. However, if the candidate is less likely, we only accept a fraction of the time &amp;#8212; with rate equal to the relative probability ratio of the two states. For example, if the candidate is only &lt;span class="math"&gt;\(80%\)&lt;/span&gt; as likely as the current sample, we accept it &lt;span class="math"&gt;\(80%\)&lt;/span&gt; of the time. That&amp;#8217;s it for Metropolis &amp;#8212; a simple &lt;span class="caps"&gt;MCMC&lt;/span&gt; algorithm, guaranteed to satisfy (\ref{cond}), and to therefore equilibrate to (\ref{Bayes})! An example&amp;nbsp;follows.&lt;/p&gt;
&lt;h3&gt;Coding&amp;nbsp;example&lt;/h3&gt;
&lt;p&gt;The following python snippet illustrates the Metropolis algorithm in action. Here, we take 15 samples from a Normal distribution of variance one and true mean also equal to one. We pretend not to know the mean (but assume we do know the variance), assume a uniform prior for the mean, and then run the algorithm to obtain two hundred thousand samples from the mean&amp;#8217;s posterior. &lt;a href="https://efavdb.com/wp-content/uploads/2016/08/result-1.png"&gt;&lt;img alt="result" src="https://efavdb.com/wp-content/uploads/2016/08/result-1.png"&gt;&lt;/a&gt; The histogram at right summarizes the results, obtained by dropping the first 1% of the samples (to protect against bias towards the initialization value). Averaging over the samples returns a mean estimate of &lt;span class="math"&gt;\(\mu \approx 1.4 \pm 0.5\)&lt;/span&gt; (95% confidence interval), consistent with the true value of &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# Take some samples&lt;/span&gt;
&lt;span class="n"&gt;true_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;true_mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;total_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;200000&lt;/span&gt;

&lt;span class="c1"&gt;# Function used to decide move acceptance&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;posterior_numerator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="kp"&gt;prod&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="kp"&gt;prod&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kp"&gt;prod&lt;/span&gt;

&lt;span class="c1"&gt;# Initialize MCMC, then iterate&lt;/span&gt;
&lt;span class="n"&gt;z1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;posterior_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;z1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;posterior_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;total_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;z_current&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;posterior_samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;z_candidate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;z_current&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;
    &lt;span class="n"&gt;rel_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;posterior_numerator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;z_candidate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;posterior_numerator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_current&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;rel_prob&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;posterior_samples&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_candidate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;trial_toss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;trial_toss&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;rel_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;posterior_samples&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_candidate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;posterior_samples&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_current&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Drop some initial samples and thin&lt;/span&gt;
&lt;span class="n"&gt;thinned_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;posterior_samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2000&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;thinned_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Histogram of MCMC samples&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;To summarize, we have reviewed the application of &lt;span class="caps"&gt;MCMC&lt;/span&gt; to Bayesian statistics. &lt;span class="caps"&gt;MCMC&lt;/span&gt; is a general tool for obtaining samples from a probability distribution. It can be applied whenever one can conveniently specify the relative probability of two states &amp;#8212; and so is particularly apt for situations where only the normalization constant of a distribution is difficult to evaluate, precisely the problem with the posterior (\ref{Bayes}). The method entails carrying out an iterative try-accept algorithm, where the rates of trial and acceptance can be adjusted, but must be balanced so that the equilibrium distribution that results approaches the desired form. The key equation enabling us to strike this balance is (\ref{inter}) &amp;#8212; the zero flux condition (aka the &lt;em&gt;detailed balance&lt;/em&gt; condition to physicists) that holds between states at&amp;nbsp;equilibrium.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>Interpreting the results of linear regression</title><link href="https://efavdb.com/interpret-linear-regression" rel="alternate"></link><published>2016-06-29T14:54:00-07:00</published><updated>2016-06-29T14:54:00-07:00</updated><author><name>Cathy Yeh</name></author><id>tag:efavdb.com,2016-06-29:/interpret-linear-regression</id><summary type="html">&lt;p&gt;Our &lt;a href="http://efavdb.github.io/linear-regression"&gt;last post&lt;/a&gt; showed how to obtain the least-squares solution for linear regression and discussed the idea of sampling variability in the best estimates for the coefficients. In this post, we continue the discussion about uncertainty in linear regression &amp;#8212; both in the estimates of individual linear regression coefficients and the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Our &lt;a href="http://efavdb.github.io/linear-regression"&gt;last post&lt;/a&gt; showed how to obtain the least-squares solution for linear regression and discussed the idea of sampling variability in the best estimates for the coefficients. In this post, we continue the discussion about uncertainty in linear regression &amp;#8212; both in the estimates of individual linear regression coefficients and the quality of the overall&amp;nbsp;fit.&lt;/p&gt;
&lt;p&gt;Specifically, we&amp;#8217;ll discuss how to calculate the 95% confidence intervals and p-values from hypothesis tests that are output by many statistical packages like python&amp;#8217;s statsmodels or R. An example with code is provided at the&amp;nbsp;end.&lt;/p&gt;
&lt;h2&gt;Review&lt;/h2&gt;
&lt;p&gt;We wish to predict a scalar response variable &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; given a vector of predictors &lt;span class="math"&gt;\(\vec{x}_i\)&lt;/span&gt; of dimension &lt;span class="math"&gt;\(K\)&lt;/span&gt;. In linear regression, we assume that &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; is a linear function of &lt;span class="math"&gt;\(\vec{x}_i\)&lt;/span&gt;, parameterized by a set of coefficients &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; and an error term &lt;span class="math"&gt;\(\epsilon_i\)&lt;/span&gt;. The linear model (in matrix format and dropping the arrows over the vectors) for predicting &lt;span class="math"&gt;\(N\)&lt;/span&gt; response variables&amp;nbsp;is
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{1}
y = X\beta + \epsilon.
\end{align}&lt;/div&gt;
&lt;p&gt;The dimensions of each component are: dim(&lt;span class="math"&gt;\(X\)&lt;/span&gt;) = (&lt;span class="math"&gt;\(N\)&lt;/span&gt;,&lt;span class="math"&gt;\(K\)&lt;/span&gt;), dim(&lt;span class="math"&gt;\(\beta\)&lt;/span&gt;) = (&lt;span class="math"&gt;\(K\)&lt;/span&gt;,1), dim(&lt;span class="math"&gt;\(y\)&lt;/span&gt;) = dim(&lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;) = (&lt;span class="math"&gt;\(N\)&lt;/span&gt;,1), where &lt;span class="math"&gt;\(N\)&lt;/span&gt; = # of examples, &lt;span class="math"&gt;\(K\)&lt;/span&gt; = # of regressors / predictors, counting an optional intercept/constant&amp;nbsp;term.&lt;/p&gt;
&lt;p&gt;The ordinary least-squares best estimator of the coefficients, &lt;span class="math"&gt;\(\hat{\beta}\)&lt;/span&gt;, was &lt;a href="http://efavdb.github.io/linear-regression"&gt;derived last time&lt;/a&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{2}\label{optimal}
\hat{\beta} = (X'X)^{-1}X'y,
\end{align}&lt;/div&gt;
&lt;p&gt;where the hat &amp;#8220;^&amp;#8221; denotes an estimator, not a true population&amp;nbsp;parameter.&lt;/p&gt;
&lt;p&gt;(\ref{optimal}) is a point estimate, but fitting different samples of data from the population will cause the best estimators to shift around. The amount of shifting can be explained by the variance-covariance matrix of &lt;span class="math"&gt;\(\hat{\beta}\)&lt;/span&gt;, &lt;a href="http://efavdb.github.io/linear-regression"&gt;also derived&lt;/a&gt; last time (independent of assumptions of&amp;nbsp;normality):
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{3}\label{cov}
cov(\hat{\beta}, \hat{\beta}) = \sigma^2 (X'X)^{-1}.
\end{align}&lt;/div&gt;
&lt;h2&gt;Goodness of fit - &lt;span class="math"&gt;\(R^2\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;To get a better feel for (\ref{cov}), it&amp;#8217;s helpful to rewrite it in terms of the coefficient of determination &lt;span class="math"&gt;\(R^2\)&lt;/span&gt;. &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; measures how much of the variation in the response variable &lt;span class="math"&gt;\(y\)&lt;/span&gt; is explained by variation in the regressors &lt;span class="math"&gt;\(X\)&lt;/span&gt; (as opposed to the unexplained variation from &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The variation in &lt;span class="math"&gt;\(y\)&lt;/span&gt;, i.e. the &amp;#8220;total sum of squares&amp;#8221; &lt;span class="math"&gt;\(SST\)&lt;/span&gt;, can be partitioned into the sum of two terms, &amp;#8220;regression sum of squares&amp;#8221; and &amp;#8220;error sum of squares&amp;#8221;: &lt;span class="math"&gt;\(SST = SSR + SSE\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For convenience, let&amp;#8217;s center &lt;span class="math"&gt;\(y\)&lt;/span&gt; and &lt;span class="math"&gt;\(X\)&lt;/span&gt; around their means, e.g. &lt;span class="math"&gt;\(y \rightarrow y - \bar{y}\)&lt;/span&gt; so that the mean &lt;span class="math"&gt;\(\bar{y}=0\)&lt;/span&gt; for the centered variables.&amp;nbsp;Then,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{4}\label{SS}
SST &amp;amp;= \sum_i^N (y - \bar{y})^2 = y'y \\
SSR &amp;amp;= \sum_i^N (X\hat{\beta} - \bar{y})^2 = \hat{y}'\hat{y} \\
SSE &amp;amp;= \sum_i^N (y - \hat{y})^2 = e'e,
\end{align}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\hat{y} \equiv X\hat{\beta}\)&lt;/span&gt;. Then &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; is defined as the ratio of the regression sum of squares to the total sum of&amp;nbsp;squares:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{5}\label{R2}
R^2 \equiv \frac{SSR}{SST} = 1 - \frac{SSE}{SST}
\end{align}&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(R^2\)&lt;/span&gt; ranges between 0 and 1, with 1 being a perfect fit. According to (\ref{cov}), the variance of a single coefficient &lt;span class="math"&gt;\(\hat{\beta}_k\)&lt;/span&gt; is proportional to the quantity &lt;span class="math"&gt;\((X'X)_{kk}^{-1}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(k\)&lt;/span&gt; denotes the kth diagonal element of &lt;span class="math"&gt;\((X'X)^{-1}\)&lt;/span&gt;, and can be rewritten&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{6}\label{cov2}
var(\hat{\beta}_k) &amp;amp;= \sigma^2 (X'X)_{kk}^{-1} \\ &amp;amp;= \frac{\sigma^2}{(1 - R_k^2)\sum_i^N (x_{ik} - \bar{x}_k)^2},
\end{align}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(R_k^2\)&lt;/span&gt; is the &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; in the regression of the kth variable, &lt;span class="math"&gt;\(x_k\)&lt;/span&gt;, against the other predictors &lt;a href="#A1"&gt;[A1]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The key observation from (\ref{cov2}) is that the precision in the estimator decreases if the fit is made over highly correlated regressors, for which &lt;span class="math"&gt;\(R_k^2\)&lt;/span&gt; approaches 1. This problem of multicollinearity in linear regression will be manifested in our simulated&amp;nbsp;example.&lt;/p&gt;
&lt;p&gt;(\ref{cov2}) is also consistent with the observation from our previous post that, all things being equal, the precision in the estimator increases if the fit is made over a direction of greater variance in the&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;In the next section, &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; will again be useful for interpreting the behavior of one of our test&amp;nbsp;statistics.&lt;/p&gt;
&lt;h2&gt;Calculating test&amp;nbsp;statistics&lt;/h2&gt;
&lt;p&gt;If we assume that the vector of residuals has a multivariate normal distribution, &lt;span class="math"&gt;\(\epsilon \sim N(0, \sigma^2I)\)&lt;/span&gt;, then we can construct test statistics to characterize the uncertainty in the regression. In this section, we&amp;#8217;ll&amp;nbsp;calculate&lt;/p&gt;
&lt;p&gt;​(a) &lt;strong&gt;confidence intervals&lt;/strong&gt; - random intervals around individual estimators &lt;span class="math"&gt;\(\hat{\beta}_k\)&lt;/span&gt; that, if constructed for regressions over multiple samples, would contain the true population parameter, &lt;span class="math"&gt;\(\beta_k\)&lt;/span&gt;, a certain fraction, e.g. 95%, of the time.
(b) &lt;strong&gt;p-value&lt;/strong&gt; - the probability of events as extreme or more extreme than an observed value (a test statistic) occurring under the null hypothesis. If the p-value is less than a given significance level &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; (a common choice is &lt;span class="math"&gt;\(\alpha = 0.05\)&lt;/span&gt;), then the null hypothesis is rejected, e.g. a regression coefficient is said to be&amp;nbsp;significant.&lt;/p&gt;
&lt;p&gt;From the assumption of the distribution of &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;, it follows that &lt;span class="math"&gt;\(\hat{\beta}\)&lt;/span&gt; has a multivariate normal distribution &lt;a href="#A2"&gt;[A2]&lt;/a&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{7}
\hat{\beta} \sim N(\beta, \sigma^2 (X'X)^{-1}).
\end{align}&lt;/div&gt;
&lt;p&gt; To be explicit, a single coefficient, &lt;span class="math"&gt;\(\hat{\beta}_k\)&lt;/span&gt;, is distributed&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{8}
\hat{\beta}_k \sim N(\beta_k, \sigma^2 (X'X)_{kk}^{-1}).
\end{align}&lt;/div&gt;
&lt;p&gt;This variable can be standardized as a&amp;nbsp;z-score:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{9}
z_k = \frac{\hat{\beta}_k - \beta_k}{\sigma^2 (X'X)_{kk}^{-1}} \sim N(0,1)
\end{align}&lt;/div&gt;
&lt;p&gt;In practice, we don&amp;#8217;t know the population parameter, &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;, so we can&amp;#8217;t use the z-score. Instead, we can construct a pivotal quantity, a t-statistic. The t-statistic for &lt;span class="math"&gt;\(\hat{\beta}_k\)&lt;/span&gt; follows a t-distribution with n-K degrees of freedom &lt;a href="#ref1"&gt;[1]&lt;/a&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{10}\label{tstat}
t_{\hat{\beta}_k} = \frac{\hat{\beta}_k - \beta_k}{s(\hat{\beta}_k)} \sim t_{n-K},
\end{align}&lt;/div&gt;
&lt;p&gt; where &lt;span class="math"&gt;\(s(\hat{\beta}_k)\)&lt;/span&gt; is the standard error of &lt;span class="math"&gt;\(\hat{\beta}_k\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{11}
s(\hat{\beta}_k)^2 = \hat{\sigma}^2 (X'X)_{kk}^{-1},
\end{align}&lt;/div&gt;
&lt;p&gt; and &lt;span class="math"&gt;\(\hat{\sigma}^2\)&lt;/span&gt; is the unbiased estimator of &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{12}
\hat{\sigma}^2 = \frac{\epsilon'\epsilon}{n - K}.
\end{align}&lt;/div&gt;
&lt;h3&gt;Confidence intervals around regression&amp;nbsp;coefficients&lt;/h3&gt;
&lt;p&gt;The &lt;span class="math"&gt;\((1-\alpha)\)&lt;/span&gt; confidence interval around an estimator, &lt;span class="math"&gt;\(\hat{\beta}_k \pm \Delta\)&lt;/span&gt;, is defined such that the probability of a random interval containing the true population parameter is &lt;span class="math"&gt;\((1-\alpha)\)&lt;/span&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{13}
P[\hat{\beta}_k - \Delta &amp;lt; \beta_k &amp;lt; \hat{\beta}_k + \Delta ] = 1 - \alpha,
\end{align}&lt;/div&gt;
&lt;p&gt; where &lt;span class="math"&gt;\(\Delta = t_{1-\alpha/2, n-K} s(\hat{\beta}_k)\)&lt;/span&gt;, and &lt;span class="math"&gt;\(t_{1-\alpha/2, n-K}\)&lt;/span&gt; is the &lt;span class="math"&gt;\(\alpha/2\)&lt;/span&gt;-level critical value for the t-distribution with &lt;span class="math"&gt;\(n-K\)&lt;/span&gt; degrees of&amp;nbsp;freedom.&lt;/p&gt;
&lt;h3&gt;t-test for the significance of a&amp;nbsp;predictor&lt;/h3&gt;
&lt;p&gt;Directly related to the calculation of confidence intervals is testing whether a regressor, &lt;span class="math"&gt;\(\hat{\beta}_k\)&lt;/span&gt;, is statistically significant. The t-statistic for the kth regression coefficient under the null hypothesis that &lt;span class="math"&gt;\(x_k\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; are independent follows a t-distribution with n-K degrees of freedom, c.f. (\ref{tstat}) with &lt;span class="math"&gt;\(\beta_k = 0\)&lt;/span&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{14}
t = \frac{\hat{\beta}_k - 0}{s(\hat{\beta}_k)} \sim t_{n-K}.
\end{align}&lt;/div&gt;
&lt;p&gt;We reject the null-hypothesis if &lt;span class="math"&gt;\(P[t] &amp;lt; \alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;According to (\ref{cov2}), &lt;span class="math"&gt;\(s(\hat{\beta}_k)\)&lt;/span&gt; increases with multicollinearity. Hence, the estimator must be more &amp;#8220;extreme&amp;#8221; in order to be statistically significant in the presence of&amp;nbsp;multicollinearity.&lt;/p&gt;
&lt;h3&gt;F-test for the significance of the&amp;nbsp;regression&lt;/h3&gt;
&lt;p&gt;Whereas the t-test considers the significance of a single regressor, the F-test evaluates the significance of the entire regression, where the null hypothesis is that &lt;em&gt;all&lt;/em&gt; the regressors except the constant are equal to zero: &lt;span class="math"&gt;\(\hat{\beta}_1 = \hat{\beta}_2 = ... = \hat{\beta}_{K-1} = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The F-statistic under the null hypothesis follows an F-distribution with {K-1, N-K} degrees of freedom &lt;a href="#ref1"&gt;[1]&lt;/a&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{15}\label{F}
F = \frac{SSR/(K-1)}{SSE/(N-K)} \sim F_{K-1, N-K}.
\end{align}&lt;/div&gt;
&lt;p&gt;It is useful to rewrite the F-statistic in terms of &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; by substituting the expressions from (\ref{&lt;span class="caps"&gt;SS&lt;/span&gt;}) and&amp;nbsp;(\ref{R2}):
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{16}\label{F2}
F = \frac{(N-K) R^2}{(K-1) (1-R^2)}
\end{align}&lt;/div&gt;
&lt;p&gt;Notice how, for fixed &lt;span class="math"&gt;\(R^2\)&lt;/span&gt;, the F-statistic decreases with an increasing number of predictors &lt;span class="math"&gt;\(K\)&lt;/span&gt;. Adding uninformative predictors to the model will decrease the significance of the regression, which motivates parsimony in constructing linear&amp;nbsp;models.&lt;/p&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;With these formulas in hand, let&amp;#8217;s consider the problem of predicting the weight of adult women using some simulated data (loosely based on reality). We&amp;#8217;ll look at two models:
(1) &lt;strong&gt;weight ~ height&lt;/strong&gt;.
As expected, height will be a strong predictor of weight, corroborated by a significant p-value for the coefficient of height in the model.
(2) &lt;strong&gt;weight ~ height + shoe size&lt;/strong&gt;.
Height and shoe size are strongly correlated in the simulated data, while height is still a strong predictor of weight. We&amp;#8217;ll find that neither of the predictors has a significant individual p-value, a consequence of&amp;nbsp;collinearity.&lt;/p&gt;
&lt;p&gt;First, import some libraries. We use &lt;code&gt;statsmodels.api.OLS&lt;/code&gt; for the linear regression since it contains a much more detailed report on the results of the fit than &lt;code&gt;sklearn.linear_model.LinearRegression&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.api&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sm&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next, set the population parameters for the simulated&amp;nbsp;data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mean_height&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;65&lt;/span&gt;
&lt;span class="n"&gt;std_height&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;shoe&lt;/span&gt; &lt;span class="k"&gt;size&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mean_shoe_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;std_shoe_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;correlation&lt;/span&gt; &lt;span class="k"&gt;between&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;shoe&lt;/span&gt; &lt;span class="k"&gt;size&lt;/span&gt;
&lt;span class="n"&gt;r_height_shoe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;98&lt;/span&gt; &lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;shoe&lt;/span&gt; &lt;span class="k"&gt;size&lt;/span&gt; &lt;span class="k"&gt;are&lt;/span&gt; &lt;span class="n"&gt;highly&lt;/span&gt; &lt;span class="n"&gt;correlated&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;covariance&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;shoe&lt;/span&gt; &lt;span class="k"&gt;size&lt;/span&gt;
&lt;span class="n"&gt;var_height_shoe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r_height_shoe&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;std_height&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;std_shoe_size&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;covariance&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;
&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean_height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mean_shoe_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cov&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std_height&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;var_height_shoe&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;var_height_shoe&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std_shoe_size&lt;/span&gt;&lt;span class="p"&gt;)]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Generate the simulated&amp;nbsp;data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="nb"&gt;number&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="k"&gt;data&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;

&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;85&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;shoe&lt;/span&gt; &lt;span class="k"&gt;size&lt;/span&gt;
&lt;span class="n"&gt;X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alone&lt;/span&gt;
&lt;span class="n"&gt;X0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;220&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X0&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Below is the simulated data plotted against each other.
&lt;a href="https://efavdb.com/wp-content/uploads/2016/06/scatter_height_weight_shoesize_cropped.png"&gt;&lt;img alt="scatterplots" src="https://efavdb.com/wp-content/uploads/2016/06/scatter_height_weight_shoesize_cropped.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fit the linear&amp;nbsp;models:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="k"&gt;add&lt;/span&gt; &lt;span class="k"&gt;column&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;ones&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;intercept&lt;/span&gt;
&lt;span class="n"&gt;X0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;OLS&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;stands&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Ordinary&lt;/span&gt; &lt;span class="n"&gt;Least&lt;/span&gt; &lt;span class="n"&gt;Squares&lt;/span&gt;
&lt;span class="n"&gt;sm0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OLS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X0&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;sm1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OLS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Look at the summary report, &lt;code&gt;sm0.summary()&lt;/code&gt;, for the weight ~ height&amp;nbsp;model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;OLS Regression Results&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;span class="err"&gt;Dep. Variable: y R-squared: 0.788&lt;/span&gt;
&lt;span class="c"&gt;Model: OLS Adj. R-squared: 0.776&lt;/span&gt;
&lt;span class="c"&gt;Method: Least Squares F-statistic: 66.87&lt;/span&gt;
&lt;span class="c"&gt;Date: Wed, 29 Jun 2016 Prob (F-statistic): 1.79e-07&lt;/span&gt;
&lt;span class="c"&gt;Time: 14:28:08 Log-Likelihood: -70.020&lt;/span&gt;
&lt;span class="err"&gt;No. Observations: 20 AIC: 144.0&lt;/span&gt;
&lt;span class="err"&gt;Df Residuals: 18 BIC: 146.0&lt;/span&gt;
&lt;span class="err"&gt;Df Model: 1&lt;/span&gt;
&lt;span class="err"&gt;Covariance Type: nonrobust&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;span class="err"&gt;coef std err t P&amp;gt;|t| [95.0% Conf. Int.]&lt;/span&gt;
&lt;span class="err"&gt;------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class="err"&gt;const -265.2764 49.801 -5.327 0.000 -369.905 -160.648&lt;/span&gt;
&lt;span class="err"&gt;x1 6.1857 0.756 8.178 0.000 4.596 7.775&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;span class="c"&gt;Omnibus: 0.006 Durbin-Watson: 2.351&lt;/span&gt;
&lt;span class="err"&gt;Prob(Omnibus): 0.997 Jarque-Bera (JB): 0.126&lt;/span&gt;
&lt;span class="c"&gt;Skew: 0.002 Prob(JB): 0.939&lt;/span&gt;
&lt;span class="c"&gt;Kurtosis: 2.610 Cond. No. 1.73e+03&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The height variable, &lt;code&gt;x1&lt;/code&gt;, is significant according to the t-test, as is the intercept, denoted &lt;code&gt;const&lt;/code&gt; in the report. Also, notice the coefficient used to simulate the dependence of weight on height (&lt;span class="math"&gt;\(\beta_1\)&lt;/span&gt; = 5.5), is contained in the 95% confidence interval of &lt;code&gt;x1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, let&amp;#8217;s look at the summary report, &lt;code&gt;sm1.summary()&lt;/code&gt;, for the weight ~ height + shoe_size&amp;nbsp;model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;OLS Regression Results&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;span class="err"&gt;Dep. Variable: y R-squared: 0.789&lt;/span&gt;
&lt;span class="c"&gt;Model: OLS Adj. R-squared: 0.765&lt;/span&gt;
&lt;span class="c"&gt;Method: Least Squares F-statistic: 31.86&lt;/span&gt;
&lt;span class="c"&gt;Date: Wed, 29 Jun 2016 Prob (F-statistic): 1.78e-06&lt;/span&gt;
&lt;span class="c"&gt;Time: 14:28:08 Log-Likelihood: -69.951&lt;/span&gt;
&lt;span class="err"&gt;No. Observations: 20 AIC: 145.9&lt;/span&gt;
&lt;span class="err"&gt;Df Residuals: 17 BIC: 148.9&lt;/span&gt;
&lt;span class="err"&gt;Df Model: 2&lt;/span&gt;
&lt;span class="err"&gt;Covariance Type: nonrobust&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;span class="err"&gt;coef std err t P&amp;gt;|t| [95.0% Conf. Int.]&lt;/span&gt;
&lt;span class="err"&gt;------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class="err"&gt;const -333.1599 204.601 -1.628 0.122 -764.829 98.510&lt;/span&gt;
&lt;span class="err"&gt;x1 7.4944 3.898 1.923 0.071 -0.729 15.718&lt;/span&gt;
&lt;span class="err"&gt;x2 -2.3090 6.739 -0.343 0.736 -16.527 11.909&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;span class="c"&gt;Omnibus: 0.015 Durbin-Watson: 2.342&lt;/span&gt;
&lt;span class="err"&gt;Prob(Omnibus): 0.993 Jarque-Bera (JB): 0.147&lt;/span&gt;
&lt;span class="c"&gt;Skew: 0.049 Prob(JB): 0.929&lt;/span&gt;
&lt;span class="c"&gt;Kurtosis: 2.592 Cond. No. 7.00e+03&lt;/span&gt;
&lt;span class="err"&gt;==============================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Neither of the regressors &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt; is significant at a significance level of &lt;span class="math"&gt;\(\alpha=0.05\)&lt;/span&gt;. In the simulated data, adult female weight has a positive linear correlation with height and shoe size, but the strong collinearity of the predictors (simulated with a correlation coefficient of 0.98) causes each variable to fail a t-test in the model &amp;#8212; and even results in the wrong sign for the dependence on shoe&amp;nbsp;size.&lt;/p&gt;
&lt;p&gt;Although the predictors fail individual t-tests, the overall regression &lt;em&gt;is&lt;/em&gt; significant, i.e. the predictors are jointly informative, according to the&amp;nbsp;F-test.&lt;/p&gt;
&lt;p&gt;Notice, however, that the p-value of the F-test has decreased compared to the simple linear model, as expected from (\ref{F2}), since including the extra variable, shoe size, did not improve &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; but did increase &lt;span class="math"&gt;\(K\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s manually calculate the standard error, t-statistics, F-statistic, corresponding p-values, and confidence intervals using the equations from&amp;nbsp;above.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;OLS&lt;/span&gt; &lt;span class="n"&gt;solution&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eqn&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;form&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;X)*beta_hat = X&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;
&lt;span class="n"&gt;beta_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;solve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;residuals&lt;/span&gt;
&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;degrees&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;freedom&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;residuals&lt;/span&gt;
&lt;span class="n"&gt;dof&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;estimator&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;
&lt;span class="n"&gt;sigma_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;dof&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;standard&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;
&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigma_hat&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="mi"&gt;95&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;confidence&lt;/span&gt; &lt;span class="n"&gt;intervals&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="o"&gt;+/-&lt;/span&gt;&lt;span class="n"&gt;t_&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;interval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dof&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;conf_intervals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;interval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;95&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dof&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;statistics&lt;/span&gt; &lt;span class="k"&gt;under&lt;/span&gt; &lt;span class="k"&gt;null&lt;/span&gt; &lt;span class="n"&gt;hypothesis&lt;/span&gt;
&lt;span class="n"&gt;t_stat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;values&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;survival&lt;/span&gt; &lt;span class="k"&gt;function&lt;/span&gt; &lt;span class="n"&gt;sf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;CDF&lt;/span&gt;
&lt;span class="n"&gt;p_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t_stat&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dof&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;SSR&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;regression&lt;/span&gt; &lt;span class="k"&gt;sum&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;squares&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mean_SSR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y_hat&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_mu&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_hat&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_mu&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;
&lt;span class="n"&gt;f_stat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_SSR&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_hat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;f-statistic:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f_stat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;
&lt;span class="n"&gt;p_values_f_stat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f_stat&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dfn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dfd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dof&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;p-value of f-statistic:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p_values_f_stat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The output values, below, from printing the manual calculations are consistent with the summary&amp;nbsp;report:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;333&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;15990097&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;49444671&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;30898743&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;degrees&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;freedom&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;residuals&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;17&lt;/span&gt;

&lt;span class="n"&gt;sigma_hat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;66991550428&lt;/span&gt;

&lt;span class="n"&gt;standard&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;204&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;60056111&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;89776076&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;73900599&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;confidence&lt;/span&gt; &lt;span class="n"&gt;intervals&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;64829352&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;85095501&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;29109662&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;57180031&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;65270473&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;19090724&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;statistics&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;62834305&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;92275698&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;34263027&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;values&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;statistics&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1218417&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;07142839&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;73607656&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;31&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;8556171105&lt;/span&gt;

&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;77777555162&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;06&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The full code is available as an &lt;a href="https://github.com/EFavDB/linear-regression"&gt;IPython notebook on github&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Assuming a multivariate normal distribution for the residuals in linear regression allows us to construct test statistics and therefore specify uncertainty in our&amp;nbsp;fits.&lt;/p&gt;
&lt;p&gt;A t-test judges the explanatory power of a predictor in isolation, although the standard error that appears in the calculation of the t-statistic is a function of the other predictors in the model. On the other hand, an F-test is a global test that judges the explanatory power of all the predictors together, and we&amp;#8217;ve seen that parsimony in choosing predictors can improve the quality of the overall&amp;nbsp;regression.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ve also seen that multicollinearity can throw off the results of individual t-tests as well as obscure the interpretation of the signs of the fitted coefficients. A symptom of multicollinearity is when none of the individual coefficients are significant but the overall F-test is&amp;nbsp;significant.&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;p&gt;[1] Greene, W., Econometric Analysis, Seventh edition, Prentice Hall, 2011 - &lt;a href="http://people.stern.nyu.edu/wgreene/MathStat/Outline.htm"&gt;chapters available&amp;nbsp;online&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Appendix&lt;/h3&gt;
&lt;p&gt;[A1]
We specifically want the kth diagonal element from the inverse moment matrix, &lt;span class="math"&gt;\((X'X)^{-1}\)&lt;/span&gt;. The matrix &lt;span class="math"&gt;\(X\)&lt;/span&gt; can be &lt;a href="https://en.wikipedia.org/wiki/Block_matrix"&gt;partitioned&lt;/a&gt;&amp;nbsp;as &lt;/p&gt;
&lt;div class="math"&gt;$$[X_{(k)} \vec{x}_k],$$&lt;/div&gt;
&lt;p&gt; where &lt;span class="math"&gt;\(\vec{x}_k\)&lt;/span&gt; is an N x 1 column vector containing the kth variable of each of the N samples, and &lt;span class="math"&gt;\(X_{(k)}\)&lt;/span&gt; is the N x (K-1) matrix containing the rest of the variables and constant intercept. For convenience, let &lt;span class="math"&gt;\(X_{(k)}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{x}_k\)&lt;/span&gt; be centered about their (column-wise)&amp;nbsp;means.&lt;/p&gt;
&lt;p&gt;Matrix multiplication of the block-partitioned form of &lt;span class="math"&gt;\(X\)&lt;/span&gt; with its transpose results in the following block&amp;nbsp;matrix:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
(X'X) =
\begin{bmatrix}
X_{(k)}'X_{(k)} &amp;amp; X_{(k)}'\vec{x}_k \
\vec{x}_k'X_{(k)} &amp;amp; \vec{x}_k'\vec{x}_k
\end{bmatrix}
\end{align}&lt;/div&gt;
&lt;p&gt;The above matrix has four blocks, and &lt;a href="https://en.wikipedia.org/wiki/Block_matrix#Block_matrix_inversion"&gt;can be inverted blockwise&lt;/a&gt; to obtain another matrix with four blocks. The lower right block corresponding to the kth diagonal element of the inverted matrix is a&amp;nbsp;scalar:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
(X'X)^{-1}_{kk} &amp;amp;= [\vec{x}_k'\vec{x}_k - \vec{x}_k'X_{(k)}(X_{(k)}'X_{(k)})^{-1}X_{(k)}'\vec{x}_k]^{-1} \\
&amp;amp;= \left[\vec{x}_k'\vec{x}_k \left( 1 - \frac{\vec{x}_k'X_{(k)}(X_{(k)}'X_{(k)})^{-1}X_{(k)}'\vec{x}_k}{\vec{x}_k'\vec{x}_k} \right)\right]^{-1}
\end{align}&lt;/div&gt;
&lt;p&gt;Then the numerator of the fraction in the parentheses above can be&amp;nbsp;simplified:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\vec{x}_k'X_{(k)} ((X_{(k)}'X_{(k)})^{-1}X_{(k)}'\vec{x}_k) &amp;amp;= \vec{x}_k' X_{(k)} \hat{\beta}_{(k)} \\
&amp;amp;= (X_{(k)}\hat{\beta}_{(k)} + \epsilon_k)'X_{(k)}\hat{\beta}_{(k)} \\
&amp;amp;= \hat{x}_k'\hat{x}_k,
\end{align}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\hat{\beta}_{(k)}\)&lt;/span&gt; is the &lt;span class="caps"&gt;OLS&lt;/span&gt; solution for the coefficients in the regression on the &lt;span class="math"&gt;\(\vec{x}_k\)&lt;/span&gt; by the remaining variables &lt;span class="math"&gt;\(X_{(k)}\)&lt;/span&gt;: &lt;span class="math"&gt;\(\vec{x}_k = X_{(k)} \beta_{(k)} + \epsilon_k\)&lt;/span&gt;. In the last line, we used one of the constraints on the residuals &amp;#8212; that the residuals and predictors are uncorrelated, &lt;span class="math"&gt;\(\epsilon_k'X_{(k)} = 0\)&lt;/span&gt;. Plugging in this simplification for the numerator and using the definition of &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; from (\ref{R2}), we obtain our final&amp;nbsp;result:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
(X'X)^{-1}_{kk} &amp;amp;= \left[\vec{x}_k'\vec{x}_k \left( 1 - \frac{\hat{x}_k'\hat{x}_k}{\vec{x}_k'\vec{x}_k} \right)\right]^{-1} \\
&amp;amp;= \left[\vec{x}_k'\vec{x}_k ( 1 - R_k^2 )\right]^{-1}
\end{align}&lt;/div&gt;
&lt;p&gt;[A2]
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\hat{\beta} &amp;amp;= (X'X)^{-1}X'y \\
&amp;amp;= (X'X)^{-1}X'(X\beta + \epsilon) \\
&amp;amp;= \beta + (X'X)^{-1}X'N(0, \sigma^2I) \\
&amp;amp; \sim N(\beta, \sigma^2 (X'X)^{-1})
\end{align}&lt;/div&gt;
&lt;p&gt; The last line is by properties of &lt;a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Affine_transformation"&gt;affine transformations on multivariate normal distributions&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category><category term="statistics"></category></entry><entry><title>Linear Regression</title><link href="https://efavdb.com/linear-regression" rel="alternate"></link><published>2016-05-29T11:27:00-07:00</published><updated>2016-05-29T11:27:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2016-05-29:/linear-regression</id><summary type="html">&lt;p&gt;We review classical linear regression using vector-matrix notation. In particular, we derive a) the least-squares solution, b) the fit&amp;#8217;s coefficient covariance matrix &amp;#8212; showing that the coefficient estimates are most precise along directions that have been sampled over a large range of values (the high variance directions, a la &lt;span class="caps"&gt;PCA …&lt;/span&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;We review classical linear regression using vector-matrix notation. In particular, we derive a) the least-squares solution, b) the fit&amp;#8217;s coefficient covariance matrix &amp;#8212; showing that the coefficient estimates are most precise along directions that have been sampled over a large range of values (the high variance directions, a la &lt;span class="caps"&gt;PCA&lt;/span&gt;), and c) an unbiased estimate for the underlying sample variance (assuming normal sample variance in this last case). We then review how these last two results can be used to provide confidence intervals / hypothesis tests for the coefficient estimates. Finally, we show that similar results follow from a Bayesian&amp;nbsp;approach.&lt;/p&gt;
&lt;p&gt;Last edited July 23,&amp;nbsp;2016.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Here, we consider the problem of fitting a linear curve to &lt;span class="math"&gt;\(N\)&lt;/span&gt; data points of the form &lt;span class="math"&gt;\((\vec{x}_i, y_i),\)&lt;/span&gt; where the &lt;span class="math"&gt;\(\{\vec{x}_i\}\)&lt;/span&gt; are column vectors of predictors that sit in an &lt;span class="math"&gt;\(L\)&lt;/span&gt;-dimensional space and the &lt;span class="math"&gt;\(\{y_i\}\)&lt;/span&gt; are the response values we wish to predict given the &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;. The linear approximation will be defined by a set of coefficients, &lt;span class="math"&gt;\(\{\beta_j\}\)&lt;/span&gt; so&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\hat{y}_i \equiv \sum_j x_{i,j} \beta_j = \vec{x}_i^T \cdot \vec{\beta} . \tag{1} \label{1}
\end{align}&lt;/div&gt;
&lt;p&gt;
We seek the &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; that minimizes the average squared &lt;span class="math"&gt;\(y\)&lt;/span&gt;&amp;nbsp;error,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{2} \label{2}
J = \sum_i \left ( y_i - \hat{y}_i \right)^2 = \sum_i \left (y_i - \vec{x}_i^T \cdot \vec{\beta} \right)^2.
\end{align}&lt;/div&gt;
&lt;p&gt;
It turns out that this is a problem where one can easily derive an analytic expression for the optimal solution. It&amp;#8217;s also possible to derive an expression for the variance in the optimal solution &amp;#8212; that is, how much we might expect the optimal parameter estimates to change were we to start with some other &lt;span class="math"&gt;\(N\)&lt;/span&gt; data points instead. These estimates can then be used to generate confidence intervals for the coefficient estimates. Here, we review these results, give a simple interpretation to the theoretical variance, and finally show that the same results follow from a Bayesian&amp;nbsp;approach.&lt;/p&gt;
&lt;h3&gt;Optimal&amp;nbsp;solution&lt;/h3&gt;
&lt;p&gt;We seek the coefficient vector that minimizes (\ref{2}). We can find this by differentiating this cost function with respect to &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;, setting the result to zero. This&amp;nbsp;gives,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \tag{3}
\partial_{\beta_j} J = 2 \sum_i \left (y_i - \sum_k x_{i,k} \beta_k \right) x_{i,j} = 0.
\end{align}&lt;/div&gt;
&lt;p&gt;
We next define the matrix &lt;span class="math"&gt;\(X\)&lt;/span&gt; so that &lt;span class="math"&gt;\(X_{i,j} = \vec{x}_{i,j}\)&lt;/span&gt;. Plugging this into the above, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\partial_{\beta_j} J &amp;amp;= 2 \sum_i X_{j,i}^T \left (y_i - \sum_k X_{i,k} \beta_k \right) = 0 \\
&amp;amp;= X^T \cdot \left ( \vec{y} - X \cdot \vec{\beta}\right ) = 0.\tag{4}
\end{align}&lt;/div&gt;
&lt;p&gt;
Rearranging&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
X^T X \cdot \vec{\beta} = X^T \cdot \vec{y} \to
\vec{\beta} = (X^T X)^{-1} \cdot X^T \cdot \vec{y} \tag{5} \label{optimal}
\end{align}&lt;/div&gt;
&lt;p&gt;
This is the squared-error-minimizing&amp;nbsp;solution.&lt;/p&gt;
&lt;h3&gt;Parameter covariance&amp;nbsp;matrix&lt;/h3&gt;
&lt;p&gt;Now, when one carries out a linear fit to some data, the best line often does not go straight through all of the data. Here, we consider the case where the reason for the discrepancy is not that the posited linear form is incorrect, but that there are some hidden variables not measured that the &lt;span class="math"&gt;\(y\)&lt;/span&gt;-values also depend on. Assuming our data points represent random samples over these hidden variables, we can model their effect as adding a random noise term to the form (\ref{1}), so&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{6} \label{noise}
y_i = \vec{x}_i^T \cdot \vec{\beta}_{true} + \epsilon_i,
\end{align}&lt;/div&gt;
&lt;p&gt;
with &lt;span class="math"&gt;\(\langle \epsilon_i \rangle =0\)&lt;/span&gt;, &lt;span class="math"&gt;\(\langle \epsilon_i^2 \rangle = \sigma^2\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\vec{\beta}_{true}\)&lt;/span&gt; the exact (but unknown) coefficient&amp;nbsp;vector.&lt;/p&gt;
&lt;p&gt;Plugging (\ref{noise}) into (\ref{optimal}), we see that &lt;span class="math"&gt;\(\langle \vec{\beta} \rangle = \vec{\beta}_{true}\)&lt;/span&gt;. However, the variance of the &lt;span class="math"&gt;\(\epsilon_i\)&lt;/span&gt; injects some uncertainty into our fit: Each realization of the noise will generate slightly different &lt;span class="math"&gt;\(y\)&lt;/span&gt; values, causing the &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; fit coefficients to vary. To estimate the magnitude of this effect, we can calculate the covariance matrix of &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;. At fixed (constant) &lt;span class="math"&gt;\(X\)&lt;/span&gt;, plugging in (\ref{optimal}) for &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
cov(\vec{\beta}, \vec{\beta}) &amp;amp;= cov \left( (X^T X)^{-1} \cdot X^T \cdot \vec{y} , \vec{y}^T \cdot X \cdot (X^T X)^{-1, T} \right) \\
&amp;amp;= (X^T X)^{-1} \cdot X^T \cdot cov(\vec{y}^T, \vec{y} ) \cdot X \cdot (X^T X)^{-1, T}
\\
&amp;amp;= \sigma^2 \left( X^T X \right)^{-1} \cdot X^T X \cdot \left( X^T X \right)^{-1, T} \\
&amp;amp;= \sigma^2 \left( X^T X \right)^{-1}. \tag{7} \label{cov}
\end{align}&lt;/div&gt;
&lt;p&gt;
In the third line here, note that we have assumed that the &lt;span class="math"&gt;\(\epsilon_i\)&lt;/span&gt; are independent, so that &lt;span class="math"&gt;\(cov(\vec{y},\vec{y}) = \sigma^2 I.\)&lt;/span&gt; We&amp;#8217;ve also used the fact that &lt;span class="math"&gt;\(X^T X\)&lt;/span&gt; is&amp;nbsp;symmetric.&lt;/p&gt;
&lt;p&gt;To get a feel for the significance of (\ref{cov}), it is helpful to consider the case where the average &lt;span class="math"&gt;\(x\)&lt;/span&gt; values are zero. In this&amp;nbsp;case,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\left( X^T X \right)_{i,j} &amp;amp;\equiv&amp;amp; \sum_k \delta X_{k,i} \delta X_{k,j} \equiv N \times \langle x_i, x_j\rangle. \tag{8} \label{corr_mat}
\end{align}&lt;/div&gt;
&lt;p&gt;
&lt;a href="https://efavdb.com/wp-content/uploads/2016/05/scatter.jpg"&gt;&lt;img alt="margin around decision boundary" src="https://efavdb.com/wp-content/uploads/2016/05/scatter.jpg"&gt;&lt;/a&gt; That is, &lt;span class="math"&gt;\(X^T X\)&lt;/span&gt; is proportional to the correlation matrix of our &lt;span class="math"&gt;\(x\)&lt;/span&gt; values. This correlation matrix is real and symmetric, and thus has an orthonormal set of eigenvectors. The eigenvalue corresponding to the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th eigenvector gives the variance of our data set&amp;#8217;s &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th component values in this basis &amp;#8212; details can be found in our &lt;a href="http://efavdb.github.io/principal-component-analysis"&gt;article on &lt;span class="caps"&gt;PCA&lt;/span&gt;&lt;/a&gt;. This implies a simple interpretation of (\ref{cov}): The variance in the &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; coefficients will be lowest for predictors parallel to the highest variance &lt;span class="caps"&gt;PCA&lt;/span&gt; components (eg &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; in the figure shown) and highest for predictors parallel to the lowest variance &lt;span class="caps"&gt;PCA&lt;/span&gt; components (&lt;span class="math"&gt;\(x_2\)&lt;/span&gt; in the figure). This observation can often be exploited during an experiment&amp;#8217;s design: If a particular coefficient is desired to high accuracy, one should make sure to sample the corresponding predictor over a wide&amp;nbsp;range.&lt;/p&gt;
&lt;p&gt;[Note: Cathy gives an interesting, alternative interpretation for the parameter estimate variances in a follow-up post, &lt;a href="http://efavdb.github.io/interpret-linear-regression"&gt;here&lt;/a&gt;.]&lt;/p&gt;
&lt;h3&gt;Unbiased estimator for &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The result (\ref{cov}) gives an expression for the variance of the parameter coefficients in terms of the underlying sample variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;. In practice, &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; is often not provided and must be estimated from the observations at hand. Assuming that the &lt;span class="math"&gt;\(\{\epsilon_i\}\)&lt;/span&gt; in (\ref{noise}) are independent &lt;span class="math"&gt;\(\mathcal{N}(0, \sigma^2)\)&lt;/span&gt; random variables, we now show that the following provides an unbiased estimate for this&amp;nbsp;variance:
&lt;/p&gt;
&lt;div class="math"&gt;$$
S^2 \equiv \frac{1}{N-L} \sum_i \left ( y_i - \vec{x}_i^T \cdot \vec{\beta} \right) ^2. \tag{9} \label{S}
$$&lt;/div&gt;
&lt;p&gt;
Note that this is a normalized sum of squared residuals from our fit, with &lt;span class="math"&gt;\((N-L)\)&lt;/span&gt; as the normalization constant &amp;#8212; the number of samples minus the number of fit parameters. To prove that &lt;span class="math"&gt;\(\langle S^2 \rangle = \sigma^2\)&lt;/span&gt;, we plug in (\ref{optimal}) for &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;, combining with (\ref{noise}) for &lt;span class="math"&gt;\(\vec{y}\)&lt;/span&gt;. This&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \nonumber
S^2 &amp;amp;= \frac{1}{N-L} \sum_i \left ( y_i - \vec{x}_i^T \cdot (X^T X)^{-1} \cdot X^T \cdot \{ X \cdot \vec{\beta}_{true} + \vec{\epsilon} \} \right) ^2 \\ \nonumber
&amp;amp;= \frac{1}{N-L} \sum_i \left ( \{y_i - \vec{x}_i^T \cdot\vec{\beta}_{true} \} - \vec{x}_i^T \cdot (X^T X)^{-1} \cdot X^T \cdot \vec{\epsilon} \right) ^2 \\
&amp;amp;= \frac{1}{N-L} \sum_i \left ( \epsilon_i - \vec{x}_i^T \cdot (X^T X)^{-1} \cdot X^T \cdot \vec{\epsilon} \right) ^2 \tag{10}. \label{S2}
\end{align}&lt;/div&gt;
&lt;p&gt;
The second term in the last line is the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th component of the&amp;nbsp;vector
&lt;/p&gt;
&lt;div class="math"&gt;$$
X \cdot (X^T X)^{-1} \cdot X^T \cdot \vec{\epsilon} \equiv \mathbb{P} \cdot \vec{\epsilon}. \tag{11} \label{projection}
$$&lt;/div&gt;
&lt;p&gt;
Here, &lt;span class="math"&gt;\(\mathbb{P}\)&lt;/span&gt; is a projection operator &amp;#8212; this follows from the fact that &lt;span class="math"&gt;\(\mathbb{P}^2 = \mathbb{P}\)&lt;/span&gt;. When it appears in (\ref{projection}), &lt;span class="math"&gt;\(\mathbb{P}\)&lt;/span&gt; maps &lt;span class="math"&gt;\(\vec{\epsilon}\)&lt;/span&gt; into the &lt;span class="math"&gt;\(L\)&lt;/span&gt;-dimensional coordinate space spanned by the &lt;span class="math"&gt;\(\{\vec{x_i}\}\)&lt;/span&gt;, scales the result using (\ref{corr_mat}), then maps it back into its original &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dimensional space. The net effect is to project &lt;span class="math"&gt;\(\vec{\epsilon}\)&lt;/span&gt; into an &lt;span class="math"&gt;\(L\)&lt;/span&gt;-dimensional subspace of the full &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dimensional space (more on the &lt;span class="math"&gt;\(L\)&lt;/span&gt;-dimensional subspace just below). Plugging (\ref{projection}) into (\ref{S2}), we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$
S^2 = \frac{1}{N-L} \sum_i \left ( \epsilon_i - (\mathbb{P} \cdot \vec{\epsilon})_i \right)^2 \equiv \frac{1}{N-L} \left \vert \vec{\epsilon} - \mathbb{P} \cdot \vec{\epsilon} \right \vert^2. \label{S3} \tag{12}
$$&lt;/div&gt;
&lt;p&gt;
This final form gives the result: &lt;span class="math"&gt;\(\vec{\epsilon}\)&lt;/span&gt; is an &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dimensional vector of independent, &lt;span class="math"&gt;\(\mathcal{N}(0, \sigma^2)\)&lt;/span&gt; variables, and (\ref{S3}) shows that &lt;span class="math"&gt;\(S^2\)&lt;/span&gt; is equal to &lt;span class="math"&gt;\(1/(N-L)\)&lt;/span&gt; times the squared length of an &lt;span class="math"&gt;\((N-L)\)&lt;/span&gt;-dimensional projection of it (the part along &lt;span class="math"&gt;\(\mathbb{I} - \mathbb{P}\)&lt;/span&gt;). The length of this projection will on average be &lt;span class="math"&gt;\((N-L) \sigma^2\)&lt;/span&gt;, so that &lt;span class="math"&gt;\(\langle S^2 \rangle = \sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We need to make two final points before moving on. First, because &lt;span class="math"&gt;\(S^2\)&lt;/span&gt; is a sum of &lt;span class="math"&gt;\((N-L)\)&lt;/span&gt; independent &lt;span class="math"&gt;\(\mathcal{N}(0, \sigma^2)\)&lt;/span&gt; random variables, it follows&amp;nbsp;that
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{(N-L) S^2}{\sigma^2} \sim \chi_{N-L}^2. \tag{13} \label{chi2}
$$&lt;/div&gt;
&lt;p&gt;
Second, &lt;span class="math"&gt;\(S^2\)&lt;/span&gt; is independent of &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;: We can see this by rearranging (\ref{optimal})&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;$$
\vec{\beta} = \vec{\beta}_{true} + (X^T X)^{-1} \cdot X^T \cdot \vec{\epsilon}. \tag{14} \label{beta3}
$$&lt;/div&gt;
&lt;p&gt;
We can left multiply this by &lt;span class="math"&gt;\(X\)&lt;/span&gt; without loss to&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;$$
X \cdot \vec{\beta} = X \cdot \vec{\beta}_{true} + \mathbb{P} \cdot \vec{\epsilon}, \tag{15} \label{beta2}
$$&lt;/div&gt;
&lt;p&gt;
where we have used (\ref{projection}). Comparing (\ref{beta2}) and (\ref{S3}), we see that the components of &lt;span class="math"&gt;\(\vec{\epsilon}\)&lt;/span&gt; that inform &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; are in the subspace fixed by &lt;span class="math"&gt;\(\mathbb{P}\)&lt;/span&gt;. This is the space complementary to that informing &lt;span class="math"&gt;\(S^2\)&lt;/span&gt;, implying that &lt;span class="math"&gt;\(S^2\)&lt;/span&gt; is independent of &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Confidence intervals and hypothesis&amp;nbsp;tests&lt;/h3&gt;
&lt;p&gt;The results above immediately provide us with a method for generating confidence intervals for the individual coefficient estimates (continuing with our Normal error assumption): From (\ref{beta3}), it follows that the coefficients are themselves Normal random variables, with variance given by (\ref{cov}). Further, &lt;span class="math"&gt;\(S^2\)&lt;/span&gt; provides an unbiased estimate for &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;, proportional to a &lt;span class="math"&gt;\(\chi^2_{N-L}\)&lt;/span&gt; random variable. Combining these results&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\beta_{i,true} - \beta_{i}}{\sqrt{\left(X^T X\right)^{-1}_{ii} S^2}} \sim t_{(N-L)}. \tag{16}
$$&lt;/div&gt;
&lt;p&gt;
That is, the pivot at left follows a Student&amp;#8217;s &lt;span class="math"&gt;\(t\)&lt;/span&gt;-distribution with &lt;span class="math"&gt;\((N-L)\)&lt;/span&gt; degrees of freedom (i.e., it&amp;#8217;s proportional to the ratio of a standard Normal and the square root of a chi-squared variable with that many degrees of freedom). A rearrangement of the above gives the following level &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; confidence interval for the true&amp;nbsp;value:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\beta_i - t_{(N-L), \alpha /2} \sqrt{\left(X^T X \right)^{-1}_{ii} S^2}\leq \beta_{i, true} \leq \beta_i + t_{(N-L), \alpha /2} \sqrt{\left(X^T X \right)^{-1}_{ii} S^2} \tag{17} \label{interval},
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\beta_i\)&lt;/span&gt; is obtained from the solution (\ref{optimal}). The interval above can be inverted to generate level &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; hypothesis tests. In particular, we note that a test of the null &amp;#8212; that a particular coefficient is actually zero &amp;#8212; would not be rejected if (\ref{interval}) contains the origin. This approach is often used to test whether some data is consistent with the assertion that a predictor is linearly related to the&amp;nbsp;response.&lt;/p&gt;
&lt;p&gt;[Again, see Cathy&amp;#8217;s follow-up post &lt;a href="http://efavdb.github.io/interpret-linear-regression"&gt;here&lt;/a&gt; for an alternate take on these&amp;nbsp;results.]&lt;/p&gt;
&lt;h3&gt;Bayesian&amp;nbsp;analysis&lt;/h3&gt;
&lt;p&gt;The final thing we wish to do here is consider the problem from a Bayesian perspective, using a flat prior on the &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;. In this case, assuming a Gaussian form for the &lt;span class="math"&gt;\(\epsilon_i\)&lt;/span&gt; in (\ref{noise})&amp;nbsp;gives
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{18} \label{18}
p(\vec{\beta} \vert \{y_i\}) \propto p(\{y_i\} \vert \vec{\beta}) p(\vec{\beta}) = \mathcal{N} \exp \left [ -\frac{1}{2 \sigma^2}\sum_i \left (y_i - \vec{\beta} \cdot \vec{x}_i \right)^2\right].
\end{align}&lt;/div&gt;
&lt;p&gt;
Notice that this posterior form for &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; is also Gaussian, and is centered about the solution (\ref{optimal}). Formally, we can write the exponent here in the&amp;nbsp;form
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
-\frac{1}{2 \sigma^2}\sum_i \left (y_i - \vec{\beta} \cdot \vec{x}_i \right)^2 \equiv -\frac{1}{2} \vec{\beta}^T \cdot \frac{1}{\Sigma^2} \cdot \vec{\beta}, \tag{19}
\end{align}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\Sigma^2\)&lt;/span&gt; is the covariance matrix for the components of &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;, as implied by the posterior form (\ref{18}). We can get the components of its inverse by differentiating (\ref{18}) twice. This&amp;nbsp;gives,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\left ( \frac{1}{\Sigma^2}\right)_{jk} &amp;amp;= \frac{1}{2 \sigma^2} \partial_{\beta_j} \partial_{\beta_k} \sum_i \left (y_i - \vec{\beta} \cdot \vec{x}_i \right)^2 \\
&amp;amp;= -\frac{1}{\sigma^2}\partial_{\beta_j} \sum_i \left (y_i - \vec{\beta} \cdot \vec{x}_i \right) x_{i,k} \\
&amp;amp;= \frac{1}{\sigma^2} \sum_i x_{i,j} x_{i,k} = \frac{1}{\sigma^2} (X^T X)_{jk}. \tag{20}
\end{align}&lt;/div&gt;
&lt;p&gt;
In other words, &lt;span class="math"&gt;\(\Sigma^2 = \sigma^2 (X^T X)^{-1}\)&lt;/span&gt;, in agreement with the classical expression&amp;nbsp;(\ref{cov}).&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;In summary, we&amp;#8217;ve gone through one quick derivation of linear fit solution that minimizes the sum of squared &lt;span class="math"&gt;\(y\)&lt;/span&gt; errors for a given set of data. We&amp;#8217;ve also considered the variance of this solution, showing that the resulting form is closely related to the principal components of the predictor variables sampled. The covariance solution (\ref{cov}) tells us that all parameters have standard deviations that decrease like &lt;span class="math"&gt;\(1/\sqrt{N}\)&lt;/span&gt;, with &lt;span class="math"&gt;\(N\)&lt;/span&gt; the number of samples. However, the predictors that are sampled over wider ranges always have coefficient estimates that more precise. This is due to the fact that sampling over many different values allows one to get a better read on how the underlying function being fit varies with a predictor. Following this, assuming normal errors, we showed that &lt;span class="math"&gt;\(S^2\)&lt;/span&gt; provides an unbiased estimate, chi-squared estimator for the sample variance &amp;#8212; one that is independent of parameter estimates. This allowed us to then write down a confidence interval for the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th coefficient. The final thing we have shown is that the Bayesian, Gaussian approximation gives similar results: In this approach, the posterior that results is centered about the classical solution, and has a covariance matrix equal to that obtained by classical&amp;nbsp;approach.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>Independent component analysis</title><link href="https://efavdb.com/independent-component-analysis" rel="alternate"></link><published>2016-02-14T00:00:00-08:00</published><updated>2016-02-14T00:00:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2016-02-14:/independent-component-analysis</id><summary type="html">&lt;p&gt;Two microphones are placed in a room where two conversations are taking place simultaneously. Given these two recordings, can one &amp;#8220;remix&amp;#8221; them in some prescribed way to isolate the individual conversations? Yes! In this post, we review one simple approach to solving this type of problem, Independent Component Analysis (&lt;span class="caps"&gt;ICA …&lt;/span&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Two microphones are placed in a room where two conversations are taking place simultaneously. Given these two recordings, can one &amp;#8220;remix&amp;#8221; them in some prescribed way to isolate the individual conversations? Yes! In this post, we review one simple approach to solving this type of problem, Independent Component Analysis (&lt;span class="caps"&gt;ICA&lt;/span&gt;). We share an ipython document implementing &lt;span class="caps"&gt;ICA&lt;/span&gt; and link to a youtube video illustrating its application to audio&amp;nbsp;de-mixing.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;To formalize the problem posed in the abstract, let two desired conversation signals be represented by &lt;span class="math"&gt;\(c_1(t)\)&lt;/span&gt; and &lt;span class="math"&gt;\(c_2(t)\)&lt;/span&gt;, and two mixed microphone recordings of these by &lt;span class="math"&gt;\(m_1(t)\)&lt;/span&gt; and &lt;span class="math"&gt;\(m_2(t)\)&lt;/span&gt;. We&amp;#8217;ll assume that the latter are both linear combinations of the former,&amp;nbsp;with
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align} \label{mean}
m_1(t) &amp;amp;= a_1 c_1(t) + a_2 c_2(t) \\
m_2(t) &amp;amp;= a_3 c_1(t) + a_4 c_2(t). \label{1} \tag{1}
\end{align}&lt;/div&gt;
&lt;p&gt;
Here, we stress that the &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; coefficients in (\ref{1}) are hidden from us: We only have access to the &lt;span class="math"&gt;\(m_i\)&lt;/span&gt;. Hypothetical illustrations are given in the figure below. Given only these mixed signals, we&amp;#8217;d like to recover the underlying &lt;span class="math"&gt;\(c_i\)&lt;/span&gt; used to construct them (spoiler: a sine wave and a saw-tooth function were used for this&amp;nbsp;figure).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2016/02/mixed2.jpg"&gt;&lt;img alt="mixed" src="https://efavdb.com/wp-content/uploads/2016/02/mixed2.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Amazingly, it turns out that with the introduction of a modest assumption, a simple solution to our problem can be obtained: We need only assume that the desired &lt;span class="math"&gt;\(c_i\)&lt;/span&gt; are mutually independent&lt;span class="math"&gt;\(^1\)&lt;/span&gt;. This assumption is helpful because it turns out that when two independent signals are added together, the resulting mixture is always &amp;#8220;more Gaussian&amp;#8221; than either of the individual, independent signals (a la the central limit theorem). Seeking linear combinations of the available &lt;span class="math"&gt;\(m_i\)&lt;/span&gt; that locally extremize their non-Gaussian character therefore provides a way to identify the pure, unmixed signals. This approach to solving the problem is called &amp;#8220;Independent Component Analysis&amp;#8221;, or &lt;span class="caps"&gt;ICA&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here, we demonstrate the principle of &lt;span class="caps"&gt;ICA&lt;/span&gt; through consideration of the audio de-mixing problem. This is a really impressive application. However, one should strive to remember that the algorithm is not a one-trick-pony. &lt;span class="caps"&gt;ICA&lt;/span&gt; is an unsupervised machine learning algorithm of general applicability &amp;#8212; similar in nature, and complementary to, the more familiar &lt;a href="http://efavdb.github.io/principal-component-analysis"&gt;&lt;span class="caps"&gt;PCA&lt;/span&gt;&lt;/a&gt; algorithm. Whereas in &lt;span class="caps"&gt;PCA&lt;/span&gt; we seek the feature-space directions that maximize captured variance, in &lt;span class="caps"&gt;ICA&lt;/span&gt; we seek those directions that maximize the &amp;#8220;interestingness&amp;#8221; of the distribution &amp;#8212; i.e., the non-Gaussian character of the resulting projections. It can be fruitfully applied in many contexts&lt;span class="math"&gt;\(^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We turn now to the problem of audio de-mixing via &lt;span class="caps"&gt;ICA&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Audio&amp;nbsp;de-mixing&lt;/h3&gt;
&lt;p&gt;In this post, we use the kurtosis of a signal to quantify its degree of &amp;#8220;non-Gaussianess&amp;#8221;. For a given signal &lt;span class="math"&gt;\(x(t)\)&lt;/span&gt;, this is defined&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;$$
\kappa(x) \equiv \left \langle \left (x- \langle x \rangle \right)^4 \right \rangle - 3 \left \langle \left (x- \langle x \rangle \right)^2 \right \rangle^2, \label{2} \tag{2}
$$&lt;/div&gt;
&lt;p&gt;
where brackets represent an average over time (or index). It turns out that the kurtosis is always zero for a Gaussian-distributed signal, so (\ref{2}) is a natural choice of score function for measuring deviation away from Gaussian behavior&lt;span class="math"&gt;\(^3\)&lt;/span&gt;. Essentially, it&amp;#8217;s a measure of how flat a distribution is &amp;#8212; with numbers greater (smaller) than 0 corresponding to distributions that are more (less) flat than a&amp;nbsp;Gaussian.&lt;/p&gt;
&lt;p&gt;With (\ref{2}) chosen as our score function, we can now jump right into applying &lt;span class="caps"&gt;ICA&lt;/span&gt;. The code snippet below considers all possible mixtures of two mixed signals &lt;span class="math"&gt;\(m_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(m_2\)&lt;/span&gt;, obtains the resulting signal kurtosis values, and plots the&amp;nbsp;result.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;kurtosis_of_mixture&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;c2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;c1&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;c1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;m1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;c2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;m2&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;

&lt;span class="n"&gt;c_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;k_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;kurtosis_of_mixture&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;c_array&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c_array&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k_array&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2016/02/k3.jpg"&gt;&lt;img alt="k" src="https://efavdb.com/wp-content/uploads/2016/02/k3.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In line &lt;span class="math"&gt;\((3)\)&lt;/span&gt; of the code here, we define the &amp;#8220;remixed&amp;#8221; signal &lt;span class="math"&gt;\(s\)&lt;/span&gt;, which is a linear combination of the two mixed signals &lt;span class="math"&gt;\(m_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(m_2\)&lt;/span&gt;. Note that in line &lt;span class="math"&gt;\((4)\)&lt;/span&gt;, we normalize the signal so that it always has variance &lt;span class="math"&gt;\(1\)&lt;/span&gt; &amp;#8212; this simply eliminates an arbitrary scale factor from the analysis. Similarly in line &lt;span class="math"&gt;\((2)\)&lt;/span&gt;, we specify &lt;span class="math"&gt;\(c_2\)&lt;/span&gt; as a function of &lt;span class="math"&gt;\(c_1\)&lt;/span&gt;, requiring the sum of their squared values to equal one &amp;#8212; this fixes another arbitrary scale&amp;nbsp;factor.&lt;/p&gt;
&lt;p&gt;When we applied the code above to the two signals shown in the introduction, we obtained the top plot at right. This shows the kurtosis of &lt;span class="math"&gt;\(s\)&lt;/span&gt; as a function of &lt;span class="math"&gt;\(c_1\)&lt;/span&gt;, the weight applied to signal &lt;span class="math"&gt;\(m_1\)&lt;/span&gt;. Notice that there are two internal extrema in this plot: a peak near &lt;span class="math"&gt;\(-0.9\)&lt;/span&gt; and a local minimum near &lt;span class="math"&gt;\(-0.7\)&lt;/span&gt;. These are the two &lt;span class="math"&gt;\(c_1\)&lt;/span&gt; weight choices that &lt;span class="caps"&gt;ICA&lt;/span&gt; suggests may relate to the pure, underlying signals we seek. To plot each of these signals, we used code similar to the following (the code shown is just for the&amp;nbsp;maximum)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;index1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k_array&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k_array&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;c1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;c_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;c2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;c1&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;int16&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;c1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;c2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This code finds the index where the kurtosis was maximized, generates the corresponding remix, and plots the result. Applying this, the bottom figure at right popped out. It worked! &amp;#8212; and with just a few lines of code, which makes it seem all the more amazing. In summary, we looked for linear combinations of the &lt;span class="math"&gt;\(m_i\)&lt;/span&gt; shown in the introduction that resulted in a stationary kurtosis &amp;#8212; plotting these combinations, we found that these were precisely the pure signals we sought&lt;span class="math"&gt;\(^4\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A second application to actual audio clips is demoed in our youtube video linked below. The full ipython file utilized in the video can be downloaded on our github page, &lt;a href="https://github.com/EFavDB/ICA"&gt;here&lt;/a&gt;&lt;span class="math"&gt;\(^5\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We hope this little post has you convinced that &lt;span class="caps"&gt;ICA&lt;/span&gt; is a powerful, yet straightforward algorithm&lt;span class="math"&gt;\(^6\)&lt;/span&gt;. Although we&amp;#8217;ve only discussed one application here, many others can be found online: Analysis of financial data, an idea to use &lt;span class="caps"&gt;ICA&lt;/span&gt; to isolate a desired wifi signal from a crowded frequency band, and the analysis of brain waves &amp;#8212; see discussion in the article mentioned in reference 2 &amp;#8212; etc. In general, the potential application set of &lt;span class="caps"&gt;ICA&lt;/span&gt; may be as large as that for &lt;span class="caps"&gt;PCA&lt;/span&gt;. Next time you need to do some unsupervised learning or data compression, definitely keep it in&amp;nbsp;mind.&lt;/p&gt;
&lt;h3&gt;Footnotes and&amp;nbsp;references&lt;/h3&gt;
&lt;p&gt;[1] Formally, saying that two signals are independent means that the evolution of one conveys no information about that of the&amp;nbsp;other.&lt;/p&gt;
&lt;p&gt;[2] For those interested in further reading on the theory and applications of &lt;span class="caps"&gt;ICA&lt;/span&gt;, we can recommend the review article by Hyvärinen and Oja &amp;#8212; &amp;#8220;Independent Component Analysis: Algorithms and Applications&amp;#8221; &amp;#8212; available for free&amp;nbsp;online.&lt;/p&gt;
&lt;p&gt;[3] Other metrics can also be used in the application of &lt;span class="caps"&gt;ICA&lt;/span&gt;. The kurtosis is easy to evaluate and is also well-motivated because of the fact that it is zero for any Gaussian. However, there are non-Gaussian distributions that also have zero kurtosis. Further, as seen in our linked youtube video, peaks in the kurtosis plot need not always correspond to the pure signals. A much more rigorous approach is to use the mutual information of the signals as your score. This function is zero if and only if you&amp;#8217;ve found a projection that results in a fully independent set of signals. Thus, it will always work. The problem with this choice is that it is much harder to evaluate &amp;#8212; thus, simpler scores are often used in practice, even though they aren&amp;#8217;t necessarily rigorously correct. The article mentioned in footnote 2 gives a good review of some other popular score function&amp;nbsp;choices.&lt;/p&gt;
&lt;p&gt;[4] In general, symmetry arguments imply that the pure signals will correspond to local extrema in the kurtosis landscape. This works because the kurtosis of &lt;span class="math"&gt;\(x_1 + a x_2\)&lt;/span&gt; is the same as that of &lt;span class="math"&gt;\(x_1 - a x_2\)&lt;/span&gt;, when &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; are independent. To complete the argument, you need to consider coefficient expansions in the mixed space. The fact that the pure signals can sometimes sit at kurtosis local minima doesn&amp;#8217;t really jive with the intuitive argument about mixtures being more Gaussian &amp;#8212; but that was a vague statement anyways. A rigorous, alternative introduction could be made via mutual information, as mentioned in the previous&amp;nbsp;footnote.&lt;/p&gt;
&lt;p&gt;[5] To run the script, you&amp;#8217;ll need ipython installed, as well as the python packages: scipy, numpy, matplotlib, and pyaudio &amp;#8212; see instructions for the latter &lt;a href="https://people.csail.mit.edu/hubert/pyaudio/"&gt;here&lt;/a&gt;. The pip install command for pyaudio didn&amp;#8217;t work for me on my mac, but the following line did:
&lt;code&gt;pip install --global-option='build_ext' --global-option='-I/usr/local/include' --global-option='-L/usr/local/lib' pyaudio&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;[6] Of course, things get a bit more complicated when you have a large number of signals. However, fast, simple algorithms have been found to carry this out even in high dimensions. See the reference in footnote 2 for&amp;nbsp;discussion.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>Principal component analysis</title><link href="https://efavdb.com/principal-component-analysis" rel="alternate"></link><published>2015-12-05T22:22:00-08:00</published><updated>2015-12-05T22:22:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-12-05:/principal-component-analysis</id><summary type="html">&lt;p&gt;We review the two essentials of principal component analysis (&amp;#8220;&lt;span class="caps"&gt;PCA&lt;/span&gt;&amp;#8221;): 1) The principal components of a set of data points are the eigenvectors of the correlation matrix of these points in feature space. 2) Projecting the data onto the subspace spanned by the first &lt;span class="math"&gt;\(k\)&lt;/span&gt; of these &amp;#8212; listed in descending …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We review the two essentials of principal component analysis (&amp;#8220;&lt;span class="caps"&gt;PCA&lt;/span&gt;&amp;#8221;): 1) The principal components of a set of data points are the eigenvectors of the correlation matrix of these points in feature space. 2) Projecting the data onto the subspace spanned by the first &lt;span class="math"&gt;\(k\)&lt;/span&gt; of these &amp;#8212; listed in descending eigenvalue order &amp;#8212; provides the best possible &lt;span class="math"&gt;\(k\)&lt;/span&gt;-dimensional approximation to the data, in the sense of captured&amp;nbsp;variance.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;One way to introduce principal component analysis is to consider the problem of least-squares fits: Consider, for example, the figure shown below. To fit a line to this data, one might attempt to minimize the squared &lt;span class="math"&gt;\(y\)&lt;/span&gt; residuals (actual minus fit &lt;span class="math"&gt;\(y\)&lt;/span&gt; values). However, if the &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; values are considered to be on an equal footing, this &lt;span class="math"&gt;\(y\)&lt;/span&gt;-centric approach is not quite appropriate. A natural alternative is to attempt instead to find the line that minimizes the &lt;em&gt;total squared projection error&lt;/em&gt;: If &lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt; is a data point, and &lt;span class="math"&gt;\((\hat{x}_i, \hat{y}_i)\)&lt;/span&gt; is the point closest to it on the regression line (aka, its &amp;#8220;projection&amp;#8221; onto the line), we attempt to&amp;nbsp;minimize
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{1} \label{score}
J = \sum_i (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2.
$$&lt;/div&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/12/projection.png"&gt;&lt;img alt="margin around decision boundary" src="https://efavdb.com/wp-content/uploads/2015/12/projection.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The summands here are illustrated in the figure: The dotted lines shown are the projection errors for each data point relative to the red line. The minimizer of (\ref{score}) is the line that minimizes the sum of the squares of these&amp;nbsp;values.&lt;/p&gt;
&lt;p&gt;Generalizing the above problem, one could ask which &lt;span class="math"&gt;\(k\)&lt;/span&gt;-dimensional hyperplane passes closest to a set of data points in &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dimensions. Being able to identify the solution to this problem can be very helpful when &lt;span class="math"&gt;\(N \gg 1\)&lt;/span&gt;. The reason is that in high-dimensional, applied problems, many features are often highly-correlated. When this occurs, projection of the data onto a &lt;span class="math"&gt;\(k\)&lt;/span&gt;-dimensional subspace can often result in a great reduction in memory usage (one moves from needing to store &lt;span class="math"&gt;\(N\)&lt;/span&gt; values for each data point to &lt;span class="math"&gt;\(k\)&lt;/span&gt;) with minimal loss of information (if the points are all near the plane, replacing them by their projections causes little distortion). Projection onto subspaces can also be very helpful for visualization: For example, plots of &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dimensional data projected onto a best two-dimensional subspace can allow one to get a feel for a dataset&amp;#8217;s&amp;nbsp;shape.&lt;/p&gt;
&lt;p&gt;At first glance, the task of actually minimizing (\ref{score}) may appear daunting. However, it turns out this can be done easily using linear algebra. One need only carry out the following three&amp;nbsp;steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Preprocessing: If appropriate, shift features and normalize so that they all have mean &lt;span class="math"&gt;\(\mu = 0\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(\sigma^2 = 1\)&lt;/span&gt;. The latter, scaling step is needed to account for differences in units, which may cause variations along one component to look artificially large or small relative to those along other components (eg, one raw component might be a measure in centimeters, and another in&amp;nbsp;kilometers).&lt;/li&gt;
&lt;li&gt;Compute the covariance matrix. Assuming there are &lt;span class="math"&gt;\(m\)&lt;/span&gt; data points, the &lt;span class="math"&gt;\(i\)&lt;/span&gt;, &lt;span class="math"&gt;\(j\)&lt;/span&gt; component of this matrix is given by:
    &lt;div class="math"&gt;$$\tag{2} \label{2} \Sigma_{ij}^2 = \frac{1}{m}\sum_{l=1}^m \langle (f_{l,i} - \mu_i) (f_{l,j} - \mu_j) \rangle\\ = \langle x_i \vert \left (\frac{1}{m} \sum_{l=1}^m \vert \delta f_l \rangle \langle \delta f_l \vert \right) \vert x_j \rangle.$$&lt;/div&gt;
    Note that, at right, we are using bracket notation for vectors. We make further use of this below &amp;#8212; see footnote [1] at bottom for review. We&amp;#8217;ve also written &lt;span class="math"&gt;\(\vert \delta f_l \rangle\)&lt;/span&gt; for the vector &lt;span class="math"&gt;\(\vert f_l \rangle - \sum_{i = 1}^n \mu_i \vert x_i \rangle\)&lt;/span&gt; &amp;#8212; the vector &lt;span class="math"&gt;\(\vert f_l \rangle\)&lt;/span&gt; with the dataset&amp;#8217;s centroid subtracted&amp;nbsp;out.&lt;/li&gt;
&lt;li&gt;Project all feature vectors onto the &lt;span class="math"&gt;\(k\)&lt;/span&gt; eigenvectors &lt;span class="math"&gt;\(\{\vert v_j \rangle\)&lt;/span&gt;, &lt;span class="math"&gt;\(j = 1 ,2 \ldots, k\}\)&lt;/span&gt; of &lt;span class="math"&gt;\(\Sigma^2\)&lt;/span&gt; that have the largest eigenvalues &lt;span class="math"&gt;\(\lambda_j\)&lt;/span&gt;, writing
    &lt;div class="math"&gt;$$\tag{3} \label{3}
    \vert \delta f_i \rangle \approx \sum_{j = 1}^k \langle v_j \vert \delta f_i \rangle \times \vert v_j\rangle.
    $$&lt;/div&gt;
    The term &lt;span class="math"&gt;\(\langle v_j \vert \delta f_i \rangle\)&lt;/span&gt; above is the coefficient of the vector &lt;span class="math"&gt;\(\vert \delta f_i \rangle\)&lt;/span&gt; along the &lt;span class="math"&gt;\(j\)&lt;/span&gt;-th principal component. If we set &lt;span class="math"&gt;\(k = N\)&lt;/span&gt; above, (\ref{3}) becomes an identity. However, when &lt;span class="math"&gt;\(k &amp;lt; N\)&lt;/span&gt;, the expression represents an approximation only, with the vector &lt;span class="math"&gt;\(\vert \delta f_i \rangle\)&lt;/span&gt; approximated by its projection into the subspace spanned by the largest &lt;span class="math"&gt;\(k\)&lt;/span&gt; principal&amp;nbsp;components.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The steps above are all that are needed to carry out a &lt;span class="caps"&gt;PCA&lt;/span&gt; analysis/compression of any dataset. We show in the next section why this solution will indeed provide the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-dimensional hyperplane resulting in minimal dataset projection&amp;nbsp;error.&lt;/p&gt;
&lt;h3&gt;Mathematics of &lt;span class="caps"&gt;PCA&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;To understand &lt;span class="caps"&gt;PCA&lt;/span&gt;, we proceed in three&amp;nbsp;steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Significance of a partial trace: Let &lt;span class="math"&gt;\(\{\textbf{u}_j \}\)&lt;/span&gt; be some arbitrary orthonormal basis set that spans our full &lt;span class="math"&gt;\(N\)&lt;/span&gt;-dimensional space, and consider the sum
    &lt;div class="math"&gt;\begin{align}\tag{4} \label{4}
    \sum_{j = 1}^k \Sigma^2_{jj} = \frac{1}{m} \sum_{i,j} \langle u_j \vert \delta f_i \rangle \langle \delta f_i \vert u_j \rangle\\ = \frac{1}{m} \sum_{i,j} \langle \delta f_i \vert u_j \rangle \langle u_j \vert \delta f_i \rangle\\ \equiv \frac{1}{m} \sum_{i} \langle \delta f_i \vert P \vert \delta f_i \rangle.
    \end{align}&lt;/div&gt;
    To obtain the first equality here, we have used &lt;span class="math"&gt;\(\Sigma^2 = \frac{1}{m} \sum_{i} \vert \delta f_i \rangle \langle \delta f_i \vert\)&lt;/span&gt;, which follows from (\ref{2}). To obtain the last, we have written &lt;span class="math"&gt;\(P\)&lt;/span&gt; for the projection operator onto the space spanned by the first &lt;span class="math"&gt;\(k\)&lt;/span&gt; &lt;span class="math"&gt;\(\{\textbf{u}_j \}\)&lt;/span&gt;. Note that this last equality implies that the partial trace is equal to the average squared length of the projected feature vectors &amp;#8212; that is, the variance of the projected data&amp;nbsp;set.&lt;/li&gt;
&lt;li&gt;Notice that the projection error is simply given by the total trace of &lt;span class="math"&gt;\(\Sigma^2\)&lt;/span&gt;, minus the partial trace above. Thus, minimization of the projection error is equivalent to maximization of the projected variance,&amp;nbsp;(\ref{4}).&lt;/li&gt;
&lt;li&gt;We now consider which basis maximizes (\ref{4}). To do that, we decompose the &lt;span class="math"&gt;\(\{\textbf{u}_i \}\)&lt;/span&gt; in terms of the eigenvectors &lt;span class="math"&gt;\(\{\textbf{v}_j\}\)&lt;/span&gt; of &lt;span class="math"&gt;\(\Sigma^2\)&lt;/span&gt;, writing
    &lt;div class="math"&gt;\begin{align} \tag{5} \label{5}
    \vert u_i \rangle = \sum_j \vert v_j \rangle \langle v_j \vert u_i \rangle \equiv \sum_j u_{ij} \vert v_j \rangle.
    \end{align}&lt;/div&gt;
    Here, we&amp;#8217;ve inserted the identity in the &lt;span class="math"&gt;\(\{v_j\}\)&lt;/span&gt; basis, and written &lt;span class="math"&gt;\( \langle v_j \vert u_i \rangle \equiv u_{ij}\)&lt;/span&gt;. With these definitions, the partial trace becomes
    &lt;div class="math"&gt;\begin{align}\tag{6} \label{6}
    \sum_{i=1}^k \langle u_i \vert \Sigma^2 \vert u_i \rangle = \sum_{i,j,l} u_{ij}u_{il} \langle v_j \vert \Sigma^2 \vert v_l \rangle \\= \sum_{i=1}^k\sum_{j} u_{ij}^2 \lambda_j.
    \end{align}&lt;/div&gt;
    The last equality here follows from the fact that the &lt;span class="math"&gt;\(\{\textbf{v}_i\}\)&lt;/span&gt; are the eigenvectors of &lt;span class="math"&gt;\(\Sigma^2\)&lt;/span&gt; &amp;#8212; we have also used the fact that they are orthonormal, which follows from the fact that &lt;span class="math"&gt;\(\Sigma^2\)&lt;/span&gt; is a real, symmetric matrix. The sum (\ref{6}) is proportional to a weighted average of the eigenvalues of &lt;span class="math"&gt;\(\Sigma^2\)&lt;/span&gt;. We have a total mass of &lt;span class="math"&gt;\(k\)&lt;/span&gt; to spread out amongst the &lt;span class="math"&gt;\(N\)&lt;/span&gt; eigenvalues. The maximum mass that can sit on any one eigenvalue is one. This follows since &lt;span class="math"&gt;\(\sum_{i = 1}^k u_{ij}^2 \leq \sum_{i = 1}^N u_{ij}^2 =1\)&lt;/span&gt;, the latter equality following from the fact that &lt;span class="math"&gt;\( \sum_{i = 1}^N u_{ij}^2\)&lt;/span&gt; is an expression for the squared length of &lt;span class="math"&gt;\(\vert v_j\rangle\)&lt;/span&gt; in the &lt;span class="math"&gt;\(\{u_i\}\)&lt;/span&gt; basis. Under these constraints, the maximum possible average one can get in (\ref{6}) occurs when all the mass sits on the largest &lt;span class="math"&gt;\(k\)&lt;/span&gt; eigenvalues, with each of these eigenvalues weighted with mass one. This condition occurs if and only if the first &lt;span class="math"&gt;\(k\)&lt;/span&gt; &lt;span class="math"&gt;\(\{\textbf{u}_i\}\)&lt;/span&gt; span the same space as that spanned by the first &lt;span class="math"&gt;\(k\)&lt;/span&gt; &lt;span class="math"&gt;\(\{\textbf{v}_j\}\)&lt;/span&gt; &amp;#8212; those with the &lt;span class="math"&gt;\(k\)&lt;/span&gt; largest&amp;nbsp;eigenvalues.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That&amp;#8217;s it for the mathematics of &lt;span class="caps"&gt;PCA&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Footnotes&lt;/h3&gt;
&lt;p&gt;[1] &lt;em&gt;Review of bracket notation&lt;/em&gt;: &lt;span class="math"&gt;\(\vert x \rangle\)&lt;/span&gt; represents a regular vector, &lt;span class="math"&gt;\(\langle x \vert\)&lt;/span&gt; is its transpose, and &lt;span class="math"&gt;\(\langle y \vert x \rangle\)&lt;/span&gt; represents the dot product of &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt;. So, for example, when the term in parentheses at the right side of (\ref{2}) acts on the vector &lt;span class="math"&gt;\(\vert x_j \rangle\)&lt;/span&gt; to its right, you get &lt;span class="math"&gt;\( \frac{1}{m} \sum_{k=1}^m \vert \delta f_k \rangle \left (\langle \delta f_k \vert x_j \rangle\right).\)&lt;/span&gt; Here, &lt;span class="math"&gt;\( \left (\langle \delta f_k \vert x_j \rangle\right)\)&lt;/span&gt; is a dot product, a scalar, and &lt;span class="math"&gt;\(\vert \delta f_k \rangle\)&lt;/span&gt; is a vector. The result is thus a weighted sum of vectors. In other words, the bracketed term (\ref{2}) acts on a vector and returns a linear combination of other vectors. That means it is a matrix, as is any other object of form &lt;span class="math"&gt;\(\sum_i \vert a_i \rangle \langle b_i \vert\)&lt;/span&gt;. A special, important example is the identity matrix: Given any complete, orthonormal set of vectors &lt;span class="math"&gt;\(\{x_j\}\)&lt;/span&gt;, the identity matrix &lt;span class="math"&gt;\(I\)&lt;/span&gt; can be written as &lt;span class="math"&gt;\(I = \sum_i \vert x_i \rangle \langle x_i \vert\)&lt;/span&gt;. This identity is often used to make a change of&amp;nbsp;basis.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>Leave-one-out cross-validation</title><link href="https://efavdb.com/leave-one-out-cross-validation" rel="alternate"></link><published>2015-08-01T16:08:00-07:00</published><updated>2015-08-01T16:08:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-08-01:/leave-one-out-cross-validation</id><summary type="html">&lt;p&gt;This will be the first of a series of short posts relating to subject matter discussed in the text, &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;&amp;#8220;An Introduction to Statistical Learning&amp;#8221;&lt;/a&gt;. This is an interesting read, but it often skips over statement proofs &amp;#8212; that&amp;#8217;s where this series of posts comes in! Here, I consider the content …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This will be the first of a series of short posts relating to subject matter discussed in the text, &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;&amp;#8220;An Introduction to Statistical Learning&amp;#8221;&lt;/a&gt;. This is an interesting read, but it often skips over statement proofs &amp;#8212; that&amp;#8217;s where this series of posts comes in! Here, I consider the content of Section 5.1.2: This gives a lightning-quick &amp;#8220;short cut&amp;#8221; method for evaluating a regression&amp;#8217;s leave-one-out cross-validation error. The method is applicable to any least-squares linear&amp;nbsp;fit.&lt;/p&gt;
&lt;h3&gt;Introduction: Leave-one-out&amp;nbsp;cross-validation&lt;/h3&gt;
&lt;p&gt;When carrying out a &lt;a href="https://en.wikipedia.org/wiki/Regression_analysis"&gt;regression analysis&lt;/a&gt;, one is often interested in two types of error measurement. The first is the training set error and the second is the generalization error. The former relates to how close the regression is to the data being fit. In contrast, the generalization error relates to how accurate the model will be when applied to other points. The latter is of particular interest whenever the regression will be used to make predictions on new&amp;nbsp;points.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)"&gt;Cross-validation&lt;/a&gt; provides one method for estimating generalization errors. The approach centers around splitting the training data available into two sets, &lt;em&gt;a cross-validation training set&lt;/em&gt; and &lt;em&gt;cross-validation test set&lt;/em&gt;. The first of these is used for training a regression model. Its accuracy on the test set then provides a generalization error estimate. Here, we focus on a special form of cross-validation, called &lt;em&gt;leave-one-out cross-validation&lt;/em&gt; (&lt;span class="caps"&gt;LOOCV&lt;/span&gt;). In this case, we pick only one point as the test set. We then build a model on all the remaining, complementary points, and evaluate its error on the single-point held out. A generalization error estimate is obtained by repeating this procedure for each of the training points available, averaging the&amp;nbsp;results.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;LOOCV&lt;/span&gt; can be computationally expensive because it generally requires one to construct many models &amp;#8212; equal in number to the size of the training set. However, for the special case of least-squares polynomial regression we have the following &amp;#8220;short cut&amp;#8221; identity:&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \label{theorem} \tag{1}  
\sum_i \left ( \tilde{y}_i - y_i\right)^2 = \sum_i \left ( \frac{\hat{y}_i - y_i}{1 - h_i}\right)^2.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; is the actual label value of training point &lt;span class="math"&gt;\(i\)&lt;/span&gt;, &lt;span class="math"&gt;\(\tilde{y}_i\)&lt;/span&gt; is the value predicted by the cross-validation model trained on all points except &lt;span class="math"&gt;\(i\)&lt;/span&gt;, &lt;span class="math"&gt;\(\hat{y}_i\)&lt;/span&gt; is the value predicted by the regression model trained on all points (including point &lt;span class="math"&gt;\(i\)&lt;/span&gt;), and &lt;span class="math"&gt;\(h_i\)&lt;/span&gt; is a function of the coordinate &lt;span class="math"&gt;\(\vec{x}_i\)&lt;/span&gt; &amp;#8212; this is defined further below. Notice that the left side of (\ref{theorem}) is the &lt;span class="caps"&gt;LOOCV&lt;/span&gt; sum of squares error (the quantity we seek), while the right can be evaluated given only the model trained on the full data set. Fantastically, this allows us to evaluate the &lt;span class="caps"&gt;LOOCV&lt;/span&gt; error using only a single&amp;nbsp;regression!&lt;/p&gt;
&lt;h3&gt;Statement&amp;nbsp;proof&lt;/h3&gt;
&lt;p&gt;Consider the &lt;span class="caps"&gt;LOOCV&lt;/span&gt; step where we construct a model trained on all points except training example &lt;span class="math"&gt;\(k\)&lt;/span&gt;. Using a linear model of form &lt;span class="math"&gt;\(\tilde{y}(\vec{x}) \equiv \vec{x}^T \cdot \vec{\beta}_k\)&lt;/span&gt; &amp;#8212; with &lt;span class="math"&gt;\(\vec{\beta}_k\)&lt;/span&gt; a coefficient vector &amp;#8212; the sum of squares that must be minimized is&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{2} \label{error_sum}  
J_k \equiv \sum_{i \not = k} \left ( \tilde{y}_i - y_i \right)^2 = \sum_{i \not = k} \left (\vec{x}^T_i \cdot \vec{\beta}_k - y_i \right)^2.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, we&amp;#8217;re using a subscript &lt;span class="math"&gt;\(k\)&lt;/span&gt; on &lt;span class="math"&gt;\(\vec{\beta}_k\)&lt;/span&gt; to highlight the fact that the above corresponds to the case where example &lt;span class="math"&gt;\(k\)&lt;/span&gt; is held out. We minimize (\ref{error_sum}) by taking the gradient with respect to &lt;span class="math"&gt;\(\vec{\beta}_k\)&lt;/span&gt;. Setting this to zero gives the equation&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{3}  
\left( \sum_{i \not = k} \vec{x}_i \vec{x}_i^T \right) \cdot \vec{\beta}_k = \sum_{i \not = k} y_i \vec{x}_i.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Similarly, the full model (trained on all points) coefficient vector &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; satisfies&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{4} \label{full_con}  
\left( \sum_{i} \vec{x}_i \vec{x}_i^T \right) \cdot \vec{\beta} \equiv M \cdot \vec{\beta} = \sum_{i} y_i \vec{x}_i.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Combining the prior two equations gives,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{5}  
\left (M - \vec{x}_k \vec{x}_k^T \right) \cdot \vec{\beta}_k = \left (\sum_{i} y_i \vec{x}_i\right) - y_k \vec{x}_k = M\cdot \vec{\beta} - y_k \vec{x}_k.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Using the definition of &lt;span class="math"&gt;\(\tilde{y}_k\)&lt;/span&gt;, rearrangement of the above leads to the identity&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{6}  
M \cdot \left ( \vec{\beta}_k - \vec{\beta} \right) = \left (\tilde{y}_k - y_k \right) \vec{x}_k.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Left multiplication by &lt;span class="math"&gt;\(\vec{x}_k^T M^{-1}\)&lt;/span&gt; gives,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{7}  
\tilde{y}_k - \hat{y}_k = \left( \tilde{y}_k - y_k\right) - \left( \hat{y}_k - y_k \right) = \vec{x}_k^T M^{-1} \vec{x}_k \left (\tilde{y}_k - y_k \right).  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Finally, combining like-terms, squaring, and summing gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{8}  
\sum_k \left (\tilde{y}_k - y_k \right) ^2 = \sum_k \left (\frac{\hat{y}_k - y_k}{1 -\vec{x}_k^T M^{-1} \vec{x}_k } \right)^2.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This is (\ref{theorem}), where we now see the parameter &lt;span class="math"&gt;\(h_k \equiv \vec{x}_k^T M^{-1} \vec{x}_k\)&lt;/span&gt;. This is referred to as the &amp;#8220;leverage&amp;#8221; of &lt;span class="math"&gt;\(\vec{x}_k\)&lt;/span&gt; in the text. Notice also that &lt;span class="math"&gt;\(M\)&lt;/span&gt; is proportional to the correlation matrix of the &lt;span class="math"&gt;\(\{\vec{x}_i\}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\blacksquare\)&lt;/span&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>The mean shift clustering algorithm</title><link href="https://efavdb.com/mean-shift" rel="alternate"></link><published>2015-04-21T09:17:00-07:00</published><updated>2015-04-21T09:17:00-07:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2015-04-21:/mean-shift</id><summary type="html">&lt;h3&gt;Mean shift&amp;nbsp;clustering&lt;/h3&gt;
&lt;p&gt;Mean shift clustering is a general non-parametric cluster finding procedure &amp;#8212; introduced by Fukunaga and Hostetler [&lt;a href="#1"&gt;1&lt;/a&gt;], and popular within the computer vision field. Nicely, and in contrast to the more-well-known K-means clustering algorithm, the output of mean shift does not depend on any explicit assumptions on the …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Mean shift&amp;nbsp;clustering&lt;/h3&gt;
&lt;p&gt;Mean shift clustering is a general non-parametric cluster finding procedure &amp;#8212; introduced by Fukunaga and Hostetler [&lt;a href="#1"&gt;1&lt;/a&gt;], and popular within the computer vision field. Nicely, and in contrast to the more-well-known K-means clustering algorithm, the output of mean shift does not depend on any explicit assumptions on the shape of the point distribution, the number of clusters, or any form of random&amp;nbsp;initialization.&lt;/p&gt;
&lt;p&gt;We describe the mean shift algorithm in some detail in the &lt;a href="#Tech"&gt;technical background section&lt;/a&gt; at the end of this post. However, its essence is readily explained in a few words: Essentially, mean shift treats the clustering problem by supposing that all points given represent samples from some underlying probability density function, with regions of high sample density corresponding to the local maxima of this distribution. To find these local maxima, the algorithm works by allowing the points to attract each other, via what might be considered a short-ranged &amp;#8220;gravitational&amp;#8221; force. Allowing the points to gravitate towards areas of higher density, one can show that they will eventually coalesce at a series of points, close to the local maxima of the distribution. Those data points that converge to the same local maxima are considered to be members of the same&amp;nbsp;cluster.&lt;/p&gt;
&lt;p&gt;In the next couple of sections, we illustrate application of the algorithm to a couple of problems. We make use of the python package &lt;a href="http://scikit-learn.org/stable/"&gt;SkLearn&lt;/a&gt;, which contains a mean shift implementation. Following this, we provide a quick discussion and an appendix on technical&amp;nbsp;details.&lt;/p&gt;
&lt;h3&gt;Mean shift clustering in&amp;nbsp;action&lt;/h3&gt;
&lt;p&gt;In today&amp;#8217;s post we will have two examples. First, we will show how to use mean shift clustering to identify clusters of data in a 2D data set. Second, we will use the algorithm to segment a picture based on the colors in the image. To do this we need a handful of libraries from sklearn, numpy, matplotlib, and the Python Imaging Library (&lt;span class="caps"&gt;PIL&lt;/span&gt;) to handle reading in a jpeg&amp;nbsp;image.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cluster&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MeanShift&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;estimate_bandwidth&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets.samples_generator&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;make_blobs&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;cycle&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;PIL&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Finding clusters in a 2D data&amp;nbsp;set&lt;/h4&gt;
&lt;p&gt;This first example is based off of the sklearn &lt;a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html"&gt;tutorial&lt;/a&gt; for mean shift clustering: We generate data points centered at 4 locations, making use of sklearn&amp;#8217;s make_blobs library. To apply the clustering algorithm to the points generated, we must first set the attractive interaction length between examples, also know as the algorithm&amp;#8217;s bandwidth. Sklearn&amp;#8217;s implementation contains a built-in function that allows it to automatically estimate a reasonable value for this, based upon the typical distance between examples. We make use of that below, carry out the clustering, and then plot the&amp;nbsp;results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Generate&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;data&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;centers&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;[1, 1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;-.75, -1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;1, -1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;-3, 2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;make_blobs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;centers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;centers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cluster_std&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;Compute&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;clustering&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;MeanShift&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;The&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bandwidth&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;can&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;be&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;automatically&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;estimated&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;bandwidth&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;estimate_bandwidth&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;quantile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;MeanShift&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bandwidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;bandwidth&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bin_seeding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels_&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;cluster_centers&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cluster_centers_&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;n_clusters_&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Plot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;result&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;colors&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;cycle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bgrcmykbgrcmykbgrcmykbgrcmyk&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_clusters_&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;colors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;my_members&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;cluster_center&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cluster_centers&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;my_members, 0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;my_members, 1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cluster_center&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cluster_center&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;markerfacecolor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;markeredgecolor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;k&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;markersize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Estimated number of clusters: %d&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_clusters_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As you can see below, the algorithm has found clusters centered on each of the blobs we&amp;nbsp;generated.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/plot1.png"&gt;&lt;img alt="Plot 1" src="https://efavdb.com/wp-content/uploads/2015/03/plot1.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Segmenting a color&amp;nbsp;photo&lt;/h4&gt;
&lt;p&gt;In the first example, we were using mean shift clustering to look for spatial clusters. In our second example, we will instead explore 3D color space, &lt;span class="caps"&gt;RGB&lt;/span&gt;, by considering pixel values taken from an image of a toy car. The procedure is similar &amp;#8212; here, we cluster points in 3d, but instead of having data(x,y) we have data(r,g,b) taken from the image&amp;#8217;s &lt;span class="caps"&gt;RGB&lt;/span&gt; pixel values. Clustering these color values in this 3d space returns a series of clusters, where the pixels in those clusters are similar in &lt;span class="caps"&gt;RGB&lt;/span&gt; space. Recoloring pixels according to their cluster, we obtain a segmentation of the original&amp;nbsp;image.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#%%&lt;/span&gt; &lt;span class="n"&gt;Part&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Color&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="n"&gt;segmentation&lt;/span&gt; &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="n"&gt;shift&lt;/span&gt;

&lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;toy.jpg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Need&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="k"&gt;convert&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="k"&gt;into&lt;/span&gt; &lt;span class="n"&gt;feature&lt;/span&gt; &lt;span class="nb"&gt;array&lt;/span&gt; &lt;span class="n"&gt;based&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="n"&gt;rgb&lt;/span&gt; &lt;span class="n"&gt;intensities&lt;/span&gt;
&lt;span class="n"&gt;flat_image&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Estimate&lt;/span&gt; &lt;span class="n"&gt;bandwidth&lt;/span&gt;
&lt;span class="n"&gt;bandwidth2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;estimate_bandwidth&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flat_image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;quantile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MeanShift&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bandwidth2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bin_seeding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flat_image&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels_&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Plot&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="n"&gt;vs&lt;/span&gt; &lt;span class="n"&gt;segmented&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;off&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;851&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1280&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;off&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The bottom image below illustrates that one can effectively use this approach to identify the key shapes within an image, all without doing any image processing to get rid of glare or background &amp;#8212; pretty great!
&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/test-e1428358370930.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2015/03/test-e1428358370930.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;Although mean shift is a reasonably versatile algorithm, it has primarily been applied to problems in computer vision, where it has been used for image segmentation, clustering, and video tracking. Application to big data problems can be challenging due to the fact the algorithm can become relatively slow in this limit. However, research is presently underway to speed up its convergence, which should enable its application to larger data&amp;nbsp;sets.&lt;/p&gt;
&lt;p&gt;Mean shift&amp;nbsp;pros:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;No assumptions on the shape or number of data&amp;nbsp;clusters.&lt;/li&gt;
&lt;li&gt;The procedure only has one parameter, the&amp;nbsp;bandwidth.&lt;/li&gt;
&lt;li&gt;Output doesn&amp;#8217;t depend on&amp;nbsp;initializations.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mean shift&amp;nbsp;cons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Output does depend on bandwidth: too small and convergence is slow, too large and some clusters may be&amp;nbsp;missed.&lt;/li&gt;
&lt;li&gt;Computationally expensive for large feature&amp;nbsp;spaces.&lt;/li&gt;
&lt;li&gt;Often slower than K-Means&amp;nbsp;clustering&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Technical details&amp;nbsp;follow.&lt;/p&gt;
&lt;h3 id="Tech"&gt;Technical&amp;nbsp;background&lt;/h3&gt;
&lt;h4&gt;Kernel density&amp;nbsp;estimation&lt;/h4&gt;
&lt;p&gt;A general formulation of the mean shift algorithm can be developed through consideration of density kernels. These effectively work by smearing out each point example in space over some small window. Summing up the mass from each of these smeared units gives an estimate for the probability density at every point in space (by smearing, we are able to obtain estimates at locations that do not sit exactly atop any example). This approach is often referred to as &lt;a href="http://en.wikipedia.org/wiki/Kernel_density_estimation"&gt;kernel density estimation&lt;/a&gt; &amp;#8212; a method for density estimation that often converges more quickly than binning, or histogramming, and one that also nicely returns a continuous estimate for the density&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;To illustrate, suppose we are given a data set &lt;span class="math"&gt;\(\{\textbf{u}_i\}\)&lt;/span&gt; of points in d-dimensional space, sampled from some larger population, and that we have chosen a kernel &lt;span class="math"&gt;\(K\)&lt;/span&gt; having bandwidth parameter &lt;span class="math"&gt;\(h\)&lt;/span&gt;. Together, these data and kernel function return the following kernel density estimator for the full population&amp;#8217;s density&amp;nbsp;function
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
f_K(\textbf{u}) = \frac{1}{nh^d}\sum\limits_{i=1}^n K(\frac{\textbf{u}-\textbf{u}_i}{h})
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The kernel (smearing) function here is required to satisfy the following two&amp;nbsp;conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\int K(\textbf{u})d\textbf{u} =&amp;nbsp;1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(K(\textbf{u})=K(\vert \textbf{u} \vert)\)&lt;/span&gt; for all values of &lt;span class="math"&gt;\(\textbf{u}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first requirement is needed to ensure that our estimate is normalized, and the second is associated with the symmetry of our space. Two popular kernel functions that satisfy these conditions are given&amp;nbsp;by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Flat/Uniform &lt;span class="math"&gt;\(&lt;div class="math"&gt;\begin{align}
    K(\textbf{u}) = \frac{1}{2}\left\{
    \begin{array}{lr}
    1 &amp;amp; -1 \le \vert \textbf{u} \vert \le 1\
    0 &amp;amp; else
    \end{array}
    \right.
    \end{align}&lt;/div&gt;\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Gaussian = &lt;span class="math"&gt;\(K(\textbf{u}) = \frac{1}{\left(2\pi\right)^{d/2}} e^{-\frac{1}{2} \vert \textbf{u}&amp;nbsp;\vert^2}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Below we plot an example in 1-d using the gaussian kernel to estimate the density of some population along the x-axis. You can see that each sample point adds a small Gaussian to our estimate, centered about it: The equations above may look a bit intimidating, but the graphic here should clarify that the concept is pretty&amp;nbsp;straightforward.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/KDE-plot.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2015/03/KDE-plot.png"&gt;&lt;/a&gt;
Example of a kernel density estimation using a gaussian kernel for each data point: Adding up small Gaussians about each example returns our net estimate for the total density, the black&amp;nbsp;curve.&lt;/p&gt;
&lt;h4&gt;Mean shift&amp;nbsp;algorithm&lt;/h4&gt;
&lt;p&gt;Recall that the basic goal of the mean shift algorithm is to move particles in the direction of local increasing density. To obtain an estimate for this direction, a gradient is applied to the kernel density estimate discussed above. Assuming an angularly symmetric kernel function, &lt;span class="math"&gt;\(K(\textbf{u}) = K(\vert \textbf{u} \vert)\)&lt;/span&gt;, one can show that this gradient takes the&amp;nbsp;form
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\nabla f_K(\textbf{u}) = \frac{2}{nh^{d+2}} \left ( \sum\limits_{i=1}^n g(\left \vert \frac{\textbf{u}-\textbf{u}_i}{h} \right \vert) \right ) \textbf{m}(\textbf{u}).
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \textbf{m}(\textbf{u}) = \left ( \frac{\sum\limits_{i=1}^n \textbf{u}_i g(\left \vert \frac{\textbf{u}-\textbf{u}_i}{h} \right \vert)}{\sum\limits_{i=1}^n g(\left \vert \frac{\textbf{u}-\textbf{u}_i}{h} \right \vert)}-\textbf{u} \right ),
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
and &lt;span class="math"&gt;\(g(\vert \textbf{u} \vert ) = -K'(\vert \textbf{u} \vert)\)&lt;/span&gt; is the derivative of the selected kernel profile. The vector &lt;span class="math"&gt;\(\textbf{m}(\textbf{u})\)&lt;/span&gt; here, called the mean shift vector, points in the direction of increasing density &amp;#8212; the direction we must move our example. With this estimate, then, the mean shift algorithm protocol&amp;nbsp;becomes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute the mean shift vector &lt;span class="math"&gt;\(\textbf{m}(\textbf{u}_i)\)&lt;/span&gt;, evaluated at the location of each training example &lt;span class="math"&gt;\(\textbf{u}_i\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Move each example from &lt;span class="math"&gt;\(\textbf{u}_i \to \textbf{u}_i +&amp;nbsp;\textbf{m}(\textbf{u}_i)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Repeat until convergence &amp;#8212; ie, until the particles have reached&amp;nbsp;equilibrium.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a final step, one determines which examples have ended up at the same points, marking them as members of the same&amp;nbsp;cluster.&lt;/p&gt;
&lt;p&gt;For a proof of convergence and further mathematical details, see &lt;a href="https://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf"&gt;Comaniciu &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Meer (2002)&lt;/a&gt; [&lt;a href="#2"&gt;2&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;​1. Fukunaga and Hostetler, &amp;#8220;The Estimation of the Gradient of a Density Function, with Applications in Pattern Recognition&amp;#8221;, &lt;span class="caps"&gt;IEEE&lt;/span&gt; Transactions on Information Theory vol 21 , pp 32-40 ,1975
2. Dorin Comaniciu and Peter Meer, Mean Shift : A Robust approach towards feature space analysis, &lt;span class="caps"&gt;IEEE&lt;/span&gt; Transactions on Pattern Analysis and Machine Intelligence vol 24 No 5 May&amp;nbsp;2002.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>Machine Learning Methods: Decision trees and forests</title><link href="https://efavdb.com/notes-on-trees" rel="alternate"></link><published>2015-03-13T09:40:00-07:00</published><updated>2015-03-13T09:40:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2015-03-13:/notes-on-trees</id><summary type="html">&lt;p&gt;This post contains our crib notes on the basics of decision trees and forests. We first discuss the construction of individual trees, and then introduce random and boosted forests. We also discuss efficient implementations of greedy tree construction algorithms, showing that a single tree can be constructed in &lt;span class="math"&gt;\(O(k …&lt;/span&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post contains our crib notes on the basics of decision trees and forests. We first discuss the construction of individual trees, and then introduce random and boosted forests. We also discuss efficient implementations of greedy tree construction algorithms, showing that a single tree can be constructed in &lt;span class="math"&gt;\(O(k \times n \log n)\)&lt;/span&gt; time, given &lt;span class="math"&gt;\(n\)&lt;/span&gt; training examples having &lt;span class="math"&gt;\(k\)&lt;/span&gt; features each. We provide exercises on interesting related points and an appendix containing relevant python/sk-learn function&amp;nbsp;calls.  &lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Decision trees constitute a class of simple functions that are frequently used for carrying out regression and classification. They are constructed by hierarchically splitting a feature space into disjoint regions, where each split divides into two one of the already existing regions. In most common implementations, the splits are always taken along one of the feature axes, which causes the regions to be rectangular in shape. An example is shown in Fig. 1 below. In this example, a two-dimensional feature space is first split by a tree on &lt;span class="math"&gt;\(f_1\)&lt;/span&gt; &amp;#8212; one of the two features characterizing the space &amp;#8212; at value &lt;span class="math"&gt;\(s_a\)&lt;/span&gt;. This separates the space into two sets, that where &lt;span class="math"&gt;\(f_1 &amp;lt; s_a\)&lt;/span&gt; and that where &lt;span class="math"&gt;\(f_1 \geq s_a\)&lt;/span&gt;. Next, the tree further splits the first of these sets on feature &lt;span class="math"&gt;\(f_2\)&lt;/span&gt; at value &lt;span class="math"&gt;\(s_b\)&lt;/span&gt;. With these combined splits, the tree partitions the space into three disjoint regions, labeled &lt;span class="math"&gt;\(R_1, R_2,\)&lt;/span&gt; and &lt;span class="math"&gt;\(R_3\)&lt;/span&gt;, where, e.g., &lt;span class="math"&gt;\(R_1 = \{ \textbf{f} \vert f_1 &amp;lt; s_a, f_2 &amp;lt; s_b \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/tree1.jpg"&gt;&lt;img alt="tree1" src="https://efavdb.com/wp-content/uploads/2015/03/tree1.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Once a decision tree is constructed, it can be used for making predictions on unlabeled feature vectors &amp;#8212; i.e., points in feature space not included in our training set. This is done by first deciding which of the regions a new feature vector belongs to, and then returning as its hypothesis label an average over the training example labels within that region: The mean of the region&amp;#8217;s training labels is returned for regression problems and the mode for classification problems. For instance, the tree in Fig. 1 would return an average of the five training examples in &lt;span class="math"&gt;\(R_1\)&lt;/span&gt; (represented by red dots) when asked to make a hypothesis for any and all other points in that&amp;nbsp;region.&lt;/p&gt;
&lt;p&gt;The art and science of tree construction is in deciding how many splits should be taken and where those splits should take place. The goal is to find a tree that provides a reasonable, piece-wise constant approximation to the underlying distribution or function that has generated the training data provided. This can be attempted through choosing a tree that breaks space up into regions such that the examples in any given region have identical &amp;#8212; or at least similar &amp;#8212; labels. We discuss some common approaches to finding such trees in the next&amp;nbsp;section.&lt;/p&gt;
&lt;p&gt;Individual trees have the important benefit of being easy to interpret and visualize, but they are often not as accurate as other common machine learning algorithms. However, individual trees can be used as simple building blocks with which to construct more complex, competitive models. In the third section of this note, we discuss three very popular constructions of this sort: bagging, random forests (a variant on bagging), and boosting. We then discuss the runtime complexity of tree/forest construction and conclude with a summary, exercises, and an appendix containing example python&amp;nbsp;code.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Constructing individual decision&amp;nbsp;trees&lt;/strong&gt;&lt;/h2&gt;
&lt;h4&gt;&lt;strong&gt;Regression&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Regression tree construction typically proceeds by attempting to minimize a squared error cost function: Given a training set &lt;span class="math"&gt;\(T \equiv \{t_j = (\textbf{f}_j, y_j) \}\)&lt;/span&gt; of feature vectors and corresponding real-valued labels, this is given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{treecost} \tag{1} 
J = \sum_{R_i} \sum_{t_j \in R_i } \left ( \overline{y}_{R_i} - y_j \right)^2,  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where &lt;span class="math"&gt;\(\overline{y}_{R_i}\)&lt;/span&gt; is the mean training label in region &lt;span class="math"&gt;\(R_i\)&lt;/span&gt;. This mean training label is the hypothesis returned by the tree for all points in &lt;span class="math"&gt;\(R_i\)&lt;/span&gt;, including its training examples. Therefore, (\ref{treecost}) is a measure of the accuracy of the tree as applied to the training&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;Unfortunately, actually minimizing (\ref{treecost}) over any large subset of trees can be a numerically challenging task. This is true whenever you have a large number of features or training examples. Consequently, different approximate methods are generally taken to find good candidate trees. Two typical methods&amp;nbsp;follow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Greedy algorithm&lt;/em&gt;: The tree is constructed recursively, one branching step at a time. At each step, one takes the split that will most significantly reduce the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt;, relative to its current value. In this way, after &lt;span class="math"&gt;\(k-1\)&lt;/span&gt; splits, a tree with &lt;span class="math"&gt;\(k\)&lt;/span&gt; regions (leaves) is obtained &amp;#8212; Fig. 2 provides an illustration of this process. The algorithm terminates whenever some specified stopping criterion is satisfied, examples of which are given&amp;nbsp;below.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Randomized algorithm&lt;/em&gt;: Randomized tree-search protocols can sometimes find global minima inaccessible to the gradient-descent-like greedy algorithm. These randomized protocols also proceed recursively. However, at each step, some randomization is introduced by hand. For example, one common approach is to select &lt;span class="math"&gt;\(r\)&lt;/span&gt; candidate splits through random sampling at each branching point. The candidate split that most significantly reduces &lt;span class="math"&gt;\(J\)&lt;/span&gt; is selected, and the process repeats. The benefit of this approach is that it can sometimes find paths that appear suboptimal in their first few steps, but are ultimately&amp;nbsp;favorable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/treebuild.jpg"&gt;&lt;img alt="treebuild" src="https://efavdb.com/wp-content/uploads/2015/03/treebuild.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Classification&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;In classification problems, the training labels take on a discrete set of values, often having no numerical significance. This means that a squared-error cost function, like that in (\ref{treecost}) &amp;#8212; cannot be directly applied as a useful accuracy score for guiding classification tree construction. Instead, three other cost functions are often considered, each providing a different measure of the class purity of the different regions &amp;#8212; that is, they attempt to measure whether or not a given region consists of training examples that are mostly of the same class. These three measures are the error rate (&lt;span class="math"&gt;\(E\)&lt;/span&gt;), the Gini index (&lt;span class="math"&gt;\(G\)&lt;/span&gt;), and the cross-entropy (&lt;span class="math"&gt;\(CE\)&lt;/span&gt;): If we write &lt;span class="math"&gt;\(N_i\)&lt;/span&gt; for the number of training examples in region &lt;span class="math"&gt;\(R_i\)&lt;/span&gt;, and &lt;span class="math"&gt;\(p_{i,j}\)&lt;/span&gt; for the fraction of these that have class label &lt;span class="math"&gt;\(j\)&lt;/span&gt;, then these three cost functions are given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{errorrate}  \tag{2} 
E &amp;amp;=&amp;amp; \sum_{R_i} N_i \times \left ( 1 - \max_{j} p_{i,j}\right) \\ \label{gini} \tag{3} 
G &amp;amp;=&amp;amp; \sum_{R_i, j}N_i \times p_{i,j}\left ( 1 - p_{i,j} \right) \\ \label{crossentropy} \tag{4} 
CE &amp;amp;=&amp;amp; - \sum_{R_i, j} N_i \times p_{i,j} \log p_{i,j}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Each of the summands here are plotted in Fig. 3 for the special case of binary classification (two labels only). Each is unfavorably maximized at the most mixed state, where &lt;span class="math"&gt;\(p_1 = 0.5\)&lt;/span&gt;, and minimized in the pure states, where &lt;span class="math"&gt;\(p_1 = 0,1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/tree_errors.jpg"&gt;&lt;img alt="tree_errors" src="https://efavdb.com/wp-content/uploads/2015/03/tree_errors.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Although &lt;span class="math"&gt;\(E\)&lt;/span&gt; is perhaps the most intuitive of the three measures above (it&amp;#8217;s simply the number of training examples misclassified by the tree &amp;#8212; this follows from the fact that the tree returns as hypothesis the mode in each region) the latter two have the benefit of being characterized by negative curvature as a function of the &lt;span class="math"&gt;\(p_{i,j}\)&lt;/span&gt;. This property tends to enhance the favorability of splits that generate region pairs where at least one is highly pure. At times, this can simultaneously result in the other region of the pair ending up relatively impure &amp;#8212; see Exercise 1 for details. Such moves are often ultimately beneficial, since any highly impure node that results can always be broken up in later splits anyways. The plot in Fig. 3 shows that the cross-entropy has the larger curvature of the two, and so should more highly favor such splits, at least in the binary classification case. Another nice feature of the Gini and cross-entropy functions is that &amp;#8212; in contrast to the error rate &amp;#8212; they are both smooth functions of the &lt;span class="math"&gt;\(p_{i,j}\)&lt;/span&gt;, which facilitates numerical optimization. For these reasons, one of these two functions is typically used to guide tree construction, even if &lt;span class="math"&gt;\(E\)&lt;/span&gt; is the quantity one would actually like to minimize. Tree construction proceeds as in the regression case, typically by a greedy or randomized construction, each step taken so as to minimize (\ref{gini}) or (\ref{crossentropy}), whichever is&amp;nbsp;chosen.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Bias-variance trade-off and stopping&amp;nbsp;conditions&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Decision trees that are allowed to split indefinitely will have low bias but will over-fit their training data. Placing different stopping criteria on a tree&amp;#8217;s growth can ameliorate this latter effect. Two typical conditions often used for this purpose are given by a) placing an upper bound on the number of levels permitted in the tree, or b) requiring that each region (tree leaf) retains at least some minimum number of training examples. To optimize over such constraints, one can apply&amp;nbsp;cross-validation.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Bagging, random forests, and&amp;nbsp;boosting&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Another approach to alleviating the high-variance, over-fitting issue associated with decision trees is to average over many of them. This approach is motivated by the observation that the sum of &lt;span class="math"&gt;\(N\)&lt;/span&gt; independent random variables &amp;#8212; each with variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; &amp;#8212; has a relatively reduced variance, &lt;span class="math"&gt;\(\sigma^2/N\)&lt;/span&gt;. Two common methods for carrying out summations of this sort are discussed&amp;nbsp;below.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Bagging and random&amp;nbsp;forests&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Bootstrap aggregation&lt;/em&gt;, or &amp;#8220;bagging&amp;#8221;, provides one common method for constructing ensemble tree models. In this approach, one samples with replacement to obtain &lt;span class="math"&gt;\(k\)&lt;/span&gt; separate bootstrapped training sets from the original training data. To obtain a bootstrapped subsample of a data set of size &lt;span class="math"&gt;\(N\)&lt;/span&gt;, one draws randomly from the set &lt;span class="math"&gt;\(N\)&lt;/span&gt; times with replacement. Because one samples with replacement, each bootstrapped set can contain multiple copies of some examples. The average number of unique examples in a given bootstrap is simply &lt;span class="math"&gt;\(N\)&lt;/span&gt; times the probability that any individual example makes it into the training set. This is&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{5} 
N \left [ 1 - \left(\frac{N-1}{N} \right)^N \right ] \approx N (1 - e^{-1}) \approx 0.63N,  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where the latter forms are accurate in the large &lt;span class="math"&gt;\(N\)&lt;/span&gt; limit. Once the bootstrapped data sets are constructed, an individual decision tree is fit to each, and an average or majority rule vote over the full set is used to provide the final&amp;nbsp;prediction.&lt;/p&gt;
&lt;p&gt;One nice thing about bagging methods, in general, is that one can train on the entire set of available labeled training data and still obtain an estimate of the generalization error. Such estimates are obtained by considering the error on each point in the training set, in each case averaging only over those trees that did not train on the point in question. The resulting estimate, called the out-of-bag error, typically provides a slight overestimate to the generalization error. This is because accuracy generally improves with growing ensemble size, and the full ensemble is usually about three times larger than the sub-ensemble used to vote on any particular training example in the out-of-bag error&amp;nbsp;analysis.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Random forests&lt;/em&gt; provide a popular variation on the bagging method. The individual decision trees making up a random forest are, again, each fit to an independent, bootstrapped subsample of the training data. However, at each step in their recursive construction process, these trees are restricted in that they are only allowed to split on &lt;span class="math"&gt;\(r\)&lt;/span&gt; randomly selected candidate feature directions; a new set of &lt;span class="math"&gt;\(r\)&lt;/span&gt; directions is chosen at random for each step in the tree construction. These restrictions serve to effect a greater degree of independence in the set of trees averaged over in a random forest, which in turn serves to reduce the ensemble&amp;#8217;s variance &amp;#8212; see Exercise 5 for related analysis. In general, the value of &lt;span class="math"&gt;\(r\)&lt;/span&gt; should be optimized through&amp;nbsp;cross-validation.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Boosting&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The final method we&amp;#8217;ll discuss is &lt;em&gt;boosting&lt;/em&gt;, which again consists of a set of individual trees that collectively determine the ultimate prediction returned by the model. However, in the boosting scenario, one fits each of the trees to the full data set, rather than to a small sample. Because they are fit to the full data set, these trees are usually restricted to being only two or three levels deep, so as to avoid over-fitting. Further, the individual trees in a boosted forest are constructed sequentially. For instance, in regression, the process typically works as follows: In the first step, a tree is fit to the full, original training set &lt;span class="math"&gt;\(T = \{t_i = (\textbf{f}_i, y_i)\}\)&lt;/span&gt;. Next, a second tree is constructed on the same training feature vectors, but with the original labels replaced by residuals. These residuals are obtained by subtracting out a scaled version of the predictions &lt;span class="math"&gt;\(\hat{y}^1\)&lt;/span&gt; returned by the first tree,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{6} 
y_i^{(1)} \equiv y_i - \alpha \hat{y}_i^1.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Here, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is the scaling factor, or learning rate &amp;#8212; choosing its value small results in a gradual learning process, which often leads to very good predictions. Once the second tree is constructed, a third tree is fit to the new residuals, obtained by subtracting out the scaled hypothesis of the second tree, &lt;span class="math"&gt;\(y_i^{(2)} \equiv y_i^{(1)} - \alpha \hat{y}_i^2\)&lt;/span&gt;. The process repeats until &lt;span class="math"&gt;\(m\)&lt;/span&gt; trees are constructed, with their &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;-scaled hypotheses summing to a good estimate to the underlying&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;Boosted classification tree ensembles are constructed in a fashion similar to that above. However, in contrast to the regression scenario, the same, original training labels are used to fit each new tree in the ensemble (as opposed to an evolving residual). To bring about a similar, gradual learning process, boosted classification ensembles instead sample from the training set with weights that are sample-dependent and that change over time: When constructing a new tree for the ensemble, one more heavily weights those examples that have been poorly fit in prior iterations. AdaBoost is a popular algorithm for carrying out boosted classification. This and other generalizations are covered in the text &lt;a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/"&gt;Elements of Statistical Learning&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Implementation runtime&amp;nbsp;complexity&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Before concluding, we take here a moment to consider the runtime complexity of tree construction. This exercise gives one a sense of how tree algorithms are constructed in practice. We begin by considering the greedy construction of a single classification tree. The extension to regression trees is&amp;nbsp;straightforward.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Individual decision&amp;nbsp;trees&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Consider the problem of greedily training a single classification tree on a set of &lt;span class="math"&gt;\(n\)&lt;/span&gt; training examples having &lt;span class="math"&gt;\(k\)&lt;/span&gt; features. In order to construct our tree, we take as a first step the sorting of the &lt;span class="math"&gt;\(n\)&lt;/span&gt; training vectors along each of the &lt;span class="math"&gt;\(k\)&lt;/span&gt; directions, which will facilitate later optimal split searches. Recall that optimized algorithms, e.g. &lt;a href="http://en.wikipedia.org/wiki/Merge_sort"&gt;merge-sort&lt;/a&gt;, require &lt;span class="math"&gt;\(O(n \log n)\)&lt;/span&gt; time to sort along any one feature direction, so sorting along all &lt;span class="math"&gt;\(k\)&lt;/span&gt; will require &lt;span class="math"&gt;\(O(k \times n \log n)\)&lt;/span&gt; time. After this pre-sort step is complete, we must seek the currently optimal split, carry it out, and then iterate. We will show that &amp;#8212; with care &amp;#8212; the full iterative process can also be carried out in &lt;span class="math"&gt;\(O(k \times n \log n)\)&lt;/span&gt;&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;Focus on an intermediate moment in the construction process where one particular node has just been split, resulting in two new regions, &lt;span class="math"&gt;\(R_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(R_2\)&lt;/span&gt; containing &lt;span class="math"&gt;\(n_{R_1}\)&lt;/span&gt; and &lt;span class="math"&gt;\(n_{R_2}\)&lt;/span&gt; training examples, respectively. We can assume that we have already calculated and stored the optimal split for every other region in the tree during prior iterations. Therefore, to determine which region contains the next optimal split, the only new searches we need to carry out are within regions &lt;span class="math"&gt;\(R_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(R_2\)&lt;/span&gt;. Focus on &lt;span class="math"&gt;\(R_1\)&lt;/span&gt; and suppose that we have been passed down the following information characterizing it: the number of training examples of each class that it contains, its total number of training examples &lt;span class="math"&gt;\(n_{R_1}\)&lt;/span&gt;, its cost function value &lt;span class="math"&gt;\(J\)&lt;/span&gt; (cross entropy, say), and for each of the &lt;span class="math"&gt;\(k\)&lt;/span&gt; feature directions, a separate list of the region&amp;#8217;s examples, sorted along that direction. To find the optimal split, we must consider all &lt;span class="math"&gt;\(k \times (n_{R_1}-1)\)&lt;/span&gt; possible cuts of this region [&lt;em&gt;Aside&lt;/em&gt;: We must check all possible cuts because the cost function can have many local minima. The precludes the use of gradient-descent-like algorithms to find the optimal split.], evaluating the cost function reduction for&amp;nbsp;each.&lt;/p&gt;
&lt;p&gt;The left side of Fig. 4 illustrates one method for efficiently carrying out these test cuts: For each feature direction, we proceed sequentially through that direction&amp;#8217;s ordered list, considering one cut at a time. In the first cut, we take only one example in the left sub-region induced, and all others on the right. In the second cut, we have the first two examples in the left sub-region, etc. Proceeding in this way, it turns out that the cost function of each new candidate split considered can always be evaluated in &lt;span class="math"&gt;\(O(1)\)&lt;/span&gt; time. This is because we start with knowledge of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; before any cut is taken, and the cost functions we consider here can each be updated in &lt;span class="math"&gt;\(O(1)\)&lt;/span&gt; time whenever only a single example is either added to or removed from a given region &amp;#8212; see exercises 3 and 4 for details. Using this approach, we can therefore try all possible cuts of region &lt;span class="math"&gt;\(R_1\)&lt;/span&gt; in &lt;span class="math"&gt;\(O(k \times n_{R_1})\)&lt;/span&gt;&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/tree_complexity.jpg"&gt;&lt;img alt="tree_complexity" src="https://efavdb.com/wp-content/uploads/2015/03/tree_complexity.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The above analysis gives the time needed to search for the optimal split within &lt;span class="math"&gt;\(R_1\)&lt;/span&gt;, and a similar form holds for &lt;span class="math"&gt;\(R_2\)&lt;/span&gt;. Once these are determined, we can quickly select the current, globally-optimal split [&lt;em&gt;Aside&lt;/em&gt;: Using a heap data structure, the global minimum can be obtained in at most &lt;span class="math"&gt;\(O(\log n)\)&lt;/span&gt; time. Summing this effort over all nodes of the tree will lead to roughly &lt;span class="math"&gt;\(O(n \log n)\)&lt;/span&gt; evaluations.]. Carrying out this split entails partitioning the region selected into two and passing the necessary information down to each. We leave as an exercise the fact that the passing of needed information &amp;#8212; ordered lists, etc. &amp;#8212; can be carried out in &lt;span class="math"&gt;\(O(k \times n_s)\)&lt;/span&gt; time, with &lt;span class="math"&gt;\(n_s\)&lt;/span&gt; the size of the parent region being split. The total tree construction time can now be obtained by summing up each node&amp;#8217;s search and split work, which both require &lt;span class="math"&gt;\(O(k \times n_s\)&lt;/span&gt;) computations. Assuming a roughly balanced tree having about &lt;span class="math"&gt;\(\log n\)&lt;/span&gt; layers &amp;#8212; see right side of Fig. 4 &amp;#8212; we obtain &lt;span class="math"&gt;\(O(k \times n \log n)\)&lt;/span&gt;, the runtime scaling&amp;nbsp;advertised.&lt;/p&gt;
&lt;p&gt;In summary, we see that achieving &lt;span class="math"&gt;\(O(k \times n \log n)\)&lt;/span&gt; scaling requires a) a pre-sort, b) a data structure for storing certain important facts about each region, including its optimal split, once determined, and also pointers to its parent and children, c) an efficient method for passing relevant information down to daughter regions during a split instance, d) a heap to enable quick selection of the currently optimal split, and e) a cost function that can be updated efficiently under single training example insertions or&amp;nbsp;removals.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Forests,&amp;nbsp;parallelization&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;If a forest of &lt;span class="math"&gt;\(N\)&lt;/span&gt; trees is to be constructed, each will require &lt;span class="math"&gt;\(O(k \times n \log n)\)&lt;/span&gt; time to construct. Recall, however, that the trees of a bagged forest can be constructed independently of one another. This allows for bagged forest constructions to take advantage of parallelization, facilitating their application in the large &lt;span class="math"&gt;\(N\)&lt;/span&gt; limit. In contrast, the trees of a boosted forest are constructed in sequence and so cannot be parallelized in a similar manner. However, note that optimal split searches along different feature directions can always be run in parallel. This can speed up individual tree construction times in either&amp;nbsp;case.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In this note, we&amp;#8217;ve quickly reviewed the basics of tree-based models and their constructions. Looking back over what we have learned, we can now consider some of the reasons why tree-based methods are so popular among practitioners. First &amp;#8212; and very importantly &amp;#8212; individual trees are often useful for gaining insight into the geometry of datasets in high dimensions. This is because tree structures can be visualized using simple diagrams, like that in Fig. 1. In contrast, most other machine learning algorithm outputs cannot be easily visualized &amp;#8212; consider, e.g., support-vector machines, which return hyper-plane decision boundaries. A related point is that tree-based approaches are able to automatically fit non-linear decision boundaries. In contrast, linear algorithms can only fit such boundaries if appropriate non-linear feature combinations are constructed. This requires that one first identify these appropriate feature combinations, which can be a challenging task for feature spaces that cannot be directly visualized. Three additional positive qualities of decision trees are given by a) the fact that they are insensitive to feature scale, which reduces the need for related data preprocessing, b) the fact that they can make use of data missing certain feature values, and c) that they are relatively robust against outliers and noisy-labeling&amp;nbsp;issues.&lt;/p&gt;
&lt;p&gt;Although boosted and random forests are not as easily visualized as individual decision trees, these ensemble methods are popular because they are often quite competitive. Boosted forests typically have a slightly lower generalization error than their random forest counterparts. For this reason, they are often used when accuracy is highly-valued &amp;#8212; see last figure for an example learning curve consistent with this rule of thumb: Generalization error rate versus training set size for a hand-written digits learning problem. However, the individual trees in a bagged forest can be constructed in parallel. This benefit &amp;#8212; not shared by boosted forests &amp;#8212; can favor random forests as a go-to, out-of-box approach for treating large-scale machine learning&amp;nbsp;problems.&lt;/p&gt;
&lt;p&gt;Exercises follow that detail some further points of interest relating to decision trees and their&amp;nbsp;construction.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/tree_learning.jpg"&gt;&lt;img alt="tree_learning" src="https://efavdb.com/wp-content/uploads/2015/03/tree_learning.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;[1] &lt;a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/"&gt;Elements of Statistical Learning&lt;/a&gt;, by Hastie, Tibshirani, Friedman&lt;br&gt;
[2] &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;An Introduction to Statistical Learning&lt;/a&gt;, by James, Witten, Hastie, and Tibshirani&lt;br&gt;
[3] &lt;a href="http://link.springer.com/article/10.1023%2FA%3A1010933404324"&gt;Random Forests&lt;/a&gt;, by Breiman (Machine Learning, 45, 2001).&lt;br&gt;
[4] &lt;a href="http://scikit-learn.org/stable/modules/tree.html"&gt;Sk-learn documentation&lt;/a&gt; on runtime complexity, see section&amp;nbsp;1.8.4.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Exercises&lt;/strong&gt;&lt;/h2&gt;
&lt;h4&gt;1) Jensen&amp;#8217;s inequality and classification tree cost&amp;nbsp;functions&lt;/h4&gt;
&lt;p&gt;​a) Consider a real function &lt;span class="math"&gt;\(y(x)\)&lt;/span&gt; with non-positive curvature. Consider sampling &lt;span class="math"&gt;\(y\)&lt;/span&gt; at values &lt;span class="math"&gt;\(\{x_1, x_2, \ldots, x_m\}\)&lt;/span&gt;. By considering graphically the centroid of the points &lt;span class="math"&gt;\(\{(x_i, y(x_i))\}\)&lt;/span&gt;, prove Jensen&amp;#8217;s inequality,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{7}
y\left ( \frac{1}{m} \sum_i x_i \right) \geq \frac{1}{m}\sum_i y(x_i).  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
When does equality&amp;nbsp;hold?&lt;/p&gt;
&lt;p&gt;​b) Consider binary tree classification guided by the minimization of the error rate (\ref{errorrate}). If all possible cuts of a particular region always leave class &lt;span class="math"&gt;\(0\)&lt;/span&gt; in the minority in both resulting sub-regions, will a cut here ever be&amp;nbsp;made?&lt;/p&gt;
&lt;p&gt;​c) How about if (\ref{gini}) or (\ref{crossentropy}) is used as the cost&amp;nbsp;function?&lt;/p&gt;
&lt;h4&gt;2) Decision tree prediction runtime&amp;nbsp;complexity&lt;/h4&gt;
&lt;p&gt;Suppose one has constructed an approximately balanced decision tree, where each node contains one of the &lt;span class="math"&gt;\(n\)&lt;/span&gt; training examples used for its construction. In general, approximately how long will it take to determine the region &lt;span class="math"&gt;\(R_i\)&lt;/span&gt; to which a supplied feature vector belongs? How about for ensemble models? Any difference between typical bagged and boosted&amp;nbsp;forests?&lt;/p&gt;
&lt;h4&gt;3) Classification tree construction runtime&amp;nbsp;complexity&lt;/h4&gt;
&lt;p&gt;​a) Consider a region &lt;span class="math"&gt;\(R\)&lt;/span&gt; within a classification tree containing &lt;span class="math"&gt;\(n_i\)&lt;/span&gt; training examples of class &lt;span class="math"&gt;\(i\)&lt;/span&gt;, with &lt;span class="math"&gt;\(\sum_i n_i = N\)&lt;/span&gt;. Now, suppose a cut is considered in which a single training example of class &lt;span class="math"&gt;\(1\)&lt;/span&gt; is removed from the region. If the region&amp;#8217;s cross-entropy before the cut is given by &lt;span class="math"&gt;\(CE_0\)&lt;/span&gt;, show that its entropy after the cut will be given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{DEntropy} \tag{8}
CE_f = CE_0 - N \log\left (\frac{N}{N-1} \right) + \log \left (\frac{n_1}{N-1} \right) - (n_1 -1) \log \left (\frac{n_1 - 1}{n_1} \right).  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
If &lt;span class="math"&gt;\(CE_0\)&lt;/span&gt;, &lt;span class="math"&gt;\(N\)&lt;/span&gt;, and the &lt;span class="math"&gt;\(\{n_i\}\)&lt;/span&gt; values are each stored in memory for a given region, this equation can be used to evaluate in &lt;span class="math"&gt;\(O(1)\)&lt;/span&gt; time the change in its entropy with any single example removal. Similarly, the change in entropy of a region upon addition of a single training example can also be evaluated in &lt;span class="math"&gt;\(O(1)\)&lt;/span&gt; time. Taking advantage of this is essential for obtaining an efficient tree construction&amp;nbsp;algorithm.&lt;/p&gt;
&lt;p&gt;​b) Show that a region&amp;#8217;s Gini coefficient (\ref{gini}) can also be updated in &lt;span class="math"&gt;\(O(1)\)&lt;/span&gt; time with any single training example&amp;nbsp;removal.&lt;/p&gt;
&lt;h4&gt;4)Regression tree construction runtime&amp;nbsp;complexity.&lt;/h4&gt;
&lt;p&gt;Consider a region &lt;span class="math"&gt;\(R\)&lt;/span&gt; within a regression tree containing &lt;span class="math"&gt;\(N\)&lt;/span&gt; training examples, characterized by mean label value &lt;span class="math"&gt;\(\overline{y}\)&lt;/span&gt; and cost value (\ref{treecost}) given by &lt;span class="math"&gt;\(J\)&lt;/span&gt; (&lt;span class="math"&gt;\( N\)&lt;/span&gt; times the region&amp;#8217;s label variance). Suppose a cut is considered in which a single training example having label &lt;span class="math"&gt;\(y\)&lt;/span&gt; is removed from the region. Show that after the cut is taken the new mean training label and cost function values within the region are given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{9}
\overline{y}_f &amp;amp;=&amp;amp; \frac{1}{N-1} \left ( N \overline{y} - y \right) \ \label{regression_cost_change}  
J_f &amp;amp;=&amp;amp; J - \frac{N}{N-1} \left ( \overline{y} - y\right)^2.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
These results allow for the cost function of a region to be updated in &lt;span class="math"&gt;\(O(1)\)&lt;/span&gt; time as single examples are either inserted or removed from it. Their simplicity is a special virtue of the squared error cost function. Other cost function choices will generally require significant increases in tree construction runtime complexity, as most require a fresh evaluation with each new subset of examples&amp;nbsp;considered.&lt;/p&gt;
&lt;h4&gt;5) Chebychev&amp;#8217;s inequality and random forest classifier&amp;nbsp;accuracy&lt;/h4&gt;
&lt;p&gt;Adapted from&amp;nbsp;[3].&lt;/p&gt;
&lt;p&gt;​a) Let &lt;span class="math"&gt;\(x\)&lt;/span&gt; be a random variable with well-defined mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;. Prove Chebychev&amp;#8217;s inequality,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{Cheby} \tag{10}
P(x \geq \mu + t) \leq \frac{\sigma^2}{t^2}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;​b) Consider a binary classification problem aimed at fitting a sampled function &lt;span class="math"&gt;\(y(\textbf{f})\)&lt;/span&gt; that takes values in &lt;span class="math"&gt;\(\{ 0,1\}\)&lt;/span&gt;. Suppose a decision tree &lt;span class="math"&gt;\(h_{\theta}(\textbf{f})\)&lt;/span&gt; is constructed on the samples using a greedy, randomized approach, where the randomization is characterized by the parameter &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. Define the classifier&amp;#8217;s &lt;em&gt;margin&lt;/em&gt; &lt;span class="math"&gt;\(m\)&lt;/span&gt; at &lt;span class="math"&gt;\(\textbf{f}\)&lt;/span&gt; by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{tree_margin_def} \tag{11}
m(\theta, \textbf{f}) =-1 + 2 \left [ y * h_{\theta}+ (1- y) * (1 - h_{\theta}) \right ]  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This is equal to &lt;span class="math"&gt;\(1\)&lt;/span&gt; if &lt;span class="math"&gt;\(h_{\theta}\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; agree at &lt;span class="math"&gt;\(\textbf{f}\)&lt;/span&gt;, and &lt;span class="math"&gt;\(-1\)&lt;/span&gt; otherwise. Now, consider a random forest, consisting of many such trees, each obtained by sampling from the same &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; distribution. Argue using (\ref{Cheby}), (\ref{tree_margin_def}), and the law of large numbers that the generalization error &lt;span class="math"&gt;\(GE\)&lt;/span&gt; of the forest is bounded by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{rf_bound} \tag{12}
GE \leq \frac{var_{\textbf{f}}\left( \langle m(\theta, \textbf{f}) \rangle_{\theta} \right)}{\langle m(\theta, \textbf{f}) \rangle_{\theta, \textbf{f}}^2 }  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;​c) Show that&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{margin_var} \tag{13} 
var_{\textbf{f}}\left( \langle m(\theta, \textbf{f}) \rangle_{\theta} \right) = \langle cov_{\textbf{f}}(m(\theta, \textbf{f}), m(\theta^{\prime},\textbf{f})) \rangle_{\theta, \theta^{\prime}}  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;​d) Writing,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{14} 
\rho \equiv \frac{\langle cov_{\textbf{f}}(m(\theta, \textbf{f}), m(\theta^{\prime},\textbf{f})) \rangle_{\theta, \theta^{\prime}}}  
{\langle \sqrt{var_{\textbf{f}}(m(\theta, \textbf{f}))} \rangle_{\theta}^2},  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
for the &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, &lt;span class="math"&gt;\(\theta^{\prime}\)&lt;/span&gt;-averaged margin-margin correlation coefficient, show that&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  
var_{\textbf{f}}\left( \langle m(\theta, \textbf{f}) \rangle_{\theta} \right) \leq \rho \langle var_{\textbf{f}}(m(\theta, \textbf{f})) \rangle_{\theta} \leq \rho \left ( 1 - \langle m(\theta, \textbf{f}) \rangle_{\theta, \textbf{f}}^2\right).  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Combining with (\ref{rf_bound}), this gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\label{tree_bound_final} \tag{15} 
GE \leq \rho \times \frac{ 1 - \langle m(\theta, \textbf{f}) \rangle_{\theta, \textbf{f}}^2 }{ \langle m(\theta, \textbf{f}) \rangle_{\theta, \textbf{f}}^2 }.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The bound (\ref{tree_bound_final}) implies that a random forest&amp;#8217;s generalization error is reduced if the individual trees making up the forest have a large average margin, and also if the trees are relatively-uncorrelated with each&amp;nbsp;other.&lt;/p&gt;
&lt;p&gt;Cover image by &lt;a href="https://www.flickr.com/photos/roberts87/2798303714"&gt;roberts87&lt;/a&gt;, &lt;a href="https://creativecommons.org/licenses/by-nc-sa/2.0/legalcode"&gt;creative commons license&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Appendix: python/sk-learn&amp;nbsp;implementations&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Here, we provide the python/sk-learn code used to construct the final figure in the body of this note: Learning curves on sk-learn&amp;#8217;s &amp;#8220;digits&amp;#8221; dataset for a single tree, a random forest, and a boosted&amp;nbsp;forest.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_digits&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GradientBoostingClassifier&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# load data: digits.data and digits.target,  &lt;/span&gt;
&lt;span class="c1"&gt;# array of features and labels, resp.  &lt;/span&gt;
&lt;span class="n"&gt;digits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_digits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_class&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;n_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;  
&lt;span class="n"&gt;t1_accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;  
&lt;span class="n"&gt;t2_accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;  
&lt;span class="n"&gt;t3_accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="c1"&gt;# below, we average over &amp;quot;trials&amp;quot; num of fits for each sample  &lt;/span&gt;
&lt;span class="c1"&gt;# size in order to estimate the average generalization error.  &lt;/span&gt;
&lt;span class="n"&gt;trials&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;

&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;clf2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;clf3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;num_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;

&lt;span class="c1"&gt;# loop over different training set sizes  &lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num_train&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

&lt;span class="n"&gt;acc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;acc2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;acc3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
    &lt;span class="n"&gt;perm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;]]))&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  
        &lt;span class="n"&gt;perm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;permutation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;  
    &lt;span class="n"&gt;acc1&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;:]],&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;:]])&lt;/span&gt;

    &lt;span class="n"&gt;clf2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;  
    &lt;span class="n"&gt;acc2&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;clf2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;:]],&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;:]])&lt;/span&gt;

    &lt;span class="n"&gt;clf3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;  
    &lt;span class="n"&gt;acc3&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;clf3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;:]],&lt;/span&gt;
        &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;:]])&lt;/span&gt;

    &lt;span class="n"&gt;n_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
    &lt;span class="n"&gt;t1_accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;acc1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
    &lt;span class="n"&gt;t2_accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;acc2&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
    &lt;span class="n"&gt;t3_accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;acc3&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;pylab&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;t1_accuracy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;t2_accuracy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;green&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;t3_accuracy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category></entry><entry><title>Machine Learning Methods: Classification without negative examples</title><link href="https://efavdb.com/methods-regression-without-negative-examples" rel="alternate"></link><published>2014-12-20T09:58:00-08:00</published><updated>2014-12-20T09:58:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2014-12-20:/methods-regression-without-negative-examples</id><summary type="html">&lt;p&gt;Here, we discuss some methods for carrying out classification when only positive examples are available. The latter half of our discussion borrows heavily from &lt;span class="caps"&gt;W.S.&lt;/span&gt; Lee and B. Liu, Proc. &lt;span class="caps"&gt;ICML&lt;/span&gt;-2003 (2003), which we supplement&amp;nbsp;somewhat.  &lt;/p&gt;
&lt;h2&gt;Generic logistic&amp;nbsp;regression&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/Logistic_regression"&gt;Logistic regression&lt;/a&gt; is a commonly used tool for estimating …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here, we discuss some methods for carrying out classification when only positive examples are available. The latter half of our discussion borrows heavily from &lt;span class="caps"&gt;W.S.&lt;/span&gt; Lee and B. Liu, Proc. &lt;span class="caps"&gt;ICML&lt;/span&gt;-2003 (2003), which we supplement&amp;nbsp;somewhat.  &lt;/p&gt;
&lt;h2&gt;Generic logistic&amp;nbsp;regression&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/Logistic_regression"&gt;Logistic regression&lt;/a&gt; is a commonly used tool for estimating the level sets of a Boolean function &lt;span class="math"&gt;\(y\)&lt;/span&gt; on a set of feature vectors &lt;span class="math"&gt;\(\textbf{F}\)&lt;/span&gt;: In a sense, you can think of it as a method for playing the game &amp;#8220;Battleship&amp;#8221; on whatever data set you&amp;#8217;re interested in. Its application requires knowledge of the &lt;span class="math"&gt;\(\{(\textbf{f}_i,y_i)\}\)&lt;/span&gt; pairs on a training set &lt;span class="math"&gt;\(\textbf{E} \subseteq \textbf{F}\)&lt;/span&gt;, with label &lt;span class="math"&gt;\(y_i = 0,1\)&lt;/span&gt; for negative and positive examples, respectively. Given these training examples, logistic regression estimates for arbitrary feature vector &lt;span class="math"&gt;\(\textbf{f}\)&lt;/span&gt;,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
h(\textbf{f}) = \frac{1}{1 + \exp \left [ - \textbf{T} \cdot \textbf{f} \right]} \approx y \tag{1}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where the coefficient vector &lt;span class="math"&gt;\(\textbf{T}\)&lt;/span&gt; is taken to be that vector that minimizes&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{2}
J(h) \equiv -\frac{1}{\vert \textbf{E} \vert}\sum_{i=1}^{\vert \textbf{E} \vert} y_i \log(h_i) + (1-y_i) \log(1- h_i) + \frac{\Lambda}{2}\sum_j T_j^2,
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
a convex cost function that strongly penalizes poor estimates on the training&amp;nbsp;set.&lt;/p&gt;
&lt;h2&gt;Problem statement: no negative&amp;nbsp;examples&lt;/h2&gt;
&lt;p&gt;Consider now a situation where all training examples given are positive &amp;#8212; i.e., no negative examples are available. One realistic realization of this scenario might involve a simple data set of movies already viewed by some Netflix customer. From this information, one would like to estimate the full subset of the available movies that the customer would watch, given time. We&amp;#8217;ll assign value &lt;span class="math"&gt;\(y = 1\)&lt;/span&gt; to such movies and &lt;span class="math"&gt;\(y=0\)&lt;/span&gt; to movies he wouldn&amp;#8217;t watch. Notice that the generic logistic regression approach outlined above would return a default-positive result if applied to this problem: Assigning &lt;span class="math"&gt;\(h = 1\)&lt;/span&gt; to all of &lt;span class="math"&gt;\(\textbf{F}\)&lt;/span&gt; minimizes &lt;span class="math"&gt;\(J\)&lt;/span&gt;. This means that no information contained in &lt;span class="math"&gt;\(\textbf{E}\)&lt;/span&gt; is actually utilized in the logistic learning process &amp;#8212; a counterintuitive choice for structured &lt;span class="math"&gt;\(\textbf{E}\)&lt;/span&gt; (e.g., the case where all movies watched thus far have been in a single category &amp;#8212; martial arts films,&amp;nbsp;say).&lt;/p&gt;
&lt;h2&gt;Noisy&amp;nbsp;labeling&lt;/h2&gt;
&lt;p&gt;Some reasonable, alternative approaches do not return the default-positive response in the situation above. To see this, we first review here noisy labeling problems. Suppose we are given a training set with noisy labeling &lt;span class="math"&gt;\(y^{\prime}\)&lt;/span&gt;: Truly-positive examples &lt;span class="math"&gt;\((y = 1)\)&lt;/span&gt; are stochastically mislabeled in this set with frequency &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; as negative &lt;span class="math"&gt;\((y^{\prime} = 0)\)&lt;/span&gt;, and truly-negative examples &lt;span class="math"&gt;\((y=0)\)&lt;/span&gt; are mislabeled with frequency &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; as positive &lt;span class="math"&gt;\((y^{\prime} = 1)\)&lt;/span&gt;. For hypothesis &lt;span class="math"&gt;\(h\)&lt;/span&gt;,&amp;nbsp;let
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{3}
C(h) = Pr[h = 0 \vert y = 1]+ Pr[h = 1 \vert y= 0],
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
the rate at which &lt;span class="math"&gt;\(h\)&lt;/span&gt; mislabels positive examples in the training set added to the rate at which it mislabels negative examples. Similarly, we define &lt;span class="math"&gt;\(C^{\prime}(h)\)&lt;/span&gt; as above, but with &lt;span class="math"&gt;\(y\)&lt;/span&gt; replaced by &lt;span class="math"&gt;\(y^{\prime}\)&lt;/span&gt;. Because &lt;span class="math"&gt;\(y^{\prime}\)&lt;/span&gt; is stochastic, we also average it in this case,&amp;nbsp;giving
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{4}
C^{\prime}(h) = \left \langle Pr[h = 0 \vert y^{\prime} = 1]+ Pr[h = 1 \vert y^{\prime}= 0] \right \rangle_{y^{\prime}}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
With these definitions, we have [see Blum and Michael (1998) or derive&amp;nbsp;yourself] 
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{5}
C(h) \propto C^{\prime}(h),
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
with &lt;span class="math"&gt;\(\text{sign}(C) = \text{sign}(1 - \alpha - \beta) \times \text{sign}(C^{\prime})\)&lt;/span&gt;. This result is very useful whenever we take &lt;span class="math"&gt;\(C(h)\)&lt;/span&gt; as our cost function&lt;span class="math"&gt;\(^1\)&lt;/span&gt;: Provided the total noise rate &lt;span class="math"&gt;\(\alpha + \beta &amp;lt;1\)&lt;/span&gt;, it implies that we can find the &amp;#8220;&lt;span class="math"&gt;\(C\)&lt;/span&gt;-optimizing&amp;#8221; &lt;span class="math"&gt;\(h\)&lt;/span&gt; within any class of hypotheses by optimizing instead &lt;span class="math"&gt;\(C^{\prime}\)&lt;/span&gt; &amp;#8212; a quantity that we can estimate given any particular noisy labeling realization &lt;span class="math"&gt;\(y^{\prime}_0\)&lt;/span&gt;&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{6}
C^{\prime}(h) \approx \left (Pr[h = 0 \vert y^{\prime} = 1]+ Pr[h = 1 \vert y^{\prime}= 0] \right ) \vert_{y^{\prime} =y^{\prime}_0}.
\end{eqnarray}&lt;/div&gt;
&lt;h2&gt;Application to no-negatives&amp;nbsp;problem&lt;/h2&gt;
&lt;p&gt;To make connection between the no-negatives and noisy-labeling problems, one can remodel the former as one where all unlabeled examples are considered to actually be negative examples (&lt;span class="math"&gt;\(y^{\prime}_0 = 0\)&lt;/span&gt;). This relabeling gives a correct label to all examples in the original training set &lt;span class="math"&gt;\(\textbf{E}\)&lt;/span&gt; (where &lt;span class="math"&gt;\(y = y^{\prime}_0 = 1\)&lt;/span&gt;) as well as to all truly-negative examples (where &lt;span class="math"&gt;\(y = y^{\prime}_0 = 0\)&lt;/span&gt;). However, all positive examples not in &lt;span class="math"&gt;\(\textbf{E}\)&lt;/span&gt; are now incorrectly labeled (they are assigned &lt;span class="math"&gt;\(y^{\prime}_0 = 0\)&lt;/span&gt;): This new labeling &lt;span class="math"&gt;\(y^{\prime}_0\)&lt;/span&gt; is noisy, with &lt;span class="math"&gt;\(\alpha = Pr(y^{\prime}_0 =0 \vert y =1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta = Pr(y^{\prime}_0 =1 \vert y = 0 ) = 0\)&lt;/span&gt;. We can now apply the Blum and Michael approach: We first approximate &lt;span class="math"&gt;\(C^{\prime}\)&lt;/span&gt; as above, making use of the particular noisy label we have access to. Second, we minimize the approximated &lt;span class="math"&gt;\(C^{\prime}\)&lt;/span&gt; over some class of hypotheses &lt;span class="math"&gt;\(\{h\}\)&lt;/span&gt;. This will in general return a non-uniform hypothesis (i.e., one that now makes use of the information contained in &lt;span class="math"&gt;\(\textbf{E}\)&lt;/span&gt;).&lt;/p&gt;
&lt;h2&gt;Hybrid noisy-logistic approach of Lee and Liu (plus a&amp;nbsp;tweak)&lt;/h2&gt;
&lt;p&gt;The &lt;span class="math"&gt;\(C \propto C^{\prime}\)&lt;/span&gt; result is slick and provides a rigorous method for attacking the no-negatives problem. Unfortunately, &lt;span class="math"&gt;\(C^{\prime}\)&lt;/span&gt; is not convex, and as a consequence it can be difficult to minimize for large &lt;span class="math"&gt;\(\vert \textbf{F} \vert\)&lt;/span&gt; &amp;#8212; in fact, its minimization is &lt;span class="caps"&gt;NP&lt;/span&gt;-hard. To mitigate this issue, Lee and Liu combine the noisy relabeling idea &amp;#8212; now well-motivated by the Blum and Michael analysis &amp;#8212; with logistic regression. They also suggest a particular re-weighting of the observed samples. However, we think that their particular choice of weighting is not very well-motivated, and we suggest here that one should instead pick an optimal weighting through consideration of a cross-validation set. With this approach, the method&amp;nbsp;becomes:&lt;/p&gt;
&lt;p&gt;​1) As above, assign examples in &lt;span class="math"&gt;\(\textbf{E}\)&lt;/span&gt; label &lt;span class="math"&gt;\(y^{\prime} = 1\)&lt;/span&gt; and examples in &lt;span class="math"&gt;\(\textbf{F} - \textbf{E}\)&lt;/span&gt; label &lt;span class="math"&gt;\(y^{\prime} = 0\)&lt;/span&gt;.&lt;br&gt;
2) Construct the weighted logistic cost&amp;nbsp;function
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{7}
J(h; \rho) \equiv -\frac{1}{\vert \textbf{E} \vert}\sum_{i=1}^{\vert \textbf{E} \vert}  
\rho y^{\prime}_i \log(h_i) + (1-\rho) (1-y^{\prime}_i) \log(1- h_i) + \frac{\Lambda}{2}\sum_j T_j^2,
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
with &lt;span class="math"&gt;\(\rho \in [0,1]\)&lt;/span&gt;, a re-weighting factor. (Lee and Liu suggest&lt;span class="math"&gt;\(^2\)&lt;/span&gt; using &lt;span class="math"&gt;\(\rho = 1-\frac{\vert \textbf{E} \vert}{\vert \textbf{F} \vert}\)&lt;/span&gt;).&lt;br&gt;
3) Minimize &lt;span class="math"&gt;\(J\)&lt;/span&gt;. By evaluating performance on a cross-validation set using your favorite criteria, optimize &lt;span class="math"&gt;\(\rho\)&lt;/span&gt; and &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Toy&amp;nbsp;example&lt;/h2&gt;
&lt;p&gt;Here, we provide a toy system that allows for a sense of how the latter method discussed above works in practice. Given is a set of &lt;span class="math"&gt;\(60\)&lt;/span&gt; grid points in the plane, which can be added/subtracted individually to the positive training set (&lt;span class="math"&gt;\(\textbf{E}\)&lt;/span&gt;, green fill) by mouse click (a few are selected by default). The remaining points are considered to not be in the training set, but are relabeled as negative examples &amp;#8212; this introduces noise, as described above. Clicking compute returns the &lt;span class="math"&gt;\(h\)&lt;/span&gt; values for each grid point, determined by minimizing the weighted cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; above: Here, we use the features &lt;span class="math"&gt;\(\{1,x,y,x^2,xy,\)&lt;/span&gt; &lt;span class="math"&gt;\(y^2,x^3, x^2 y,\)&lt;/span&gt; &lt;span class="math"&gt;\(x y^2, y^3\}\)&lt;/span&gt; to characterize each point. Those points with &lt;span class="math"&gt;\(h\)&lt;/span&gt; values larger than &lt;span class="math"&gt;\(0.5\)&lt;/span&gt; (i.e., those the hypothesis estimates as positive) are outlined in black. We have found that by carefully choosing the &lt;span class="math"&gt;\(\rho\)&lt;/span&gt; and &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; values (often to be large and small, respectively), one can get a good fit to most training sets. By eye, the optimal weighting seems to often be close &amp;#8212; but not necessarily equal to &amp;#8212; the value suggested by Lee and&amp;nbsp;Liu.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 1: Interactive weighted noisy-no-negatives solver. Click &amp;#8220;compute&amp;#8221; to run logistic regression.&lt;/em&gt;
[&lt;span class="caps"&gt;NOTE&lt;/span&gt;:  new site does not yet support processing - I hope to reinsert the interactive object here as soon as&amp;nbsp;possible].&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;In this note, we have discussed methods for tackling classification sans negative examples &amp;#8212; a problem that we found perplexing at first sight. It is interesting that standard logistic regression returns a default-positive result for such problems, while the two latter methods we discussed here are based on assuming that all points in &lt;span class="math"&gt;\(\textbf{F} - \textbf{E}\)&lt;/span&gt; are negatives. In fact, this assumption seems to be the essence of all the other methods referenced in Lee and Liu&amp;#8217;s paper. Ultimately, these methods will only work if the training set provides a good sampling of the truly-positive space. If this is the case, then &amp;#8220;defocusing&amp;#8221; a bit, or blurring one&amp;#8217;s eyes, will give a good sense of where the positive space sits. In the noisy-logistic approach, a good choice of &lt;span class="math"&gt;\(\rho\)&lt;/span&gt; and &lt;span class="math"&gt;\(\Lambda\)&lt;/span&gt; should effect a good result. Of course, when the training set does not sample the full positive space well, one can still use this approach to get a good approximation for the outline of the subspace&amp;nbsp;sampled.&lt;/p&gt;
&lt;h2&gt;Footnotes&lt;/h2&gt;
&lt;p&gt;&lt;span class="math"&gt;\([1]\)&lt;/span&gt;: The target function &lt;span class="math"&gt;\(y\)&lt;/span&gt; provides the unique minimum of &lt;span class="math"&gt;\(C\)&lt;/span&gt;. Therefore, choosing &lt;span class="math"&gt;\(C\)&lt;/span&gt; as our cost function and minimizing it over some class of hypotheses &lt;span class="math"&gt;\(\{h\}\)&lt;/span&gt; should return a reasonable estimate for &lt;span class="math"&gt;\(y\)&lt;/span&gt; (indeed, if &lt;span class="math"&gt;\(y\)&lt;/span&gt; is in the search class, we will find&amp;nbsp;it).&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\([2]\)&lt;/span&gt;: Lee and Liu justify their weighting suggestion on the basis that it means that a randomly selected positive example contributes with expected weight &lt;span class="math"&gt;\(&amp;gt;0.5\)&lt;/span&gt; (see their paper). Yet, other weighting choices give even larger expected weights to the positive examples, so this is a poor justification. Nevertheless, their weighting choice does have the nice feature that the positive and negative spaces are effectively sampled with equal frequency. If optimizing over &lt;span class="math"&gt;\(\rho\)&lt;/span&gt; is too resource-costly for some application, using their weighting suggestion may be reasonable for this&amp;nbsp;reason.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Theory"></category><category term="methods"></category></entry></feed>