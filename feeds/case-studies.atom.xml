<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>EFAVDB - Case studies</title><link href="https://efavdb.com/" rel="alternate"></link><link href="https://efavdb.com/feeds/case-studies.atom.xml" rel="self"></link><id>https://efavdb.com/</id><updated>2022-08-07T00:00:00-07:00</updated><subtitle>Everybody's Favorite Data Blog</subtitle><entry><title>Spaced repetition can allow for infinite recall</title><link href="https://efavdb.com/memory%20recall" rel="alternate"></link><published>2022-08-07T00:00:00-07:00</published><updated>2022-08-07T00:00:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2022-08-07:/memory recall</id><summary type="html">&lt;p align="center"&gt;
         &lt;img src="images/jeopardy.jpg"&gt;
&lt;/p&gt;

&lt;p&gt;My friend Andrew is an advocate of the &amp;#8220;spaced repetition&amp;#8221; technique for
memorization of a great many facts [1].  The ideas behind this are&amp;nbsp;two-fold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When one first &amp;#8220;learns&amp;#8221; a new fact, it needs to be reviewed frequently in
  order to not forget it.  However, with each additional review, the …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p align="center"&gt;
         &lt;img src="images/jeopardy.jpg"&gt;
&lt;/p&gt;

&lt;p&gt;My friend Andrew is an advocate of the &amp;#8220;spaced repetition&amp;#8221; technique for
memorization of a great many facts [1].  The ideas behind this are&amp;nbsp;two-fold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When one first &amp;#8220;learns&amp;#8221; a new fact, it needs to be reviewed frequently in
  order to not forget it.  However, with each additional review, the fact can
be retained longer before a refresher is needed to maintain it in&amp;nbsp;recall.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Because of this, one can maintain a large, growing body of facts in recall
  through daily review:  Each day, one need only review for ten minutes or so,
covering a small number of facts. The facts included should be sampled from the
full library in a way that prefers newer entries, but that also sprinkles in
older facts often enough so that none are ever forgotten.  Apps have been
written to intelligently take care of the sampling process for&amp;nbsp;us.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Taking this framework as correct motivates questioning exactly how far it can
be pushed:  &lt;em&gt;Would an infinitely-long-lived, but forgetful person be able to
recall an infinite number of facts using this method? &lt;/em&gt;  &lt;span class="math"&gt;\(\ldots\)&lt;/span&gt; Below, we
show that the answer is: &lt;em&gt;&lt;span class="caps"&gt;YES&lt;/span&gt;!&lt;/em&gt;&lt;/p&gt;
&lt;h5&gt;Proof:&lt;/h5&gt;
&lt;p&gt;We first posit that the number of days &lt;span class="math"&gt;\(T\)&lt;/span&gt; that a fact can be retained before
it needs to be reviewed grows as a power-law in &lt;span class="math"&gt;\(s\)&lt;/span&gt;, the number of times it&amp;#8217;s
been reviewed so&amp;nbsp;far,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1}\label{1}
T(s) \sim s^{\gamma},
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
with &lt;span class="math"&gt;\(\gamma &amp;gt; 0\)&lt;/span&gt;. With this assumption, if &lt;span class="math"&gt;\(N(t)\)&lt;/span&gt; facts are to be recalled
from &lt;span class="math"&gt;\(t\)&lt;/span&gt; days ago, one can show that the amount of work needed today to retain
these will go like (see appendix for a proof of this&amp;nbsp;line)&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{2}\label{2}
w(t) \sim \frac{N(t)}{t^{\gamma / (\gamma + 1)}}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The total work needed today is then the sum of work needed for each past day&amp;#8217;s&amp;nbsp;facts,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{3} \label{3}
W(total) = \int_1^{\infty} \frac{N(t)}{t^{\gamma / (\gamma + 1)}} dt.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Now, each day we only have a finite amount of time to study.  However, the
above total work integral will diverge at large &lt;span class="math"&gt;\(t\)&lt;/span&gt; unless it decays faster
than &lt;span class="math"&gt;\(1/t\)&lt;/span&gt;.  To ensure this, we can limit the number of facts retained from
from &lt;span class="math"&gt;\(t\)&lt;/span&gt; days ago to go&amp;nbsp;as&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{4} \label{4}
N(t) \sim \frac{1}{t^{\epsilon}} \times \frac{1}{t^{1 / (\gamma + 1)}},
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; is some small, positive constant.  Plugging (\ref{4}) into
(\ref{3}) shows that we are guaranteed a finite required study time each day.
However, after &lt;span class="math"&gt;\(t\)&lt;/span&gt; days of study, the total number of facts retained scales&amp;nbsp;as&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
N_{total}(t) &amp;amp;\sim &amp;amp; \int_1^{t} N(t) dt \\
&amp;amp;\sim &amp;amp; \int_0^{t} \frac{1}{t^{1 / (\gamma + 1)}} \\
&amp;amp;\sim &amp;amp; t^{ \gamma / (\gamma + 1)}. \tag{5} \label{5}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Because we assume that &lt;span class="math"&gt;\(\gamma &amp;gt; 0\)&lt;/span&gt;, this grows without bound over time,
eventually allowing for an infinitely large&amp;nbsp;library.&lt;/p&gt;
&lt;p&gt;We conclude that &amp;#8212; though we can&amp;#8217;t remember a fixed number of facts from each
day in the past using spaced repetition &amp;#8212; we can ultimately recall an infinite
number of facts using this method.  To do this only requires that we gradually
curate our previously-introduced facts so that the scaling (\ref{4}) holds at
all&amp;nbsp;times.&lt;/p&gt;
&lt;h3&gt;Appendix: Proof of&amp;nbsp;(2)&lt;/h3&gt;
&lt;p&gt;Recall that we assume &lt;span class="math"&gt;\(N(s)\)&lt;/span&gt; facts have been reviewed exactly &lt;span class="math"&gt;\(s\)&lt;/span&gt; times.  On a
given day, the number of these that need to be reviewed then goes&amp;nbsp;like&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{A1}\label{A1}
W(s) \sim \frac{N(s)}{T(s)}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(T(s)\)&lt;/span&gt; is given in (\ref{1}).  This holds because each of the &lt;span class="math"&gt;\(N(s)\)&lt;/span&gt;
facts that have been studied &lt;span class="math"&gt;\(s\)&lt;/span&gt; times so far must be reviewed within &lt;span class="math"&gt;\(T(s)\)&lt;/span&gt;
days, or one will be forgotten.  During these &lt;span class="math"&gt;\(T(s)\)&lt;/span&gt; days, each will move to
having been reviewed &lt;span class="math"&gt;\(s+1\)&lt;/span&gt; times.&amp;nbsp;Therefore,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{A2} \label{A2}
\frac{ds}{dt} &amp;amp;\sim &amp;amp; \frac{1}{T(s)}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Integrating this gives &lt;span class="math"&gt;\(s\)&lt;/span&gt; as a function of &lt;span class="math"&gt;\(t\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{A3} \label{A3}
s \sim t^{1 / (\gamma + 1)}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;Plugging this last line and (1) into (A1), we get&amp;nbsp;(2).&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] See Andrew&amp;#8217;s blog post on spaced repetition &lt;a
href="https://andrewjudson.com/spaced-repitition/2022/06/03/spaced-repitition.html"&gt;
here&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category></entry><entry><title>Utility engines</title><link href="https://efavdb.com/utility-engines" rel="alternate"></link><published>2020-09-13T09:24:00-07:00</published><updated>2020-09-13T09:24:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2020-09-13:/utility-engines</id><summary type="html">&lt;p&gt;A person&amp;#8217;s happiness does not depend only on their current lot in life, but
also on the rate of change of their lot.  This is because a person&amp;#8217;s prior
history informs their expectations.  Here, we build a model that highlights
this emotional &amp;#8220;path-dependence&amp;#8221; quality of utility.  Interestingly, we …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A person&amp;#8217;s happiness does not depend only on their current lot in life, but
also on the rate of change of their lot.  This is because a person&amp;#8217;s prior
history informs their expectations.  Here, we build a model that highlights
this emotional &amp;#8220;path-dependence&amp;#8221; quality of utility.  Interestingly, we find
that it can be gamed: One can increase net happiness via a process of gradual
increased deprivation, followed by sudden jolts in increased consumption, as
shown in the cartoon below &amp;#8212; this is the best approach.  In particular, it
beats the steady consumption rate&amp;nbsp;strategy.&lt;/p&gt;
&lt;p align="center"&gt;
         &lt;img src="images/engine.png"&gt;
&lt;/p&gt;

&lt;h3&gt;The utility function&amp;nbsp;model&lt;/h3&gt;
&lt;p&gt;In this post, we assume that the utility realized by a person over a time &lt;span class="math"&gt;\(T\)&lt;/span&gt;
is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1} \label{1}  U(t) = \int_0^T \left (a \vert
x^{\prime}(t) \vert + b \vert x^{\prime}(t) \vert^2 \right)
\text{sign}(x^{\prime}(t)) dt.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here, &lt;span class="math"&gt;\(x(t)\)&lt;/span&gt; is a measure of consumption (one&amp;#8217;s &amp;#8220;lot in life&amp;#8221;) at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;.
Our model here is a natural Taylor expansion, relevant for small changes in
&lt;span class="math"&gt;\(x(t)\)&lt;/span&gt;.  It is positive when consumption is going up and negative when
consumption is going down. We&amp;#8217;ll be interested to learn whether varying &lt;span class="math"&gt;\(x(t)\)&lt;/span&gt;
subject to a fixed average constraint can result in increased net happiness,
relative to the steady state consumption solution.  The answer is yes, and this
can be understood qualitatively by considering the two terms&amp;nbsp;above:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First term,  &lt;span class="math"&gt;\(a \vert x^{\prime}(t) \vert \text{sign}(x^{\prime}(t))\)&lt;/span&gt;:  This
  term is proportional to the rate of change of consumption.
  We assume that &lt;span class="math"&gt;\(a &amp;gt; 0\)&lt;/span&gt;, so that as we start to consume less, it is negative,
as we go back up it is positive.  We will be interested in cycles that repeat
here, so will assume that our &lt;span class="math"&gt;\(x(t)\)&lt;/span&gt; is periodic with period &lt;span class="math"&gt;\(T\)&lt;/span&gt;.   In this
case, the linear term &amp;#8212; while possibly acutely felt at each moment &amp;#8212; will
integrate to an average of zero over the long&amp;nbsp;term.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Second term, &lt;span class="math"&gt;\(b \vert x^{\prime}(t) \vert^2 \text{sign}(x^{\prime}(t))\)&lt;/span&gt;: This
  term is non-linear.  It is very weak for small rates of change but kicks in
strongly whenever we have an abrupt change.  We again assume that &lt;span class="math"&gt;\(b&amp;gt;0\)&lt;/span&gt; so that
big drops in consumption are very painful,&amp;nbsp;etc.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the above comments in place, we can now see how our figure gives a net
gain in utility:  On average, only the quadratic term matters and this will
effectively only contribute during sudden jumps.  The declines in our figure
are gradual, and so contribute only weakly in this term.  However, the
increases are sudden and each give a significant utility &amp;#8220;fix&amp;#8221; as a&amp;nbsp;consequence.&lt;/p&gt;
&lt;p&gt;For those interested, we walk through the simple mathematics needed to exactly
optimize our utility function in an appendix.  Concluding comments on the
practical application of these ideas are covered&amp;nbsp;next.&lt;/p&gt;
&lt;h3&gt;Practical&amp;nbsp;considerations&lt;/h3&gt;
&lt;p&gt;A few&amp;nbsp;comments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Many people treat themselves on occasion &amp;#8212; with chocolates, vacations, etc.
  &amp;#8212; perhaps empirically realizing that varying things improves their long
  term happiness.  It is interesting to consider the possibility of optimizing
  this effect, which we do with our toy model here:  In this model, we do not
  want to live in a steady state salted with occasional treats:  Instead, we
  want the saw-tooth shape of consumption shown in our&amp;nbsp;figure.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A sad fact of life is that progress tends to be gradual, while set backs tend
  to occur suddenly &amp;#8212; e.g., stocks tend to move in patterns like this.  This
  is the worst way things could go, according to our&amp;nbsp;model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;True, human utility functions are certainly more complex than what we have
  considered&amp;nbsp;here.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is interesting to contrast models of utility with conservative physical
  systems, where the energy of a state is not path dependent, but depends only
on the current state.  Path dependence means that two identical people in the
same current situation can have very different valuations of their lot in&amp;nbsp;life.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The appendix below discusses the mathematical optimization of&amp;nbsp;(\ref{1}).&lt;/p&gt;
&lt;h3&gt;Appendix &amp;#8212; optimizing&amp;nbsp;(\ref{1})&lt;/h3&gt;
&lt;p&gt;For simplicity, we consider a path that goes down from &lt;span class="math"&gt;\(t=0\)&lt;/span&gt; to &lt;span class="math"&gt;\(t_0\)&lt;/span&gt; &amp;#8212; making
its way down by &lt;span class="math"&gt;\(\Delta x\)&lt;/span&gt;, then goes back up to where it started from &lt;span class="math"&gt;\(t_0\)&lt;/span&gt; to
&lt;span class="math"&gt;\(T\)&lt;/span&gt;.  It is easy to see that the first term integrates to zero in this case,
provided we start and end at the same value of &lt;span class="math"&gt;\(x\)&lt;/span&gt;.  Now, consider the second
term.  On the way down, we&amp;nbsp;have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} 
\int_0^{t_0} \vert x^{\prime} \vert^2 dt &amp;amp;\equiv &amp;amp; t_0 \langle \vert x^{\prime}
\vert^2 \rangle_{t_0}
\\ &amp;amp;\geq &amp;amp; t_0 \langle \vert x^{\prime} \vert \rangle^2_{t_0}
\\
&amp;amp;=&amp;amp; t_0 \left( \frac{\Delta x}{t_0} \right)^2
\tag{2}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
The inequality here is equivalent to the statement that the variance of the
rate of change of our consumption is positive.  We get equality &amp;#8212; and minimal
loss from the quadratic term on the way down &amp;#8212; if the slope is constant
throughout.  That is, we want a linear drop in &lt;span class="math"&gt;\(x(t)\)&lt;/span&gt; from &lt;span class="math"&gt;\(0\)&lt;/span&gt; to &lt;span class="math"&gt;\(t_0\)&lt;/span&gt;.  With
this choice, we&amp;nbsp;get
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\int_0^{t_0} \vert x^{\prime} \vert^2 dt = \frac{\Delta x^2}{t_0}.  \tag{3}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
On the way back up, we&amp;#8217;d like to max out the inequality analogous to above.
This is achieved by having the recovery occur as quickly as possible, say over
a window of time &lt;span class="math"&gt;\(t_{r}\)&lt;/span&gt; with &lt;span class="math"&gt;\(r\)&lt;/span&gt; standing for recovery.  We can decrease our
loss by increasing &lt;span class="math"&gt;\(t_0\)&lt;/span&gt; up to &lt;span class="math"&gt;\(t_0 \to T\)&lt;/span&gt;.  In this case, our integral of
the quadratic over all time goes&amp;nbsp;to
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\text{gain} = b \Delta x^2 \left (\frac{1}{t_r} - \frac{1}{T} \right)
\tag{4}.
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
This gives the optimal lift possible &amp;#8212; that realized by the saw-tooth approach
shown in our first figure&amp;nbsp;above. &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category></entry><entry><title>Utility functions and immigration</title><link href="https://efavdb.com/utility-functions-and-immigration" rel="alternate"></link><published>2019-06-21T09:24:00-07:00</published><updated>2019-06-21T09:24:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2019-06-21:/utility-functions-and-immigration</id><summary type="html">&lt;p&gt;We consider how the &lt;span class="caps"&gt;GDP&lt;/span&gt; or utility output of a city depends on the number of people living within it. From this, we derive some interesting consequences that can inform both government and individual attitudes towards&amp;nbsp;newcomers.&lt;/p&gt;
&lt;p&gt;Edit 9/2022: The model here can&amp;#8217;t be complete because it doesn …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We consider how the &lt;span class="caps"&gt;GDP&lt;/span&gt; or utility output of a city depends on the number of people living within it. From this, we derive some interesting consequences that can inform both government and individual attitudes towards&amp;nbsp;newcomers.&lt;/p&gt;
&lt;p&gt;Edit 9/2022: The model here can&amp;#8217;t be complete because it doesn&amp;#8217;t take into account
city capacity.  The true utility functions should be shaped like &lt;span class="math"&gt;\(S\)&lt;/span&gt;-curves.
Nevertheless, the arguments here can provide some insight into &amp;#8220;growth&amp;#8221;-phase
dynamics and&amp;nbsp;preferences.&lt;/p&gt;
&lt;h3&gt;The utility function and benefit per&amp;nbsp;person&lt;/h3&gt;
&lt;p&gt;In this post, we will consider an idealized town whose net output &lt;span class="math"&gt;\(U\)&lt;/span&gt; (the &lt;span class="caps"&gt;GDP&lt;/span&gt;) scales as a power law with the number of people &lt;span class="math"&gt;\(N\)&lt;/span&gt; living within it. That is, we&amp;#8217;ll assume,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1} \label{1}  
U(N) = a N^{\gamma}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
We&amp;#8217;ll assume that the average benefit captured per person is their share of this utility,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{2} \label{2}  
BPP(N) = U(N) / N = a N^{\gamma -1}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
What can we say about the above &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;? Well, we must have &lt;span class="math"&gt;\(a&amp;gt; 0\)&lt;/span&gt; if the society is productive. Further, because we know that cities allow for more complex economies as the number of occupants grow, we must have &lt;span class="math"&gt;\(\gamma &amp;gt; 1\)&lt;/span&gt;. These are the only assumptions we will make here. Below, we&amp;#8217;ll see that these assumptions imply some interesting&amp;nbsp;consequences.&lt;/p&gt;
&lt;h3&gt;Marginal&amp;nbsp;benefits&lt;/h3&gt;
&lt;p&gt;When a new person immigrates to a city, its &lt;span class="math"&gt;\(N\)&lt;/span&gt; value goes up by one. Here, we consider how the utility and benefit per person changes when this occurs. The increase in net utility is simply&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{3} \label{3}  
\partial_N U(N) = a \gamma N^{\gamma -1}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Notice that because we have &lt;span class="math"&gt;\(\gamma &amp;gt; 1\)&lt;/span&gt;, (\ref{3}) is a function that increases with &lt;span class="math"&gt;\(N\)&lt;/span&gt;. That is, cities with larger populations benefit more (as a collective) per immigrant newcomer than those cities with smaller &lt;span class="math"&gt;\(N\)&lt;/span&gt; would. This implies that the governments of large cities should be more enthusiastic about welcoming of newcomers than those of smaller&amp;nbsp;cities.&lt;/p&gt;
&lt;p&gt;Now consider the marginal benefit per person when one new person moves to this city. This is simply&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{4} \label{4}  
\partial_N BPP(N) = a (\gamma - 1) N^{\gamma -2}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Notice that this is different from the form (\ref{3}) that describes the marginal increase in total city utility. In particular, while (\ref{4}) is positive, it is not necessarily increasing with &lt;span class="math"&gt;\(N\)&lt;/span&gt;: If &lt;span class="math"&gt;\(\gamma &amp;lt; 2\)&lt;/span&gt;, (\ref{4}) decreases with &lt;span class="math"&gt;\(N\)&lt;/span&gt;. Cities having &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; values like this are such that the net new wealth captured per existing citizen &amp;#8212; thanks to each new immigrant &amp;#8212; quickly decays to zero. The consequence is that city governments and existing citizens can have a conflict of interest when it comes to&amp;nbsp;immigration.&lt;/p&gt;
&lt;h3&gt;Equilibration&lt;/h3&gt;
&lt;p&gt;In a local population that has freedom of movement, we can expect the migration of people to push the benefit per person to be equal across cities. In cases like this, we should then have&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{5} \label{5}  
a_i N^{\gamma_i -1} \approx a_j N^{\gamma_j -1},  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
for each city &lt;span class="math"&gt;\(i\)&lt;/span&gt; and &lt;span class="math"&gt;\(j\)&lt;/span&gt; for which there is low mutual migration costs. We point out that this is not the same result required to maximize the net, global output. This latter score is likely that which an authoritarian government might try to maximize. To maximize net utility, we need to have the marginal utility per city equal across cities, which means&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{6} \label{6}  
\partial_N U_i(N) = \partial_N U_j(N)  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
or,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{7} \label{7}  
a_i \gamma_i N^{\gamma_i -1} = a_j \gamma_j N^{\gamma_j -1}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
We see that (\ref{5}) and (\ref{7}) differ in that there are &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; factors in (\ref{7}) that are not present in (\ref{5}). This implies that as long as the &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; values differ across cities, there will be a conflict of interest between the migrants and the&amp;nbsp;government.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category></entry><entry><title>The speed of traffic</title><link href="https://efavdb.com/the-speed-of-traffic" rel="alternate"></link><published>2019-06-14T09:21:00-07:00</published><updated>2019-06-14T09:21:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2019-06-14:/the-speed-of-traffic</id><summary type="html">&lt;p&gt;We use a simple argument to estimate the speed of traffic on a highway as a function of the density of cars. The idea is to simply calculate the maximum speed that traffic could go without supporting a growing traffic&amp;nbsp;jam.&lt;/p&gt;
&lt;h3&gt;Jam dissipation&amp;nbsp;argument&lt;/h3&gt;
&lt;p&gt;To estimate the speed of traffic …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We use a simple argument to estimate the speed of traffic on a highway as a function of the density of cars. The idea is to simply calculate the maximum speed that traffic could go without supporting a growing traffic&amp;nbsp;jam.&lt;/p&gt;
&lt;h3&gt;Jam dissipation&amp;nbsp;argument&lt;/h3&gt;
&lt;p&gt;To estimate the speed of traffic as a function of density, we&amp;#8217;ll calculate an upper bound and argue that actual traffic speeds must be described by an equation similar to that obtained. To derive our upper bound, we&amp;#8217;ll consider what happens when a small traffic jam forms. If the speed of cars is such that the rate of exit from the jam is larger than the rate at which new cars enter the jam, then the jam will dissipate. On the other hand, if this doesn&amp;#8217;t hold, the jam will grow, causing the speed to drop until a speed is obtained that allows the jam to dissipate. This sets the bound. Although we consider a jam to make the argument simple, what we really have in mind is any other sort of modest slow-down that may&amp;nbsp;occur.&lt;/p&gt;
&lt;p&gt;To begin, we introduce some definitions. (1) Let &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; be the density of cars in units of &lt;span class="math"&gt;\([cars / mile]\)&lt;/span&gt;. (2) Next we consider the rate of exit from a jam: Note that when traffic is stopped, a car cannot move until the car in front of it does. Because a human is driving the car, there is a slight delay between the time that one car moves and the car behind it moves. Let &lt;span class="math"&gt;\(T\)&lt;/span&gt; be this delay time in &lt;span class="math"&gt;\([hours]\)&lt;/span&gt;. (3) Let &lt;span class="math"&gt;\(v\)&lt;/span&gt; be the speed of traffic outside the jam in units of &lt;span class="math"&gt;\([miles / hour]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;With the above definitions, we now consider the rate at which cars exit a jam. This is the number of cars that can exit the jam per hour, which is simply&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{1} \label{1}  
r_{out} = \frac{1}{T}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Next, the rate at which cars enter the jam is given by&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{2} \label{2}  
r_{in} = \lambda v.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Requiring that &lt;span class="math"&gt;\(r_{out} &amp;gt; r_{in}\)&lt;/span&gt; we get&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \label{3} \tag{3}  
v &amp;lt; \frac{1}{\lambda T}.  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This is our bound and estimate for the speed of traffic. We note that this form for &lt;span class="math"&gt;\(v\)&lt;/span&gt; follows from dimensional analysis, so the actual rate of traffic must have the same algebraic form as our upper bound (\ref{3}) &amp;#8212; it can differ by a constant factor in front, but should have the same &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; and &lt;span class="math"&gt;\(T\)&lt;/span&gt;&amp;nbsp;dependence.&lt;/p&gt;
&lt;h3&gt;Plugging in&amp;nbsp;numbers&lt;/h3&gt;
&lt;p&gt;I estimate &lt;span class="math"&gt;\(T\)&lt;/span&gt;, the delay time between car movements to be about one second, which in hours is&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray} \tag{4} \label{4}  
T \approx 0.00028\ [hour].  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Next for &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, note that a typical car is about 10 feet long and a mile is around 5000 feet, so the maximum for &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; is around &lt;span class="math"&gt;\( \lambda \lesssim 500 [cars / mile]\)&lt;/span&gt;. Consider a case where there is a car every 10 car lengths or so. In this case, the density will go down from the maximum by a factor of 10, or&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}\tag{5} \label{5}  
\lambda \approx 50 \ [cars / mile].  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Plugging (\ref{4}) and (\ref{5}) into (\ref{3}), we obtain&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}  \tag{6}
v \lesssim \frac{1}{0.00028 * 50} \approx 70\ [mile / hour],  
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
quite close to our typical highway traffic speeds (and speed&amp;nbsp;limits).&lt;/p&gt;
&lt;h3&gt;Final&amp;nbsp;comments&lt;/h3&gt;
&lt;p&gt;The above bound clearly depends on what values you plug in &amp;#8212; I picked numbers that seemed reasonable, but admit I adjusted them a bit till I got the final number I wanted for &lt;span class="math"&gt;\(v\)&lt;/span&gt;. Anecdotally, I&amp;#8217;ve found the result to work well at other densities: For example, when traffic is slow on the highway near my house, if I see that there is a car every 5 car lengths, the speed tends to be about &lt;span class="math"&gt;\(30 [miles / hour]\)&lt;/span&gt; &amp;#8212; so scaling rule seems to work. The last thing I should note is that wikipedia has an article outlining some of the extensive research literature that&amp;#8217;s been done on traffic flows &amp;#8212; you can see that &lt;a href="https://en.wikipedia.org/wiki/Traffic_flow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category><category term="theory"></category><category term="traffic"></category></entry><entry><title>Machine learning to predict San Francisco crime</title><link href="https://efavdb.com/predicting-san-francisco-crimes" rel="alternate"></link><published>2015-07-20T03:01:00-07:00</published><updated>2015-07-20T03:01:00-07:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2015-07-20:/predicting-san-francisco-crimes</id><summary type="html">&lt;p&gt;In today&amp;#8217;s post, we document our submission to the recent &lt;a href="https://www.kaggle.com/c/sf-crime"&gt;Kaggle&lt;/a&gt; competition aimed at predicting the category of San Francisco crimes, given only their time and location of occurrence. As a reminder, Kaggle is a site where one can compete with other data scientists on various data challenges.  We …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In today&amp;#8217;s post, we document our submission to the recent &lt;a href="https://www.kaggle.com/c/sf-crime"&gt;Kaggle&lt;/a&gt; competition aimed at predicting the category of San Francisco crimes, given only their time and location of occurrence. As a reminder, Kaggle is a site where one can compete with other data scientists on various data challenges.  We took this competition as an opportunity to explore the Naive Bayes algorithm. With the few steps discussed below, we were able to quickly move from the middle of the pack to the top 33% on the competition leader board, all the while continuing with this simple&amp;nbsp;model!&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As in all cities, crime is a reality San Francisco: Everyone who lives in San Francisco seems to know someone whose car window has been smashed in, or whose bicycle was stolen within the past year or two. Even Prius&amp;#8217; car batteries are apparently considered &lt;a href="http://abc7news.com/news/exclusive-car-battery-thefts-from-hybrid-cars-on-the-rise-in-san-francisco-/725532/"&gt;fair game&lt;/a&gt; by the city&amp;#8217;s diligent thieves.  The challenge we tackle today involves attempting to guess the class of a crime committed within the city, given the time and location it took place. Such studies are representative of efforts by many police forces today: Using machine learning approaches, one can get an improved understanding of which crimes occur where and when in a city &amp;#8212; this then allows for better, &lt;a href="http://www.forbes.com/sites/emc/2014/06/03/data-analysis-helps-police-departments-fight-crime/"&gt;dynamic allocation of police resources&lt;/a&gt;. To aid in the &lt;span class="caps"&gt;SF&lt;/span&gt; &lt;a href="https://www.kaggle.com/c/sf-crime"&gt;challenge&lt;/a&gt;, Kaggle has provided about 12 years of crime reports from all over the city &amp;#8212; a data set that is pretty interesting to comb&amp;nbsp;through.&lt;/p&gt;
&lt;p&gt;Here, we outline our approach to tackling this problem, using the Naive Bayes classifier. This is one of the simplest classification algorithms, the essential ingredients of which include combining &lt;a href="https://en.wikipedia.org/wiki/Bayes%27_theorem" title="Bayes' theorem"&gt;Bayes&amp;#8217; theorem&lt;/a&gt; with an independence assumption on the features (this is the &amp;#8220;naive&amp;#8221; part).  Although simple, it is still a popular method for text categorization. For example, using word frequencies as features, this approach can accurately classify emails as spam, or whether a particular a piece of text was written by a specific author.  In fact, with careful preprocessing, the algorithm is often &lt;a href="http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf"&gt;competitive&lt;/a&gt; with more advanced methods, including support vector&amp;nbsp;machines.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Loading package and&amp;nbsp;data&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Below, we show the relevant commands needed to load all the packages and training/test data we will be using. As in previous posts, we will work with &lt;a href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt; for quick and easy data loading and wrangling. We will be having a post dedicated to Pandas in the near future, so stay tuned! We start off with using the parse_dates method to convert the Dates column of our provided data &amp;#8212; which can be downloaded &lt;a href="https://www.kaggle.com/c/sf-crime/data"&gt;here&lt;/a&gt;&amp;#8212; from string to datetime&amp;nbsp;format.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;preprocessing&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;log_loss&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.naive_bayes&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BernoulliNB&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;#Load Data with pandas, and parse the first column into datetime&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;train.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parse_dates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Dates&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="kp"&gt;test&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parse_dates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Dates&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The training data provided contains the following&amp;nbsp;fields:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Date&lt;/em&gt;&lt;/strong&gt; -  date + timestamp
&lt;strong&gt;&lt;em&gt;Category&lt;/em&gt;&lt;/strong&gt; - The type of crime, Larceny, etc.
&lt;strong&gt;&lt;em&gt;Descript&lt;/em&gt;&lt;/strong&gt; - A more detailed description of the crime.
&lt;strong&gt;&lt;em&gt;DayOfWeek&lt;/em&gt;&lt;/strong&gt; - Day of crime: Monday, Tuesday, etc.
&lt;strong&gt;&lt;em&gt;PdDistrict &lt;/em&gt;&lt;/strong&gt;- Police department district.
&lt;strong&gt;&lt;em&gt;Resolution&lt;/em&gt;&lt;/strong&gt;- What was the outcome, Arrest, Unfounded, None, etc.
&lt;strong&gt;&lt;em&gt;Address&lt;/em&gt;&lt;/strong&gt; - Street address of crime.
&lt;strong&gt;&lt;em&gt;X and Y&lt;/em&gt;&lt;/strong&gt; - &lt;span class="caps"&gt;GPS&lt;/span&gt; coordinates of&amp;nbsp;crime.&lt;/p&gt;
&lt;p&gt;As we mentioned earlier, the provided data spans almost 12 years, and both the training data set and the testing data set each have about 900k records. At this point we have all the data in memory. However, the majority of this data is categorical in nature, and so will require some more&amp;nbsp;preprocessing.&lt;/p&gt;
&lt;h2&gt;How to handle categorical&amp;nbsp;data&lt;/h2&gt;
&lt;p&gt;Many machine learning algorithms &amp;#8212; including that which we apply below &amp;#8212; will not accept categorical, or text, features. What is the best way to convert such data into numerical values? A natural idea is to convert each unique string to a unique value.  For example, in our data set we might take the crime category value to correspond to one numerical feature, with Larceny set to 1, Homicide to 2, etc.  However, this scheme can cause problems for many algorithms, because they will incorrectly assume that nearby numerical values imply some sort of similarity between the underlying categorical&amp;nbsp;values.&lt;/p&gt;
&lt;p&gt;To avoid the problem noted above, we will instead binarize our categorical data, using vectors of 1&amp;#8217;s and 0&amp;#8217;s. For example, we will&amp;nbsp;write&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;larceny = 1,0,0,0,...&lt;/span&gt;
&lt;span class="err"&gt;homicide = 0,1,0,0,...&lt;/span&gt;
&lt;span class="err"&gt;prostitution  = 0,0,1,0,...&lt;/span&gt;
&lt;span class="err"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are a variety of methods to do this encoding, but Pandas has a particularly nice method called &lt;a href="http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.get_dummies.html"&gt;get_dummies()&lt;/a&gt; that can go straight from your column of text to a binarized array.  Below, we also convert the crime category labels to integer values using the method &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"&gt;LabelEncoder&lt;/a&gt;, and use Pandas to extract the hour from each time point. We then convert the districts, weekday, and hour into binarized arrays and combine them into a new dataframe. &lt;strong&gt; &lt;/strong&gt; We then split up the train_data into a training and validation set so that we have a way of accessing the model performance while leaving the test data&amp;nbsp;untouched.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="k"&gt;Convert&lt;/span&gt; &lt;span class="n"&gt;crime&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;numbers&lt;/span&gt;
&lt;span class="n"&gt;le_crime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preprocessing&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LabelEncoder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;crime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;le_crime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Category&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="k"&gt;Get&lt;/span&gt; &lt;span class="n"&gt;binarized&lt;/span&gt; &lt;span class="n"&gt;weekdays&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;districts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;hours&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DayOfWeek&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;district&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PdDistrict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;hour&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dates&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;
&lt;span class="n"&gt;hour&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Build&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nb"&gt;array&lt;/span&gt;
&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;district&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;crime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;crime&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Repeat&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="k"&gt;data&lt;/span&gt;
&lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DayOfWeek&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;district&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PdDistrict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;hour&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dates&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;
&lt;span class="n"&gt;hour&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;district&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;&lt;strong&gt;Model&amp;nbsp;development&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For this competition the metric used to rate the performance of the model is the multi-class &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html"&gt;log_loss&lt;/a&gt; &amp;#8212; smaller values of this loss correspond to improved&amp;nbsp;performance.&lt;/p&gt;
&lt;h4&gt;First&amp;nbsp;pass&lt;/h4&gt;
&lt;p&gt;For our first quick pass, we used just the day of the week and district for features in our classifier training. We also carried out a Logistic Regression (&lt;span class="caps"&gt;LR&lt;/span&gt;) on the data in order to get a feel for how the Naive Bayes (&lt;span class="caps"&gt;NB&lt;/span&gt;) model was performing. The results from the &lt;span class="caps"&gt;NB&lt;/span&gt; model gave us a log-loss of 2.62, while &lt;span class="caps"&gt;LR&lt;/span&gt; after tuning was able to give 2.62. However, &lt;span class="caps"&gt;LR&lt;/span&gt; took 60 seconds to run, while &lt;span class="caps"&gt;NB&lt;/span&gt; took only 1.5 seconds! As a reference, the current top score on the leader board is about 2.27, while the worst is around 35. Not bad&amp;nbsp;performance!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;Friday&amp;#39;, &amp;#39;Monday&amp;#39;, &amp;#39;Saturday&amp;#39;, &amp;#39;Sunday&amp;#39;, &amp;#39;Thursday&amp;#39;, &amp;#39;Tuesday&amp;#39;,&lt;/span&gt;
&lt;span class="n"&gt;&amp;#39;Wednesday&amp;#39;, &amp;#39;BAYVIEW&amp;#39;, &amp;#39;CENTRAL&amp;#39;, &amp;#39;INGLESIDE&amp;#39;, &amp;#39;MISSION&amp;#39;,&lt;/span&gt;
&lt;span class="n"&gt;&amp;#39;NORTHERN&amp;#39;, &amp;#39;PARK&amp;#39;, &amp;#39;RICHMOND&amp;#39;, &amp;#39;SOUTHERN&amp;#39;, &amp;#39;TARAVAL&amp;#39;, &amp;#39;TENDERLOIN&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;train_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.60&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;BernoulliNB&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;crime&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;log_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;crime&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;#Logistic&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Regression&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;comparison&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;crime&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;log_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;crime&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Submission&amp;nbsp;code&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;BernoulliNB&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;crime&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;#Write&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;result&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;le_crime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classes_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;testResult.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;index&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;index_label&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Id&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With the above model performing well, we used our code to write out our predictions on the test set to csv format, and submitted this to Kaggle. It turns out we got a score of 2.61 which is slightly better than our validation set estimate. The was a good enough score to put us in the to 50%. Pretty good for a first&amp;nbsp;try!&lt;/p&gt;
&lt;h4&gt;Second&amp;nbsp;pass&lt;/h4&gt;
&lt;p&gt;To improve the model further, we next added the time to the feature list used in training. This clearly provides some relevant information, as some types of crime happen more during the day than the night. For example, we expect public drunkenness to probably go up in the late evening.  Adding this feature we were able to push our log-loss score down to 2.58 &amp;#8212; quick and easy progress!  As a side note, we also tried leaving the hours as a continuous variable, but this did not lead to any score improvements.  After training on the whole data set again, we also get 2.58 on the test date. This moved us up another 32 spots, giving a final placement of&amp;nbsp;76/226!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Friday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Monday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Saturday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Sunday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Thursday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Tuesday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;Wednesday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;BAYVIEW&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;CENTRAL&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;INGLESIDE&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;MISSION&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;NORTHERN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;PARK&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;RICHMOND&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;SOUTHERN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;TARAVAL&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;TENDERLOIN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;features2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;features2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Although Naive Bayes is a fairly simple model, properly wielded it can give great results.  In fact, in this competition our results were competitive with teams who were using much more complicated models, e.g. neural nets. We also learned a few other interesting things here: For example, Pandas&amp;#8217; get_dummies() method looks like it will be a huge timesaver when dealing with categorical data. Till next time &amp;#8212; keep your Prius safe!
&lt;a href="https://github.com/EFavDB/SF-Crime" title="GitHub Repo"&gt;&lt;img alt="Open GitHub Repo" src="https://efavdb.com/wp-content/uploads/2015/03/GitHub_Logo.png"&gt;&lt;/a&gt;&lt;/p&gt;</content><category term="Case studies"></category></entry><entry><title>Forecasting Bike Sharing Demand</title><link href="https://efavdb.com/bike-share-forecasting" rel="alternate"></link><published>2015-03-26T09:20:00-07:00</published><updated>2015-03-26T09:20:00-07:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2015-03-26:/bike-share-forecasting</id><summary type="html">&lt;p&gt;In today&amp;#8217;s post, we document our efforts at applying a gradient boosted trees model to forecast bike sharing demand &amp;#8212; a problem posed in a recent &lt;a href="https://www.kaggle.com/c/bike-sharing-demand"&gt;Kaggle&lt;/a&gt;competition. For those not familiar, Kaggle is a site where one can compete with other data scientists on various data challenges. Top scorers …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In today&amp;#8217;s post, we document our efforts at applying a gradient boosted trees model to forecast bike sharing demand &amp;#8212; a problem posed in a recent &lt;a href="https://www.kaggle.com/c/bike-sharing-demand"&gt;Kaggle&lt;/a&gt;competition. For those not familiar, Kaggle is a site where one can compete with other data scientists on various data challenges. Top scorers often win prize money, but the site more generally serves as a great place to grab interesting datasets to explore and play with. With the simple optimization steps discussed below, we managed to quickly move from the bottom 10% of the competition &amp;#8212; our first-pass attempt&amp;#8217;s score &amp;#8212; to the top 10%: no&amp;nbsp;sweat!&lt;/p&gt;
&lt;p&gt;Our work here was inspired by a &lt;a href="http://blog.dato.com/using-gradient-boosted-trees-to-predict-bike-sharing-demand"&gt;post&lt;/a&gt; by the people at &lt;a href="http://blog.dato.com/"&gt;Dato.com&lt;/a&gt;, who used the bike sharing competition as an opportunity to demonstrate their software. Here, we go through a similar, but more detailed discussion using the python package &lt;a href="http://scikit-learn.org/stable/"&gt;SKlearn&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Bike sharing systems are gaining popularity around the world &amp;#8212; there are over 500 different programs currently operating in various cities, and counting!  These programs are generally funded through rider membership fees, or through pay-to-ride one time rental fees. Key to the convenience of these programs is the fact that riders who pick up a bicycle from one station can return the bicycle to any other in the network.  These systems generate a great deal of data relating to various ride details, including travel time, departure location, arrival location, and so on.  This data has the potential to be very useful for studying city mobility. The data we look at today comes from Washington D. C.&amp;#8217;s &lt;a href="https://www.capitalbikeshare.com/"&gt;Capital Bikeshare&lt;/a&gt; program. The goal of the Kaggle competition is to leverage the historical data provided in order to forecast future bike rental demand within the&amp;nbsp;city.&lt;/p&gt;
&lt;p&gt;As we detailed in an earlier &lt;a href="http://efavdb.github.io/notes-on-trees"&gt;post&lt;/a&gt;, boosting provides a general method for increasing a machine learning algorithm&amp;#8217;s performance. Here, in order to model the Capital Bikeshare program&amp;#8217;s demand curves, we&amp;#8217;ll be applying a gradient boosted trees model (&lt;span class="caps"&gt;GBM&lt;/span&gt;).  Simply put, &lt;span class="caps"&gt;GBM&lt;/span&gt;&amp;#8217;s are constructed by iteratively fitting a series of simple trees to a training set, where each new tree attempts to fit the residuals, or errors, of the trees that came before it. With the addition of each new tree the training error is further reduced, typically asymptoting to a reasonably accurate model &amp;#8212; but one must watch out for overfitting &amp;#8212; see&amp;nbsp;below!&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Loading package and&amp;nbsp;data&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Below, we show the relevant commands needed to load all the packages and training/test data we will be using. We work with the package &lt;a href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;, whose DataFrame data structure enables quick and easy data loading and wrangling. We take advantage of this package immediately below, where in the last lines we use its parse_dates method to convert the first column of our provided data &amp;#8212; which can be downloaded &lt;a href="https://www.kaggle.com/c/bike-sharing-demand"&gt;here&lt;/a&gt; &amp;#8212; from string to datetime&amp;nbsp;format.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ensemble&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;mean_absolute_error&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.grid_search&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;

&lt;span class="c1"&gt;#Load Data with pandas, and parse the&lt;/span&gt;
&lt;span class="c1"&gt;#first column into datetime&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;train.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parse_dates&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="kp"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parse_dates&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The training data provided contains the following&amp;nbsp;fields:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;datetime&lt;/em&gt;&lt;/strong&gt; - hourly date + timestamp
&lt;strong&gt;&lt;em&gt;season&lt;/em&gt;&lt;/strong&gt; -  1 = spring, 2 = summer, 3 = fall, 4 = winter
&lt;strong&gt;&lt;em&gt;holiday&lt;/em&gt;&lt;/strong&gt; - whether the day is considered a holiday
&lt;strong&gt;&lt;em&gt;workingday&lt;/em&gt;&lt;/strong&gt; - whether the day is neither a weekend nor holiday
&lt;strong&gt;&lt;em&gt;weather&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clear, Few clouds, Partly cloudy, Partly&amp;nbsp;cloudy&lt;/li&gt;
&lt;li&gt;Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds,&amp;nbsp;Mist&lt;/li&gt;
&lt;li&gt;Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered&amp;nbsp;clouds&lt;/li&gt;
&lt;li&gt;Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow +&amp;nbsp;Fog&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;temp&lt;/em&gt;&lt;/strong&gt; - temperature in Celsius
&lt;strong&gt;&lt;em&gt;atemp&lt;/em&gt;&lt;/strong&gt; - &amp;#8220;feels like&amp;#8221; temperature in Celsius
&lt;strong&gt;&lt;em&gt;humidity&lt;/em&gt;&lt;/strong&gt; - relative humidity
&lt;strong&gt;&lt;em&gt;windspeed&lt;/em&gt;&lt;/strong&gt; - wind speed
&lt;strong&gt;&lt;em&gt;casual&lt;/em&gt;&lt;/strong&gt; - number of non-registered user rentals initiated
&lt;strong&gt;&lt;em&gt;registered&lt;/em&gt;&lt;/strong&gt; - number of registered user rentals initiated
&lt;strong&gt;&lt;em&gt;count&lt;/em&gt;&lt;/strong&gt; - number of total&amp;nbsp;rentals&lt;/p&gt;
&lt;p&gt;The data provided spans two years. The training set contains the first 19 days of each month considered, while the test set data corresponds to the remaining days in each&amp;nbsp;month.&lt;/p&gt;
&lt;p&gt;Looking ahead, we anticipate that the year, month, day of week, and hour will serve as important features for characterizing the bike demand at any given moment.  These features are easily extracted from the datetime formatted-values loaded above. In the following lines, we add these features to our&amp;nbsp;DataFrames.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Feature&lt;/span&gt; &lt;span class="n"&gt;engineering&lt;/span&gt;
&lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DatetimeIndex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;year&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;month&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;month&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;hour&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;weekday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weekday&lt;/span&gt;

&lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DatetimeIndex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;year&lt;/span&gt;
&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;month&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;month&lt;/span&gt;
&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;hour&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;
&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;weekday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weekday&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Define&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="n"&gt;vector&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;season&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;holiday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;workingday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;weather&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;temp&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;atemp&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;humidity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;windspeed&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s1"&gt;&amp;#39;month&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;weekday&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;hour&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;&lt;strong&gt;Evaluation&amp;nbsp;metric&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The evaluation metric that Kaggle uses to rank competing algorithms is the Root Mean Squared Logarithmic Error (&lt;span class="caps"&gt;RMSLE&lt;/span&gt;).&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
J = \sqrt{\frac{1}{n} \sum_{i=1}^n [\ln(p_i + 1) - \ln(a_i+1)]^2 }
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
Here,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(n\)&lt;/span&gt; is the number of hours in the test&amp;nbsp;set&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p_i\)&lt;/span&gt; is the predicted number of bikes rented in a given&amp;nbsp;hour&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(a_i\)&lt;/span&gt; is the actual rent&amp;nbsp;count&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(ln(x)\)&lt;/span&gt; is the natural&amp;nbsp;logarithm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With ranking determined as above, our aim becomes to accurately guess the natural logarithm of bike demand at different times (actually demand count plus one, in order to avoid infinities associated with times where demand is nil). To facilitate this, we add the logarithm of the casual, registered, and total counts to our training DataFrame&amp;nbsp;below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#the evaluation metric is the RMSE in the log domain,&lt;/span&gt;
&lt;span class="c1"&gt;#so we should transform the target columns into log domain as well.&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;casual&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;registered&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
  &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log1p&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that in the code above we use the &lt;span class="math"&gt;\(log1p()\)&lt;/span&gt; function instead of the more familiar &lt;span class="math"&gt;\(log(1+x)\)&lt;/span&gt;. For large values of &lt;span class="math"&gt;\(x\)&lt;/span&gt;, these two functions are actually equivalent. However, at very small values of &lt;span class="math"&gt;\(x\)&lt;/span&gt;, the two can disagree. The source of the discrepancy is floating point error: For very small &lt;span class="math"&gt;\(x\)&lt;/span&gt;, python will send &lt;span class="math"&gt;\(1+x \to 1\)&lt;/span&gt;, which when supplied as an argument to &lt;span class="math"&gt;\(log(1+x)\)&lt;/span&gt; will return &lt;span class="math"&gt;\(log(1)=0\)&lt;/span&gt;. The function &lt;span class="math"&gt;\(log1p(x) \sim x\)&lt;/span&gt; in this limit. The difference is not very important when the result is being added to other numbers, but can be very important in a multiplicative operation. We use this function instead for this reason. The inverse of &lt;span class="math"&gt;\(log(x+1)\)&lt;/span&gt; is &lt;span class="math"&gt;\(e^{x} -1\)&lt;/span&gt; &amp;#8212; an operation we will also need to make use of later, in order to return linear-scale demand values. We&amp;#8217;ll use an analog of the &lt;span class="math"&gt;\(log1p()\)&lt;/span&gt; function, numpy&amp;#8217;s &lt;span class="math"&gt;\(expm1()\)&lt;/span&gt; function, to carry out this&amp;nbsp;inversion.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Model&amp;nbsp;development&lt;/strong&gt;&lt;/h2&gt;
&lt;h4&gt;&lt;strong&gt;A first&amp;nbsp;pass&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The Gradient Boosting Machine (&lt;span class="caps"&gt;GBM&lt;/span&gt;) we will be using has some associated hyperparameters that will eventually need to be optimized. These&amp;nbsp;include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;n_estimators = the number of boosting stages, or trees, to&amp;nbsp;use.&lt;/li&gt;
&lt;li&gt;max_depth = maximum depth of the individual regression&amp;nbsp;trees.&lt;/li&gt;
&lt;li&gt;learning_rate = shrinks the contribution of each tree by the learning&amp;nbsp;rate.&lt;/li&gt;
&lt;li&gt;in_samples_leaf = the minimum number of samples required to be at a leaf&amp;nbsp;node&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, in order to get our feet wet, we&amp;#8217;ll begin by just picking some ad hoc values for these parameters. The code below fits a &lt;span class="caps"&gt;GBM&lt;/span&gt; to the log-demand training data, and then converts predicted log-demand into the competition&amp;#8217;s required format &amp;#8212; in particular, the demand is output in linear&amp;nbsp;scale.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ensemble&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;log-count&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="k"&gt;result&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;results1.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;index&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;datetime&amp;#39;,&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the last lines above, we have used the DataFrames to_csv() method in order to output results for competition submission. Example output is shown below. Without a hitch, we successfully submitted the results of this preliminary analysis to Kaggle. The only bad news was that our model scored in the bottom 10%. Fortunately, some simple optimizations that follow led to significant improvements in our&amp;nbsp;standing.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;datetime&lt;/th&gt;
&lt;th&gt;count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2011-01-20 0:00:00&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2011-01-20 0:01:00&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2011-01-20 0:02:00&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;strong&gt;Hyperparameter&amp;nbsp;tuning&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;We now turn to the challenge of tuning our &lt;span class="caps"&gt;GBM&lt;/span&gt;&amp;#8217;s hyperparameters. In order to carry this out, we segmented our training data into a training set and a validation set. The validation set allowed us to check the accuracy of our model locally, without having to submit to Kaggle. This also helped us to avoid overfitting&amp;nbsp;issues.&lt;/p&gt;
&lt;p&gt;As mentioned earlier, the training data provided covers the first 19 days of each month. In segmenting this data, we opted to use days 17-19 for validation. We then used this validation set to optimize the model&amp;#8217;s hyperparameters. As a first-pass at this, we again chose an ad hoc value for n_estimators, but optimized over the remaining degrees of freedom. The code follows, where we make use of GridSearchCV() to perform our parameter&amp;nbsp;sweep.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#Split data into training and validation sets&lt;/span&gt;
&lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DatetimeIndex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;day&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;validation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;temp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;day&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;param_grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;learning_rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;max_depth&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;min_samples_leaf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;est&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensemble&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# this may take awhile&lt;/span&gt;
&lt;span class="n"&gt;gs_cv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;est&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param_grid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# best hyperparameter setting&lt;/span&gt;
&lt;span class="n"&gt;gs_cv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_params_&lt;/span&gt;

&lt;span class="c1"&gt;#Baseline error&lt;/span&gt;
&lt;span class="n"&gt;error_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_absolute_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;gs_cv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gs_cv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;results2.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Note: If you want to run n_jobs &amp;gt; 1 on a Windows machine, the script needs to be in an &amp;#8220;if &lt;strong&gt;name&lt;/strong&gt; == &amp;#8216;&lt;strong&gt;main&lt;/strong&gt;&amp;#8216;:&amp;#8221; block. Otherwise the script will&amp;nbsp;fail.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;day&lt;/th&gt;
&lt;th&gt;Best Parms&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;learning_rate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;max_depth&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;min_samples_leaf&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The optimized parameters are shown above. Submitting the resulting model to Kaggle, we found that we had moved from the bottom 10% of models to the top 20%!  An awesome improvement, but we still have one final hyperparameter to&amp;nbsp;optimize.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Tuning the number of&amp;nbsp;estimators&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;In boosted models, training set performance will always improve as the number of estimators is increased. However, at large estimator number, overfitting can start to become an issue. Learning curves provide a method for optimization. These are constructed by plotting the error on both the training and validation sets as a function of the number of estimators used. The code below generates such a curve for our&amp;nbsp;model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;error_train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;error_validation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;501&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensemble&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_depth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;min_samples_leaf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
  &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
  &lt;span class="n"&gt;error_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;mean_absolute_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

  &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
  &lt;span class="n"&gt;error_validation&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;mean_absolute_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

&lt;span class="c1"&gt;#Plot the data&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;501&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;use&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ggplot&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;error_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;k&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;error_validation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Number of Estimators&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Error&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Train&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Validation&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Error vs. Number of Estimators&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="Error vs Number of Estimators" src="https://efavdb.com/wp-content/uploads/2015/03/figure_1-e1427234375629.png"&gt;&lt;/p&gt;
&lt;p&gt;Notice in the plot that by the time the number estimators in our &lt;span class="caps"&gt;GBM&lt;/span&gt; reaches about 80, the error of our model as applied to the validation set starts to slowly increase, though the error on the training set continues to decrease steadily. The diagnosis is that the model begins to overfit at this point. Moving forward, we will set n_estimators to 80, rather than 500, the value we were using above. Reducing the number of estimators reduced the calculated error and moved us to a higher position on the&amp;nbsp;leaderboard.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Separate models for registered and casual&amp;nbsp;users&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Reviewing the data, we see that we have info regarding two types of riders: casual and registered riders. It is plausible that each group&amp;#8217;s behavior differs, and that we might be able to improve our performance by modeling each separately. Below, we carry this out, and then also merge the two group&amp;#8217;s predicted values to obtain a net predicted demand. We also repeat the hyperparameter sweep steps covered above &amp;#8212; this returned similar values. Resubmitting the resulting model, we found we had increased our standing in the competition by a few&amp;nbsp;percent.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;merge_predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="c1"&gt;# Combine the predictions of two separately trained models.&lt;/span&gt;
  &lt;span class="c1"&gt;# The input models are in the log domain and returns the predictions&lt;/span&gt;
  &lt;span class="c1"&gt;# in original domain.&lt;/span&gt;
  &lt;span class="n"&gt;p1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="n"&gt;p2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="n"&gt;p_total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;p2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p_total&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;est_casual&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensemble&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;est_registered&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensemble&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;param_grid2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;max_depth&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
               &lt;span class="s1"&gt;&amp;#39;_samples_leaf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;gs_casual&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;est_casual&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param_grid2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-casual&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;gs_registered&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;est_registered&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param_grid2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-registered&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;result3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;merge_predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gs_casual&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gs_registered&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;result3&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;results3.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The last step is to submit a final set of model predictions, this time training on the full labeled dataset provided. With these simple steps, we ended up in the top 11% on the competition&amp;#8217;s leaderboard with a rank of&amp;nbsp;280/2467!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/03/score.png"&gt;&lt;img alt="score" src="https://efavdb.com/wp-content/uploads/2015/03/score.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;pre&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="n"&gt;est_casual&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensemble&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_depth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;min_samples_leaf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;est_registered&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensemble&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientBoostingRegressor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_depth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;min_samples_leaf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;est_casual&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-casual&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;est_registered&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log-registered&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;result4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;merge_predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;est_casual&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;est_registered&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;result4&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;results4.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;DISCUSSION&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By iteratively tuning a &lt;span class="caps"&gt;GBM&lt;/span&gt;, we were able to quickly climb the leaderboard for this particular Kaggle competition. With further feature extraction work, we believe further improvements could readily be made. However, our goal here was only to practice our rapid development skills, so we won&amp;#8217;t be spending much time on further fine-tuning. At any rate, our results have convinced us that simple boosted models can often provide excellent&amp;nbsp;results.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/EFavDB/bike-forecast" title="GitHub Repo"&gt;&lt;img alt="Open GitHub Repo" src="https://efavdb.com/wp-content/uploads/2015/03/GitHub_Logo.png"&gt;&lt;/a&gt;
Open GitHub&amp;nbsp;Repo&lt;/p&gt;
&lt;p&gt;Note: With this post, we have begun to post our python scripts and data at GitHub. Clicking on the icon at left will take you to our repository. Feel free to stop by and take a&amp;nbsp;look!&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category></entry><entry><title>Measles vaccination rate by USA state and relation to mean outbreak size</title><link href="https://efavdb.com/vaccination-rates" rel="alternate"></link><published>2015-02-25T13:40:00-08:00</published><updated>2015-02-25T13:40:00-08:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2015-02-25:/vaccination-rates</id><summary type="html">&lt;p&gt;In this post, we provide a quick overview of the data and science of measles spread. Making use of python (code provided) we extract from a &lt;span class="caps"&gt;CDC&lt;/span&gt; data set the 2012 youth vaccination rate for each &lt;span class="caps"&gt;USA&lt;/span&gt; state &amp;#8212; see figure below. To aid in the interpretation of this data, we …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post, we provide a quick overview of the data and science of measles spread. Making use of python (code provided) we extract from a &lt;span class="caps"&gt;CDC&lt;/span&gt; data set the 2012 youth vaccination rate for each &lt;span class="caps"&gt;USA&lt;/span&gt; state &amp;#8212; see figure below. To aid in the interpretation of this data, we also review and describe the results of a generalized &amp;#8220;&lt;span class="caps"&gt;SIR&lt;/span&gt;&amp;#8221; model for disease spread. The model &amp;#8212; analyzed in &lt;a href="http://http://efavdb.github.io/math-of-measles"&gt;our last post&lt;/a&gt; &amp;#8212; predicts that measles outbreaks are supported only if the vaccination rate is below 94%. At higher rates, infection spread is suppressed, and outbreaks do not occur. As seen in the figure, the majority of the states sit just below this critical number, and so are predicted to support youth&amp;nbsp;outbreaks.&lt;/p&gt;
&lt;iframe width="600" height="533" frameborder="0" src="//plot.ly/~damienrj/179.embed"&gt;&lt;/iframe&gt;

&lt;p&gt;Measles vaccination rate by &lt;span class="caps"&gt;USA&lt;/span&gt; state for youths under 36 months in age, 2012. Dashed line at 94% is estimated critical vaccination rate needed to suppress outbreaks. The mean &lt;span class="caps"&gt;USA&lt;/span&gt; youth rate is about&amp;nbsp;91%.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Introduction: History of measles in&amp;nbsp;America&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Measles is a highly-contagious, serious illness estimated to currently be contracted by about 20 million individuals each year, globally. Just prior to the arrival of the first measles vaccine in 1963 (developed by &lt;a href="http://en.wikipedia.org/wiki/John_Franklin_Enders"&gt;John Enders&lt;/a&gt; and his colleagues) approximately 500,000 Americans contracted the disease annually, approximately 50,000 of which required hospitalization &amp;#8212; a rate of 1/10. Among these individuals, approximately 500 would die each year, equating to a 1/1000 mortality rate. As yet, there is &lt;a href="http://www.mayoclinic.org/diseases-conditions/measles/basics/treatment/con-20019675"&gt;no treatment&lt;/a&gt; available to combat the measles virus, once contracted. Consequently, similar hospitalization and mortality rates &lt;a href="http://en.wikipedia.org/wiki/Measles#Epidemiology"&gt;continue to hold today&lt;/a&gt;. Sadly, the mortality rate among the malnourished can be as high as&amp;nbsp;1/10.&lt;/p&gt;
&lt;iframe width="600" height="533" frameborder="0" src="//plot.ly/~damienrj/262.embed"&gt;&lt;/iframe&gt;

&lt;p&gt;Here, the vaccination rate shown is for youths, under 48 months in first brach (up to 1985), and under 36 months in the latter&amp;nbsp;branch.&lt;/p&gt;
&lt;p&gt;Contemporary contraction rates in the &lt;span class="caps"&gt;USA&lt;/span&gt; are now extremely low, on the order of &lt;a href="http://www.cdc.gov/measles/cases-outbreaks.html"&gt;50-500 per year&lt;/a&gt;. This is likely a consequence of the strong local adoption of measles vaccinations, with 91% of young children here now receiving the vaccine prior to their third birthday. The most recent large-scale &lt;span class="caps"&gt;USA&lt;/span&gt; outbreak of the disease happened between the years of 1989 and 1991, when the vaccination rates of children were significantly lower, hovering around 70%. This outbreak centered within poorer, urban areas where the vaccination rates were &lt;a href="http://www.cdc.gov/vaccines/pubs/pinkbook/meas.html"&gt;substantially lower&lt;/a&gt; than the national average. A summary plot of &lt;span class="caps"&gt;USA&lt;/span&gt; contraction counts and youth vaccination rates by year is shown above &amp;#8212; the two curves are highly anti-correlated. Note that this plot is &lt;strong&gt;click and drag zoomable&lt;/strong&gt;, which is useful for setting the scale appropriately for recent years. Data &lt;a href="http://jid.oxfordjournals.org/content/189/Supplement_1/S17.long"&gt;source 1&lt;/a&gt;, &lt;a href="http://www.cdc.gov/mmwr/preview/mmwrhtml/mm6316a4.htm"&gt;source 2&lt;/a&gt;; no youth vaccination rate data available between 1985-1991. Full-population averages have been in the 90&amp;#8217;s &lt;a href="http://www.nature.com/news/measles-by-the-numbers-a-race-to-eradication-1.16897"&gt;for some decades&lt;/a&gt;, likely due to elementary school matriculation&amp;nbsp;requirements.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Modeling disease&amp;nbsp;spread&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The striking &lt;span class="caps"&gt;USA&lt;/span&gt; historical data suggests a very strong relationship between the vaccination rate within a community and the frequency and size of the measles outbreaks that it supports. In fact, simple models for disease spread suggest a phase-transition-like (exhibiting abrupt changes) outbreak size dependence on vaccination rates. This is illustrated below, where we plot the predicted measles contraction rate against a population&amp;#8217;s vaccination rate &amp;#8212; as returned by a generalized &lt;a href="http://en.wikipedia.org/wiki/Epidemic_model#The_SIR_model"&gt;&lt;span class="caps"&gt;SIR&lt;/span&gt; model for disease spread&lt;/a&gt;: Notice that in the far left side of this plot, the model predicts that disease spread is completely suppressed. However, below a critical vaccination rate (the stated 94% mark &amp;#8212; a number consistent with &lt;a href="http://jid.oxfordjournals.org/content/196/10/1433.full"&gt;published estimates for measles&lt;/a&gt;), outbreaks begin to be supported, growing in size with further decrease of the vaccination&amp;nbsp;rate.&lt;/p&gt;
&lt;iframe width="600" height="533" frameborder="0" src="//plot.ly/~damienrj/275.embed"&gt;&lt;/iframe&gt;

&lt;p&gt;The generalized &lt;span class="caps"&gt;SIR&lt;/span&gt; model predicts that a measles outbreak size changes in a phase-transition-like manner with vaccination rate. Further, at about 85% vaccination, the outbreak population fraction and the unvaccinated fraction curves cross. At this point, the outbreak captures nearly all unvaccinated and also a finite fraction of the vaccinated, who begin to become infected due to frequent encounters with&amp;nbsp;disease.&lt;/p&gt;
&lt;p&gt;A detailed study of the &lt;span class="caps"&gt;SIR&lt;/span&gt; model&amp;#8217;s &lt;a href="http://efavdb.github.io/math-of-measles"&gt;solution&lt;/a&gt; is not necessary to understand why disease spread exhibits a phase-transition-like form. Qualitatively, this behavior is a consequence of a simple balance of rates: If the rate at which a disease spreads is greater than the rate at which the ill recover, outbreaks grow and expand. However, if patients recover more quickly than they can spread the disease, outbreak expansion is suppressed, and the number of infected individuals will decrease with time. The balance of these competing effects is tuned by the frequency of vaccination, which directly affects the first of these rates &amp;#8212; that at which the disease can spread. Because measles is &lt;a href="http://www.cdc.gov/measles/about/transmission.html"&gt;highly contagious&lt;/a&gt;, its balance point occurs around the relatively-high 94% mark seen in the figure &amp;#8212; this is the vaccination rate needed to have the average rate of disease spread just equal to the average rate of recovery. An info-graphic illustrating these points can be found &lt;a href="http://www.vaccines.gov/basics/protection/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Model implications for &lt;span class="caps"&gt;USA&lt;/span&gt;&amp;nbsp;youth&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;At the start of this post, we presented &lt;span class="caps"&gt;CDC&lt;/span&gt; estimates for the mean, by-state vaccination rates within the &lt;span class="caps"&gt;USA&lt;/span&gt;. Now that we have reviewed the results of the &lt;span class="caps"&gt;SIR&lt;/span&gt; model, we can begin to appreciate the significance of this data more deeply. First &amp;#8212; as we noted above &amp;#8212; we see that many states have youth vaccination rates that sit just below the critical 94% level, and thus on average should support small measles outbreaks. The graph of the previous section also allows us to estimate the mean size of a measles outbreak (once sparked) within any community below the critical vaccination rate: The further a state is from the 94% mark, the larger its mean outbreak size should be. Although all states are doing reasonably well, when considered on &lt;a href="http://www.npr.org/blogs/goatsandsoda/2015/02/06/384068229/measles-vaccination-rates-tanzania-does-better-than-u-s"&gt;a global scale&lt;/a&gt;, it is important to realize that many sit in a region of the plot where the response to a decrease in vaccination rate is most dramatic &amp;#8212; the curve&amp;#8217;s slope is largest just to the right of 94% vaccination. This means, e.g., that a 1% decrease in vaccination rate will result in a greater than 1% increase in the size of the average outbreak supported within that state. This observation is modestly worrisome, as the risk of contraction for all individuals &amp;#8212; even those vaccinated &amp;#8212; is always proportional to the number of cases present in any&amp;nbsp;outbreak.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important&amp;nbsp;caveats:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vaccination rates fluctuate between cities, neighborhoods, etc. This means that simple state averages may not accurately characterize your local community&amp;#8217;s vaccination rate &amp;#8212; the quantity most relevant to your personal infection risk. See, for instance, the plot for California by county shown &lt;a href="http://www.huffingtonpost.com/2015/02/03/measles-us-facts_n_6581922.html"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Our results are based on a descriptive, but simple model. They are intended only to provide one with a qualitative picture of the forces governing measles spread. Detailed, peer-reviewed treatments (this is just a blog&amp;#8230;) can be found in the&amp;nbsp;literature.&lt;/li&gt;
&lt;li&gt;Consult a licensed physician for qualified information on vaccines, measles&amp;nbsp;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Interestingly, &lt;a href="http://en.wikipedia.org/wiki/Measles#Cause"&gt;humans are the only known carriers&lt;/a&gt; of the measles virus. Consequently, with &lt;a href="http://www.nature.com/news/measles-by-the-numbers-a-race-to-eradication-1.16897"&gt;growing global vaccination rates&lt;/a&gt;, it may one day soon be possible to totally extinguish the disease. Looking back on the historical &lt;span class="caps"&gt;USA&lt;/span&gt; data helps one realize that this would be a truly remarkable accomplishment! In fact, were the &lt;span class="caps"&gt;USA&lt;/span&gt; in isolation, our current vaccination rates would likely be sufficient to bring this about. However, we are not, and sporadic outbreaks continue to occur here, ignited through international travel of infected individuals. These outbreaks can occur because our youth vaccination rates are below the critical 94% level needed to suppress&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;The science of disease spread is very interesting. For those with a mathematical background, we suggest taking a look at &lt;a href="http://http://efavdb.github.io/math-of-measles"&gt;our prior post&lt;/a&gt;, where we solve the &lt;span class="caps"&gt;SIR&lt;/span&gt; model analytically. Many additional insights into disease spread &amp;#8212; not covered here &amp;#8212; can be gleaned through the study of this model. Likewise, those with a programming background can play with the code that we provide below to sort through the &lt;span class="caps"&gt;CDC&lt;/span&gt;&amp;#8217;s data sets in different ways. For instance, one can easily alter this code to view how vaccination rates vary by socio-economic background, rather than by state, etc. One can also use the same procedures to sort through many other interesting data sets provided by the &lt;span class="caps"&gt;CDC&lt;/span&gt; and&amp;nbsp;others.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Methods: Wrangling the &lt;span class="caps"&gt;CDC&lt;/span&gt; measles data&amp;nbsp;sets&lt;/strong&gt;&lt;/h4&gt;
&lt;h5&gt;&lt;strong&gt;Grabbing and loading&amp;nbsp;data&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;In this section, we outline our numerical analysis of the &lt;span class="caps"&gt;CDC&lt;/span&gt; measles vaccination rate data set. To follow along, you must first download the files.  Current data can be found &lt;a href="http://www.cdc.gov/nchs/nis/data_files.htm"&gt;here&lt;/a&gt;, and data corresponding to years before 2009 &lt;a href="http://www.cdc.gov/nchs/nis/data_files_09_prior.htm"&gt;here&lt;/a&gt;.  Three files are needed.  The &lt;a href="ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/nis/nispuf12.dat"&gt;Dataset file&lt;/a&gt;, the &lt;a href="ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Dataset_Documentation/NIS/NISPUF12_CODEBOOK.PDF"&gt;Codebook&lt;/a&gt; that explains how to read the dataset files, and the &lt;a href="ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Dataset_Documentation/NIS/NISPUF12_DUG.PDF"&gt;Data User’s Guide&lt;/a&gt; which provides details about the data, including methodology and statistics&amp;nbsp;descriptions.&lt;/p&gt;
&lt;p&gt;In our analysis, we will make use of a few python packages.  In particular, we will use &lt;a href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt; to construct high performance data structures, called DataFrames. These are easy to use, and they allow for fast, straightforward data manipulation &amp;#8212; both helpful features for data wrangling. We will utilize the groupby DataFrame method, which enables one to easily segment data according to values along different feature directions. To illustrate, we will split our data by state name.  We will then use the &lt;a href="https://plot.ly/"&gt;Plot.ly&lt;/a&gt; package to generate the interactive plots included&amp;nbsp;above.&lt;/p&gt;
&lt;p&gt;To get a feel for the &lt;span class="caps"&gt;CDC&lt;/span&gt; data, a good first step is to look at the data in a text editor.  Doing this, we quickly notice multiple rows of characters having no vernacular significance.  The codebook allows one to interpret these characters.  It also explains that each row corresponds to a different child surveyed, and that each row has a fixed number of entries, many corresponding to different vaccines. We will read these rows in one at a time &amp;#8212; making use of a for loop &amp;#8212; to save on memory. As each line is processed, we keep only what we need &amp;#8212; here that info relevant to the &lt;span class="caps"&gt;MMR&lt;/span&gt;&amp;nbsp;vaccine.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;plotly.plotly&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;py&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;plotly.graph_objs&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="n"&gt;py&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sign_in&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;username&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;password&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#Create some empty lists to store the data&lt;/span&gt;
&lt;span class="n"&gt;child_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;house_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;patient_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;measles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;stratum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="c1"&gt;#No virgin islands&lt;/span&gt;
&lt;span class="n"&gt;weights_v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;mmr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="c1"&gt;#First we need to read in the data file, and parse the data&lt;/span&gt;
&lt;span class="c1"&gt;#using the datasheet&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;nispuf12.dat&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;child_ID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;house_ID&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;patient_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;measles&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;260&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;261&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;181&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;183&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;stratum&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;88&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;92&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;69&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;weights_v&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;31&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;mmr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;261&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;262&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;&lt;strong&gt;Cleaning&amp;nbsp;data&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;Now that everything is parsed and loaded into arrays, we will insert our data into a dictionary with key given by the column header.  We then convert this into a DataFrame. To streamline the process, we will also replace all the missing values with a &lt;span class="caps"&gt;NAN&lt;/span&gt;, and also convert all relevant entries from string to number format.  We will also replace the stateID numbers with their written names.  Lastly, we will remove all the incomplete patient&amp;nbsp;files.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#Create a dataframe in pandas for data manipulation&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;child&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;child_ID&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;house&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;house_ID&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;patient_data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;patient_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;measles&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;measles&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;MMR&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;mmr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;stratum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;stratum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;weights&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;weights_v&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;weights_v&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="c1"&gt;#Clean up the data to assign . to NAN&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;measles&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;measles&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;NAN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;#Convert the data to numberic values&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convert_objects&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;convert_numeric&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#Replace the state ID code with the state&amp;#39;s name&lt;/span&gt;
&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loadtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;state_ID.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;unpack&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;delimiter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#Find all the values where there is complete provider data.&lt;/span&gt;
&lt;span class="c1"&gt;#We will look at only the complete patient records,&lt;/span&gt;
&lt;span class="c1"&gt;#and remove records column which is now only one value.&lt;/span&gt;
&lt;span class="n"&gt;ind&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;patient_data&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ind&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;patient_data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Below, we provide some examples of our resulting, cleaned data points. The weight columns here are described further below. The first of these is used for analyses including the Virgin Islands, the other when they are&amp;nbsp;not.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;MMR child house measles state stratum weights weights_v&lt;/span&gt;
&lt;span class="err"&gt;0 1 11 1 1 Texas 1054 65.155698 120.163273&lt;/span&gt;
&lt;span class="err"&gt;1 1 21 2 1 Texas 2055 33.652064 52.360361&lt;/span&gt;
&lt;span class="err"&gt;3 1 41 4 1 Massachusetts 1002 216.529889 271.502218&lt;/span&gt;
&lt;span class="err"&gt;5 1 61 6 1 Georgia 1025 231.557156 562.130094&lt;/span&gt;
&lt;span class="err"&gt;6 1 71 7 1 South Carolina 1030 150.109737 238.018808&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;&lt;strong&gt;Weighting to get averaged&amp;nbsp;statistics&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;We now have our data cleaned and ready to go. However, some additional work needs to be done before we can evaluate various statistics of interest. This is because the &lt;span class="caps"&gt;CDC&lt;/span&gt; data set is not a random sample, but instead a &lt;a href="http://en.wikipedia.org/wiki/Stratified_sampling"&gt;stratified sample&lt;/a&gt; &amp;#8212; i.e. one geared towards obtaining reasonable accuracy among many minority groups, and not simply among the averaged population. The weight factors are the key to extracting averaged statistics from this data, as explained in the user guide. For example, the average is essentially just a weighted average, and the standard error can be calculated using a Taylor-Series approach. The easiest way to apply this to our data is to make use of custom functions. Once constructed, these can then be easily applied to different DataFrame&amp;nbsp;groupings.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Lets define a function to caluclate the vaccination rate&lt;/span&gt;
&lt;span class="c1"&gt;# for the group we are looking at, and the standard error&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;calculate_rate_and_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# The rate is calucated useing a weighted average&lt;/span&gt;
    &lt;span class="n"&gt;rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MMR&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_v&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c1"&gt;# The error is calculated using the formula from the data sheet&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Z&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_v&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MMR&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_v&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;zhi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;stratum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;house&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Z&lt;/span&gt;
    &lt;span class="n"&gt;zh&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;zhi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;stratum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="c1"&gt;# Number of households per stratum&lt;/span&gt;
    &lt;span class="n"&gt;nk&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;zhi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;stratum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;zh&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;zh&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nk&lt;/span&gt;

    &lt;span class="n"&gt;stratum_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;zh&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
    &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nk&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;ind&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;stratum_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;delta2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zhi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;zh&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="n"&gt;delta2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;delta2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ind&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nk&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nk&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;delta2&lt;/span&gt;
        &lt;span class="n"&gt;ind&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="n"&gt;standard_error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;standard_error&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we apply this function to our whole DataFrame, we will get the national &lt;span class="caps"&gt;MMR&lt;/span&gt; vaccination rate and standard&amp;nbsp;error.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;calculate_rate_and_error(data)&lt;/span&gt;
&lt;span class="err"&gt;# output: (0.90767842904073048, 0.0043003916851249895)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;&lt;strong&gt;Data segmentation &amp;#8212; stats by&amp;nbsp;group&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;But we can also do more! If we first apply the DateFrame&amp;#8217;s groupby method, we can split the data along any feature of interest. For example, below we split the data along the state column. This generates subgroups for each state. Next, we use the apply method to run our custom function on all the different groups of data. We then clean up the output, unzip the tuple and generate a graph showing the &lt;span class="caps"&gt;MMR&lt;/span&gt; vaccination rate by state. It should be evident that with only modest effort, one can modify this code to group the data in many varying ways &amp;#8212; all that needs to be done is to adjust the arguments of the groupby&amp;nbsp;command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Now that we have our function it is easy to calculate values&lt;/span&gt;
&lt;span class="c1"&gt;# for any group.&lt;/span&gt;

&lt;span class="c1"&gt;# To examine rates by state, we will group by state&lt;/span&gt;
&lt;span class="n"&gt;grouped&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# We then apply our function to the grouped data, and save&lt;/span&gt;
&lt;span class="c1"&gt;# it as a data frame&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grouped&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;calculate_rate_and_error&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# To conver the results from:&lt;/span&gt;
&lt;span class="c1"&gt;# state&lt;/span&gt;
&lt;span class="c1"&gt;# Alabama (0.931323319509, 0.017837146103)&lt;/span&gt;
&lt;span class="c1"&gt;# Alaska (0.862202058663, 0.0257844894834)&lt;/span&gt;
&lt;span class="c1"&gt;# to&lt;/span&gt;
&lt;span class="c1"&gt;# Rate Standard Error&lt;/span&gt;
&lt;span class="c1"&gt;# state&lt;/span&gt;
&lt;span class="c1"&gt;# Alabama 0.931323 0.017837&lt;/span&gt;
&lt;span class="c1"&gt;# Alaska 0.862202 0.025784&lt;/span&gt;
&lt;span class="c1"&gt;# We use the following code&lt;/span&gt;

&lt;span class="n"&gt;new_col_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Standard Error&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_col_list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Make a sorted copy of the data&lt;/span&gt;
&lt;span class="nb"&gt;sorted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;

&lt;span class="c1"&gt;# Save a csv file if wanted&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate 2012.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;float_format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;%5.2f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Generate an online plot&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Data&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="n"&gt;Bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;orientation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h&amp;#39;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plot_url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;py&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;plot&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="Case studies"></category></entry><entry><title>Analyzing Analysts</title><link href="https://efavdb.com/analyzinganalysts" rel="alternate"></link><published>2015-02-08T14:15:00-08:00</published><updated>2015-02-08T14:15:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2015-02-08:/analyzinganalysts</id><summary type="html">&lt;p&gt;In this post, Dustin provides an overview of some of his work from his time in the &lt;a href="http://insightdatascience.com/"&gt;Insight Data Science Fellowship&lt;/a&gt; program — work done in collaboration with &lt;a href="https://modeanalytics.com/"&gt;Mode Analytics&lt;/a&gt;, an online collaborative &lt;span class="caps"&gt;SQL&lt;/span&gt;&amp;nbsp;platform.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="//www.slideshare.net/DustinMcIntosh/insight-demo-44864932" title="Insight Demo"&gt;Insight Demo&lt;/a&gt;&lt;/strong&gt; from &lt;strong&gt;&lt;a href="//www.slideshare.net/DustinMcIntosh"&gt;Dustin&amp;nbsp;McIntosh&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many high-value business insights have their answers rooted in data. Companies …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post, Dustin provides an overview of some of his work from his time in the &lt;a href="http://insightdatascience.com/"&gt;Insight Data Science Fellowship&lt;/a&gt; program — work done in collaboration with &lt;a href="https://modeanalytics.com/"&gt;Mode Analytics&lt;/a&gt;, an online collaborative &lt;span class="caps"&gt;SQL&lt;/span&gt;&amp;nbsp;platform.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="//www.slideshare.net/DustinMcIntosh/insight-demo-44864932" title="Insight Demo"&gt;Insight Demo&lt;/a&gt;&lt;/strong&gt; from &lt;strong&gt;&lt;a href="//www.slideshare.net/DustinMcIntosh"&gt;Dustin&amp;nbsp;McIntosh&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many high-value business insights have their answers rooted in data. Companies hire data analysts to extract these insights by dissecting their data, largely through querying databases with &lt;span class="caps"&gt;SQL&lt;/span&gt; (Structured Query&amp;nbsp;Language).&lt;/p&gt;
&lt;p&gt;However, data analysis can often be difficult. &lt;a href="https://modeanalytics.com"&gt;Mode Analytics&lt;/a&gt; is a company that aims to streamline the process of learning and engaging in data analysis with &lt;span class="caps"&gt;SQL&lt;/span&gt;, both for experts and novices alike. As an Insight Data Science Fellow, I have been working with Mode to help them analyze their users&amp;#8217; &lt;span class="caps"&gt;SQL&lt;/span&gt; code, mistakes and all, to identify opportunities for product improvement. Specifically, I&amp;#8217;ve been trying to address the following&amp;nbsp;questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What types of &lt;span class="caps"&gt;SQL&lt;/span&gt; errors do users make? Do these vary with user ability&amp;nbsp;level?&lt;/li&gt;
&lt;li&gt;Do early struggles dishearten beginners, hampering them from becoming power&amp;nbsp;users?&lt;/li&gt;
&lt;li&gt;More generally, what characteristics differentiate the &lt;span class="caps"&gt;SQL&lt;/span&gt; queries of aspiring and expert&amp;nbsp;analysts?&lt;/li&gt;
&lt;li&gt;Given answers to these questions, how can Mode modify their product to make analysis easier for their&amp;nbsp;users?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My analysis uncovered a number of actionable insights for&amp;nbsp;Mode:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Mostly, analysts make the same sorts of errors regardless of their ability level. The good news is that many of these errors are avoidable: Mode can&amp;nbsp;help.&lt;/li&gt;
&lt;li&gt;Novice users actually do not seem to churn due to frustration with errors. It&amp;#8217;s more likely that they leave due simply to having completed the tutorials - Mode can increase user retention by proactively encouraging users to upload and interact with their own private data after finishing the&amp;nbsp;tutorials.&lt;/li&gt;
&lt;li&gt;Mode can customize the user experience soon after a user joins the site in order to better serve users based on their &lt;span class="caps"&gt;SQL&lt;/span&gt; skills. To this end, I built a model to classify users as experts or novices based on the content of their queries. This model reveals that experts take extra care in the formatting of their queries, implying an advanced knowledge of the &lt;span class="caps"&gt;SQL&lt;/span&gt; structure that could be emphasized in the&amp;nbsp;tutorials.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;WHAT&lt;/span&gt; &lt;span class="caps"&gt;IS&lt;/span&gt; &lt;span class="caps"&gt;MODE&lt;/span&gt; &lt;span class="caps"&gt;AND&lt;/span&gt; &lt;span class="caps"&gt;WHO&lt;/span&gt; &lt;span class="caps"&gt;ARE&lt;/span&gt; &lt;span class="caps"&gt;THEIR&lt;/span&gt; &lt;span class="caps"&gt;USERS&lt;/span&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://modeanalytics.com/"&gt;Mode Analytics&lt;/a&gt; provides a one-stop shop for all the needs of a data analyst. Users can easily connect their data to Mode&amp;#8217;s web-based app, query it with &lt;span class="caps"&gt;SQL&lt;/span&gt;, and create and share visualizations of their analysis to convey the insights they&amp;#8217;ve extracted. In addition, Mode offers free instruction via their &lt;a href="http://sqlschool.modeanalytics.com/"&gt;&lt;span class="caps"&gt;SQL&lt;/span&gt; school&lt;/a&gt;, which instructs novices on the basics of &lt;span class="caps"&gt;SQL&lt;/span&gt; while querying some public, tutorial datasets.&lt;br&gt;
&lt;a href="https://efavdb.com/wp-content/uploads/2015/02/errorRate31.png"&gt;&lt;img alt="errorRate3" src="https://efavdb.com/wp-content/uploads/2015/02/errorRate31.png"&gt;&lt;/a&gt;&lt;br&gt;
With the goal of cohorting users based on their ability level, I group users based on the number of queries they have submitted to Mode (see plot at right of tutorial reference rate and error rate by&amp;nbsp;cohort):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Power users (1000 + queries): this group primarily queries private data sources with no need for the tutorials, making only occasional&amp;nbsp;errors.&lt;/li&gt;
&lt;li&gt;Novices (10-1000 queries): the majority of these users are heavily invested in the tutorials and are making a lot more errors than the experts as they&amp;nbsp;learn.&lt;/li&gt;
&lt;li&gt;Infrequent queriers (&amp;lt;10 queries): these users have only visited Mode&amp;#8217;s website long enough to make a few queries - most of which are easy tutorial exercises on which they are making very few&amp;nbsp;errors.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The demarcations at 10 and 1000 queries are somewhat arbitrary at this point. Undoubtedly, there are many experts hidden in the 10-1000 category who already possess advanced &lt;span class="caps"&gt;SQL&lt;/span&gt; knowledge, but are yet to extensively use Mode’s platform for their analysis. As we will see, we can build a model to distinguish these users based on the content of their queries rather than their query&amp;nbsp;count.&lt;/p&gt;
&lt;p&gt;The primary goal, from a business perspective, is to move users up this list: get infrequent queriers invested in the product and get novice tutorial users up to speed with &lt;span class="caps"&gt;SQL&lt;/span&gt; so they can enjoy all aspects of Mode&amp;#8217;s product. For the remainder of the post, I ignore the infrequent queriers [1] in favor of analyzing novices and the differences between them and the&amp;nbsp;experts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;WHAT&lt;/span&gt; &lt;span class="caps"&gt;ERRORS&lt;/span&gt; &lt;span class="caps"&gt;DO&lt;/span&gt; &lt;span class="caps"&gt;SQL&lt;/span&gt; &lt;span class="caps"&gt;USERS&lt;/span&gt; &lt;span class="caps"&gt;MAKE&lt;/span&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;SQL&lt;/span&gt; is a very simple language. With a simple, declarative style and only about 200 keywords in total, of which only a few dozen are in common usage [2], there is very little to memorize for &lt;span class="caps"&gt;SQL&lt;/span&gt; users. Thus, most errors should be recognizable and avoidable in product design. Examining the types of errors &lt;span class="caps"&gt;SQL&lt;/span&gt; users make informs us how we can make an analyst&amp;#8217;s experience better through the Mode platform.  In the plot below, I count the number of each type of error made by the two cohorts (experts and novices).&lt;br&gt;
&lt;a href="https://efavdb.com/wp-content/uploads/2015/02/errorClass1.png"&gt;&lt;img alt="errorClass" src="https://efavdb.com/wp-content/uploads/2015/02/errorClass1.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The most striking thing about this analysis is that the two most common error types are forgetting and/or misspelling table and column names. Fortunately, these errors are easily addressed through product design: simply prominently displaying the table/column names may significantly reduce errors. It may even be possible in some instances to auto-fill the&amp;nbsp;names.&lt;/p&gt;
&lt;p&gt;There are a few subtle differences between expert and novice errors worth noting. Novice users tend to make more syntax errors, which is not a surprise given they are less familiar with the language. Power users, on the other hand, make a broader distribution of errors (e.g., a lot more rare errors). Further, experts much more frequently run into the limits of the system (e.g., timeouts, internal errors), an indication that they are running more complex queries. From a product perspective, correcting errors for novices is the priority as they make the most errors and are less likely to know how to correct them. Further, as novices tend to make a narrower range of errors, it is likely possible to parse out many of these errors and either auto-correct them or provide more personalized help to the user than the standard &lt;span class="caps"&gt;SQL&lt;/span&gt; error&amp;nbsp;messages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;DO&lt;/span&gt; &lt;span class="caps"&gt;NOVICES&lt;/span&gt; &lt;span class="caps"&gt;GET&lt;/span&gt; &lt;span class="caps"&gt;FRUSTRATED&lt;/span&gt; &lt;span class="caps"&gt;AND&lt;/span&gt; &lt;span class="caps"&gt;QUIT&lt;/span&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A principal concern for educating the novice users is that they may consistently run into certain types of errors, not understand how to correct them, and quit out of frustration. Should Mode customize the standard error messages to better direct novices to their problems? To see whether or not this is the case, I looked at the set of churned novice users (those that have not made a query since November) and examined their final few queries. If user frustration is causing churn, we expect the error rate to increase as the users approach churn.&lt;br&gt;
&lt;a href="https://efavdb.com/wp-content/uploads/2015/02/errorChurn.png"&gt;&lt;img alt="errorChurn" src="https://efavdb.com/wp-content/uploads/2015/02/errorChurn.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Surprisingly, the rate at which users commit errors before they churn is actually lower than the group&amp;#8217;s average error rate and decreases until they churn. Thus, we can infer that the average novice does &lt;strong&gt;not&lt;/strong&gt; quit due to frustration with errors they are making - this is not a major concern for customer&amp;nbsp;retention.&lt;/p&gt;
&lt;p&gt;However, Mode could be more proactive about transitioning customers from their &lt;span class="caps"&gt;SQL&lt;/span&gt; school to connecting their own data and using their visualization tools. 88% of users that churn have not connected their own data sources and are working exclusively on the tutorial datasets. One potential product enhancement is to periodically remind tutorial users of the possibility of connecting their own data and to add tutorial exercises introducing the visualization&amp;nbsp;tools.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;WHAT&lt;/span&gt;&amp;#8217;S &lt;span class="caps"&gt;IN&lt;/span&gt; A &lt;span class="caps"&gt;SQL&lt;/span&gt; &lt;span class="caps"&gt;EXPERT&lt;/span&gt;&amp;#8217;S &lt;span class="caps"&gt;QUERY&lt;/span&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As discussed above, drawing a line at 1000 queries is not a very useful classification metric for experts versus novices. There are many experts that come to Mode with a lot of expertise in &lt;span class="caps"&gt;SQL&lt;/span&gt;, but have not made 1000 queries yet.  Thus, I developed a model based on a Random Forest classifier to differentiate expert and novice &lt;span class="caps"&gt;SQL&lt;/span&gt; users based on the content of their queries [3]. This model accurately classifies users based on a single query roughly 65% of the time for both classes. Accuracy of the prediction will go up the more unique queries a user submits; for example, after ten queries accuracy may be as high as 90%&amp;nbsp;[4].&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/02/featureImportance5.png"&gt;&lt;img alt="featureImportance5" src="https://efavdb.com/wp-content/uploads/2015/02/featureImportance5.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;One interesting aspect of the Random Forest model is that it determines the most important features that define an expert&amp;#8217;s query from a novice&amp;#8217;s. The result is clear: Of the top five most important features, three of them have to do with formatting; experts take more care in their use of white space, line breaks, and parentheses. In particular, in their queries, experts have a higher density of white space, a lower density of line breaks, and a higher density of parentheses. Thus, experts tend to have longer lines of code with more white space. The two remaining features in the top five, query length and frequency of “select” and “from”, indicate that experts tend to write longer queries with more&amp;nbsp;subqueries.&lt;/p&gt;
&lt;p&gt;Being able to classify a user as an expert or novice based on the content of their queries would be extremely useful for personalizing user experience. If a user connects their data early on and it becomes clear that they have limited experience with &lt;span class="caps"&gt;SQL&lt;/span&gt;, Mode would like to be able to direct those users to relevant tutorials or perhaps to work related to theirs that was performed by experts. Likewise, if a user immediately demonstrates advanced &lt;span class="caps"&gt;SQL&lt;/span&gt; knowledge, Mode would like to direct them to, for example, the data visualization tools that make Mode&amp;#8217;s platform uniquely useful to the experienced analyst. Implementation of my model would permit&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;CONCLUSIONS&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This analysis provides a number of actionable insights for Mode in serving their user&amp;nbsp;base:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Everyone using the platform is affected by errors relating to misspelling or altogether forgetting the names of the tables and their contained fields in their databases. Implementing an auto-complete of these names or prominently displaying them for the user could considerably reduce the number of errors encountered on&amp;nbsp;Mode.&lt;/li&gt;
&lt;li&gt;Most users that churn are not leaving out of frustration with the errors they are making. The vast majority of churning users, however, are exclusively querying tutorial datasets. Mode can try to get tutorial users more invested in their product by prompting them to connect their data periodically throughout the&amp;nbsp;tutorial.&lt;/li&gt;
&lt;li&gt;In addition to tutorial-using novices and the true &lt;span class="caps"&gt;SQL&lt;/span&gt; experts, Mode has a third class of users. Their query history does not reveal much about them: they only visit Mode briefly, submitting just a few tutorial exercises without making many errors. Looking into who these visitors are via, for example, google analytics data, may provide insight into how to retain these potential&amp;nbsp;users.&lt;/li&gt;
&lt;li&gt;User ability level can be determined based on the content of their first several queries to Mode. This is useful as Mode can effectively personalize response to users as they join the&amp;nbsp;platform.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;FOOTNOTES&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[1] The query history of the infrequent queriers does not provide very much information as to who these users are. Gathering additional information on these users may prove useful in determining how to retain these&amp;nbsp;customers.&lt;/p&gt;
&lt;p&gt;[2] &lt;em&gt;Top-Heavy Nature of &lt;span class="caps"&gt;SQL&lt;/span&gt; -&lt;/em&gt;Of the approximately 200 keywords in the &lt;span class="caps"&gt;SQL&lt;/span&gt; vocabulary, a very small subset are even remotely common. Looking at the fraction of queries containing each keyword from the expert dataset demonstrates just how top heavy the language is. Each keyword not present on this plot appears in fewer than 4% of all unique queries in the dataset.&lt;br&gt;
&lt;a href="https://efavdb.com/wp-content/uploads/2015/02/SQL_keywords_expert.png"&gt;&lt;img alt="SQL_keywords_expert" src="https://efavdb.com/wp-content/uploads/2015/02/SQL_keywords_expert.png"&gt;&lt;/a&gt;[3] &lt;em&gt;Labeling the Training Set -&lt;/em&gt;I define users with &amp;gt;1000 queries as experts and take only a subset of their most recent queries (those submitted since November 2014) - assuming that they have become better queriers with time. For the novice training set, it is not possible to strictly take the users from the 10-1000 query group as there are likely some new-comer experts in that category (experienced &lt;span class="caps"&gt;SQL&lt;/span&gt; users that have just recently joined Mode). Instead, I took only users with 100-1000 queries that had participated extensively in the tutorials. I then remove those tutorial queries from their set and label those remaining as novice queries for training.&lt;br&gt;
&lt;em&gt;Feature Selection/Engineering&lt;/em&gt;- I use a bag-of-words approach with language defined by &lt;span class="caps"&gt;SQL&lt;/span&gt;&amp;#8217;s keywords. Additional features include query length, number of unique keywords in the query (diversity), as well as fraction of the query that is white space, line breaks, and parentheses. Several keywords are strongly correlated with one another; for this reason, I combine some together (e.g., “select” and “from”) and remove others from the analysis entirely (e.g., “as”, “by” and&amp;nbsp;“on”).&lt;/p&gt;
&lt;p&gt;[4]&lt;em&gt;Growing confidence of classification -&lt;/em&gt;If a user&amp;#8217;s successive queries were independent of one another we would expect confidence in the prediction to grow in accordance with a binomial distribution: &lt;span class="math"&gt;\(P(\)&lt;/span&gt;misclassify&lt;span class="math"&gt;\() = \sum_{i = 0}^{\lfloor n/2 \rfloor} \binom{n}{i} p^i (1-p)^{n-i}\)&lt;/span&gt; with &lt;span class="math"&gt;\(p \approx 0.65\)&lt;/span&gt; and &lt;span class="math"&gt;\(n\)&lt;/span&gt; the number of queries. After ten queries the misclassification error reduces to roughly 10%; after 20 queries - 5%. However, a user&amp;#8217;s queries are typically not independent of one another, so this is likely a generous&amp;nbsp;estimate.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category></entry><entry><title>Machine learning for facial recognition</title><link href="https://efavdb.com/machine-learning-for-facial-recognition-3" rel="alternate"></link><published>2015-01-21T17:12:00-08:00</published><updated>2015-01-21T17:12:00-08:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2015-01-21:/machine-learning-for-facial-recognition-3</id><summary type="html">&lt;p&gt;A guest post, contributed by Damien Ramunno-Johnson (&lt;a href="https://www.linkedin.com/profile/view?id=60223336&amp;amp;authType=NAME_SEARCH&amp;amp;authToken=LOV_&amp;amp;locale=en_US&amp;amp;trk=tyah2&amp;amp;trkInfo=tarId%3A1420748440448%2Ctas%3Adamien%2Cidx%3A1-1-1"&gt;LinkedIn&lt;/a&gt;, &lt;a href="https://efavdb.github.io/pages/authors.html"&gt;bio-sketch&lt;/a&gt;)&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The ability to identify faces is a skill that people develop very early in life and can apply almost effortlessly. One reason for this is that our brains are very well adapted for pattern recognition. In contrast, facial recognition can …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A guest post, contributed by Damien Ramunno-Johnson (&lt;a href="https://www.linkedin.com/profile/view?id=60223336&amp;amp;authType=NAME_SEARCH&amp;amp;authToken=LOV_&amp;amp;locale=en_US&amp;amp;trk=tyah2&amp;amp;trkInfo=tarId%3A1420748440448%2Ctas%3Adamien%2Cidx%3A1-1-1"&gt;LinkedIn&lt;/a&gt;, &lt;a href="https://efavdb.github.io/pages/authors.html"&gt;bio-sketch&lt;/a&gt;)&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The ability to identify faces is a skill that people develop very early in life and can apply almost effortlessly. One reason for this is that our brains are very well adapted for pattern recognition. In contrast, facial recognition can be a somewhat difficult problem for computers. Today, given a full frontal image of a face, computer facial recognition software works well. However, problems can arise given large camera angles, poor lighting, or exaggerated facial expressions: Computers have a ways to go before they catch up with us in this&amp;nbsp;arena.&lt;/p&gt;
&lt;p&gt;Although facial recognition algorithms remain imperfect, the methods that exist now are already quite useful and are being applied by many different companies. Two examples, first up Facebook: When you upload pictures to their website, it will now automatically suggest names for the people in your photos. This application is well-suited for machine learning for two reasons. First, every tagged photo already uploaded to the site provides labeled examples on which to train an algorithm, and second, people often post full face images in decent lighting. A second example is provided by Google&amp;#8217;s Android phone &lt;span class="caps"&gt;OS&lt;/span&gt;, which has a face unlock mode. To get this to work, you first have to train your phone by taking images of your face in different lighting conditions and from different angles. After training, the phone can attempt to recognize you. This is another cool application that also often works&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;In this post, we&amp;#8217;re going to develop our own basic facial learning algorithm. We&amp;#8217;ll find that it is actually pretty straightforward to set one up that is reasonably accurate. Our post follows and expands upon the tutorial found &lt;a href="http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Loading packages and&amp;nbsp;data&lt;/strong&gt;&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;print_function&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fetch_lfw_people&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.grid_search&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;classification_report&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;confusion_matrix&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.decomposition&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomizedPCA&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.svm&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SVC&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The sklearn function &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html"&gt;fetch_lfw_people&lt;/a&gt;, imported above, will download the data that we need, if not already present in the faces folder. The dataset we are downloading consists of a set of preprocessed images from &lt;a href="http://vis-www.cs.umass.edu/lfw/"&gt;Labeled Faces in the Wild (&lt;span class="caps"&gt;LFW&lt;/span&gt;)&lt;/a&gt;, a database designed for studying unconstrained face recognition. The data set contains more than 13,000 images of faces collected from the web, each labeled with the name of the person pictured. 1680 of the people pictured have two or more distinct photos in the data&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;In our analysis here, we will impose two&amp;nbsp;conditions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First, we will only consider folks that have a minimum of 70 pictures in the data&amp;nbsp;set.&lt;/li&gt;
&lt;li&gt;We will resize the images so that they each have a 0.4 aspect&amp;nbsp;ratio.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;print(&amp;#39;Loading Data&amp;#39;)&lt;/span&gt;
&lt;span class="err"&gt;people = fetch_lfw_people(&lt;/span&gt;
&lt;span class="err"&gt;&amp;#39;./faces&amp;#39;, min_faces_per_person=70, resize=0.4)&lt;/span&gt;
&lt;span class="err"&gt;print(&amp;#39;Done!&amp;#39;)&lt;/span&gt;
&lt;span class="err"&gt;&amp;amp;gt;&amp;amp;gt;&lt;/span&gt;
&lt;span class="err"&gt;Loading Data&lt;/span&gt;
&lt;span class="err"&gt;Done!&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The object &lt;strong&gt;people&lt;/strong&gt; contains the following&amp;nbsp;data.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;people.data: a numpy array with the shape(n_samples, h*w), each row corresponds to a unravelled&amp;nbsp;face.&lt;/li&gt;
&lt;li&gt;people.images: a numpy array with the shape(n_samples, h, w), where each row corresponds to a face. The remaining indices here contain gray-scale values for the pixels of each&amp;nbsp;image.&lt;/li&gt;
&lt;li&gt;people.target: a numpy array with the shape(n_samples), where each row is the label for the&amp;nbsp;face.&lt;/li&gt;
&lt;li&gt;people.target_name: a numpy array with the shape(n_labels), where each row is the name for the&amp;nbsp;label.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the algorithm we will be using, we don&amp;#8217;t need the relative position data, so we will use the unraveled&amp;nbsp;people.data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="nn"&gt;Find&lt;/span&gt; &lt;span class="nt"&gt;out&lt;/span&gt; &lt;span class="nt"&gt;how&lt;/span&gt; &lt;span class="nt"&gt;many&lt;/span&gt; &lt;span class="nt"&gt;faces&lt;/span&gt; &lt;span class="nt"&gt;we&lt;/span&gt; &lt;span class="nt"&gt;have&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;and&lt;/span&gt;
&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="nn"&gt;the&lt;/span&gt; &lt;span class="nt"&gt;size&lt;/span&gt; &lt;span class="nt"&gt;of&lt;/span&gt; &lt;span class="nt"&gt;each&lt;/span&gt; &lt;span class="nt"&gt;picture&lt;/span&gt; &lt;span class="nt"&gt;from&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="nt"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;h&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;people&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;images&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;shape&lt;/span&gt;

&lt;span class="nt"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;people&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;data&lt;/span&gt;
&lt;span class="nt"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;

&lt;span class="nt"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;people&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;target&lt;/span&gt;
&lt;span class="nt"&gt;target_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;people&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;target_names&lt;/span&gt;
&lt;span class="nt"&gt;n_classes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;

&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;Total&lt;/span&gt; &lt;span class="nt"&gt;dataset&lt;/span&gt; &lt;span class="nt"&gt;size&lt;/span&gt;&lt;span class="o"&gt;:&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;)&lt;/span&gt;
&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;n_images&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="nt"&gt;d&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nt"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="nt"&gt;d&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nt"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;n_classes&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="nt"&gt;d&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nt"&gt;n_classes&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="nt"&gt;Total&lt;/span&gt; &lt;span class="nt"&gt;dataset&lt;/span&gt; &lt;span class="nt"&gt;size&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="nt"&gt;n_images&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;1288&lt;/span&gt;
&lt;span class="nt"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;1850&lt;/span&gt;
&lt;span class="nt"&gt;n_classes&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;7&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Looking above we see that our dataset currently has 1288 images. Each image has 1850 pixels, or features. We also have 7 classes, meaning images of 7 different&amp;nbsp;people.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Data segmentation and dimensional&amp;nbsp;reduction&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;At this point we need to segment our data. We are going to use &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html"&gt;train_test_split&lt;/a&gt;, which will take care of splitting our data into random training and testing data sets. Next, we note that we have a lot of features and that there are advantages to having fewer: First, the computational cost is reduced. Second, having fewer features reduces the data’s dimension which can also reduce the complexity of the model and help avoid overfitting. Instead of dropping individual pixels outright, we will carry out a dimensional reduction via a Principle Component Analysis &lt;a href="http://en.wikipedia.org/wiki/Principal_component_analysis"&gt;&lt;span class="caps"&gt;PCA&lt;/span&gt;&lt;/a&gt;. &lt;span class="caps"&gt;PCA&lt;/span&gt; works by attempting to represent the variance in the training data with as few dimensions as possible. So instead of dropping features, as we did in our &lt;a href="http://efavdb.github.io/machine-learning-with-wearable-sensors"&gt;wearable sensor example&lt;/a&gt; analysis, here we will compress features together, and then use only the most important feature combinations. When this is done to images, the features returned by &lt;span class="caps"&gt;PCA&lt;/span&gt; are commonly called eigenfaces (some examples are given&amp;nbsp;below).&lt;/p&gt;
&lt;p&gt;The function we are going to use to carry out our &lt;span class="caps"&gt;PCA&lt;/span&gt; is &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.RandomizedPCA.html"&gt;RandomizedPCA&lt;/a&gt;. We&amp;#8217;ll keep the top 150 eigenfaces, and we&amp;#8217;ll also whiten the data &amp;#8212; ie normalize our new, principal component feature set. The goal of whitening is to make the input less redundant. Whitening is performed by rotating into the coordinate space of the principal components, dividing each dimension by square root of variance in that direction (giving the feature unit variance), and then rotating back to pixel&amp;nbsp;space.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;split&lt;/span&gt; &lt;span class="k"&gt;into&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;testing&lt;/span&gt; &lt;span class="k"&gt;set&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Compute&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;PCA&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eigenfaces&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;face&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;
&lt;span class="n"&gt;n_components&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;150&lt;/span&gt;

&lt;span class="n"&gt;pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandomizedPCA&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;whiten&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;eigenfaces&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;components_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;X_train_pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;&lt;strong&gt;Visualizing the&amp;nbsp;eigenfaces&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Let&amp;#8217;s now take a moment to examine the dataset&amp;#8217;s principal eigenfaces: the set of images that we will project each example onto to obtain independent features. To do that we will use the following helper function to make life easier &amp;#8212; visual&amp;nbsp;follows.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;helper&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;to&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;make&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;plots&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;faces&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;plot_gallery&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;titles&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_row&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_col&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.8&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_col&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;2.4&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_row&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots_adjust&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bottom&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;left&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;right&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.99&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;top&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.90&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;hspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.35&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_row&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_col&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_row&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_col&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;titles&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;(())&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yticks&lt;/span&gt;&lt;span class="p"&gt;(())&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Plot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gallery&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;most&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;significative&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;eigenfaces&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;eigenface_titles&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&lt;/span&gt;
&lt;span class="n"&gt;&amp;amp;quot;eigenface %d&amp;amp;quot; % i for i in range(eigenfaces.shape[0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;plot_gallery&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eigenfaces&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;eigenface_titles&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/01/Screen-Shot-2015-01-21-at-12.36.09-PM.png"&gt;&lt;img alt="Screen Shot 2015-01-21 at 12.36.09 PM" src="https://efavdb.com/wp-content/uploads/2015/01/Screen-Shot-2015-01-21-at-12.36.09-PM.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Training a&amp;nbsp;model&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Now that we have reduced the dimensionality of the data it is time to go ahead and train a model. I am going to use the same &lt;span class="caps"&gt;SVM&lt;/span&gt; and GridSearchCV method I explained in my previous &lt;a href="http://efavdb.github.io/machine-learning-with-wearable-sensors"&gt;post&lt;/a&gt;. However, instead of using a linear kernel, as we did last time, I&amp;#8217;ll use instead a &lt;a href="http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html"&gt;radial basis function (&lt;span class="caps"&gt;RBF&lt;/span&gt;)&lt;/a&gt; kernel. The &lt;span class="caps"&gt;RBF&lt;/span&gt; kernel is a good choice here since we&amp;#8217;d like to have non-linear decision boundaries &amp;#8212; in general, it&amp;#8217;s a reasonable idea to try this out whenever the number of training examples outnumbers the number of features characterizing those examples. The parameter C here acts as a regularization term: Small C values give you smooth decision boundaries, while large C values give complicated boundaries that attempt to fit/accommodate all training data. The gamma parameter defines how far the influence of a single point example extends (the width of the &lt;span class="caps"&gt;RBF&lt;/span&gt;&amp;nbsp;kernel).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="nn"&gt;Train&lt;/span&gt; &lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="nt"&gt;SVM&lt;/span&gt; &lt;span class="nt"&gt;classification&lt;/span&gt; &lt;span class="nt"&gt;model&lt;/span&gt;

&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;Fitting&lt;/span&gt; &lt;span class="nt"&gt;the&lt;/span&gt; &lt;span class="nt"&gt;classifier&lt;/span&gt; &lt;span class="nt"&gt;to&lt;/span&gt; &lt;span class="nt"&gt;the&lt;/span&gt; &lt;span class="nt"&gt;training&lt;/span&gt; &lt;span class="nt"&gt;set&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;)&lt;/span&gt;
&lt;span class="nt"&gt;t0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;time&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;
&lt;span class="nt"&gt;param_grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;C&amp;#39;:&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="nx"&gt;e3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="nx"&gt;e3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="nx"&gt;e4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="nx"&gt;e4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="nx"&gt;e5&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="err"&gt;,&lt;/span&gt;
&lt;span class="err"&gt;&amp;#39;gamma&amp;#39;:&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0005&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.005&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="err"&gt;,&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nt"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;GridSearchCV&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;
&lt;span class="nt"&gt;SVC&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rbf&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;class_weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;auto&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="nt"&gt;param_grid&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;fit&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;X_train_pca&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;y_train&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;done&lt;/span&gt; &lt;span class="nt"&gt;in&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;3fs&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;time&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="nt"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;t0&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="nt"&gt;Best&lt;/span&gt; &lt;span class="nt"&gt;estimator&lt;/span&gt; &lt;span class="nt"&gt;found&lt;/span&gt; &lt;span class="nt"&gt;by&lt;/span&gt; &lt;span class="nt"&gt;grid&lt;/span&gt; &lt;span class="nt"&gt;search&lt;/span&gt;&lt;span class="o"&gt;:&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;quot&lt;/span&gt;&lt;span class="o"&gt;;)&lt;/span&gt;
&lt;span class="nt"&gt;print&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;best_estimator_&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&amp;amp;&lt;/span&gt;&lt;span class="nt"&gt;gt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="nt"&gt;Fitting&lt;/span&gt; &lt;span class="nt"&gt;the&lt;/span&gt; &lt;span class="nt"&gt;classifier&lt;/span&gt; &lt;span class="nt"&gt;to&lt;/span&gt; &lt;span class="nt"&gt;the&lt;/span&gt; &lt;span class="nt"&gt;training&lt;/span&gt; &lt;span class="nt"&gt;set&lt;/span&gt;
&lt;span class="nt"&gt;done&lt;/span&gt; &lt;span class="nt"&gt;in&lt;/span&gt; &lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;056s&lt;/span&gt;
&lt;span class="nt"&gt;Best&lt;/span&gt; &lt;span class="nt"&gt;estimator&lt;/span&gt; &lt;span class="nt"&gt;found&lt;/span&gt; &lt;span class="nt"&gt;by&lt;/span&gt; &lt;span class="nt"&gt;grid&lt;/span&gt; &lt;span class="nt"&gt;search&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="nt"&gt;SVC&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;cache_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;200&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;class_weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;auto&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;coef0&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;degree&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;3&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;gamma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;001&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rbf&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;max_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;-1&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;probability&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;False&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;None&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;shrinking&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;True&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;tol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;001&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;False&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;&lt;strong&gt;Model&amp;nbsp;validation&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;That&amp;#8217;s it for training! Next we&amp;#8217;ll validate our model on the testing data set. Below, we first use our &lt;span class="caps"&gt;PCA&lt;/span&gt; model to transform the testing data into our current feature space. Then, we apply our model to make predictions on this set. To get a feel for how well the model is doing, we print a &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"&gt;classification_report&lt;/a&gt; and a &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"&gt;confusion matrix&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Quantitative&lt;/span&gt; &lt;span class="n"&gt;evaluation&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="n"&gt;quality&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="k"&gt;set&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Validate&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="k"&gt;data&lt;/span&gt;
&lt;span class="n"&gt;X_test_pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test_pca&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classification_report&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Confusion Matrix&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Make&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="k"&gt;data&lt;/span&gt; &lt;span class="n"&gt;frame&lt;/span&gt; &lt;span class="n"&gt;so&lt;/span&gt; &lt;span class="n"&gt;we&lt;/span&gt; &lt;span class="n"&gt;can&lt;/span&gt; &lt;span class="n"&gt;have&lt;/span&gt; &lt;span class="k"&gt;some&lt;/span&gt; &lt;span class="n"&gt;nice&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;
&lt;span class="n"&gt;cm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_classes&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;precision&lt;/span&gt; &lt;span class="n"&gt;recall&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt; &lt;span class="n"&gt;support&lt;/span&gt;

&lt;span class="n"&gt;Ariel&lt;/span&gt; &lt;span class="n"&gt;Sharon&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;81&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;85&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;83&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;span class="n"&gt;Colin&lt;/span&gt; &lt;span class="n"&gt;Powell&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;82&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;78&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="mi"&gt;54&lt;/span&gt;
&lt;span class="n"&gt;Donald&lt;/span&gt; &lt;span class="n"&gt;Rumsfeld&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;78&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;67&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;72&lt;/span&gt; &lt;span class="mi"&gt;27&lt;/span&gt;
&lt;span class="n"&gt;George&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="n"&gt;Bush&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;87&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;95&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;91&lt;/span&gt; &lt;span class="mi"&gt;139&lt;/span&gt;
&lt;span class="n"&gt;Gerhard&lt;/span&gt; &lt;span class="n"&gt;Schroeder&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;86&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;73&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt; &lt;span class="mi"&gt;26&lt;/span&gt;
&lt;span class="n"&gt;Hugo&lt;/span&gt; &lt;span class="n"&gt;Chavez&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;00&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;75&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;86&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;span class="n"&gt;Tony&lt;/span&gt; &lt;span class="n"&gt;Blair&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;84&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;89&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;86&lt;/span&gt; &lt;span class="mi"&gt;36&lt;/span&gt;

&lt;span class="k"&gt;avg&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;85&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;85&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;85&lt;/span&gt; &lt;span class="mi"&gt;322&lt;/span&gt;

&lt;span class="n"&gt;Confusion&lt;/span&gt; &lt;span class="n"&gt;Matrix&lt;/span&gt;
&lt;span class="n"&gt;Ariel&lt;/span&gt; &lt;span class="n"&gt;Sharon&lt;/span&gt; &lt;span class="n"&gt;Colin&lt;/span&gt; &lt;span class="n"&gt;Powell&lt;/span&gt; &lt;span class="n"&gt;Donald&lt;/span&gt; &lt;span class="n"&gt;Rumsfeld&lt;/span&gt; &lt;span class="n"&gt;George&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="n"&gt;Bush&lt;/span&gt; &lt;span class="err"&gt;\&lt;/span&gt;
&lt;span class="n"&gt;Ariel&lt;/span&gt; &lt;span class="n"&gt;Sharon&lt;/span&gt; &lt;span class="mi"&gt;17&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;Colin&lt;/span&gt; &lt;span class="n"&gt;Powell&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;
&lt;span class="n"&gt;Donald&lt;/span&gt; &lt;span class="n"&gt;Rumsfeld&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;18&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;George&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="n"&gt;Bush&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;132&lt;/span&gt;
&lt;span class="n"&gt;Gerhard&lt;/span&gt; &lt;span class="n"&gt;Schroeder&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;Hugo&lt;/span&gt; &lt;span class="n"&gt;Chavez&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="n"&gt;Tony&lt;/span&gt; &lt;span class="n"&gt;Blair&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

&lt;span class="n"&gt;Gerhard&lt;/span&gt; &lt;span class="n"&gt;Schroeder&lt;/span&gt; &lt;span class="n"&gt;Hugo&lt;/span&gt; &lt;span class="n"&gt;Chavez&lt;/span&gt; &lt;span class="n"&gt;Tony&lt;/span&gt; &lt;span class="n"&gt;Blair&lt;/span&gt;
&lt;span class="n"&gt;Ariel&lt;/span&gt; &lt;span class="n"&gt;Sharon&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;Colin&lt;/span&gt; &lt;span class="n"&gt;Powell&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;Donald&lt;/span&gt; &lt;span class="n"&gt;Rumsfeld&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;George&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="n"&gt;Bush&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;Gerhard&lt;/span&gt; &lt;span class="n"&gt;Schroeder&lt;/span&gt; &lt;span class="mi"&gt;19&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;Hugo&lt;/span&gt; &lt;span class="n"&gt;Chavez&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;Tony&lt;/span&gt; &lt;span class="n"&gt;Blair&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As a quick reminder, lets define what the terms above&amp;nbsp;are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;precision is the ratio Tp / (Tp + Fp) where Tp is the number of true positives and Fp the number of false&amp;nbsp;positives.&lt;/li&gt;
&lt;li&gt;recall is the ration of Tp / (Tp + Fn) where Fn is the number of false&amp;nbsp;negatives.&lt;/li&gt;
&lt;li&gt;f1-score is (precision * recall) / (precision +&amp;nbsp;recall)&lt;/li&gt;
&lt;li&gt;support is the total number of occurrences of each&amp;nbsp;face.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In our second table here, we have printed a confusion matrix, which provides a nice summary visualization of our results: Each row is the actual class, and the columns are the predicted class. For example, in row 1 there are 17 correct identifications of Arial Sharon, and 5 wrong ones. Using our previously defined helper plotting function, we show some examples of predicted vs true names below. Our simple algorithm&amp;#8217;s accuracy is imperfect, yet&amp;nbsp;satisfying!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;#Plot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;portion&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;set&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;pred_name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;y_pred[i&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;]&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rsplit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;-1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;true_name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;y_test[i&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;]&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rsplit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;-1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;predicted: %s\ntrue: %s&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;true_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;prediction_titles&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;title(y_pred, y_test, target_names, i)&lt;/span&gt;
&lt;span class="n"&gt;for i in range(y_pred.shape[0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;plot_gallery&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;prediction_titles&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2015/01/download.png"&gt;&lt;img alt="download" src="https://efavdb.com/wp-content/uploads/2015/01/download.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;85% average accuracy shows that &lt;span class="caps"&gt;PCA&lt;/span&gt; (Eigenface analysis) can provide accurate face recognition results, given just a modest amount of training data. There are pros and cons to eigenfaces&amp;nbsp;however:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pros&lt;ol&gt;
&lt;li&gt;Training can be&amp;nbsp;automated.&lt;/li&gt;
&lt;li&gt;Once the the eigenfaces are calculated, face recognition can be performed in real&amp;nbsp;time.&lt;/li&gt;
&lt;li&gt;Eigenfaces can handle large&amp;nbsp;databases.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Cons&lt;ol&gt;
&lt;li&gt;Sensitive to lighting&amp;nbsp;conditions.&lt;/li&gt;
&lt;li&gt;Expression changes are not handled&amp;nbsp;well.&lt;/li&gt;
&lt;li&gt;Has trouble when the face angle&amp;nbsp;changes.&lt;/li&gt;
&lt;li&gt;Difficult to interpret eigenfaces: Eg, one can&amp;#8217;t easily read off from these eye separation distance,&amp;nbsp;etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are more advanced facial recognition methods that take advantage of features special to faces. One example is provided by the &lt;a href="http://en.wikipedia.org/wiki/Active_appearance_model"&gt;Active Appearance Model (&lt;span class="caps"&gt;AAM&lt;/span&gt;)&lt;/a&gt;, which finds facial features (nose, mouth, etc.), and then identifies relationships between these to carry out identifications. Whatever the approach, the overall methodology is the same for all facial recognition&amp;nbsp;algorithms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Take a labeled set of&amp;nbsp;faces.&lt;/li&gt;
&lt;li&gt;Extract features from those faces using some method of choice (eg&amp;nbsp;eigenfaces).&lt;/li&gt;
&lt;li&gt;Train a machine learning model on those&amp;nbsp;features.&lt;/li&gt;
&lt;li&gt;Extract features from a new face, and predict the&amp;nbsp;identity.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The story doesn&amp;#8217;t end with finding faces in photos. Facial recognition is just a subset of machine vision, which is currently being applied widely in industry. For example, Intel and other semiconductor manufactures use machine vision to detect defects in the chips being produced &amp;#8212; one application where by-hand (human) analysis is not possible and computers have the upper&amp;nbsp;hand.&lt;/p&gt;</content><category term="Case studies"></category><category term="machine learning"></category></entry><entry><title>Machine learning with wearable sensors</title><link href="https://efavdb.com/machine-learning-with-wearable-sensors" rel="alternate"></link><published>2015-01-09T12:34:00-08:00</published><updated>2015-01-09T12:34:00-08:00</updated><author><name>Damien RJ</name></author><id>tag:efavdb.com,2015-01-09:/machine-learning-with-wearable-sensors</id><summary type="html">&lt;p&gt;A guest post, contributed by Damien Ramunno-Johnson (&lt;a href="https://www.linkedin.com/profile/view?id=60223336&amp;amp;authType=NAME_SEARCH&amp;amp;authToken=LOV_&amp;amp;locale=en_US&amp;amp;trk=tyah2&amp;amp;trkInfo=tarId%3A1420748440448%2Ctas%3Adamien%2Cidx%3A1-1-1"&gt;LinkedIn&lt;/a&gt;, &lt;a href="http://www.efavdb.com/about"&gt;bio-sketch&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;br&gt;
Wearable sensors have become increasingly popular over the last few years with the success of smartphones, fitness trackers, and smart watches. All of these devices create a large amount of data that is ideal for machine learning. Two early examples …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A guest post, contributed by Damien Ramunno-Johnson (&lt;a href="https://www.linkedin.com/profile/view?id=60223336&amp;amp;authType=NAME_SEARCH&amp;amp;authToken=LOV_&amp;amp;locale=en_US&amp;amp;trk=tyah2&amp;amp;trkInfo=tarId%3A1420748440448%2Ctas%3Adamien%2Cidx%3A1-1-1"&gt;LinkedIn&lt;/a&gt;, &lt;a href="http://www.efavdb.com/about"&gt;bio-sketch&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;br&gt;
Wearable sensors have become increasingly popular over the last few years with the success of smartphones, fitness trackers, and smart watches. All of these devices create a large amount of data that is ideal for machine learning. Two early examples are the FitBit and Jawbone&amp;#8217;s up band, both of which analyze sensor input to determine how many steps the user has taken, a metric which is helpful for measuring physical activity. There is no reason to stop there: With all of this data available it is also possible to extract more information. For example, fitness trackers coming out now can also analyze your&amp;nbsp;sleep.&lt;/p&gt;
&lt;p&gt;In that spirit, I&amp;#8217;m going to show here that it is pretty straightforward to make an algorithm that can differentiate between 6 different&amp;nbsp;states.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Walking&lt;/li&gt;
&lt;li&gt;Walking&amp;nbsp;Upstairs&lt;/li&gt;
&lt;li&gt;Walking&amp;nbsp;Downstairs&lt;/li&gt;
&lt;li&gt;Sitting&lt;/li&gt;
&lt;li&gt;Standing&lt;/li&gt;
&lt;li&gt;Laying&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To do this I am going to use &lt;a href="https://www.python.org/"&gt;Python&lt;/a&gt;, &lt;a href="http://scikit-learn.org/"&gt;Sklearn&lt;/a&gt; and &lt;a href="https://plot.ly"&gt;Plot.ly&lt;/a&gt;. Plot.ly is a wonderful plotting package that makes interactive graphs you can share. The first step is to import all of the relevant&amp;nbsp;packages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Load packages and source data&lt;/strong&gt;&lt;br&gt;
For this example, I used one of the datasets available from the &lt;a href="https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones"&gt;&lt;span class="caps"&gt;UCI&lt;/span&gt; Machine Learning Repository&lt;/a&gt;. For this data set 30 subjects were recorded performing activities of daily living (&lt;span class="caps"&gt;ADL&lt;/span&gt;) while carrying a waist-mounted smartphone (Samsung Galaxy &lt;span class="caps"&gt;II&lt;/span&gt;) with embedded inertial sensors. A testing dataset and training dataset are provided. The dataset has 561 features which were created from the sensor data: &lt;span class="caps"&gt;XYZ&lt;/span&gt; acceleration,&amp;nbsp;etc.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="kp"&gt;loadtxt&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;svm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;grid_search&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f1_score&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SelectPercentile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f_classif&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;plotly.plotly&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;py&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;plotly.graph_objs&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that we have loaded all of our packages, it is time to import the data into memory. This data set is not large enough to cause any memory issues, so go ahead and load the whole&amp;nbsp;thing.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;data_test = loadtxt(&amp;quot;./Wearable/UCI_HAR_Dataset/test/X_test.txt&amp;quot;)  &lt;/span&gt;
&lt;span class="err"&gt;label_test=loadtxt(&amp;quot;./Wearable/UCI_HAR_Dataset/test/y_test.txt&amp;quot;)  &lt;/span&gt;
&lt;span class="err"&gt;data_train = loadtxt(&amp;quot;./Wearable/UCI_HAR_Dataset/train/X_train.txt&amp;quot;)  &lt;/span&gt;
&lt;span class="err"&gt;label_train = loadtxt(&amp;quot;./Wearable/UCI_HAR_Dataset/train/y_train.txt&amp;quot;)  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Feature selection&lt;/strong&gt;&lt;br&gt;
Given that this data set has training and testing data with labels, it makes sense to do supervised machine learning. We have over 500 potential features to use, which is a lot. Let&amp;#8217;s see if we can get by with fewer features. To do that, we will use &lt;span class="caps"&gt;SK&lt;/span&gt;-learn&amp;#8217;€™s SelectKBest to keep the top 20 percent of the features, and then transform the&amp;nbsp;data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;selector = SelectPercentile(f_classif, 20)  &lt;/span&gt;
&lt;span class="err"&gt;selector.fit(data_train, label_train)  &lt;/span&gt;
&lt;span class="err"&gt;data_train_transformed = selector.transform(data_train)  &lt;/span&gt;
&lt;span class="err"&gt;data_test_transformed = selector.transform(data_test)  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Machine learning&lt;/strong&gt;&lt;br&gt;
At this point you need to decide which algorithm you want to use. I tried a few of them and got the best results using a &lt;a href="http://scikit-learn.org/stable/modules/svm.html"&gt;Support Vector Machine&lt;/a&gt; (&lt;span class="caps"&gt;SVM&lt;/span&gt;). SVMs attempt to determine the decision boundary between two classes that is as far away from the data of both classes as possible. In general they work pretty&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s try some parameters and see how good our results&amp;nbsp;are.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;svm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;rbf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_train_transformed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_test_transformed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Accuracy is %.4f and the f1-score is %.4f &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;  
&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;f1_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;&amp;gt;Accuracy is 0.8812 and the f1-score is 0.8788  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Optimization&lt;/strong&gt;&lt;br&gt;
That&amp;#8217;s not too bad, but I think we can still optimize our results some more. We could change the parameters manually, or we can automate the task using a &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html"&gt;grid search&lt;/a&gt;. This is a handy module that allows you to do a parameter sweep. Below, I set up a sweep using two different kernels and various penalty term values (C) to see if we can raise our&amp;nbsp;accuracy.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;parameters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;kernel&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;linear&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;rbf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;  
&lt;span class="n"&gt;svr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;svm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;grid_search&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;svr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_train_transformed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_test_transformed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Accuracy is %.4f and the f1-score is %.4f &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;  
&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;f1_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;&amp;gt;Accuracy is 0.9430 and the f1-score is 0.9430  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Visualization&lt;/strong&gt;&lt;br&gt;
Looks like we are getting pretty good accuracy for using only 20% of the features available to us. You may have also noticed that I am outputting the &lt;a href="http://en.wikipedia.org/wiki/F1_score"&gt;F1-Score&lt;/a&gt; which is another measure of the accuracy which takes into account the precision and the&amp;nbsp;recall.&lt;/p&gt;
&lt;p&gt;Now let&amp;#8217;s plot some of these data points to see if we can visualize why this is all working. Here, I am using Plot.ly to make the plot. You can make the plots many different ways including converting matplotlib plots into these online plots. If you click on the &amp;#8220;play with this data&amp;#8221; link at the bottom of the figure (or click &lt;a href="https://plot.ly/~Damien RJ/104"&gt;here&lt;/a&gt;) you can see the code used to make the&amp;nbsp;plot.&lt;/p&gt;
&lt;p&gt;[iframe src=&amp;#8221;https://plot.ly/~Damien &lt;span class="caps"&gt;RJ&lt;/span&gt;/104&amp;#8221; width=&amp;#8221;100%&amp;#8221;&amp;nbsp;height=&amp;#8221;680&amp;#8221;]&lt;/p&gt;
&lt;p&gt;I picked two of the features to plot, the z acceleration average, and the z acceleration standard deviation. Note, the gravity component of the acceleration was removed and placed into its own feature. Only 3/6 labels are being plotted to make it a little easier to see what is going on. For example, it is easy to see that the walking profile in the top graph differs significantly from those of standing and laying in the bottom&amp;nbsp;two.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;br&gt;
From the graphs above alone, it would be difficult to differentiate between laying and standing. We might be able to comb through different combinations of features to find a set that is more easily distinguishable, but we are limited by the simple fact that it is hard to visualize data in more than 3 dimensions. If it turns out that more than a handful of features need to be considered simultaneously to separate the different classes, this approach will fail. In contrast, we have seen in our &lt;span class="caps"&gt;SVM&lt;/span&gt; analysis above that it is actually pretty easy to use machine learning to pick out, with high accuracy, a variety of motions from the sensor data. This is a neat application that is currently being applied widely in industry. It illustrates why machine learning is so interesting in general: It allows us to automate data analysis, and apply it to problems where a by-hand, visual analysis is not&amp;nbsp;possible.&lt;/p&gt;</content><category term="Case studies"></category><category term="machine learning"></category></entry><entry><title>Historic daily traffic patterns and the time scale of deviations</title><link href="https://efavdb.com/historic-daily-traffic-patterns-and-the-time-scale" rel="alternate"></link><published>2014-11-11T05:25:00-08:00</published><updated>2014-11-11T05:25:00-08:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-11-11:/historic-daily-traffic-patterns-and-the-time-scale</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/11/WeekdayModeAverageDynamics.png"&gt;&lt;img alt="WeekdayModeAverageDynamics" src="https://efavdb.com/wp-content/uploads/2014/11/WeekdayModeAverageDynamics.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Daily traffic patterns can be decomposed into a historic average plus fluctuations from this&amp;nbsp;average.&lt;/p&gt;
&lt;p&gt;Here, we examine the daily dynamics of traffic as a function of weekday to provide the first piece of this puzzle. To do this, we average the time-dependent scores &lt;span class="math"&gt;\(c_i(t)\)&lt;/span&gt; for each day of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/11/WeekdayModeAverageDynamics.png"&gt;&lt;img alt="WeekdayModeAverageDynamics" src="https://efavdb.com/wp-content/uploads/2014/11/WeekdayModeAverageDynamics.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Daily traffic patterns can be decomposed into a historic average plus fluctuations from this&amp;nbsp;average.&lt;/p&gt;
&lt;p&gt;Here, we examine the daily dynamics of traffic as a function of weekday to provide the first piece of this puzzle. To do this, we average the time-dependent scores &lt;span class="math"&gt;\(c_i(t)\)&lt;/span&gt; for each day of the week (see plot to the&amp;nbsp;right).&lt;/p&gt;
&lt;p&gt;As discussed &lt;a href="http://efavdb.github.io/daily-traffic-evolution-and-the-super-bowl"&gt;previously&lt;/a&gt;, modes one and two are general indicators of  overall traffic density and directional commuter density, respectively. Interestingly, we can clearly see systematic deviations in these two mode amplitudes across the days of the week: During rush hour, Mondays and Fridays have generally lower levels of traffic by both measures - most likely a consequence of people taking three-day weekends. In addition, if you look closely, you can actually see evidence of slackers taking off early on Friday&amp;nbsp;afternoons.&lt;/p&gt;
&lt;p&gt;Examining higher modes, the average signal dies away, eventually being lost in the noise. The final two panels in the figure have different y-axis scales - zoomed in to show the signal-to-noise ratio. By mode 100 the signal is already quite weak, but systematic deviations from zero can still be seen above the noise. However, by mode 1000, there are no systematic or significant deviations from zero.  These minor principal components likely represent rare events such as traffic accidents and thus are not reflected at all in the daily&amp;nbsp;averages.&lt;/p&gt;
&lt;p&gt;Is the historic average the best we can do for prediction?  To answer this we must examine the predictability of the fluctuations away from these means.  Here, we examine the autocorrelation&lt;span class="math"&gt;\(^1\)&lt;/span&gt; of the fluctuations from the mean to find the memory time scale of each principal component’s fluctuation memory.  This quantity characterizes the time scale over which we can extrapolate each amplitude deviation into the future (see plot&lt;span class="math"&gt;\(^2\)&lt;/span&gt; below). The time scale of initial decay of the correlation decreases monotonically with the mode index - the modes that capture the most variance have the longest memory: several&amp;nbsp;hours.&lt;/p&gt;
&lt;p&gt;This is excellent news: we can do better than just using the historic traffic patterns.  We can, in fact, project fluctuations away from the historic average several hours into the future and expect some&amp;nbsp;improvement.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/11/TuesAutocorr.png"&gt;&lt;img alt="TuesAutocorr" src="https://efavdb.com/wp-content/uploads/2014/11/TuesAutocorr.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[1] &lt;em&gt;The autocorrellation&lt;/em&gt;: The autocorrelation of a stochastic signal is a measure of its memory. In this particular case, \(R_{i}(t) \propto \mathbb{E}[\Delta c_i(s) \cdot \Delta c_i(s+t)]\) where the expectation value is over &lt;span class="math"&gt;\(s\)&lt;/span&gt; and &lt;span class="math"&gt;\(\Delta c_i(t) = c_i(t) - \langle c_i(t) \rangle\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;[2] &lt;em&gt;On this plot:&lt;/em&gt; The plot shown is for Tuesdays — other days have exhibit similar characteristics.  Note that mode 2 takes on negative autocorrelation values for a period of time (\(t =\) 6-9 hours). This is not surprising since mode 2, being an “odd” mode, tends to reverse sign between rush hours. The inset shows a few mean-subtracted signals for the first mode, ($\Delta c_1(t) $ above). The long-time correlations of these fluctuations are apparent&amp;nbsp;here.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category><category term="traffic"></category></entry><entry><title>Daily traffic evolution and the Super Bowl</title><link href="https://efavdb.com/daily-traffic-evolution-and-the-super-bowl" rel="alternate"></link><published>2014-10-31T04:02:00-07:00</published><updated>2014-10-31T04:02:00-07:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-10-31:/daily-traffic-evolution-and-the-super-bowl</id><summary type="html">&lt;p&gt;With an eye towards predicting traffic evolution, we begin by examining the time-dependence of the contribution from the first principal components on different days of the week. Traffic throughout the day \(\vert x(t) \rangle\) can be represented in the basis of principal components; \(\vert x(t) \rangle = \sum_{i …&lt;/p&gt;</summary><content type="html">&lt;p&gt;With an eye towards predicting traffic evolution, we begin by examining the time-dependence of the contribution from the first principal components on different days of the week. Traffic throughout the day \(\vert x(t) \rangle\) can be represented in the basis of principal components; \(\vert x(t) \rangle = \sum_{i} c_i(t) \vert \phi_i \rangle \)&lt;span class="math"&gt;\(^1\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\vert \phi_i \rangle\)&lt;/span&gt; is the ith principle component. The coefficients &lt;span class="math"&gt;\(c_i(t)\)&lt;/span&gt;, sometimes called the “scores” of \(\vert x(t) \rangle\) in the basis of principal components, carry all of the&amp;nbsp;dynamics.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/ModeTDependence.png"&gt;&lt;img alt="ModeTDependence" src="https://efavdb.com/wp-content/uploads/2014/10/ModeTDependence.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The largest deviations in the traffic patterns (and of the scores) are during weekday rush hours (around 8 am and 5 pm) - see plot of the scores for several modes throughout Jan. 15.  There is an abundance of interesting information to be gleaned here.  First, note that, generally, the amplitude of the lowest modes is the largest, again, &lt;a href="https://efavdb.github.io/data-reduction-by-pca"&gt;as expected&lt;/a&gt;.  In addition, there appears to be some large wavelength structure (primarily correlated with the rush hours) sitting on top of a background of higher frequency noise.  Ignoring for the moment the noise, the modes generally come in two classes - something like “even” and “odd”.  That is, some fluctuate with the same sign in both the morning and evening, while others fluctuate with opposite&amp;nbsp;signs.&lt;/p&gt;
&lt;p&gt;Let us consider the first two modes in this plot.  The first mode is an even mode and we attribute this essentially to an overall shift in general speed throughout the system. Whenever there are more cars on the road, the speed generally decreases everywhere.  Indeed, this is the only mode that deviates significantly from zero at night and it deviates positive - the roads are generally completely clear at night and thus the speed is somewhat higher than average.  The second mode is an odd mode, which we attribute generally to directional traffic - when everyone is going to work it has one sign, when they are coming home it has the other.  We have previously &lt;a href="http://efavdb.github.io/traffic-patterns-of-the-year-2014-edition"&gt;visualized these two modes&lt;/a&gt; and, indeed, these concepts are reflected in their spatial structure: the first mode is generally uniform while the second has regions with either sign and many sections of highway are positive in one direction and negative in the&amp;nbsp;other.&lt;/p&gt;
&lt;p&gt;The time-dependence of these scores is remarkably consistent from week to week (See Wednesdays plot below of the scores for modes 1 and 2).  On the right, we plot the same quantities for several Sundays as well.  Not surprisingly, the fluctuations are smaller on Sundays than on weekdays, reflecting more homogeneous speeds in sparse traffic.  However, they are still reproducible from week to week - see Sundays Jan. 5, 12, 19, and 26 - apparently there is a slight slow-down around 6 pm.  Feb. 2 was Super Bowl Sunday and the traffic pattern differs qualitatively from other Sundays.  Remarkably, we can identify the time of the Super Bowl kickoff from this data - before the kickoff there is slightly more traffic than the average Sunday and immediately after,&amp;nbsp;less.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/WednesdayModes.png"&gt;&lt;img alt="WednesdayModes" src="https://efavdb.com/wp-content/uploads/2014/10/WednesdayModes.png"&gt;&lt;/a&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/SuperBowlModes.png"&gt;&lt;img alt="SuperBowlModes" src="https://efavdb.com/wp-content/uploads/2014/10/SuperBowlModes.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[1] &lt;em&gt;On projecting into principal components&lt;/em&gt;: In the original basis, our data reads \(\vert x(t) \rangle = \sum_j a_j(t) \vert \ell_j \rangle\), where the sum runs over all loop locations(there are some 2,000 loops in the Bay area) and \(\vert \ell_j \rangle\) is a unit fluctuation in speed at the location of loop &lt;span class="math"&gt;\(j\)&lt;/span&gt;. Changing bases, to the principal components \(\vert \phi_i \rangle\), \(\vert x(t) \rangle = \sum_{i,j} a_j(t) \vert \phi_i \rangle \langle \phi_i \vert \ell_j \rangle\) \(= \sum_i c_i(t) \vert \phi_i \rangle\) where \(c_i(t) = \sum_j a_j(t) \langle \phi_i \vert \ell_j \rangle\). The coefficient \(\langle \phi_i \vert \ell_j \rangle\) is often called the “loading” of \(\vert \ell_j \rangle\) into \(\vert \phi_i&amp;nbsp;\rangle\).&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category><category term="traffic"></category></entry><entry><title>Traffic patterns of the year: 2014 edition</title><link href="https://efavdb.com/traffic-patterns-of-the-year-2014-edition" rel="alternate"></link><published>2014-10-21T23:47:00-07:00</published><updated>2014-10-21T23:47:00-07:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-10-21:/traffic-patterns-of-the-year-2014-edition</id><summary type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/Mode1.png"&gt;&lt;img alt="Mode1" src="https://efavdb.com/wp-content/uploads/2014/10/Mode1.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/Mode2.png"&gt;&lt;img alt="Mode2" src="https://efavdb.com/wp-content/uploads/2014/10/Mode2.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As we mentioned in the last post, there are currently over 2000 active speed loop detectors within the Bay Area highway system.  The information provided by these loops is often highly redundant because speeds at neighboring sites typically differ little from one another.  This observation suggests that a higher level …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/Mode1.png"&gt;&lt;img alt="Mode1" src="https://efavdb.com/wp-content/uploads/2014/10/Mode1.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/Mode2.png"&gt;&lt;img alt="Mode2" src="https://efavdb.com/wp-content/uploads/2014/10/Mode2.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As we mentioned in the last post, there are currently over 2000 active speed loop detectors within the Bay Area highway system.  The information provided by these loops is often highly redundant because speeds at neighboring sites typically differ little from one another.  This observation suggests that a higher level, “macro” picture of traffic conditions could provide more insight:  Rather than stating the speed at each detector, we might instead offer info like “101S is rather slow right now”.   In fact, we aim to characterize traffic conditions as efficiently as possible.  To move towards this goal, we have carried out a principal component analysis (&lt;span class="caps"&gt;PCA&lt;/span&gt;)&lt;span class="math"&gt;\(^1\)&lt;/span&gt; of the full 2014 (year to date) &lt;span class="caps"&gt;PEMS&lt;/span&gt; data&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;As described in [1] below, &lt;span class="caps"&gt;PCA&lt;/span&gt; provides us with a slick, automated method for identifying the most common “traffic patterns” or “modes” that get excited in our system.  By adding together these patterns — with appropriate time-specific amplitudes — we can reconstruct the site-by-site traffic conditions observed at any particular moment.  Importantly, summing over only the most significant modes will provide us with a system-tailored, minimal-loss method of data compression that will simplify our later prediction analysis. We will discuss this compression benefit further in the next post. Here, we present the two dominant modes of the Bay Area traffic system (see figures above). Notice that the first is fairly uniform, which presumably captures some nearly-site-independent changes in mean speed associated with night vs. daytime driving. In contrast, the second mode captures some interesting structure, showing slowdowns for some highways/directions and speedups for others. Evidently, this structure is the second most highly exhibited pattern in the Bay Area system; We couldn’t have intuited this pattern, but it has been captured automatically via our &lt;span class="caps"&gt;PCA&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;[1]  *Statistical physics of &lt;span class="caps"&gt;PCA&lt;/span&gt;:  *  One way of thinking about &lt;span class="caps"&gt;PCA&lt;/span&gt; as applied here is to imagine that the traffic system is harmonic.  That is, we suppose that the traffic dynamics observed can be characterized by an energy cost function that is quadratic in the speeds of the different loops, measured relative to their average values, &lt;span class="math"&gt;\(E = \frac{\beta^{-1}}{2} \delta \textbf{v}^{T}  \cdot H \cdot \delta \textbf{v}.\)&lt;/span&gt;   Here, &lt;span class="math"&gt;\(\delta v_i = v_i - \langle v_i \rangle $ and $H\)&lt;/span&gt; is a matrix Hamiltonian.  Under some effective, &lt;a href="http://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)#Canonical_partition_function"&gt;thermal driving&lt;/a&gt;, the pair correlation for two sites will be given by &lt;span class="math"&gt;\(\langle \delta v_a  \delta v_b \rangle \equiv&lt;div class="math"&gt;$$ \frac{1}{Z} \int_{{\delta \textbf{v}_i }} e^{- \frac{1}{2} \delta \textbf{v}^{T}  \cdot H \cdot \delta \textbf{v}} \delta v_a  \delta v_b =$$&lt;/div&gt; H^{-1}_{ab}\)&lt;/span&gt;.  It is this pair correlation function that is measured when one carries out a &lt;span class="caps"&gt;PCA&lt;/span&gt; analysis, and the matrix &lt;span class="math"&gt;\(H^{-1}\)&lt;/span&gt; is called the covariance matrix.  Its eigenvectors are the modes of the system — the independent traffic patterns that we discuss above.  The low lying modes are those with a larger &lt;span class="math"&gt;\(H^{-1}\)&lt;/span&gt; eigenvalue.  These have low energy, are consequently often highly excited, and generally dominate the traffic conditions that we&amp;nbsp;observe.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category><category term="Machine Learning"></category><category term="traffic"></category></entry><entry><title>Obtaining and visualizing traffic data</title><link href="https://efavdb.com/obtaining-and-visualizing-traffic-data" rel="alternate"></link><published>2014-10-16T22:07:00-07:00</published><updated>2014-10-16T22:07:00-07:00</updated><author><name>Dustin McIntosh</name></author><id>tag:efavdb.com,2014-10-16:/obtaining-and-visualizing-traffic-data</id><summary type="html">&lt;p&gt;In our first set of posts here, we explore the possibility of using historical traffic data to train a machine learning algorithm capable of predicting near-term highway conditions — say, up to an hour into the future, at any given time.  To try our hand at this, we will be working …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In our first set of posts here, we explore the possibility of using historical traffic data to train a machine learning algorithm capable of predicting near-term highway conditions — say, up to an hour into the future, at any given time.  To try our hand at this, we will be working with publicly available data provided by the &lt;a href="http://pems.dot.ca.gov/"&gt;California Performance Measurement System (&lt;span class="caps"&gt;PEMS&lt;/span&gt;)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2014/10/Jan15_1730.png"&gt;&lt;img alt="Jan15_1730" src="https://efavdb.com/wp-content/uploads/2014/10/Jan15_1730.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The data provided by &lt;span class="caps"&gt;PEMS&lt;/span&gt; takes the form of time-averaged speed measurements for a large set of points throughout the California highway system (&lt;span class="math"&gt;\(\gtrsim 2000\)&lt;/span&gt;  in the Bay Area alone). These speeds are measured using devices called “inductive-loop detectors&lt;span class="math"&gt;\(^1\)&lt;/span&gt;” that are embedded just below the pavement at each site of interest. The same sort of devices are used at traffic lights to detect waiting vehicles: If you have ever noticed what looks like a circular or rectangular cut in the concrete at a traffic light, that’s what that is — &lt;a href="https://www.youtube.com/watch?v=8GAacxGiV4A"&gt;informative youtube video on how bikers might more easily trigger these&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As we dissect the traffic data, we will require a visualization tool. We have developed a tool to do this from scratch in Python&lt;span class="math"&gt;\(^2\)&lt;/span&gt;. This tool plots by color the speed of traffic along each highway. The figure above provides an example (traffic conditions for Jan 15 2014 at 5:30 pm) on top of a silhouette background of the Bay Area (courtesy of &lt;a href="http://www.lesterlee.org/"&gt;Lester Lee&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Stay tuned for updates on this and other related&amp;nbsp;projects!&lt;/p&gt;
&lt;p&gt;[1] &lt;em&gt;Aside on inductive loop detectors&lt;/em&gt;: Inductive detectors are essentially large wire loops (solenoids) that are constantly being driven by an alternating current source. When a large metallic object (car) is above the loop, the voltage needed to drive the current changes (see below).  This effect allows the loops to infer vehicle proximity.  In order to estimate speeds, average vehicle-loop crossing times are combined with predetermined average vehicle lengths.  [To see why the voltage across a loop changes as a car passes, you can play with these equations:  &lt;span class="math"&gt;\( V  = L \partial_t I\)&lt;/span&gt;, &lt;span class="math"&gt;\(\Phi = L I\)&lt;/span&gt;, &lt;span class="math"&gt;\(\nabla \times \textbf{E} = - \partial_t \textbf{B}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\nabla \times \textbf{H} = \textbf{J}_f\)&lt;/span&gt;.  See also &lt;a href="http://en.wikipedia.org/wiki/Electromagnet#Magnetic_core"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[2] &lt;em&gt;Aside on plotting algorithm&lt;/em&gt;: Traffic data on the &lt;span class="caps"&gt;PEMS&lt;/span&gt; website is provided via downloadable text files (1 file per day). Each file provides average traffic speed data, for each five minute window period in a day, for each functioning detector. The detectors themselves are identified with a 6-digit &lt;span class="caps"&gt;ID&lt;/span&gt; number. The latitude and longitude of these detectors, as well as which highway they are on, the direction of the highway, and their absolute mile marker position, are located in separate meta data files. We simply plot a line between each adjacent pair of detectors on a given highway, with color determined by the average speed of the two. North-South / East-West highway counterparts are separated by a small space by shifting their position perpendicular to the local highway tangent vector at each point. Missing data is imputed via the value of the nearest functional&amp;nbsp;detector.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies"></category><category term="programming"></category><category term="traffic"></category></entry></feed>