<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>EFAVDB - Finance, Statistics</title><link href="http/" rel="alternate"></link><link href="http/feeds/finance-statistics.atom.xml" rel="self"></link><id>http/</id><updated>2015-09-13T06:00:00-07:00</updated><subtitle>Everybody's Favorite Data Blog</subtitle><entry><title>Stochastic geometric series</title><link href="http/stochastic-geometric-series.html" rel="alternate"></link><published>2015-09-13T06:00:00-07:00</published><updated>2015-09-13T06:00:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:None,2015-09-13:http/stochastic-geometric-series.html</id><summary type="html">&lt;p&gt;Let &lt;span class="math"&gt;\(a_1, a_2, \ldots\)&lt;/span&gt; be an infinite set of non-negative samples taken from a distribution &lt;span class="math"&gt;\(P_0(a)\)&lt;/span&gt;, and write&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{1} \label{problem}  
S = 1 + a_1 + a_1 a_2 + a_1 a_2 a_3 + \ldots.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Notice that if the &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; were all the same, &lt;span class="math"&gt;\(S\)&lt;/span&gt; would be a regular geometric series, with value …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Let &lt;span class="math"&gt;\(a_1, a_2, \ldots\)&lt;/span&gt; be an infinite set of non-negative samples taken from a distribution &lt;span class="math"&gt;\(P_0(a)\)&lt;/span&gt;, and write&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{1} \label{problem}  
S = 1 + a_1 + a_1 a_2 + a_1 a_2 a_3 + \ldots.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Notice that if the &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; were all the same, &lt;span class="math"&gt;\(S\)&lt;/span&gt; would be a regular geometric series, with value &lt;span class="math"&gt;\(S = \frac{1}{1-a}\)&lt;/span&gt;. How will the introduction of &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; randomness change this sum? Will &lt;span class="math"&gt;\(S\)&lt;/span&gt; necessarily converge? How is &lt;span class="math"&gt;\(S\)&lt;/span&gt; distributed? In this post, we discuss some simple techniques to answer these&amp;nbsp;questions.&lt;/p&gt;
&lt;p&gt;Note: This post covers work done in collaboration with my aged p, S.&amp;nbsp;Landy.&lt;/p&gt;
&lt;h3&gt;Introduction &amp;#8212; a stock dividend&amp;nbsp;problem&lt;/h3&gt;
&lt;p&gt;To motivate the sum (\ref{problem}), consider the problem of evaluating the total output of a stock that pays dividends each year in proportion to its present value &amp;#8212; say &lt;span class="math"&gt;\(x %\)&lt;/span&gt;. The price dynamics of a typical stock can be reasonably modeled as a geometric random walk&lt;span class="math"&gt;\(^1\)&lt;/span&gt;:&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\label{prod} \tag{2}  
price(t) = price(t-1) * a_t,  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where &lt;span class="math"&gt;\(a_t\)&lt;/span&gt; is a random variable, having distribution &lt;span class="math"&gt;\(P_0(a_t)\)&lt;/span&gt;. Assuming this form for our hypothetical stock, its total lifetime dividends output will be&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{3}  
x \times \sum_{t = 0}^{\infty} price(t) = x \times price(0) \left ( 1 + a_1 + a_1 a_2 + a_1 a_2 a_3 + \ldots \right)  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The inner term in parentheses here is precisely (\ref{problem}). More generally, a series of this form will be of interest pretty much whenever geometric series are: Population growth problems, the length of a cylindrical bacterium at a series of time steps&lt;span class="math"&gt;\(^2\)&lt;/span&gt;, etc. Will the nature of these sums change dramatically through the introduction of growth&amp;nbsp;variance?&lt;/p&gt;
&lt;p&gt;To characterize these types of stochastic geometric series, we will start below by considering their moments: This will allow us to determine the average value of (\ref{problem}), it&amp;#8217;s variance etc. This approach will also allow us to determine a condition that is both necessary and sufficient for the sum&amp;#8217;s convergence. Following this, we will introduce an integral equation satisfied by the &lt;span class="math"&gt;\(P(S)\)&lt;/span&gt; distribution. We demonstrate its application by solving the equation for a simple&amp;nbsp;example.&lt;/p&gt;
&lt;h3&gt;The moments of &lt;span class="math"&gt;\(S\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;To solve for the moments of &lt;span class="math"&gt;\(S\)&lt;/span&gt;, we use a trick similar to that used to sum the regular geometric series: We write&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{4} \label{trick}  
S = 1 + a_1 + a_1 a_2 + \ldots \equiv 1 + a_1 T,  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where &lt;span class="math"&gt;\(T = 1 + a_2 + a_2 a_3 + \ldots.\)&lt;/span&gt; Now, because we assume that the &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; are independent, it follows that &lt;span class="math"&gt;\(a_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(T\)&lt;/span&gt; are independent. Further, &lt;span class="math"&gt;\(S\)&lt;/span&gt; and &lt;span class="math"&gt;\(T\)&lt;/span&gt; are clearly distributed identically, since they take the same form. Subtracting &lt;span class="math"&gt;\(1\)&lt;/span&gt; from both sides of the above equation, these observations imply&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{5} \label{moments}  
\overline{(S-1)^k} = \sum_j {k \choose j} (-1)^j \overline{S^{k-j}} = \overline{ a^k S^k} = \overline{a^k} \ \overline{S^k}.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This expression can be used to relate the moments of &lt;span class="math"&gt;\(S\)&lt;/span&gt; to those of &lt;span class="math"&gt;\(a\)&lt;/span&gt; &amp;#8212; a useful result, whenever the distribution of &lt;span class="math"&gt;\(a\)&lt;/span&gt; is known, allowing for the direct evaluation of its&amp;nbsp;moments.&lt;/p&gt;
&lt;p&gt;To illustrate, let us get the first couple of moments of &lt;span class="math"&gt;\(S\)&lt;/span&gt;, using (\ref{moments}). Setting &lt;span class="math"&gt;\(k=1\)&lt;/span&gt; above, we obtain&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{6} \label{mean}  
\overline{S -1} = \overline{a} \overline{S} \ \ \to \ \ \overline{S} = \frac{1}{1 - \overline{a}}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
The right side here looks just like the usual geometric sum result, with &lt;span class="math"&gt;\(a\)&lt;/span&gt; replaced by its average value. Similarly, setting &lt;span class="math"&gt;\(k =2\)&lt;/span&gt; in (\ref{moments}), we can solve for the second moment of &lt;span class="math"&gt;\(S\)&lt;/span&gt;. Subtracting the square of the first gives the following expression for the sum&amp;#8217;s variance,&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{7} \label{var}  
var(S) = \frac{var(a)}{(1 - \overline{a})^2(1 - \overline{a^2})}.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
As one might intuit, the variance of &lt;span class="math"&gt;\(S\)&lt;/span&gt; is proportional to the variance of &lt;span class="math"&gt;\(a\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Expressions (\ref{mean}) and (\ref{var}) are the most practical results of this post: They provide formal general expressions for the mean and variance for a sum of form (\ref{problem}). They can be used to provide a statistical estimate and error bar for a sum of form &lt;span class="math"&gt;\(S\)&lt;/span&gt; in any practical context. It is interesting/nice that the mean takes such a natural looking form &amp;#8212; one that many people likely make use of already, without putting much thought&amp;nbsp;into.&lt;/p&gt;
&lt;p&gt;The expressions above are also of some theoretical interest: Note, for example, that as &lt;span class="math"&gt;\(\overline{a} \to 1\)&lt;/span&gt; from below, the average value of &lt;span class="math"&gt;\(S\)&lt;/span&gt; diverges, and then becomes negative as &lt;span class="math"&gt;\(a\)&lt;/span&gt; goes above this value. This is clearly impossible, as &lt;span class="math"&gt;\(S\)&lt;/span&gt; is a sum of positive terms. This indicates that &lt;span class="math"&gt;\(S\)&lt;/span&gt; has no first moment whenever &lt;span class="math"&gt;\(\overline{a} \geq 1\)&lt;/span&gt;, while (\ref{mean}) holds whenever &lt;span class="math"&gt;\(\overline{a} &amp;lt; 1\)&lt;/span&gt;. Similarly, (\ref{var}) indicates that the second moment of &lt;span class="math"&gt;\(S\)&lt;/span&gt; exists and is finite whenever &lt;span class="math"&gt;\(\overline{a^2} &amp;lt; 1\)&lt;/span&gt;. In fact, this pattern continues for all &lt;span class="math"&gt;\(k\)&lt;/span&gt;: &lt;span class="math"&gt;\(\overline{S^k}\)&lt;/span&gt; exists and is finite if and only if &lt;span class="math"&gt;\(\overline{a^k} &amp;lt; 1\)&lt;/span&gt; &amp;#8212; a result that can be obtained from (\ref{moments}). A rigorous and elementary proof of these statements can be found in an earlier work by Szabados and Szekeley&lt;span class="math"&gt;\(^3\)&lt;/span&gt;. The simple moment equation (\ref{moments}) can also be found&amp;nbsp;there.&lt;/p&gt;
&lt;h3&gt;Condition for the convergence of &lt;span class="math"&gt;\(S\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;A simple condition for the convergence of &lt;span class="math"&gt;\(S\)&lt;/span&gt; can also be obtained using (\ref{moments}). The trick is to consider the limit as &lt;span class="math"&gt;\(k\)&lt;/span&gt; goes to zero of the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th moments. This gives, for example, the average of &lt;span class="math"&gt;\(1\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(P(S)\)&lt;/span&gt;. If this is finite, then the distribution of &lt;span class="math"&gt;\(P\)&lt;/span&gt; is normalizable. Otherwise, &lt;span class="math"&gt;\(S\)&lt;/span&gt; must diverge: Setting &lt;span class="math"&gt;\(k = \epsilon\)&lt;/span&gt; in (\ref{moments}), expanding to first order in &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{8} \label{approximate_log}  
\overline{ \exp [\epsilon \log (S -1) ]} \sim \overline{ 1 + \epsilon \log (S -1) } \sim \overline{ 1 + \epsilon \log S } \ \overline{ 1 + \epsilon \log a}.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Solving for &lt;span class="math"&gt;\(\overline{1}_S\)&lt;/span&gt;, the average of &lt;span class="math"&gt;\(1\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(P(S)\)&lt;/span&gt;, gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{9}  
\overline{1}_S = \frac{\overline{\log( 1 - \frac{1}{S})}}{\log a} + O(\epsilon).  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Like the integer moment expressions above, the right side here is finite up to the point where its denominator diverges. That is, the series will converge, if and only if &lt;span class="math"&gt;\(\overline{\log a} &amp;lt; 0\)&lt;/span&gt;, a very simple condition&lt;span class="math"&gt;\(^4\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Integral equation for the distribution &lt;span class="math"&gt;\(P(S)\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;We have also found that one can sometimes go beyond solving for the moments of &lt;span class="math"&gt;\(S\)&lt;/span&gt;, and instead solve directly for its full distribution: Integrating (\ref{trick}) over &lt;span class="math"&gt;\(a\)&lt;/span&gt; gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{10} \label{int}  
P(S_0) = \int da P_0(a) \int dS P(S) \delta(1+ a S - S_0) = \int \frac{da}{a} P_0(a) P \left (\frac{S_0 -1}{a} \right).  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
This is a general, linear integral equation for &lt;span class="math"&gt;\(P(S)\)&lt;/span&gt;. At least in some cases, it can solved in closed-form. An example&amp;nbsp;follows.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Uniformly distributed &lt;span class="math"&gt;\(a_i\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To demonstrate how one might solve the equation (\ref{int}), we consider here the case where the &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; are uniform on &lt;span class="math"&gt;\([0,1]\)&lt;/span&gt;. In this case, writing &lt;span class="math"&gt;\(a = \frac{S_0 -1}{v}\)&lt;/span&gt;, (\ref{int}) goes to&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{11} \label{int2}  
P(S_0) = \int_{S_0-1}^{\infty} P\left (v\right) \frac{1}{v}dv.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
To progress, we differentiate with respect to &lt;span class="math"&gt;\(S_0\)&lt;/span&gt;, which gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{12} \label{delay}  
P^{\prime} (S_0)\equiv - \frac{1 }{S_0 -1}\times P\left (S_0 -1\right).  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Equation (\ref{delay}) is a &lt;a href="https://en.wikipedia.org/wiki/Delay_differential_equation"&gt;delay differential equation&lt;/a&gt;. It can be solved through iterated integrations: To initiate the process, we note that &lt;span class="math"&gt;\(P(S_0)\)&lt;/span&gt; is equal to zero for all &lt;span class="math"&gt;\(S_0&amp;lt; 1\)&lt;/span&gt;. Plugging this observation into (\ref{delay}) implies that &lt;span class="math"&gt;\(P(S_0) \equiv J\)&lt;/span&gt; &amp;#8212; some constant &amp;#8212; for &lt;span class="math"&gt;\(S \in (1,2)\)&lt;/span&gt;. Continuing in this fashion, repeated integrations of (\ref{delay}) gives&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{13}  
P (S_0) = \begin{cases}  
J, \ \ \ S_0 \in (1,2) \  
J[1 - \log (S_0 -1)], \ \ \ S_0 \in (2,3) \  
J \left [ 1 - \log(S_0 - 1) + Li_2(2-S_0) + \frac{ \log(S_0 - 2)}{\log(S_0 - 1)} - Li_2(-1) \right ], \ \ S_0 \in (3,4) \  
\ldots,  
\end{cases}  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where &lt;span class="math"&gt;\(Li_2\)&lt;/span&gt; is the polylogarithm&amp;nbsp;function.  &lt;/p&gt;
&lt;p&gt;In practice, to find &lt;span class="math"&gt;\(J\)&lt;/span&gt; one can solve (\ref{delay}) numerically, requiring &lt;span class="math"&gt;\(P(S)\)&lt;/span&gt; to be normalized. The figure below compares the result to a simulation estimate, obtained via binning the results of 250,000 random sums of form (\ref{problem}). The two agree&amp;nbsp;nicely.&lt;/p&gt;
&lt;p&gt;&lt;a href="http/wp-content/uploads/2015/08/Screen-Shot-2015-08-16-at-10.34.46-PM.png"&gt;&lt;img alt="Screen Shot 2015-08-16 at 10.34.46 PM" src="http/wp-content/uploads/2015/08/Screen-Shot-2015-08-16-at-10.34.46-PM.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;Consideration of this problem was motivated by a geometric series of type (\ref{problem}) that arose in my work at Square. In this case, I was interested in understanding the bias and variance in the natural estimate (\ref{mean}) to this problem. After some weeks of tinkering with S Landy, I was delighted to find that rigorous, simple results could be obtained to characterize these sums, the simplest being the moment and convergence results above. We now realize that these particular issues have already been well- (and better-)studied, by others&lt;span class="math"&gt;\(^3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As for the integral equation approach, we have not found any other works aimed at solving this problem in general. The method discussed in the example above can be used for any &lt;span class="math"&gt;\(P_0(a)\)&lt;/span&gt; that is uniform over a finite segment. We have also found solutions for a few other cases. Unfortunately, we have so far been unable to obtain a formal, general solution in closed form. However, we note that standard iterative approaches can always be used to estimate the solution to (\ref{int}). Finally, in cases where all moments exist, these can also be used to determine &lt;span class="math"&gt;\(P\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4&gt;References and&amp;nbsp;comments&lt;/h4&gt;
&lt;p&gt;[1] For a discussion on the geometric random walk model for stocks, see &lt;a href="http://people.duke.edu/~rnau/411georw.htm"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[2] Elongated bacteria &amp;#8212; eg., e. coli &amp;#8212; grow longer at an exponential rate &amp;#8212; see my &lt;a href="https://www.sites.google.com/site/Jonathan Landy/%282014%29cellgrowth.pdf?attredirects=0&amp;amp;d=1"&gt;paper on how cell shape affects growth rates&lt;/a&gt;. Due to randomness inherent in the growth rates, bacteria populations will have a length distribution, similar in form to &lt;span class="math"&gt;\(P(S)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;[3] &amp;#8220;An exponential functional of random walks&amp;#8221; by Szabados and Szekeley, Journal of Applied Probability&amp;nbsp;2003.&lt;/p&gt;
&lt;p&gt;[4] Although we have given only a hand-waving argument for this result, the authors of [3] state &amp;#8212; and give a reference for &amp;#8212; the fact that it can be proven using the law of large numbers: By independence of the &lt;span class="math"&gt;\(a_i\)&lt;/span&gt;, the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th term in the series approaches &lt;span class="math"&gt;\((\overline{\log a})^k\)&lt;/span&gt; with probability one, at large &lt;span class="math"&gt;\(k\)&lt;/span&gt;. Simple convergence criteria then give the&amp;nbsp;result.&lt;/p&gt;
&lt;p&gt;[5] The moment equation (\ref{moments}) can also be obtained from the integral equation (\ref{int}), where it arrises from the application of the convolution&amp;nbsp;theorem.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Finance, Statistics"></category></entry></feed>