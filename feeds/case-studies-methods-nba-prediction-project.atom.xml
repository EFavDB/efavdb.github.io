<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>EFAVDB - Case studies, Methods, NBA prediction project</title><link href="http/" rel="alternate"></link><link href="http/feeds/case-studies-methods-nba-prediction-project.atom.xml" rel="self"></link><id>http/</id><updated>2014-12-28T21:55:00-08:00</updated><subtitle>Everybody's Favorite Data Blog</subtitle><entry><title>Quantifying the NBA Christmas week flop: one in ten thousand?</title><link href="http/an-nba-christmas.html" rel="alternate"></link><published>2014-12-28T21:55:00-08:00</published><updated>2014-12-28T21:55:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:None,2014-12-28:http/an-nba-christmas.html</id><summary type="html">&lt;p&gt;There were a number of upsets in the &lt;span class="caps"&gt;NBA&lt;/span&gt; this past Christmas week. Here, we offer no explanation, but do attempt to quantify just how bad those upsets were, taken in aggregate. Short answer: real bad! To argue this point, we review and then apply a very simple predictive model …&lt;/p&gt;</summary><content type="html">&lt;p&gt;There were a number of upsets in the &lt;span class="caps"&gt;NBA&lt;/span&gt; this past Christmas week. Here, we offer no explanation, but do attempt to quantify just how bad those upsets were, taken in aggregate. Short answer: real bad! To argue this point, we review and then apply a very simple predictive model for sporting event outcomes &amp;#8212; python code given in&amp;nbsp;footnotes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quick review of x-mas week&lt;/strong&gt;&lt;br&gt;
The Christmas holiday week&lt;span class="math"&gt;\(^1\)&lt;/span&gt; (Dec. 19 - 25) provided a steady stream of frustrating upsets. The two most perplexing, perhaps, were the Lakers win over the Warriors and the Jazz win over the Grizzlies: two of this year&amp;#8217;s greats losing to two of its most lackluster. In all, &lt;span class="math"&gt;\(24\)&lt;/span&gt; of the &lt;span class="math"&gt;\(49\)&lt;/span&gt; games that week were upsets (with an upset defined here to be one where the winning team started the game with a lower win percentage than the loser). That comes out to an upset ratio just under &lt;span class="math"&gt;\(49%\)&lt;/span&gt;, much higher than the typical rate, about &lt;span class="math"&gt;\(34%\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A general sporting model&lt;/strong&gt;&lt;br&gt;
A &lt;span class="math"&gt;\(49%\)&lt;/span&gt; upset rate sounds significant. However, this metric does not quite capture the emotional magnitude of the debacle. To move towards obtaining such a metric, we first review here a &amp;#8220;standard&amp;#8221;&lt;span class="math"&gt;\(^2\)&lt;/span&gt; sporting model that will allow us to quantify the probability of observing a week as bad as this just past. For each team &lt;span class="math"&gt;\(i\)&lt;/span&gt;, we introduce a variable &lt;span class="math"&gt;\(h_i\)&lt;/span&gt; called its mean scoring potential: Subtracting from this the analogous value for team &lt;span class="math"&gt;\(j\)&lt;/span&gt; gives the expected number of points team &lt;span class="math"&gt;\(i\)&lt;/span&gt; would win by, were it to play team &lt;span class="math"&gt;\(j\)&lt;/span&gt;. More formally, if we let the win-difference for any particular game be &lt;span class="math"&gt;\(y_{ij}\)&lt;/span&gt;, we&amp;nbsp;have &lt;/p&gt;
&lt;div class="math"&gt;$$h_i - h_j \equiv \langle score(i) - score(j) \rangle \equiv \langle y_{ij} \rangle, $$&lt;/div&gt;
&lt;p&gt; where we average over hypothetical outcomes on the right in order to account for the variability characterizing each individual&amp;nbsp;game.&lt;/p&gt;
&lt;p&gt;&lt;a href="http/wp-content/uploads/2014/12/Screen-Shot-2015-01-01-at-3.51.42-AM.png"&gt;&lt;img alt="Screen Shot 2015-01-01 at 3.51.42 AM" src="http/wp-content/uploads/2014/12/Screen-Shot-2015-01-01-at-3.51.42-AM.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;By taking into account the games that have already occurred this season, one can estimate the set of &lt;span class="math"&gt;\(\{h_i\}\)&lt;/span&gt; values. For example, summing the above equation over all past games played by team &lt;span class="math"&gt;\(1\)&lt;/span&gt;, we&amp;nbsp;obtain &lt;/p&gt;
&lt;div class="math"&gt;$$ \sum_{j\text{ (past opponents of 1)}} (h_1 - h_j) = \sum_j \langle y_{1j} \rangle \approx \sum_j y_{1j}.$$&lt;/div&gt;
&lt;p&gt; Here, in the sum on right we have approximated the averaged sum in the middle by the score differences actually observed in the games already played (note that in the sum on &lt;span class="math"&gt;\(j\)&lt;/span&gt; here, each team appears exactly the number of times they have already played team &lt;span class="math"&gt;\(1\)&lt;/span&gt; &amp;#8212; this could be zero, once, twice, etc.) Writing down all equations analogous to this last one (one for each team) returns a system of &lt;span class="math"&gt;\(30\)&lt;/span&gt; linear equations in the &lt;span class="math"&gt;\(30\)&lt;/span&gt; &lt;span class="math"&gt;\(\{h_i\}\)&lt;/span&gt; variables. This system can be easily solved using a computer&lt;span class="math"&gt;\(^3\)&lt;/span&gt;. We did this, applying the algorithm to the complete set of 2014-15 games played prior to the Christmas week, and obtained the set of &lt;span class="math"&gt;\(h\)&lt;/span&gt; values shown at right&lt;span class="math"&gt;\(^4\)&lt;/span&gt;. The ranking looks quite reasonable, from top to&amp;nbsp;bottom.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A Gaussian &lt;span class="caps"&gt;NBA&lt;/span&gt;&lt;/strong&gt;&lt;br&gt;
Now that we have the &lt;span class="math"&gt;\(\{h_i\}\)&lt;/span&gt; values, we can use them to estimate the mean score difference for any game. For example, in a Warriors-76ers game, we&amp;#8217;d expect the Warriors to win, since they have the larger &lt;span class="math"&gt;\(h\)&lt;/span&gt; value. Further, on average, we&amp;#8217;d expect them to win by about &lt;span class="math"&gt;\(h_{\text{War's}} - h_{\text{76's}}\)&lt;/span&gt; &lt;span class="math"&gt;\( = 9.24 - (-11.96) \approx 21\)&lt;/span&gt; points. These two actually played this week, on Dec 30, and the Warriors won by &lt;span class="math"&gt;\(40\)&lt;/span&gt;, a much larger margin than&amp;nbsp;predicted.&lt;/p&gt;
&lt;p&gt;&lt;a href="http/wp-content/uploads/2014/12/hist.jpg"&gt;&lt;img alt="hist" src="http/wp-content/uploads/2014/12/hist.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The distinction between our predicted and the actual Warriors-76ers outcome motivates further consideration of the variability characterizing &lt;span class="caps"&gt;NBA&lt;/span&gt; games. It turns out that if we analyze the complete set of games already played this year, something simple pops out: Plotting a histogram of our estimate errors, &lt;span class="math"&gt;\(\epsilon_{ij} \equiv (h_i - h_j) - y_{ij}\)&lt;/span&gt;, we see that the actual score difference distribution of &lt;span class="caps"&gt;NBA&lt;/span&gt; games looks a lot like a &lt;a href="http://en.wikipedia.org/wiki/Gaussian_function"&gt;Gaussian&lt;/a&gt;, or bell curve. This is centered about our predicted value and has a standard deviation of &lt;span class="math"&gt;\(\sigma \approx 11\)&lt;/span&gt; points, as shown in the figure at right. These observations allow us to estimate various quantities of interest. For instance, we can estimate the frequency with which the Warriors should beat the 76ers by 40 or more points, as they did this week. This is simply equal to the frequency with which we underestimate the winning margin by at least &lt;span class="math"&gt;\(40 - 21 = 19\)&lt;/span&gt; points. This, in turn, can be estimated by counting how often this has already occurred in past games, using our histogram. Alternatively, we can use the fact that our errors are Gaussian distributed to write this&amp;nbsp;as &lt;/p&gt;
&lt;div class="math"&gt;$$ P(\epsilon \leq -19) = \frac{1}{\sqrt{2 \pi \sigma^2}} \int_{-\infty}^{-19} e^{-\frac{\epsilon^2}{2\sigma^2}} d \epsilon \approx 0.042,$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
where we have evaluated the integral by computer. This result says that a Warriors win by 40 or more points will only occur about &lt;span class="math"&gt;\(4.2%\)&lt;/span&gt; of the time. Using a similar argument, one can show that the 76ers should beat the Warriors only about &lt;span class="math"&gt;\(2.8 %\)&lt;/span&gt; of the&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Christmas week, quantified&lt;/strong&gt;&lt;br&gt;
It is now a simple matter to extend our analysis method so that we can estimate the joint likelihood of a given set of outcomes all happening the same week: We need only make use of the &lt;a href="%20http://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables"&gt;fact&lt;/a&gt; that the mean estimate error &lt;span class="math"&gt;\(\langle \epsilon \rangle\)&lt;/span&gt; of our predictions on a set of &lt;span class="math"&gt;\(N\)&lt;/span&gt; games &lt;span class="math"&gt;\((\langle \epsilon \rangle = \frac{1}{N}\sum_{\text{games }i = 1}^N \epsilon_i)\)&lt;/span&gt; will also be Gaussian distributed, but now with standard deviation &lt;span class="math"&gt;\(\sigma/ \sqrt{N}\)&lt;/span&gt;. The &lt;span class="math"&gt;\(1/\sqrt{N}\)&lt;/span&gt; factor here reduces the width of the mean error distribution, relative to that of the single games &amp;#8212; it takes into account the significant cancellations that typically occur when you sum over many games, some with positive and some with negative errors. A typical week has about &lt;span class="math"&gt;\(50\)&lt;/span&gt; games, so the mean error standard deviation will usually be about &lt;span class="math"&gt;\(11/\sqrt{50} \approx 1.6\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the four figures below, we plot histograms of our prediction errors for four separate weeks: Christmas week is shown last (in red), and the other subplots correspond to the three weeks preceding it (each in green). We also show in each subplot (in gray) a histogram of all game errors preceding the week highlighted in that subplot &amp;#8212; notice that each is quite well-fit by a Gaussian. In the first week, &lt;span class="math"&gt;\(53\)&lt;/span&gt; games were played, and our average error on these games was just &lt;span class="math"&gt;\(\langle \epsilon \rangle = 0.5\)&lt;/span&gt; points. The probability of observing an average overestimate of &lt;span class="math"&gt;\(0.5\)&lt;/span&gt; or greater in such a week is given&amp;nbsp;by, &lt;/p&gt;
&lt;div class="math"&gt;$$P(\langle \epsilon \rangle \geq 0.5) = \frac{1}{\sqrt{2 \pi \sigma^2/53}} \int_{0.5}^{\infty} e^{-\frac{\epsilon^2}{2\sigma^2/53}} d \epsilon \approx 0.38.$$&lt;/div&gt;
&lt;p&gt; That is, a weekly average overestimate of &lt;span class="math"&gt;\(\langle \epsilon \rangle \geq 0.5\)&lt;/span&gt; will happen about &lt;span class="math"&gt;\(38%\)&lt;/span&gt; of the time, and so is pretty common. Similarly, in the second, third, and fourth weeks, the number of games played and average estimate errors were &lt;span class="math"&gt;\((N,\langle \epsilon \rangle) = (52,0.8),\)&lt;/span&gt; &lt;span class="math"&gt;\((55,2.2)\)&lt;/span&gt;, and &lt;span class="math"&gt;\((49,5.7)\)&lt;/span&gt;, respectively. Calculating as above, overestimates of these magnitudes or larger occur with frequency &lt;span class="math"&gt;\(30%\)&lt;/span&gt;, &lt;span class="math"&gt;\(7%\)&lt;/span&gt;, and &lt;span class="math"&gt;\(0.01 %\)&lt;/span&gt;, respectively. The previous two are both fairly common, &lt;em&gt;but &amp;#8212; on average &amp;#8212; it would apparently take about ten thousand trials to find a week as bad as Christmas week&amp;nbsp;2014.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http/wp-content/uploads/2014/12/xmas_plots2.jpg"&gt;&lt;img alt="xmas_plots" src="http/wp-content/uploads/2014/12/xmas_plots2.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;br&gt;
A week in ten thousand is equivalent to about one week in every &lt;span class="math"&gt;\(400\)&lt;/span&gt; seasons! We don&amp;#8217;t really take this estimate too seriously. In fact, we suspect that one of the following might be happening here: a) there may have been something peculiar about the games held this Christmas week that caused their outcomes to not be distributed in the same manner as other games this season&lt;span class="math"&gt;\(^5\)&lt;/span&gt;, b) alternatively, there may be long tails in the error distribution that we can&amp;#8217;t easily observe, or c) it may be that improvements to our model (e.g., taking into account home team advantage, etc.) would result in a larger frequency estimate. Maybe all three are true, or maybe this really was a week in ten thousand. Either way, it&amp;#8217;s clear that this past Christmas week was a singular&amp;nbsp;one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Footnotes&lt;/strong&gt;&lt;br&gt;
[1] The &lt;span class="caps"&gt;NBA&lt;/span&gt; workweek starts on&amp;nbsp;Friday.&lt;/p&gt;
&lt;p&gt;[2] We first read about this modeling method &lt;a href="http://www.pro-football-reference.com/blog/?p=37"&gt;here&lt;/a&gt;. In the addendum, it&amp;#8217;s stated that the author thinks that nobody in particular is credited with having developed it, and that it&amp;#8217;s been around for a long&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;[3] Notice that we can shift all &lt;span class="math"&gt;\(h_i \to h_i +c\)&lt;/span&gt;, with &lt;span class="math"&gt;\(c\)&lt;/span&gt; some common constant. This invariance means that the solution obtained by solving the system of equations is not unique. Consequently, the matrix of coefficients is not invertible, and the system needs to be solved by Gaussian elimination, or some other irritating&amp;nbsp;means.&lt;/p&gt;
&lt;p&gt;[4] Python code and data for evaluating the &lt;span class="caps"&gt;NBA&lt;/span&gt; &lt;span class="math"&gt;\(h\)&lt;/span&gt; values given &lt;a href="%20http://efavdb.com/nba-h-model/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[5] Note, however, that carrying out a similar analysis over the past 9 seasons showed no similar anomalies in their respective Christmas&amp;nbsp;weeks.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Case studies, Methods, NBA prediction project"></category><category term="NBA"></category><category term="statistics"></category></entry></feed>