<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>EFAVDB - Methods, Statistics, Theory</title><link href="https://efavdb.com/" rel="alternate"></link><link href="https://efavdb.com/feeds/methods-statistics-theory.atom.xml" rel="self"></link><id>https://efavdb.com/</id><updated>2018-03-03T17:53:00-08:00</updated><subtitle>Everybody's Favorite Data Blog</subtitle><entry><title>Integration method to map model scores to conversion rates from example data</title><link href="https://efavdb.com/integration-method-to-map-model-scores-to-conversion-rates-from-example-data" rel="alternate"></link><published>2018-03-03T17:53:00-08:00</published><updated>2018-03-03T17:53:00-08:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2018-03-03:/integration-method-to-map-model-scores-to-conversion-rates-from-example-data</id><summary type="html">&lt;p&gt;This note addresses the typical applied problem of estimating from data how a target &amp;#8220;conversion rate&amp;#8221; function varies with some available scalar score function &amp;#8212; e.g., estimating conversion rates from some marketing campaign as a function of a targeting model score. The idea centers around estimating the integral of the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This note addresses the typical applied problem of estimating from data how a target &amp;#8220;conversion rate&amp;#8221; function varies with some available scalar score function &amp;#8212; e.g., estimating conversion rates from some marketing campaign as a function of a targeting model score. The idea centers around estimating the integral of the rate function; differentiating this gives the rate function. The method is a variation on a standard technique for estimating pdfs via fits to empirical&amp;nbsp;cdfs.&lt;/p&gt;
&lt;h3&gt;Problem definition and naive binning&amp;nbsp;solution&lt;/h3&gt;
&lt;p&gt;Here, we are interested in estimating a rate function, &lt;span class="math"&gt;\(p \equiv p(x)\)&lt;/span&gt;, representing the probability of some &amp;#8220;conversion&amp;#8221; event as a function of &lt;span class="math"&gt;\(x\)&lt;/span&gt;, some scalar model score. To do this, we assume we have access to a finite set of score-outcome data of the form &lt;span class="math"&gt;\(\{(x_i, n_i), i= 1, \ldots ,k\}\)&lt;/span&gt;. Here, &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; is the score for example &lt;span class="math"&gt;\(i\)&lt;/span&gt; and &lt;span class="math"&gt;\(n_i \in \{0,1\}\)&lt;/span&gt; is its conversion&amp;nbsp;indicator.&lt;/p&gt;
&lt;p&gt;There are a number of standard methods for estimating rate functions. For example, if the score &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a prior estimate for the conversion rate, a trivial mapping &lt;span class="math"&gt;\(p(x) = x\)&lt;/span&gt; may work. This won&amp;#8217;t work if the score function in question is not an estimate for &lt;span class="math"&gt;\(p\)&lt;/span&gt;. A more general approach is to bin together example data points that have similar scores: The observed conversion rate within each bin can then be used as an estimate for the true conversion rate in the bin&amp;#8217;s score range. An example output of this approach is shown in Fig. 1. Another option is to create a moving average, analogous to the binned&amp;nbsp;solution.&lt;/p&gt;
&lt;p&gt;The simple binning approach introduces two inefficiencies: (1) Binning coarsens a data set, resulting in a loss of information. (2) The data in one bin does not affect the data in the other bins, precluding exploitation of any global smoothness constraints that could be placed on &lt;span class="math"&gt;\(p\)&lt;/span&gt; as a function of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. The running average approach is also subject to these issues. The method we discuss below alleviates both&amp;nbsp;inefficiencies.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/03/image17.png"&gt;&lt;img alt="" src="https://efavdb.com/wp-content/uploads/2018/03/image17.png"&gt;&lt;/a&gt;&lt;br&gt;
Fig. 1. Binned probability estimate approach: All data with scores in a given range are grouped together, and the outcomes from those data points are used to estimate the conversion rate in each bin. Here, the x-axis represents score range, data was grouped into six bins, and mean and standard deviation of the outcome probabilities were estimated from the observed outcomes within each&amp;nbsp;bin.&lt;/p&gt;
&lt;h3&gt;Efficient estimates by&amp;nbsp;integration&lt;/h3&gt;
&lt;p&gt;It can be difficult to directly fit a rate function p(x) using score-outcome data because data of this type does not lie on a continuous curve (the y-values alternate between 0 and 1, depending on the outcome for each example). However, if we consider the empirical integral of the available data, we obtain a smooth, increasing function that is much easier to&amp;nbsp;fit.&lt;/p&gt;
&lt;p&gt;To evaluate the empirical integral, we assume the samples are first sorted by &lt;span class="math"&gt;\(x\)&lt;/span&gt; and define&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{1} \label{1}  
\delta x_i \equiv x_i - x_{i-1}.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
Next, the empirical integral is taken as&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$ \tag{2} \label{2}  
\hat{J}(x_j) \equiv \sum_{i=0}^{j} n_i \delta x_i,  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
which approximates the integral&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\tag{3} \label{3}  
J(x) \equiv \int_{x_0}^{x_j} p(x) dx.  
$$&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
We can think of (\ref{3}) as the number of expected conversions given density-&lt;span class="math"&gt;\(1\)&lt;/span&gt; sampling over the &lt;span class="math"&gt;\(x\)&lt;/span&gt; range noted. Taking a fit to the &lt;span class="math"&gt;\(\{(x_i, \hat{J}(x_i))\}\)&lt;/span&gt; values gives a smooth estimate for (\ref{3}). Differentiating with respect to &lt;span class="math"&gt;\(x\)&lt;/span&gt; the gives an estimate for &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;. Fig. 2 illustrates the approach. Here, I fit the available data to a quadratic, capturing the growth in &lt;span class="math"&gt;\(p\)&lt;/span&gt; with &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The example in Fig. 2 has no error bar shown. One way to obtain error bars would be to work with a particular fit form. The uncertainty in the fit coefficients could then be used to estimate uncertainties in the values at each&amp;nbsp;point.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/03/image16-1.png"&gt;&lt;img alt="image16" src="https://efavdb.com/wp-content/uploads/2018/03/image16-1.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fig. 2. (Left) A plot of the empirical integral of the data used to generate Fig. 1 is in blue. A quadratic fit is shown in red. (Right) The derivative of the red fit function at left is shown, an estimate for the rate function in question, &lt;span class="math"&gt;\(p\equiv p(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Example python&amp;nbsp;code&lt;/h3&gt;
&lt;p&gt;The code snippet below carries out the procedure described above on a simple example. One example output is shown in Fig. 3 at the bottom of the section. Running the code multiple times gives one a sense of the error that is present in the predictions. In practical applications, this can&amp;#8217;t be done so carrying out the error analysis procedure suggested above should be done to get a better sense of the error&amp;nbsp;involved.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;pylab&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;  
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.optimize&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;curve_fit&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;p_given_x&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;outcome_given_p&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Generate some random data  &lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p_given_x&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outcome_given_p&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Calculate delta x, get weighted outcomes  &lt;/span&gt;
&lt;span class="n"&gt;delta_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="n"&gt;weighted_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;delta_x&lt;/span&gt;

&lt;span class="c1"&gt;# Integrate and fit  &lt;/span&gt;
&lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weighted_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="n"&gt;popt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pcov&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;curve_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fit_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;j_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;popt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Finally, differentiate and compare to actual p  &lt;/span&gt;
&lt;span class="n"&gt;p_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j_fit&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;j_fit&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;delta_x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Plots  &lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;*&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;empirical integral&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;j_fit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fit to integral&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;p_fit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fit to p versus x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;k--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;actual p versus x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2018/03/example_fit.png"&gt;&lt;img alt="example_fit" src="https://efavdb.com/wp-content/uploads/2018/03/example_fit.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fig. 3. The result of one run of the algorithm on a data set where &lt;span class="math"&gt;\(p(x) \equiv x^2\)&lt;/span&gt;, given 200 random samples of &lt;span class="math"&gt;\(x \in (0, 1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Statistics, Theory"></category></entry><entry><title>Logistic Regression</title><link href="https://efavdb.com/logistic-regression" rel="alternate"></link><published>2017-07-29T19:10:00-07:00</published><updated>2017-07-29T19:10:00-07:00</updated><author><name>Jonathan Landy</name></author><id>tag:efavdb.com,2017-07-29:/logistic-regression</id><summary type="html">&lt;p&gt;We review binary logistic regression. In particular, we derive a) the equations needed to fit the algorithm via gradient descent, b) the maximum likelihood fit&amp;#8217;s asymptotic coefficient covariance matrix, and c) expressions for model test point class membership probability confidence intervals. We also provide python code implementing a minimal …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We review binary logistic regression. In particular, we derive a) the equations needed to fit the algorithm via gradient descent, b) the maximum likelihood fit&amp;#8217;s asymptotic coefficient covariance matrix, and c) expressions for model test point class membership probability confidence intervals. We also provide python code implementing a minimal &amp;#8220;LogisticRegressionWithError&amp;#8221; class whose &amp;#8220;predict_proba&amp;#8221; method returns prediction confidence intervals alongside its point&amp;nbsp;estimates.&lt;/p&gt;
&lt;p&gt;Our python code can be downloaded from our github page, &lt;a href="https://github.com/EFavDB/logistic-regression-with-error"&gt;here&lt;/a&gt;. Its use requires the jupyter, numpy, sklearn, and matplotlib&amp;nbsp;packages.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The logistic regression model is a linear classification model that can be used to fit binary data &amp;#8212; data where the label one wishes to predict can take on one of two values &amp;#8212; e.g., &lt;span class="math"&gt;\(0\)&lt;/span&gt; or &lt;span class="math"&gt;\(1\)&lt;/span&gt;. Its linear form makes it a convenient choice of model for fits that are required to be interpretable. Another of its virtues is that it can &amp;#8212; with relative ease &amp;#8212; be set up to return both point estimates and also confidence intervals for test point class membership probabilities. The availability of confidence intervals allows one to flag test points where the model prediction is not precise, which can be useful for some applications &amp;#8212; eg fraud&amp;nbsp;detection.&lt;/p&gt;
&lt;p&gt;In this note, we derive the expressions needed to fit the logistic model to a training data set. We assume the training data consists of a set of &lt;span class="math"&gt;\(n\)&lt;/span&gt; feature vector- label pairs, &lt;span class="math"&gt;\(\{(\vec{x}_i, y_i)\)&lt;/span&gt;, for &lt;span class="math"&gt;\(i = 1, 2, \ldots, n\}\)&lt;/span&gt;, where the feature vectors &lt;span class="math"&gt;\(\vec{x}_i\)&lt;/span&gt; belong to some &lt;span class="math"&gt;\(m\)&lt;/span&gt;-dimensional space and the labels are binary, &lt;span class="math"&gt;\(y_i \in \{0, 1\}.\)&lt;/span&gt; The logistic model states that the probability of belonging to class &lt;span class="math"&gt;\(1\)&lt;/span&gt; is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{1} \label{model1}
p(y=1 \vert \vec{x}) \equiv \frac{1}{1 + e^{- \vec{\beta} \cdot \vec{x} } },
\end{align}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; is a coefficient vector characterizing the model. Note that with this choice of sign in the exponent, predictor vectors &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; having a large, positive component along &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; will be predicted to have a large probability of being in class &lt;span class="math"&gt;\(1\)&lt;/span&gt;. The probability of class &lt;span class="math"&gt;\(0\)&lt;/span&gt; is given by the&amp;nbsp;complement,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}\tag{2} \label{model2}
p(y=0 \vert \vec{x}) \equiv 1 - p(y=1 \vert \vec{x}) = \frac{1}{1 + e^{ \vec{\beta} \cdot \vec{x} } }.
\end{align}&lt;/div&gt;
&lt;p&gt;
The latter equality above follows from simplifying algebra, after plugging in (\ref{model1}) for &lt;span class="math"&gt;\(p(y=1 \vert&amp;nbsp;\vec{x}).\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To fit the Logistic model to a training set &amp;#8212; i.e., to find a good choice for the fit parameter vector &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; &amp;#8212; we consider here only the maximum-likelihood solution. This is that &lt;span class="math"&gt;\(\vec{\beta}^*\)&lt;/span&gt; that maximizes the conditional probability of observing the training data. The essential results we review below are 1) a proof that the maximum likelihood solution can be found by gradient descent, and 2) a derivation for the asymptotic covariance matrix of &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;. This latter result provides the basis for returning point estimate confidence&amp;nbsp;intervals.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://efavdb.com/wp-content/uploads/2017/07/errorbar.png"&gt;&lt;img alt="errorbar" src="https://efavdb.com/wp-content/uploads/2017/07/errorbar.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;On our GitHub &lt;a href="https://github.com/EFavDB/logistic-regression-with-error"&gt;page&lt;/a&gt;, we provide a Jupyter notebook that contains some minimal code extending the SKLearn LogisticRegression class. This extension makes use of the results presented here and allows for class probability confidence intervals to be returned for individual test points. In the notebook, we apply the algorithm to the SKLearn Iris dataset. The figure at right illustrates the output of the algorithm along a particular cut through the Iris data set parameter space. The y-axis represents the probability of a given test point belong to Iris class &lt;span class="math"&gt;\(1\)&lt;/span&gt;. The error bars in the plot provide insight that is completely missed when considering the point estimates only. For example, notice that the error bars are quite large for each of the far right points, despite the fact that the point estimates there are each near &lt;span class="math"&gt;\(1\)&lt;/span&gt;. Without the error bars, the high probability of these point estimates might easily be misinterpreted as implying high model&amp;nbsp;confidence.&lt;/p&gt;
&lt;p&gt;Our derivations below rely on some prerequisites: Properties of covariance matrices, the multivariate Cramer-Rao theorem, and properties of maximum likelihood estimators. These concepts are covered in two of our prior posts [&lt;span class="math"&gt;\(1\)&lt;/span&gt;, &lt;span class="math"&gt;\(2\)&lt;/span&gt;].&lt;/p&gt;
&lt;h3&gt;Optimization by gradient&amp;nbsp;descent&lt;/h3&gt;
&lt;p&gt;In this section, we derive expressions for the gradient of the negative-log likelihood loss function and also demonstrate that this loss is everywhere convex. The latter result is important because it implies that gradient descent can be used to find the maximum likelihood&amp;nbsp;solution.&lt;/p&gt;
&lt;p&gt;Again, to fit the logistic model to a training set, our aim is to find &amp;#8212; and also to set the parameter vector to &amp;#8212; the maximum likelihood value. Assuming the training set samples are independent, the likelihood of observing the training set labels is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
L &amp;amp;\equiv \prod_i p(y_i \vert \vec{x}_i) \\
&amp;amp;= \prod_{i: y_i = 1} \frac{1}{1 + e^{-\vec{\beta} \cdot \vec{x}_i}} \prod_{i: y_i = 0} \frac{1}{1 + e^{\vec{\beta} \cdot \vec{x}_i}}.
\tag{3} \label{likelihood}
\end{align}&lt;/div&gt;
&lt;p&gt;
Maximizing this is equivalent to minimizing its negative logarithm &amp;#8212; a cost function that is somewhat easier to work&amp;nbsp;with,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
J &amp;amp;\equiv -\log L \\
&amp;amp;= \sum_{\{i: y_i = 1 \}} \log \left (1 + e^{- \vec{\beta} \cdot \vec{x}_i } \right ) + \sum_{\{i: y_i = 0 \}} \log \left (1 + e^{\vec{\beta} \cdot \vec{x}_i } \right ).
\tag{4} \label{costfunction}
\end{align}&lt;/div&gt;
&lt;p&gt;
The maximum-likelihood solution, &lt;span class="math"&gt;\(\vec{\beta}^*\)&lt;/span&gt;, is that coefficient vector that minimizes the above. Note that &lt;span class="math"&gt;\(\vec{\beta}^*\)&lt;/span&gt; will be a function of the random sample, and so will itself be a random variable &amp;#8212; characterized by a distribution having some mean value, covariance, etc. Given enough samples, a theorem on maximum-likelihood asymptotics (Cramer-Rao) guarantees that this distribution will be unbiased &amp;#8212; i.e., it will have mean value given by the correct parameter values &amp;#8212; and will also be of minimal covariance [&lt;span class="math"&gt;\(1\)&lt;/span&gt;]. This theorem is one of the main results motivating use of the maximum-likelihood&amp;nbsp;solution.&lt;/p&gt;
&lt;p&gt;Because &lt;span class="math"&gt;\(J\)&lt;/span&gt; is convex (demonstrated below), the logistic regression maximum-likelihood solution can always be found by gradient descent. That is, one need only iteratively update &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; in the direction of the negative &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt;-gradient of &lt;span class="math"&gt;\(J\)&lt;/span&gt;, which&amp;nbsp;is
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
- \nabla_{\vec{\beta}} J &amp;amp;= \sum_{\{i: y_i = 1 \}}\vec{x}_i \frac{ e^{- \vec{\beta} \cdot \vec{x}_i } }{1 + e^{- \vec{\beta} \cdot \vec{x}_i }}
- \sum_{\{i: y_i = 0 \}} \vec{x}_i \frac{ e^{\vec{\beta} \cdot \vec{x}_i }}{1 + e^{\vec{\beta} \cdot \vec{x}_i } } \\
&amp;amp;\equiv \sum_{\{i: y_i = 1 \}}\vec{x}_i p(y=0 \vert \vec{x}_i)
-\sum_{\{i: y_i = 0 \}} \vec{x}_i p(y= 1 \vert \vec{x}_i). \tag{5} \label{gradient}
\end{align}&lt;/div&gt;
&lt;p&gt;
Notice that the terms that contribute the most here are those that are most strongly misclassified &amp;#8212; i.e., those where the model&amp;#8217;s predicted probability for the observed class is very low. For example, a point with true label &lt;span class="math"&gt;\(y=1\)&lt;/span&gt; but large model &lt;span class="math"&gt;\(p(y=0 \vert \vec{x})\)&lt;/span&gt; will contribute a significant push on &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; in the direction of &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; &amp;#8212; so that the model will be more likely to predict &lt;span class="math"&gt;\(y=1\)&lt;/span&gt; at this point going forward. Notice that the contribution of a term above is also proportional to the length of its feature vector &amp;#8212; training points further from the origin have a stronger impact on the optimization process than those near the origin (at fixed classification&amp;nbsp;difficulty).&lt;/p&gt;
&lt;p&gt;The Hessian (second partial derivative) matrix of the cost function follows from taking a second gradient of the above. With a little algebra, one can show that this has &lt;span class="math"&gt;\(i-j\)&lt;/span&gt; component given&amp;nbsp;by,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
H(J)_{ij} &amp;amp;\equiv -\partial_{\beta_j} \partial_{\beta_i} \log L \\
&amp;amp;= \sum_k x_{k; i} x_{k; j} p(y= 0 \vert \vec{x}_k) p(y= 1 \vert \vec{x}_k). \tag{6} \label{Hessian}
\end{align}&lt;/div&gt;
&lt;p&gt;
We can prove that this is positive semi-definite using the fact that a matrix &lt;span class="math"&gt;\(M\)&lt;/span&gt; is necessarily positive semi-definite if &lt;span class="math"&gt;\(\vec{s}^T \cdot M \cdot \vec{s} \geq 0\)&lt;/span&gt; for all real &lt;span class="math"&gt;\(\vec{s}\)&lt;/span&gt; [&lt;span class="math"&gt;\(2\)&lt;/span&gt;]. Dotting our Hessian above on both sides by an arbitrary vector &lt;span class="math"&gt;\(\vec{s}\)&lt;/span&gt;, we&amp;nbsp;obtain
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\vec{s}^T \cdot H \cdot \vec{s} &amp;amp;\equiv \sum_k \sum_{ij} s_i x_{k; i} x_{k; j} s_j p(y= 0 \vert \vec{x}_k) p(y= 1 \vert \vec{x}_k) \\
&amp;amp;= \sum_k \vert \vec{s} \cdot \vec{x}_k \vert^2 p(y= 0 \vert \vec{x}_k) p(y= 1 \vert \vec{x}_k) \geq 0.
\tag{7} \label{convex}
\end{align}&lt;/div&gt;
&lt;p&gt;
The last form follows from the fact that both &lt;span class="math"&gt;\( p(y= 0 \vert \vec{x}_k) $ and $ p(y= 1 \vert \vec{x}_k) $ are non-negative. This holds for any $\vec{\beta}\)&lt;/span&gt; and any &lt;span class="math"&gt;\(\vec{s}\)&lt;/span&gt;, which implies that our Hessian is everywhere positive semi-definite. Because of this, convex optimization strategies &amp;#8212; e.g., gradient descent &amp;#8212; can always be applied to find the global maximum-likelihood&amp;nbsp;solution.&lt;/p&gt;
&lt;h3&gt;Coefficient uncertainty and significance&amp;nbsp;tests&lt;/h3&gt;
&lt;p&gt;The solution &lt;span class="math"&gt;\(\vec{\beta}^*\)&lt;/span&gt; that minimizes &lt;span class="math"&gt;\(J\)&lt;/span&gt; &amp;#8212; which can be found by gradient descent &amp;#8212; is a maximum likelihood estimate. In the asymptotic limit of a large number of samples, maximum-likelihood parameter estimates satisfy the Cramer-Rao lower bound [&lt;span class="math"&gt;\(2\)&lt;/span&gt;]. That is, the parameter covariance matrix satisfies [&lt;span class="math"&gt;\(3\)&lt;/span&gt;],
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\text{cov}(\vec{\beta}^*, \vec{\beta}^*) &amp;amp;\sim H(J)^{-1} \\
&amp;amp;\approx \frac{1}{\sum_k \vec{x}_{k} \vec{x}_{k}^T p(y= 0 \vert \vec{x}_k) p(y= 1 \vert \vec{x}_k)}.
\tag{8} \label{covariance}
\end{align}&lt;/div&gt;
&lt;p&gt;
Notice that the covariance matrix will be small if the denominator above is large. Along a given direction, this requires that the training set contains samples over a wide range of values in that direction (we discuss this at some length in the analogous section of our post on Linear Regression [&lt;span class="math"&gt;\(4\)&lt;/span&gt;]). For a term to contribute in the denominator, the model must also have some confusion about its values: If there are no difficult-to-classify training examples, this means that there are no examples near the decision boundary. When this occurs, there will necessarily be a lot of flexibility in where the decision boundary is placed, resulting in large parameter&amp;nbsp;variances.&lt;/p&gt;
&lt;p&gt;Although the form above only holds in the asymptotic limit, we can always use it to approximate the true covariance matrix &amp;#8212; keeping in mind that the accuracy of the approximation will degrade when working with small training sets. For example, using (\ref{covariance}), the asymptotic variance for a single parameter can be approximated&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\tag{9} \label{single_cov}
\sigma^2_{\beta^*_i} = \text{cov}(\vec{\beta}^*, \vec{\beta}^*)_{ii}.
\end{align}&lt;/div&gt;
&lt;p&gt;
In the asymptotic limit, the maximum-likelihood parameters will be Normally-distributed [&lt;span class="math"&gt;\(1\)&lt;/span&gt;], so we can provide confidence intervals for the parameters&amp;nbsp;as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\tag{10} \label{parameter_interval}
\beta_i \in \left ( \beta^*_i - z \sigma_{\beta^*_i}, \beta_i^* + z \sigma_{\beta^*_i} \right),
\end{align}&lt;/div&gt;
&lt;p&gt;
where the value of &lt;span class="math"&gt;\(z\)&lt;/span&gt; sets the size of the interval. For example, choosing &lt;span class="math"&gt;\(z = 2\)&lt;/span&gt; gives an interval construction procedure that will cover the true value approximately &lt;span class="math"&gt;\(95%\)&lt;/span&gt; of the time &amp;#8212; a result of Normal statistics [&lt;span class="math"&gt;\(5\)&lt;/span&gt;]. Checking which intervals do not cross zero provides a method for identifying which features contribute significantly to a given&amp;nbsp;fit.&lt;/p&gt;
&lt;h3&gt;Prediction confidence&amp;nbsp;intervals&lt;/h3&gt;
&lt;p&gt;The probability of class &lt;span class="math"&gt;\(1\)&lt;/span&gt; for a test point &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; is given by (\ref{model1}). Notice that this depends on &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{\beta}\)&lt;/span&gt; only through the dot product &lt;span class="math"&gt;\(\vec{x} \cdot \vec{\beta}\)&lt;/span&gt;. At fixed &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt;, the variance (uncertainty) in this dot product follows from the coefficient covariance matrix above: We have [&lt;span class="math"&gt;\(2\)&lt;/span&gt;],
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\tag{11} \label{logit_var}
\sigma^2_{\vec{x} \cdot \vec{\beta}} \equiv \vec{x}^T \cdot \text{cov}(\vec{\beta}^*, \vec{\beta}^*) \cdot \vec{x}.
\end{align}&lt;/div&gt;
&lt;p&gt;
With this result, we can obtain an expression for the confidence interval for the dot product, or equivalently a confidence interval for the class probability. For example, the asymptotic interval for class &lt;span class="math"&gt;\(1\)&lt;/span&gt; probability is given&amp;nbsp;by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\tag{12} \label{prob_interval}
p(y=1 \vert \vec{x}) \in \left ( \frac{1}{1 + e^{- \vec{x} \cdot \vec{\beta}^* + z \sigma_{\vec{x} \cdot \vec{\beta}^*}}}, \frac{1}{1 + e^{- \vec{x} \cdot \vec{\beta}^* - z \sigma_{\vec{x} \cdot \vec{\beta}^*}}} \right),
\end{align}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(z\)&lt;/span&gt; again sets the size of the interval as above (&lt;span class="math"&gt;\(z=2\)&lt;/span&gt; gives a &lt;span class="math"&gt;\(95%\)&lt;/span&gt; confidence interval, etc. [&lt;span class="math"&gt;\(5\)&lt;/span&gt;]), and &lt;span class="math"&gt;\(\sigma_{\vec{x} \cdot \vec{\beta}^*}\)&lt;/span&gt; is obtained from (\ref{covariance}) and&amp;nbsp;(\ref{logit_var}).&lt;/p&gt;
&lt;p&gt;The results (\ref{covariance}), (\ref{logit_var}), and (\ref{prob_interval}) are used in our Jupyter notebook. There we provide code for a minimal Logistic Regression class implementation that returns both point estimates and prediction confidence intervals for each test point. We used this code to generate the plot shown in the post introduction. Again, the code can be downloaded &lt;a href="https://github.com/EFavDB/logistic-regression-with-error"&gt;here&lt;/a&gt; if you are interested in trying it&amp;nbsp;out.&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;In this note, we have 1) reviewed how to fit a logistic regression model to a binary data set for classification purposes, and 2) have derived the expressions needed to return class membership probability confidence intervals for test&amp;nbsp;points.&lt;/p&gt;
&lt;p&gt;Confidence intervals are typically not available for many out-of-the-box machine learning models, despite the fact that intervals can often provide significant utility. The fact that logistic regression allows for meaningful error bars to be returned with relative ease is therefore a notable, advantageous&amp;nbsp;property.&lt;/p&gt;
&lt;h3&gt;Footnotes&lt;/h3&gt;
&lt;p&gt;[&lt;span class="math"&gt;\(1\)&lt;/span&gt;] Our notes on the maximum-likelihood estimators can be found &lt;a href="http://efavdb.github.io/maximum-likelihood-asymptotics"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[&lt;span class="math"&gt;\(2\)&lt;/span&gt;] Our notes on covariance matrices and the multivariate Cramer-Rao theorem can be found &lt;a href="http://efavdb.github.io/multivariate-cramer-rao-bound"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[&lt;span class="math"&gt;\(3\)&lt;/span&gt;] The Cramer-Rao identity [&lt;span class="math"&gt;\(2\)&lt;/span&gt;] states that covariance matrix of the maximum-likelihood estimators approaches the Hessian matrix of the log-likelihood, evaluated at their true values. Here, we approximate this by evaluating the Hessian at the maximum-likelihood point&amp;nbsp;estimate.&lt;/p&gt;
&lt;p&gt;[&lt;span class="math"&gt;\(4\)&lt;/span&gt;] Our notes on linear regression can be found &lt;a href="http://efavdb.github.io/linear-regression"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[&lt;span class="math"&gt;\(5\)&lt;/span&gt;] Our notes on Normal distributions can be found &lt;a href="http://efavdb.github.io/normal-distributions"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Methods, Statistics, Theory"></category></entry></feed>