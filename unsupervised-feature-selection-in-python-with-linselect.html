<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>Linear compression in python: PCA vs unsupervised feature selection</title>


  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
<link href="./unsupervised-feature-selection-in-python-with-linselect.html" rel="canonical" />
  <!-- Feed -->

  <link href="./theme/css/style.css" type="text/css" rel="stylesheet" />

  <!-- Code highlight color scheme -->
      <link href="./theme/css/code_blocks/github.css" rel="stylesheet">


  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->



    <meta name="description" content="We illustrate the application of two linear compression algorithms in python: Principal component analysis (PCA) and least-squares...">

    <meta name="author" content="Jonathan Landy">

    <meta name="tags" content="linselect">
    <meta name="tags" content="methods">
    <meta name="tags" content="python">




<!-- Open Graph -->
<meta property="og:site_name" content="EFAVDB"/>
<meta property="og:title" content="Linear compression in python: PCA vs unsupervised feature selection"/>
<meta property="og:description" content="We illustrate the application of two linear compression algorithms in python: Principal component analysis (PCA) and least-squares..."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="./unsupervised-feature-selection-in-python-with-linselect.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-08-11 07:30:00-07:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="./author/jonathan-landy.html">
<meta property="article:section" content="linselect"/>
<meta property="article:tag" content="linselect"/>
<meta property="article:tag" content="methods"/>
<meta property="article:tag" content="python"/>
<meta property="og:image" content="./theme/images/post-bg.jpg">

<!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@efavdb">
    <meta name="twitter:title" content="Linear compression in python: PCA vs unsupervised feature selection">
    <meta name="twitter:url" content="./unsupervised-feature-selection-in-python-with-linselect.html">

        <meta name="twitter:image:src" content="./theme/images/post-bg.jpg">

      <meta name="twitter:description" content="We illustrate the application of two linear compression algorithms in python: Principal component analysis (PCA) and least-squares...">

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "Linear compression in python: PCA vs unsupervised feature selection",
  "headline": "Linear compression in python: PCA vs unsupervised feature selection",
  "datePublished": "2018-08-11 07:30:00-07:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Jonathan Landy",
    "url": "./author/jonathan-landy.html"
  },
  "image": "./theme/images/post-bg.jpg",
  "url": "./unsupervised-feature-selection-in-python-with-linselect.html",
  "description": "We illustrate the application of two linear compression algorithms in python: Principal component analysis (PCA) and least-squares..."
}
</script>
</head>
<!-- TODO : Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>
          <li><a href="/" role="presentation">Home</a></li>
          <li><a href="/pages/about.html" role="presentation">About & Consulting</a></li>
          <li><a href="/archives.html" role="presentation">Archive</a></li>
          <li><a href="/tags.html" role="presentation">Tags</a></li>
          <li><a href="/pages/linselect.html" role="presentation">linselect - feature selection</a></li>


    </ul>
  </div>
</nav>
    <!-- Progressbar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header id="post-header" >
      <div class="inner">
        <nav id="navigation">
            <span id="home-button" class="nav-button">
                <a class="home-button" href="./" title="Home"><i class="ic ic-arrow-left"></i> Home</a>
            </span>
          <span id="menu-button" class="nav-button">
            <a class="menu-button"><i class="ic ic-menu"></i> Menu</a>
          </span>
        </nav>
        <h1 class="post-title">Linear compression in python: PCA vs unsupervised feature selection</h1>
        <!-- TODO : Proper class for headline -->
        <span class="post-meta">
                <a href="./author/jonathan-landy.html">Jonathan Landy</a>
            | <time datetime="Sat 11 August 2018">Sat 11 August 2018</time>
        </span>
        <!-- TODO : Modified check -->
      </div>
    </header>

  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
            <section class="post-content">
                <p>We illustrate the application of two linear compression algorithms in python: Principal component analysis (PCA) and least-squares feature selection. Both can be used to compress a passed array, and they both work by stripping out redundant columns from the array. The two differ in that PCA operates in a particular rotated frame, while the feature selection solution operates directly on the original columns. As we illustrate below, PCA always gives a stronger compression. However, the feature selection solution is often comparably strong, and its output has the benefit of being relatively easy to interpret -- a virtue that is important for many applications.</p>
<p>We use our python package <code>linselect</code> to carry out efficient feature selection-based compression below -- this is available on pypi (<code>pip install linselect</code>) and <a href="https://github.com/EFavDB/linselect">GitHub</a>.</p>
<h2>Linear compression algorithms</h2>
<p><a href="./wp-content/uploads/2018/06/simple_line.jpg"><img alt="simple_line" src="./wp-content/uploads/2018/06/simple_line.jpg"></a></p>
<p>To compress a data array having <span class="math">\(n\)</span> columns, linear compression algorithms begin by fitting a <span class="math">\(k\)</span>-dimensional line, or <em>hyperplane</em>, to the data (with <span class="math">\(k &lt; n\)</span>). Any point in the hyperplane can be uniquely identified using a basis of <span class="math">\(k\)</span> components. Marking down each point's projected location in the hyperplane using these components then gives a <span class="math">\(k\)</span>-column, compressed representation of the data. This idea is illustrated in Fig. 1 at right, where a line is fit to some two-component data. Projecting the points onto the line and then marking down how far along the line each projected point sits, we obtain a one-column compression. Carrying out this process can be useful if storage space is at a premium or if any operations need to be applied to the array (usually operations will run much faster on the compressed format). Further, compressed data is often easier to interpret and visualize, thanks to its reduced dimension.</p>
<p>In this post, we consider two automated linear compression algorithms: principal component analysis (PCA) and least-squares unsupervised feature selection. These differ because they are obtained from different hyperplane fitting strategies: The PCA approach is obtained from the <span class="math">\(k\)</span>-dimensional hyperplane fit that minimizes the data's total squared-projection error. In general, the independent variables of this fit -- i.e., the <span class="math">\(k\)</span> components specifying locations in the fit plane -- end up being some linear combinations of the original <span class="math">\(x_i\)</span>'s. In contrast, the feature selection strategy intelligently picks a subset of the original array columns as predictors and then applies the usual least-squares fit to the others for compression [1]. These approaches are illustrated in the left and right panels of Fig. 2 below. The two fit lines there look very similar, but the encodings returned by these strategies differ qualitatively: The 1-d compression returned by PCA is how far along the <span class="math">\(PCA_1\)</span> direction a point sits (this is some linear combination of <span class="math">\(x_1\)</span> and <span class="math">\(x_2\)</span> -- see figure), while the feature selection solution simply returns each point's <span class="math">\(x_1\)</span> value. One of our goals here is to explain why this difference can favor the feature selection approach in certain applications.</p>
<p>Our post proceeds as follows: In the next section, we consider two representative applications in python: (1) The compression of a data set of tech-sector stock price quotes, and (2) the visualization of some economic summary statistics on the G20 nations. Working through these applications, we are able to familiarize ourselves with the output of the two algorithms, and also through contrast to highlight their relative virtues. The discussion section summarizes what we learn. Finally, a short appendix covers some of the formal mathematics of compression. There, we prove that linear compression-decompression operators are always projections.</p>
<p><a href="./wp-content/uploads/2018/06/pca_vs_linselect.jpg"><img alt="pca_vs_linselect" src="./wp-content/uploads/2018/06/pca_vs_linselect.jpg"></a><br>
<strong>Fig. 2</strong>. A cartoon illustrating the projection that results when applying PCA (left) and unsupervised feature selection -- via <code>linselect</code> (right): The original 2-d big dots are replaced by their small dot, effectively-1-d approximations -- a projection.</p>
<h2>Applications</h2>
<p>Both data sets explored below are available on our Github, <a href="https://github.com/EFavDB/linselect_demos">here</a>.</p>
<h3>Stock prices</h3>
<h4>Loading and compressing the data</h4>
<p>In this section, we apply our algorithms to a prepared data set of one year's worth of daily percentage price lifts on 50 individual tech stocks [2]. We expect these stocks to each be governed by a common set of market forces, motivating the idea that a substantial compression might be possible. This is true, and the compressed arrays that result may be more efficiently operated on, as noted above. In addition, we'll see below that we can learn something about the full data set by examining the compression outputs.</p>
<p>The code below loads our data, smooths it over a running 30 day window (to remove idiosyncratic noise that is not of much interest), prints out the first three rows, compresses the data using our two methods, and then finally prints out the first five PCA components and the top five selected stocks.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>  
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>  
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>  
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>  
<span class="kn">from</span> <span class="nn">linselect</span> <span class="kn">import</span> <span class="n">FwdSelect</span>

<span class="c1"># CONSTANTS  </span>
<span class="n">KEEP</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># compression dimension  </span>
<span class="n">WINDOW_SIZE</span> <span class="o">=</span> <span class="mi">30</span> <span class="c1"># smoothing window size</span>

<span class="c1"># LOAD AND SMOOTH THE DATA  </span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;stocks.csv&#39;</span><span class="p">)</span>  
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">WINDOW_SIZE</span><span class="p">)</span><span class="o">.</span><span class="kp">mean</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">WINDOW_SIZE</span><span class="p">:]</span>  
<span class="nb">print</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>  
<span class="n">TICKERS</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span>  
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># PCA COMPRESSION  </span>
<span class="n">s</span> <span class="o">=</span> <span class="n">StandardScalar</span><span class="p">()</span>  
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">KEEP</span><span class="p">)</span>  
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>  
<span class="n">X_compressed_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># FEATURE SELECTION COMPRESSION  </span>
<span class="n">selector</span> <span class="o">=</span> <span class="n">FwdSelect</span><span class="p">()</span>  
<span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  
<span class="n">X_compressed_linselect</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">selector</span><span class="o">.</span><span class="n">ordered_features</span><span class="p">[:</span><span class="n">KEEP</span><span class="p">]]</span>

<span class="c1"># PRINT OUT FIRST FIVE PCA COMPONENTs, TOP FIVE STOCKS  </span>
<span class="nb">print</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[:</span><span class="n">KEEP</span><span class="p">]</span>  
<span class="nb">print</span> <span class="n">TICKERS</span><span class="p">[</span><span class="n">selector</span><span class="o">.</span><span class="n">ordered_features</span><span class="p">][:</span><span class="n">KEEP</span><span class="p">]</span>  
</pre></div>


<p>The output of the above print statements:</p>
<div class="highlight"><pre><span></span><span class="o">#</span> <span class="n">The</span> <span class="k">first</span> <span class="n">three</span> <span class="k">rows</span> <span class="k">of</span> <span class="n">the</span> <span class="k">data</span> <span class="n">frame</span><span class="p">:</span>  
<span class="nb">date</span> <span class="n">AAPL</span> <span class="n">ADBE</span> <span class="n">ADP</span> <span class="n">ADSK</span> <span class="n">AMAT</span> <span class="n">AMZN</span> <span class="err">\</span>  
<span class="mi">30</span> <span class="mi">2017</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">31</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002821</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002994</span> <span class="mi">0</span><span class="p">.</span><span class="mi">000248</span> <span class="mi">0</span><span class="p">.</span><span class="mi">009001</span> <span class="mi">0</span><span class="p">.</span><span class="mi">006451</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003237</span>  
<span class="mi">31</span> <span class="mi">2017</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">01</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003035</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002776</span> <span class="mi">0</span><span class="p">.</span><span class="mi">000522</span> <span class="mi">0</span><span class="p">.</span><span class="mi">008790</span> <span class="mi">0</span><span class="p">.</span><span class="mi">005487</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003450</span>  
<span class="mi">32</span> <span class="mi">2017</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">02</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003112</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002964</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">000560</span> <span class="mi">0</span><span class="p">.</span><span class="mi">008573</span> <span class="mi">0</span><span class="p">.</span><span class="mi">005523</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003705</span>

<span class="n">ASML</span> <span class="n">ATVI</span> <span class="n">AVGO</span> <span class="p">...</span> <span class="n">T</span> <span class="n">TSLA</span> <span class="n">TSM</span> <span class="err">\</span>  
<span class="mi">30</span> <span class="mi">0</span><span class="p">.</span><span class="mi">000755</span> <span class="mi">0</span><span class="p">.</span><span class="mi">005933</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003988</span> <span class="p">...</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">001419</span> <span class="mi">0</span><span class="p">.</span><span class="mi">004500</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003590</span>  
<span class="mi">31</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002174</span> <span class="mi">0</span><span class="p">.</span><span class="mi">006369</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003225</span> <span class="p">...</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">001125</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003852</span> <span class="mi">0</span><span class="p">.</span><span class="mi">004279</span>  
<span class="mi">32</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001566</span> <span class="mi">0</span><span class="p">.</span><span class="mi">006014</span> <span class="mi">0</span><span class="p">.</span><span class="mi">005343</span> <span class="p">...</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">001216</span> <span class="mi">0</span><span class="p">.</span><span class="mi">004130</span> <span class="mi">0</span><span class="p">.</span><span class="mi">004358</span>

<span class="n">TWTR</span> <span class="n">TXN</span> <span class="n">VMW</span> <span class="n">VZ</span> <span class="n">WDAY</span> <span class="n">WDC</span> <span class="n">ZNGA</span>  
<span class="mi">30</span> <span class="mi">0</span><span class="p">.</span><span class="mi">008292</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001467</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001984</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">001741</span> <span class="mi">0</span><span class="p">.</span><span class="mi">006103</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002916</span> <span class="mi">0</span><span class="p">.</span><span class="mi">007811</span>  
<span class="mi">31</span> <span class="mi">0</span><span class="p">.</span><span class="mi">008443</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001164</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002026</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">001644</span> <span class="mi">0</span><span class="p">.</span><span class="mi">006303</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003510</span> <span class="mi">0</span><span class="p">.</span><span class="mi">008379</span>  
<span class="mi">32</span> <span class="mi">0</span><span class="p">.</span><span class="mi">007796</span> <span class="mi">0</span><span class="p">.</span><span class="mi">000637</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001310</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">001333</span> <span class="mi">0</span><span class="p">.</span><span class="mi">006721</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002836</span> <span class="mi">0</span><span class="p">.</span><span class="mi">008844</span>

<span class="o">#</span> <span class="n">PCA</span> <span class="n">top</span> <span class="n">components</span><span class="p">:</span>  
<span class="p">[[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">10548148</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">20601986</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">0126039</span> <span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">20139121</span><span class="p">,</span> <span class="p">...],</span>  
<span class="p">[</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">11739195</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">02536787</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">2044143</span> <span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">08462741</span><span class="p">,</span> <span class="p">...],</span>  
<span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">03251305</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">10796197</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">00463919</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">17564998</span><span class="p">,</span> <span class="p">...],</span>  
<span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">08678107</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">1931497</span> <span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">16850867</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">16260134</span><span class="p">,</span> <span class="p">...],</span>  
<span class="p">[</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">0174396</span> <span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">01174769</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">11617622</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">01036602</span><span class="p">,</span> <span class="p">...]]</span>

<span class="o">#</span> <span class="n">Feature</span> <span class="n">selector</span> <span class="k">output</span><span class="p">:</span>  
<span class="p">[</span><span class="s1">&#39;WDAY&#39;</span><span class="p">,</span> <span class="s1">&#39;PYPL&#39;</span><span class="p">,</span> <span class="s1">&#39;AMZN&#39;</span><span class="p">,</span> <span class="s1">&#39;LRCX&#39;</span><span class="p">,</span> <span class="s1">&#39;HPQ&#39;</span><span class="p">]</span>  
</pre></div>


<p>Lines 22 and 27 in the first code block above are the two compressed versions of the original data array, line 16. For each row, the first compression stores the amplitude of that date's stock changes along each of the first five PCA components (printed below line 17 of second code block), while the second compression is simply equal to the five columns of the original array corresponding to the stocks picked out by the selector (printed below line 24 of the second code block).</p>
<h4>Exploring the encodings</h4>
<p>Working with the compressed arrays obtained above provides some immediate operational benefits: Manipulations of the compressed arrays can be carried out more quickly and they require less memory for storage. Here, we review how valuable insight can also obtained from our compressions -- via study of the compression components.</p>
<p>First, we consider the PCA components. It turns out that these components are the eigenvectors of the correlation matrix of our data set (<span class="math">\(X^T \cdot X\)</span>) -- that is, they are the collective, fluctuation modes present in the data set (for those who have studied classical mechanics, you can imagine the system as one where the different stocks are masses that are connected by springs, and these eigenvectors are the modes of the system). Using this fact, one can show that the components evolve in an uncorrelated manner. Further, one can show that projecting the data set down onto the top <span class="math">\(k\)</span> modes gives the minimum squared projection error of all possible <span class="math">\(k\)</span>-component projections. The first component then describes the largest amplitude fluctuation pattern exhibited in the data. From line 18 above, this is $[ 0.105, 0.206, -0.012, 0.201, ... $]. These coefficients tell us that when the first stock (AAPL) goes up by some amount, the second (ADBE) typically goes up by about twice as much (this follows from fact that 0.206 is about twice as big as 0.105), etc. This isn't the full story of course, because each day's movements are a superposition (sum) of the amplitudes along each of PCA components. Including more of these components in a compression allows one to capture more of the detailed correlation patterns exhibited in the data. However, each additional PCA component provides progressively less value as one moves down the ranking -- it is this fact that allows a good compression to be obtained using only a minority of these modes.</p>
<p>Whereas the PCA components directly encode the collective, correlated fluctuations exhibited in our data, the feature selection solution attempts to identify a minimally-redundant subset of the original array's columns -- one that is representative of the full set. This strategy is best understood in the limit where the original columns fall into a set of discreet clusters (in our example, we might expect the businesses operating in a particular sub-sector to fall into a single cluster). In such cases, a good compression is obtained by selecting one representative column from each cluster: Once the representatives are selected, each of the other members of a given cluster can be approximately reconstructed using its selected representative as a predictor. In the above, we see that our automated feature selector has worked well, in that the companies selected ('WDAY', 'PYPL', 'AMZN', 'LRCX', and 'HPQ') each operate in a different part of the tech landscape [3]. In general, we can expect the feature selector to attempt to mimic the PCA approach, in that it will seek columns that fluctuate in a nearly orthogonal manner. However, whereas the PCA components highlight which columns fluctuate together, the feature selector attempts to throw out all but one of the columns that fluctuate together -- a sort-of dual approach.</p>
<h4>Compression strength</h4>
<p>To decide how many compression components are needed for a given application, one need only consider the variance explained as a function of the compression dimension -- this is equal to one minus the average squared error of the projections that result from the compressions (see footnote [4] for a visualization of the error that results from compression here). In the two python packages we're using, one can access these values as follows:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;</span> <span class="n">print</span> <span class="n">np</span><span class="p">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>  
<span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">223</span> <span class="mi">0</span><span class="p">.</span><span class="mi">367</span> <span class="mi">0</span><span class="p">.</span><span class="mi">493</span> <span class="mi">0</span><span class="p">.</span><span class="mi">598</span> <span class="mi">0</span><span class="p">.</span><span class="mi">696</span><span class="p">]</span>

<span class="o">&gt;&gt;</span> <span class="n">print</span> <span class="p">[</span><span class="n">var</span> <span class="o">/</span> <span class="mi">50</span><span class="p">.</span><span class="mi">0</span> <span class="k">for</span> <span class="n">var</span> <span class="k">in</span> <span class="n">selector</span><span class="p">.</span><span class="n">ordered_cods</span><span class="p">[:</span><span class="n">KEEP</span><span class="p">]]</span>  
<span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">169</span> <span class="mi">0</span><span class="p">.</span><span class="mi">316</span> <span class="mi">0</span><span class="p">.</span><span class="mi">428</span> <span class="mi">0</span><span class="p">.</span><span class="mi">530</span> <span class="mi">0</span><span class="p">.</span><span class="mi">612</span><span class="p">]</span>  
</pre></div>


<p>The printed lines above show that both algorithms capture more than <span class="math">\(50%\)</span> of the variance exhibited in the data using only 4 of the 50 stocks. The PCA compressions are stronger in each dimension because PCA is unconstrained -- it can use any linear combination of the initial features for compression components, whereas the feature selector is constrained to use a subset of the original features.</p>
<p>A plot of the values above across all compression dimensions is shown in Fig. 3 below. Looking at this plot, we see an elbow somewhere between <span class="math">\(5\)</span> and <span class="math">\(10\)</span> retained components. This implies that our <span class="math">\(50\)</span>-dimensional data set mostly lies within a subspace of dimension <span class="math">\(k \in (5, 10)\)</span>. Using any <span class="math">\(k\)</span> in that interval will provide a decent compression, and a satisfying large dimensional reduction -- a typical result of applying these algorithms to large, raw data sets. Again, this is useful because it allows one to stop tracking redundant columns that offers little incremental value.</p>
<p><a href="./wp-content/uploads/2018/06/cod_stocks.png"><img alt="cod_stocks" src="./wp-content/uploads/2018/06/cod_stocks.png"></a> <strong>Fig. 3</strong>. Plots of the compression strength (coefficient of determination or <span class="math">\(r^2\)</span>) for our two compression algorithms versus compression dimension. We see two things: (1) PCA gives a slightly stronger compression at each dimension, and (2) The full data set spans 50 dimensions, but the elbow in the plots suggests the data largely sits in a subspace having dimension between 5 to 10.</p>
<h3>G20 economic summary stats</h3>
<h4>Loading and compressing the data</h4>
<p>In this section, we explore economic summary statistics on the 19 individual countries belonging to the G20 [5]. We scraped this data from data.un.org -- for example, the link used for the United States can be found <a href="http://data.un.org/en/iso/us.html">here</a>. Our aim here will be to illustrate how compression algorithms can be used to aid in the visualization of a data set: Plotting the rows of a data set allows one to quickly get a sense for the relationship between them (here, the different G20 countries). Because we cannot plot in more than two or three dimensions, compression is a necessary first step in this process.</p>
<p>A sample row from our data set is given below -- the values for Argentina.</p>
<p>[code language="text"]<br>
GDP growth rate(annual %, const. 2005 prices) 2.40<br>
GDP per capita(current US<span class="math">\() 14564  
Economy: Agriculture(% of GVA) 6  
Economy: Industry(% of GVA) 27.8  
Economy: Services and other activity(% of GVA) 66.2  
Employment: Agriculture(% of employed) 2  
Employment: Industry(% of employed) 24.8  
Employment: Services(% of employed) 73.1  
Unemployment(% of labour force) 6.5  
CPI: Consumer Price Index(2000=100) 332  
Agricultural production index(2004-2006=100) 119  
Food production index(2004-2006=100) 119  
International trade: Exports(million US\)</span>) / GPV 0.091<br>
International trade: Imports(million US$) / GPV 0.088<br>
Balance of payments, current account / GPV -0.025<br>
Labour force participation(female) pop. %) 48.6<br>
Labour force participation(male) pop. %) 74.4  </p>
<div class="highlight"><pre><span></span><span class="err">Comparing each of the 19 countries across these 17 fields would be a complicated task. However, by considering a plot like Fig. 3 for this data set, we learned that many of these fields are highly correlated (plot not shown). This means that we can indeed get a reasonable, approximate understanding of the relationship between these economies by compressing down to two dimensions and plotting the result. The code to obtain these compressions follows:</span>
</pre></div>


<p>from linselect import FwdSelect<br>
from sklearn.decomposition import PCA<br>
from sklearn.preprocessing import StandardScaler<br>
import pandas as pd</p>
<h1>LOADING THE DATA</h1>
<p>df = pd.read_csv('g20.csv', index_col=0)<br>
X = df.values<br>
countries = df.index.values</p>
<h1>FEATURE SELECTION</h1>
<p>selector = FwdSelect()<br>
selector.fit(X)<br>
x1, y1 = X[:, selector.ordered_features[:2]].T</p>
<h1>PRINCIPAL COMPONENT ANALYSIS</h1>
<p>pca = PCA()<br>
s = StandardScaler()<br>
x2, y2 = pca.fit_transform(s.fit_transform(X)).T[:2]<br>
```<br>
The plots of the <span class="math">\((x_1, y_1)\)</span> and <span class="math">\((x_2, y_2)\)</span> compressions obtained above are given in Fig. 4.</p>
<h4>Visualizing and interpreting the compressed data</h4>
<p>The first thing to note about Fig. 4 is that the geometries of the upper (feature selection) and lower (PCA) plots are very similar -- the neighbors of each country are the same in the two plots. As we know from our discussion above, the first two PCA components must give a stronger compressed representation of the data than is obtained from the feature selection solution. However, given that similar country relationships are suggested by the two plots, the upper, feature selection view might be preferred. <em>This is because its axes retain their original meaning and are relatively easy to interpret</em>: The y-axis is a measure of the relative scale of international trade within each of the individual economies and the x-axis is a measure of the internal makeup of the economies.</p>
<p>Examining the upper, feature selection plot of Fig. 4, a number of interesting insights can be found. One timely observation: International trade exports are a lower percentage of GDP for the US than for any other country considered (for imports, it is third, just after Argentina and Brazil). This observation might be related to the US administration's recent willingness to engage in trading tariff increases with other countries. Nations in the same quadrant include Great Britain (gb), Japan (jp), and Australia (au) -- each relatively industrialized and geographically isolated nations. In the opposite limits, we have Germany (de) and India (in). The former is relatively industrial and not isolated, while the latter's economy weights agriculture relatively highly.</p>
<h4>Summary</h4>
<p>In this section, we illustrated a general analysis method that allows one to quickly gain insight into a data set: Visual study of the compressed data via a plot. Using this approach, we first found here that the G20 nations are best differentiated economically by considering how important international trade is to their economies and also the makeup of their economies (agricultural or other) -- i.e., these are the two features that best explain the full data set of 17 columns that we started with. Plotting the data across these two variables and considering the commonalities of neighboring countries, we were able to identify some natural hypotheses influencing the individual economies. Specifically, geography appears to inform at least one of their key characteristics: more isolated countries often trade less. This is an interesting insight, and one that is quickly arrived at through the compression / plotting strategy.</p>
<p><a href="./wp-content/uploads/2018/06/pca_linselect_g20.jpg"><img alt="pca_linselect_g20" src="./wp-content/uploads/2018/06/pca_linselect_g20.jpg"></a><br>
<strong>Fig. 4</strong>. Plots of the compressed economic summary statistics on the G20 nations, taken from data.un.org: <code>linselect</code> unsupervised feature selection (upper) and PCA (lower).</p>
<h2>Discussion</h2>
<p>In this post, we have seen that carrying out compressions on a data set can provide insight into the original data. By examining the PCA components, we gain access to the collective fluctuations present within the data. The feature selection solution returns a minimal subset of the original features that captures the broad stroke information contained in the original full set -- in cases where clusters are present, the minimal set contains a representative from each. Both methods allow one to determine the effective dimension of a given data set -- when applied to raw data sets, this is often much lower than the apparent dimension due to heavy redundancy.</p>
<p>In general, compressing a data set down into lower dimensions will make the data easier to interpret. We saw in this in the second, G20 economic example above, where a feature set was originally provided that had many columns. Compressing this down into two-dimensions quickly gave us a sense of the relationships between the different economies. The PCA and feature selection solutions gave similar plots there, but the feature selection solution had the extra benefit of providing easily interpreted axes.</p>
<p>When one’s goal is to use compression for operational efficiency gains, the appropriate dimension can be identified by plotting the variance explained versus compression dimension. Because PCA is unconstrained, it will give a stronger compression at any dimension. However, the feature selection approach has its own operational advantages: Once a representative subset of features has been identified, one can often simply stop tracking the others. Doing this can result in a huge cost savings for large data pipelines. A similar savings is not possible for PCA, because evaluation of the PCA components requires one to first evaluate each of the original feature / column values for a given data point. A similar consideration is also important in some applications: For example, when developing a stock portfolio, transaction costs may make it prohibitively expensive to purchase all of the stocks present in a given sector. By purchasing only a representative subset, a minimal portfolio can be constructed without incurring a substantial transaction cost burden.</p>
<p>In summary, the two compression methods we have considered here are very similar, but subtly different. Appreciating these differences allows one to choose the best approach for a given application.</p>
<h2>Appendix: Compression as projection</h2>
<p>We can see that the composite linear compression-decompression operator is a projection operator as follows: If <span class="math">\(X\)</span> is our data array, the general equations describing compression and decompression are,<br>
</p>
<div class="math">\begin{eqnarray}  
\label{A1} \tag{A1}  
X_{compressed} &amp;=&amp; X \cdot M_{compression} \\  
\label{A2} \tag{A2}  
X_{approx} &amp;=&amp; X_{compressed} \cdot M_{decompression}.  
\end{eqnarray}</div>
<p><br>
Here, <span class="math">\(M_{compression}\)</span> is an <span class="math">\(n \times k\)</span> matrix and <span class="math">\(M_{decompression}\)</span> is a <span class="math">\(k \times n\)</span> matrix. The squared error of the approximation is,<br>
</p>
<div class="math">\begin{eqnarray}  
\Lambda &amp;=&amp; \sum_{i,j} \left (X_{ij} - X_{approx, ij}\right)^2 \\  
&amp;=&amp; \sum_j \Vert X_j - X_{compressed} \cdot M_{decompression, j} \Vert^2. \label{A3} \tag{A3}  
\end{eqnarray}</div>
<p><br>
This second line here shows that we can minimize the entire squared error by minimizing each of the column squared errors independently. Further, each of the column level minimizations is equivalent to a least-squares linear regression problem: We treat the column vector <span class="math">\(M_{compressions, j}\)</span> as an unknown coefficient vector, and attempt to set these so that the squared error of the fit to <span class="math">\(X_j\)</span> -- using the columns of <span class="math">\(X_{compressed}\)</span> as features -- is minimized. We've worked out the least-squares linear fit solution in <a href="http://efavdb.com/linear-regression/">another post</a> (it's also a well-known result). Plugging this result in, we get the optimal <span class="math">\(M_{decompression}\)</span>,<br>
</p>
<div class="math">\begin{eqnarray} \label{A4}  
M_{decompression}^* &amp;=&amp; \left ( X_{compressed}^T X_{compressed} \right)^{-1} X_{compressed}^T X \tag{A4}  
\\  
&amp;=&amp; \left ( M_{compression}^T X^T X M_{compression} \right)^{-1} M_{compression}^T X^T X.  
\end{eqnarray}</div>
<p><br>
To obtain the second line here, we have used (\ref{A1}), the definition of <span class="math">\(X_{compressed}\)</span>.</p>
<p>What happens if we try to compress our approximate matrix a second time? Nothing: The matrix product <span class="math">\(M_{compression} M_{decompression}^*\)</span> is a projection operator. That is, it satisfies the condition<br>
</p>
<div class="math">\begin{eqnarray}  
(M_{compression} M_{decompression}^*)^2 = M_{compression} M_{decompression}^*. \label{A5} \tag{A5}  
\end{eqnarray}</div>
<p><br>
This result is easy enough to confirm using (\ref{A4}). What (\ref{A5}) means geometrically is that our compression operator projects a point in <span class="math">\(n\)</span>-dimensional space onto a subspace of dimension <span class="math">\(k\)</span>. Once a point sits in this subspace, hitting the point with the composite operator has no effect, as the new point already sits in the projected subspace. This is consistent with our 2-d cartoon depicting the effect of PCA and <code>linselect</code>, above. However, this is also true for general choices of <span class="math">\(M_{compression}\)</span>, provided we use the optimal <span class="math">\(M_{decompression}\)</span> associated with it.</p>
<h2>Footnotes</h2>
<p>[1] For a discussion on how PCA selects its <span class="math">\(k\)</span> components, see our prior <a href="http://efavdb.com/principal-component-analysis/">post</a> on the topic. To identify good feature subsets, <code>linselect</code> uses the stepwise selection strategy. This is described in its <a href="https://github.com/EFavDB/linselect">readme</a>. Here, we simply use the forward selection approach, but <code>linselect</code> supports fairly general stepwise search protocols.</p>
<p>[2] The tickers included are: AAPL, ADBE, ADP, ADSK, AMAT, AMZN, ASML, ATVI, AVGO, BABA, BIDU, CRM, CSCO, CTSH, EA, FB, GOOG, GPRO, HPE, HPQ, IBM, INFY, INTC, INTU, ITW, LRCX, MSFT, NFLX, NOK, NVDA, NXPI, OMC, ORCL, PANW, PYPL, QCOM, SAP, SNAP, SQ, SYMC, T, TSLA, TSM, TWTR, TXN, VMW, VZ, WDAY, WDC, and ZNGA.</p>
<p>[3] Workday (WDAY) is a SaaS company that offers a product to businesses, Paypal (PYPL) is a company that provides payments infrastructure supporting e-commerce, Amazon (AMZN) is an e-commerce company, Lam Research (LRCX) makes chips, and Hewlett-Packard (HPQ) makes computers. Each of these are representatives of a different sub-sector.</p>
<p>[4] We can also get a sense of the compression error by plotting the compressed traces for one of the stocks. <a href="./wp-content/uploads/2018/06/sq.png"><img alt="sq" src="./wp-content/uploads/2018/06/sq.png"></a> The plot at right does this for Square inc. The ups and downs of SQ are largely captured by both methods. However, some refined details are lost in the compressions. Similar accuracy levels are seen for each of the other stocks in the full set (not shown here).</p>
<p>[5] The missing twentieth member of the G20 is the EU. We don't consider the EU here simply because the site we scraped from does not have a page dedicated to it.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </section>

            <section class="post-info">
                <div class="post-share">
                    <a class="twitter" href="https://twitter.com/share?text=Linear compression in python: PCA vs unsupervised feature selection&amp;url=./unsupervised-feature-selection-in-python-with-linselect.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="ic ic-twitter"></i><span class="hidden">Twitter</span>
                    </a>
                    <a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=./unsupervised-feature-selection-in-python-with-linselect.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="ic ic-facebook"></i><span class="hidden">Facebook</span>
                    </a>
                    <a class="googleplus" href="https://plus.google.com/share?url=./unsupervised-feature-selection-in-python-with-linselect.html" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <i class="ic ic-googleplus"></i><span class="hidden">Google+</span>
                    </a>
                    <div class="clear"></div>
                </div>

                <aside class="post-tags">
<a href="./tag/linselect.html">linselect</a><a href="./tag/methods.html">methods</a><a href="./tag/python.html">python</a>                </aside>

                <div class="clear"></div>

                <aside class="post-author">


                        <figure class="post-author-avatar">
                            <img src=".//wp-content/uploads/2014/12/JonathanLinkedIn.jpg" alt="Jonathan Landy" />
                        </figure>
                    <div class="post-author-bio">
                        <h4 class="post-author-name"><a href="./author/jonathan-landy.html">Jonathan Landy</a></h4>
                            <p class="post-author-about">Jonathan grew up in the midwest and then went to school at Caltech and UCLA. Following this, he did two postdocs, one at UCSB and one at UC Berkeley.  His academic research focused primarily on applications of statistical mechanics, but his professional passion has always been in the mastering, development, and practical application of slick math methods/tools. He worked as a data-scientist at Square for four years and is now working on a quantitative investing startup.</p>
                        <!-- Social linkes in alphabet order. -->
                    </div>
                    <div class="clear"></div>
                </aside>

                </section>


                <aside class="post-nav">
                    <a class="post-nav-next" href="./the-speed-of-traffic.html">
                        <section class="post-nav-teaser">
                            <i class="ic ic-arrow-left"></i>
                                <h2 class="post-nav-title">The speed of traffic</h2>
                            <p class="post-nav-excerpt">We use a simple argument to estimate the speed of traffic on a highway as a function...</p>
                        </section>
                    </a>
                    <a class="post-nav-prev" href="./linselect-demo.html">
                        <section class="post-nav-teaser">
                            <i class="ic ic-arrow-right"></i>
                                <h2 class="post-nav-title">linselect demo: a tech sector stock analysis</h2>
                            <p class="post-nav-excerpt">This is a tutorial post relating to our python feature selection package, linselect....</p>
                        </section>
                    </a>
                    <div class="clear"></div>
                </aside>

            </div>
        </article>
    </main>
      <!-- TODO : Body class -->
    <div id="body-class" style="display: none;" class=""></div>

    <footer id="footer">
      <div class="inner">
        <section class="credits">
        </section>
      </div>
    </footer>
  </section>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script type="text/javascript" src="./theme/js/script.js"></script>

</body>
</html>