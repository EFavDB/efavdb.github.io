<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Jonathan Landy" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="linselect, methods, python, linselect, " />

<meta property="og:title" content="Linear compression in python: PCA vs unsupervised feature selection "/>
<meta property="og:url" content="./unsupervised-feature-selection-in-python-with-linselect.html" />
<meta property="og:description" content="We illustrate the application of two linear compression algorithms in python: Principal component analysis (PCA) and least-squares feature selection. Both can be used to compress a passed array, and they both work by stripping out redundant columns from the array. The two differ in that PCA operates in a particular …" />
<meta property="og:site_name" content="EFAVDB" />
<meta property="og:article:author" content="Jonathan Landy" />
<meta property="og:article:published_time" content="2018-08-11T07:30:00-07:00" />
<meta name="twitter:title" content="Linear compression in python: PCA vs unsupervised feature selection ">
<meta name="twitter:description" content="We illustrate the application of two linear compression algorithms in python: Principal component analysis (PCA) and least-squares feature selection. Both can be used to compress a passed array, and they both work by stripping out redundant columns from the array. The two differ in that PCA operates in a particular …">

        <title>Linear compression in python: PCA vs unsupervised feature selection  · EFAVDB
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Fira+Sans:300,400,700" rel="stylesheet" type='text/css' />
        <link href="https://fonts.googleapis.com/css?family=Cardo" rel="stylesheet" type='text/css' />        
        <link rel="stylesheet" type="text/css" href="./theme/css/elegant.prod.css" media="screen">
        <link rel="stylesheet" type="text/css" href="./theme/css/custom.css" media="screen">



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="./"><span class=site-name>EFAVDB</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       .
                                    >Home</a>
                                </li>
                                <li ><a href="./pages/authors.html">Authors</a></li>
                                <li ><a href="./categories.html">Categories</a></li>
                                <li ><a href="./tags.html">Tags</a></li>
                                <li ><a href="./archives.html">Archives</a></li>
                                <li><form class="navbar-search" action="./search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="./unsupervised-feature-selection-in-python-with-linselect.html">
                Linear compression in python: <span class="caps">PCA</span> vs unsupervised feature&nbsp;selection
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <p>We illustrate the application of two linear compression algorithms in python: Principal component analysis (<span class="caps">PCA</span>) and least-squares feature selection. Both can be used to compress a passed array, and they both work by stripping out redundant columns from the array. The two differ in that <span class="caps">PCA</span> operates in a particular rotated frame, while the feature selection solution operates directly on the original columns. As we illustrate below, <span class="caps">PCA</span> always gives a stronger compression. However, the feature selection solution is often comparably strong, and its output has the benefit of being relatively easy to interpret &#8212; a virtue that is important for many&nbsp;applications.</p>
<p>We use our python package <code>linselect</code> to carry out efficient feature selection-based compression below &#8212; this is available on pypi (<code>pip install linselect</code>) and <a href="https://github.com/EFavDB/linselect">GitHub</a>.</p>
<h2>Linear compression&nbsp;algorithms</h2>
<p><a href="./wp-content/uploads/2018/06/simple_line.jpg"><img alt="simple_line" src="./wp-content/uploads/2018/06/simple_line.jpg"></a></p>
<p>To compress a data array having <span class="math">\(n\)</span> columns, linear compression algorithms begin by fitting a <span class="math">\(k\)</span>-dimensional line, or <em>hyperplane</em>, to the data (with <span class="math">\(k &lt; n\)</span>). Any point in the hyperplane can be uniquely identified using a basis of <span class="math">\(k\)</span> components. Marking down each point&#8217;s projected location in the hyperplane using these components then gives a <span class="math">\(k\)</span>-column, compressed representation of the data. This idea is illustrated in Fig. 1 at right, where a line is fit to some two-component data. Projecting the points onto the line and then marking down how far along the line each projected point sits, we obtain a one-column compression. Carrying out this process can be useful if storage space is at a premium or if any operations need to be applied to the array (usually operations will run much faster on the compressed format). Further, compressed data is often easier to interpret and visualize, thanks to its reduced&nbsp;dimension.</p>
<p>In this post, we consider two automated linear compression algorithms: principal component analysis (<span class="caps">PCA</span>) and least-squares unsupervised feature selection. These differ because they are obtained from different hyperplane fitting strategies: The <span class="caps">PCA</span> approach is obtained from the <span class="math">\(k\)</span>-dimensional hyperplane fit that minimizes the data&#8217;s total squared-projection error. In general, the independent variables of this fit &#8212; i.e., the <span class="math">\(k\)</span> components specifying locations in the fit plane &#8212; end up being some linear combinations of the original <span class="math">\(x_i\)</span><span class="quo">&#8216;</span>s. In contrast, the feature selection strategy intelligently picks a subset of the original array columns as predictors and then applies the usual least-squares fit to the others for compression [1]. These approaches are illustrated in the left and right panels of Fig. 2 below. The two fit lines there look very similar, but the encodings returned by these strategies differ qualitatively: The 1-d compression returned by <span class="caps">PCA</span> is how far along the <span class="math">\(PCA_1\)</span> direction a point sits (this is some linear combination of <span class="math">\(x_1\)</span> and <span class="math">\(x_2\)</span> &#8212; see figure), while the feature selection solution simply returns each point&#8217;s <span class="math">\(x_1\)</span> value. One of our goals here is to explain why this difference can favor the feature selection approach in certain&nbsp;applications.</p>
<p>Our post proceeds as follows: In the next section, we consider two representative applications in python: (1) The compression of a data set of tech-sector stock price quotes, and (2) the visualization of some economic summary statistics on the G20 nations. Working through these applications, we are able to familiarize ourselves with the output of the two algorithms, and also through contrast to highlight their relative virtues. The discussion section summarizes what we learn. Finally, a short appendix covers some of the formal mathematics of compression. There, we prove that linear compression-decompression operators are always&nbsp;projections.</p>
<p><a href="./wp-content/uploads/2018/06/pca_vs_linselect.jpg"><img alt="pca_vs_linselect" src="./wp-content/uploads/2018/06/pca_vs_linselect.jpg"></a><br>
<strong>Fig. 2</strong>. A cartoon illustrating the projection that results when applying <span class="caps">PCA</span> (left) and unsupervised feature selection &#8212; via <code>linselect</code> (right): The original 2-d big dots are replaced by their small dot, effectively-1-d approximations &#8212; a&nbsp;projection.</p>
<h2>Applications</h2>
<p>Both data sets explored below are available on our Github, <a href="https://github.com/EFavDB/linselect_demos">here</a>.</p>
<h3>Stock&nbsp;prices</h3>
<h4>Loading and compressing the&nbsp;data</h4>
<p>In this section, we apply our algorithms to a prepared data set of one year&#8217;s worth of daily percentage price lifts on 50 individual tech stocks [2]. We expect these stocks to each be governed by a common set of market forces, motivating the idea that a substantial compression might be possible. This is true, and the compressed arrays that result may be more efficiently operated on, as noted above. In addition, we&#8217;ll see below that we can learn something about the full data set by examining the compression&nbsp;outputs.</p>
<p>The code below loads our data, smooths it over a running 30 day window (to remove idiosyncratic noise that is not of much interest), prints out the first three rows, compresses the data using our two methods, and then finally prints out the first five <span class="caps">PCA</span> components and the top five selected&nbsp;stocks.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>  
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>  
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>  
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>  
<span class="kn">from</span> <span class="nn">linselect</span> <span class="kn">import</span> <span class="n">FwdSelect</span>

<span class="c1"># CONSTANTS  </span>
<span class="n">KEEP</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># compression dimension  </span>
<span class="n">WINDOW_SIZE</span> <span class="o">=</span> <span class="mi">30</span> <span class="c1"># smoothing window size</span>

<span class="c1"># LOAD AND SMOOTH THE DATA  </span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;stocks.csv&#39;</span><span class="p">)</span>  
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">WINDOW_SIZE</span><span class="p">)</span><span class="o">.</span><span class="kp">mean</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">WINDOW_SIZE</span><span class="p">:]</span>  
<span class="nb">print</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>  
<span class="n">TICKERS</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span>  
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># PCA COMPRESSION  </span>
<span class="n">s</span> <span class="o">=</span> <span class="n">StandardScalar</span><span class="p">()</span>  
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">KEEP</span><span class="p">)</span>  
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>  
<span class="n">X_compressed_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># FEATURE SELECTION COMPRESSION  </span>
<span class="n">selector</span> <span class="o">=</span> <span class="n">FwdSelect</span><span class="p">()</span>  
<span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  
<span class="n">X_compressed_linselect</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">selector</span><span class="o">.</span><span class="n">ordered_features</span><span class="p">[:</span><span class="n">KEEP</span><span class="p">]]</span>

<span class="c1"># PRINT OUT FIRST FIVE PCA COMPONENTs, TOP FIVE STOCKS  </span>
<span class="nb">print</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[:</span><span class="n">KEEP</span><span class="p">]</span>  
<span class="nb">print</span> <span class="n">TICKERS</span><span class="p">[</span><span class="n">selector</span><span class="o">.</span><span class="n">ordered_features</span><span class="p">][:</span><span class="n">KEEP</span><span class="p">]</span>  
</pre></div>


<p>The output of the above print&nbsp;statements:</p>
<div class="highlight"><pre><span></span><span class="o">#</span> <span class="n">The</span> <span class="k">first</span> <span class="n">three</span> <span class="k">rows</span> <span class="k">of</span> <span class="n">the</span> <span class="k">data</span> <span class="n">frame</span><span class="p">:</span>  
<span class="nb">date</span> <span class="n">AAPL</span> <span class="n">ADBE</span> <span class="n">ADP</span> <span class="n">ADSK</span> <span class="n">AMAT</span> <span class="n">AMZN</span> <span class="err">\</span>  
<span class="mi">30</span> <span class="mi">2017</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">31</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002821</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002994</span> <span class="mi">0</span><span class="p">.</span><span class="mi">000248</span> <span class="mi">0</span><span class="p">.</span><span class="mi">009001</span> <span class="mi">0</span><span class="p">.</span><span class="mi">006451</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003237</span>  
<span class="mi">31</span> <span class="mi">2017</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">01</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003035</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002776</span> <span class="mi">0</span><span class="p">.</span><span class="mi">000522</span> <span class="mi">0</span><span class="p">.</span><span class="mi">008790</span> <span class="mi">0</span><span class="p">.</span><span class="mi">005487</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003450</span>  
<span class="mi">32</span> <span class="mi">2017</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">02</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003112</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002964</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">000560</span> <span class="mi">0</span><span class="p">.</span><span class="mi">008573</span> <span class="mi">0</span><span class="p">.</span><span class="mi">005523</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003705</span>

<span class="n">ASML</span> <span class="n">ATVI</span> <span class="n">AVGO</span> <span class="p">...</span> <span class="n">T</span> <span class="n">TSLA</span> <span class="n">TSM</span> <span class="err">\</span>  
<span class="mi">30</span> <span class="mi">0</span><span class="p">.</span><span class="mi">000755</span> <span class="mi">0</span><span class="p">.</span><span class="mi">005933</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003988</span> <span class="p">...</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">001419</span> <span class="mi">0</span><span class="p">.</span><span class="mi">004500</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003590</span>  
<span class="mi">31</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002174</span> <span class="mi">0</span><span class="p">.</span><span class="mi">006369</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003225</span> <span class="p">...</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">001125</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003852</span> <span class="mi">0</span><span class="p">.</span><span class="mi">004279</span>  
<span class="mi">32</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001566</span> <span class="mi">0</span><span class="p">.</span><span class="mi">006014</span> <span class="mi">0</span><span class="p">.</span><span class="mi">005343</span> <span class="p">...</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">001216</span> <span class="mi">0</span><span class="p">.</span><span class="mi">004130</span> <span class="mi">0</span><span class="p">.</span><span class="mi">004358</span>

<span class="n">TWTR</span> <span class="n">TXN</span> <span class="n">VMW</span> <span class="n">VZ</span> <span class="n">WDAY</span> <span class="n">WDC</span> <span class="n">ZNGA</span>  
<span class="mi">30</span> <span class="mi">0</span><span class="p">.</span><span class="mi">008292</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001467</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001984</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">001741</span> <span class="mi">0</span><span class="p">.</span><span class="mi">006103</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002916</span> <span class="mi">0</span><span class="p">.</span><span class="mi">007811</span>  
<span class="mi">31</span> <span class="mi">0</span><span class="p">.</span><span class="mi">008443</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001164</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002026</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">001644</span> <span class="mi">0</span><span class="p">.</span><span class="mi">006303</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003510</span> <span class="mi">0</span><span class="p">.</span><span class="mi">008379</span>  
<span class="mi">32</span> <span class="mi">0</span><span class="p">.</span><span class="mi">007796</span> <span class="mi">0</span><span class="p">.</span><span class="mi">000637</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001310</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">001333</span> <span class="mi">0</span><span class="p">.</span><span class="mi">006721</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002836</span> <span class="mi">0</span><span class="p">.</span><span class="mi">008844</span>

<span class="o">#</span> <span class="n">PCA</span> <span class="n">top</span> <span class="n">components</span><span class="p">:</span>  
<span class="p">[[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">10548148</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">20601986</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">0126039</span> <span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">20139121</span><span class="p">,</span> <span class="p">...],</span>  
<span class="p">[</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">11739195</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">02536787</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">2044143</span> <span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">08462741</span><span class="p">,</span> <span class="p">...],</span>  
<span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">03251305</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">10796197</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">00463919</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">17564998</span><span class="p">,</span> <span class="p">...],</span>  
<span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">08678107</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">1931497</span> <span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">16850867</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">16260134</span><span class="p">,</span> <span class="p">...],</span>  
<span class="p">[</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">0174396</span> <span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="mi">01174769</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">11617622</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">01036602</span><span class="p">,</span> <span class="p">...]]</span>

<span class="o">#</span> <span class="n">Feature</span> <span class="n">selector</span> <span class="k">output</span><span class="p">:</span>  
<span class="p">[</span><span class="s1">&#39;WDAY&#39;</span><span class="p">,</span> <span class="s1">&#39;PYPL&#39;</span><span class="p">,</span> <span class="s1">&#39;AMZN&#39;</span><span class="p">,</span> <span class="s1">&#39;LRCX&#39;</span><span class="p">,</span> <span class="s1">&#39;HPQ&#39;</span><span class="p">]</span>  
</pre></div>


<p>Lines 22 and 27 in the first code block above are the two compressed versions of the original data array, line 16. For each row, the first compression stores the amplitude of that date&#8217;s stock changes along each of the first five <span class="caps">PCA</span> components (printed below line 17 of second code block), while the second compression is simply equal to the five columns of the original array corresponding to the stocks picked out by the selector (printed below line 24 of the second code&nbsp;block).</p>
<h4>Exploring the&nbsp;encodings</h4>
<p>Working with the compressed arrays obtained above provides some immediate operational benefits: Manipulations of the compressed arrays can be carried out more quickly and they require less memory for storage. Here, we review how valuable insight can also obtained from our compressions &#8212; via study of the compression&nbsp;components.</p>
<p>First, we consider the <span class="caps">PCA</span> components. It turns out that these components are the eigenvectors of the correlation matrix of our data set (<span class="math">\(X^T \cdot X\)</span>) &#8212; that is, they are the collective, fluctuation modes present in the data set (for those who have studied classical mechanics, you can imagine the system as one where the different stocks are masses that are connected by springs, and these eigenvectors are the modes of the system). Using this fact, one can show that the components evolve in an uncorrelated manner. Further, one can show that projecting the data set down onto the top <span class="math">\(k\)</span> modes gives the minimum squared projection error of all possible <span class="math">\(k\)</span>-component projections. The first component then describes the largest amplitude fluctuation pattern exhibited in the data. From line 18 above, this is $[ 0.105, 0.206, -0.012, 0.201, &#8230; $]. These coefficients tell us that when the first stock (<span class="caps">AAPL</span>) goes up by some amount, the second (<span class="caps">ADBE</span>) typically goes up by about twice as much (this follows from fact that 0.206 is about twice as big as 0.105), etc. This isn&#8217;t the full story of course, because each day&#8217;s movements are a superposition (sum) of the amplitudes along each of <span class="caps">PCA</span> components. Including more of these components in a compression allows one to capture more of the detailed correlation patterns exhibited in the data. However, each additional <span class="caps">PCA</span> component provides progressively less value as one moves down the ranking &#8212; it is this fact that allows a good compression to be obtained using only a minority of these&nbsp;modes.</p>
<p>Whereas the <span class="caps">PCA</span> components directly encode the collective, correlated fluctuations exhibited in our data, the feature selection solution attempts to identify a minimally-redundant subset of the original array&#8217;s columns &#8212; one that is representative of the full set. This strategy is best understood in the limit where the original columns fall into a set of discreet clusters (in our example, we might expect the businesses operating in a particular sub-sector to fall into a single cluster). In such cases, a good compression is obtained by selecting one representative column from each cluster: Once the representatives are selected, each of the other members of a given cluster can be approximately reconstructed using its selected representative as a predictor. In the above, we see that our automated feature selector has worked well, in that the companies selected (&#8216;<span class="caps">WDAY</span>&#8217;, &#8216;<span class="caps">PYPL</span>&#8217;, &#8216;<span class="caps">AMZN</span>&#8217;, &#8216;<span class="caps">LRCX</span>&#8217;, and &#8216;<span class="caps">HPQ</span>&#8217;) each operate in a different part of the tech landscape [3]. In general, we can expect the feature selector to attempt to mimic the <span class="caps">PCA</span> approach, in that it will seek columns that fluctuate in a nearly orthogonal manner. However, whereas the <span class="caps">PCA</span> components highlight which columns fluctuate together, the feature selector attempts to throw out all but one of the columns that fluctuate together &#8212; a sort-of dual&nbsp;approach.</p>
<h4>Compression&nbsp;strength</h4>
<p>To decide how many compression components are needed for a given application, one need only consider the variance explained as a function of the compression dimension &#8212; this is equal to one minus the average squared error of the projections that result from the compressions (see footnote [4] for a visualization of the error that results from compression here). In the two python packages we&#8217;re using, one can access these values as&nbsp;follows:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;</span> <span class="n">print</span> <span class="n">np</span><span class="p">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>  
<span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">223</span> <span class="mi">0</span><span class="p">.</span><span class="mi">367</span> <span class="mi">0</span><span class="p">.</span><span class="mi">493</span> <span class="mi">0</span><span class="p">.</span><span class="mi">598</span> <span class="mi">0</span><span class="p">.</span><span class="mi">696</span><span class="p">]</span>

<span class="o">&gt;&gt;</span> <span class="n">print</span> <span class="p">[</span><span class="n">var</span> <span class="o">/</span> <span class="mi">50</span><span class="p">.</span><span class="mi">0</span> <span class="k">for</span> <span class="n">var</span> <span class="k">in</span> <span class="n">selector</span><span class="p">.</span><span class="n">ordered_cods</span><span class="p">[:</span><span class="n">KEEP</span><span class="p">]]</span>  
<span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">169</span> <span class="mi">0</span><span class="p">.</span><span class="mi">316</span> <span class="mi">0</span><span class="p">.</span><span class="mi">428</span> <span class="mi">0</span><span class="p">.</span><span class="mi">530</span> <span class="mi">0</span><span class="p">.</span><span class="mi">612</span><span class="p">]</span>  
</pre></div>


<p>The printed lines above show that both algorithms capture more than <span class="math">\(50%\)</span> of the variance exhibited in the data using only 4 of the 50 stocks. The <span class="caps">PCA</span> compressions are stronger in each dimension because <span class="caps">PCA</span> is unconstrained &#8212; it can use any linear combination of the initial features for compression components, whereas the feature selector is constrained to use a subset of the original&nbsp;features.</p>
<p>A plot of the values above across all compression dimensions is shown in Fig. 3 below. Looking at this plot, we see an elbow somewhere between <span class="math">\(5\)</span> and <span class="math">\(10\)</span> retained components. This implies that our <span class="math">\(50\)</span>-dimensional data set mostly lies within a subspace of dimension <span class="math">\(k \in (5, 10)\)</span>. Using any <span class="math">\(k\)</span> in that interval will provide a decent compression, and a satisfying large dimensional reduction &#8212; a typical result of applying these algorithms to large, raw data sets. Again, this is useful because it allows one to stop tracking redundant columns that offers little incremental&nbsp;value.</p>
<p><a href="./wp-content/uploads/2018/06/cod_stocks.png"><img alt="cod_stocks" src="./wp-content/uploads/2018/06/cod_stocks.png"></a> <strong>Fig. 3</strong>. Plots of the compression strength (coefficient of determination or <span class="math">\(r^2\)</span>) for our two compression algorithms versus compression dimension. We see two things: (1) <span class="caps">PCA</span> gives a slightly stronger compression at each dimension, and (2) The full data set spans 50 dimensions, but the elbow in the plots suggests the data largely sits in a subspace having dimension between 5 to&nbsp;10.</p>
<h3>G20 economic summary&nbsp;stats</h3>
<h4>Loading and compressing the&nbsp;data</h4>
<p>In this section, we explore economic summary statistics on the 19 individual countries belonging to the G20 [5]. We scraped this data from data.un.org &#8212; for example, the link used for the United States can be found <a href="http://data.un.org/en/iso/us.html">here</a>. Our aim here will be to illustrate how compression algorithms can be used to aid in the visualization of a data set: Plotting the rows of a data set allows one to quickly get a sense for the relationship between them (here, the different G20 countries). Because we cannot plot in more than two or three dimensions, compression is a necessary first step in this&nbsp;process.</p>
<p>A sample row from our data set is given below &#8212; the values for&nbsp;Argentina.</p>
<p>[code language=&#8221;text&#8221;]<br>
<span class="caps">GDP</span> growth rate(annual %, const. 2005 prices) 2.40<br>
<span class="caps">GDP</span> per capita(current <span class="caps">US</span><span class="math">\() 14564  
Economy: Agriculture(% of GVA) 6  
Economy: Industry(% of GVA) 27.8  
Economy: Services and other activity(% of GVA) 66.2  
Employment: Agriculture(% of employed) 2  
Employment: Industry(% of employed) 24.8  
Employment: Services(% of employed) 73.1  
Unemployment(% of labour force) 6.5  
CPI: Consumer Price Index(2000=100) 332  
Agricultural production index(2004-2006=100) 119  
Food production index(2004-2006=100) 119  
International trade: Exports(million US\)</span>) / <span class="caps">GPV</span> 0.091<br>
International trade: Imports(million <span class="caps">US</span>$) / <span class="caps">GPV</span> 0.088<br>
Balance of payments, current account / <span class="caps">GPV</span> -0.025<br>
Labour force participation(female) pop. %) 48.6<br>
Labour force participation(male) pop. %)&nbsp;74.4  </p>
<div class="highlight"><pre><span></span><span class="err">Comparing each of the 19 countries across these 17 fields would be a complicated task. However, by considering a plot like Fig. 3 for this data set, we learned that many of these fields are highly correlated (plot not shown). This means that we can indeed get a reasonable, approximate understanding of the relationship between these economies by compressing down to two dimensions and plotting the result. The code to obtain these compressions follows:</span>
</pre></div>


<p>from linselect import FwdSelect<br>
from sklearn.decomposition import <span class="caps">PCA</span><br>
from sklearn.preprocessing import StandardScaler<br>
import pandas as&nbsp;pd</p>
<h1><span class="caps">LOADING</span> <span class="caps">THE</span> <span class="caps">DATA</span></h1>
<p>df = pd.read_csv(&#8216;g20.csv&#8217;, index_col=0)<br>
X = df.values<br>
countries =&nbsp;df.index.values</p>
<h1><span class="caps">FEATURE</span> <span class="caps">SELECTION</span></h1>
<p>selector = FwdSelect()<br>
selector.fit(X)<br>
x1, y1 = X[:,&nbsp;selector.ordered_features[:2]].T</p>
<h1><span class="caps">PRINCIPAL</span> <span class="caps">COMPONENT</span> <span class="caps">ANALYSIS</span></h1>
<p>pca = <span class="caps">PCA</span>()<br>
s = StandardScaler()<br>
x2, y2 = pca.fit_transform(s.fit_transform(X)).T[:2]<br>
&#8220;`<br>
The plots of the <span class="math">\((x_1, y_1)\)</span> and <span class="math">\((x_2, y_2)\)</span> compressions obtained above are given in Fig.&nbsp;4.</p>
<h4>Visualizing and interpreting the compressed&nbsp;data</h4>
<p>The first thing to note about Fig. 4 is that the geometries of the upper (feature selection) and lower (<span class="caps">PCA</span>) plots are very similar &#8212; the neighbors of each country are the same in the two plots. As we know from our discussion above, the first two <span class="caps">PCA</span> components must give a stronger compressed representation of the data than is obtained from the feature selection solution. However, given that similar country relationships are suggested by the two plots, the upper, feature selection view might be preferred. <em>This is because its axes retain their original meaning and are relatively easy to interpret</em>: The y-axis is a measure of the relative scale of international trade within each of the individual economies and the x-axis is a measure of the internal makeup of the&nbsp;economies.</p>
<p>Examining the upper, feature selection plot of Fig. 4, a number of interesting insights can be found. One timely observation: International trade exports are a lower percentage of <span class="caps">GDP</span> for the <span class="caps">US</span> than for any other country considered (for imports, it is third, just after Argentina and Brazil). This observation might be related to the <span class="caps">US</span> administration&#8217;s recent willingness to engage in trading tariff increases with other countries. Nations in the same quadrant include Great Britain (gb), Japan (jp), and Australia (au) &#8212; each relatively industrialized and geographically isolated nations. In the opposite limits, we have Germany (de) and India (in). The former is relatively industrial and not isolated, while the latter&#8217;s economy weights agriculture relatively&nbsp;highly.</p>
<h4>Summary</h4>
<p>In this section, we illustrated a general analysis method that allows one to quickly gain insight into a data set: Visual study of the compressed data via a plot. Using this approach, we first found here that the G20 nations are best differentiated economically by considering how important international trade is to their economies and also the makeup of their economies (agricultural or other) &#8212; i.e., these are the two features that best explain the full data set of 17 columns that we started with. Plotting the data across these two variables and considering the commonalities of neighboring countries, we were able to identify some natural hypotheses influencing the individual economies. Specifically, geography appears to inform at least one of their key characteristics: more isolated countries often trade less. This is an interesting insight, and one that is quickly arrived at through the compression / plotting&nbsp;strategy.</p>
<p><a href="./wp-content/uploads/2018/06/pca_linselect_g20.jpg"><img alt="pca_linselect_g20" src="./wp-content/uploads/2018/06/pca_linselect_g20.jpg"></a><br>
<strong>Fig. 4</strong>. Plots of the compressed economic summary statistics on the G20 nations, taken from data.un.org: <code>linselect</code> unsupervised feature selection (upper) and <span class="caps">PCA</span>&nbsp;(lower).</p>
<h2>Discussion</h2>
<p>In this post, we have seen that carrying out compressions on a data set can provide insight into the original data. By examining the <span class="caps">PCA</span> components, we gain access to the collective fluctuations present within the data. The feature selection solution returns a minimal subset of the original features that captures the broad stroke information contained in the original full set &#8212; in cases where clusters are present, the minimal set contains a representative from each. Both methods allow one to determine the effective dimension of a given data set &#8212; when applied to raw data sets, this is often much lower than the apparent dimension due to heavy&nbsp;redundancy.</p>
<p>In general, compressing a data set down into lower dimensions will make the data easier to interpret. We saw in this in the second, G20 economic example above, where a feature set was originally provided that had many columns. Compressing this down into two-dimensions quickly gave us a sense of the relationships between the different economies. The <span class="caps">PCA</span> and feature selection solutions gave similar plots there, but the feature selection solution had the extra benefit of providing easily interpreted&nbsp;axes.</p>
<p>When one’s goal is to use compression for operational efficiency gains, the appropriate dimension can be identified by plotting the variance explained versus compression dimension. Because <span class="caps">PCA</span> is unconstrained, it will give a stronger compression at any dimension. However, the feature selection approach has its own operational advantages: Once a representative subset of features has been identified, one can often simply stop tracking the others. Doing this can result in a huge cost savings for large data pipelines. A similar savings is not possible for <span class="caps">PCA</span>, because evaluation of the <span class="caps">PCA</span> components requires one to first evaluate each of the original feature / column values for a given data point. A similar consideration is also important in some applications: For example, when developing a stock portfolio, transaction costs may make it prohibitively expensive to purchase all of the stocks present in a given sector. By purchasing only a representative subset, a minimal portfolio can be constructed without incurring a substantial transaction cost&nbsp;burden.</p>
<p>In summary, the two compression methods we have considered here are very similar, but subtly different. Appreciating these differences allows one to choose the best approach for a given&nbsp;application.</p>
<h2>Appendix: Compression as&nbsp;projection</h2>
<p>We can see that the composite linear compression-decompression operator is a projection operator as follows: If <span class="math">\(X\)</span> is our data array, the general equations describing compression and decompression are,<br>
</p>
<div class="math">\begin{eqnarray}  
\label{A1} \tag{A1}  
X_{compressed} &amp;=&amp; X \cdot M_{compression} \\  
\label{A2} \tag{A2}  
X_{approx} &amp;=&amp; X_{compressed} \cdot M_{decompression}.  
\end{eqnarray}</div>
<p><br>
Here, <span class="math">\(M_{compression}\)</span> is an <span class="math">\(n \times k\)</span> matrix and <span class="math">\(M_{decompression}\)</span> is a <span class="math">\(k \times n\)</span> matrix. The squared error of the approximation is,<br>
</p>
<div class="math">\begin{eqnarray}  
\Lambda &amp;=&amp; \sum_{i,j} \left (X_{ij} - X_{approx, ij}\right)^2 \\  
&amp;=&amp; \sum_j \Vert X_j - X_{compressed} \cdot M_{decompression, j} \Vert^2. \label{A3} \tag{A3}  
\end{eqnarray}</div>
<p><br>
This second line here shows that we can minimize the entire squared error by minimizing each of the column squared errors independently. Further, each of the column level minimizations is equivalent to a least-squares linear regression problem: We treat the column vector <span class="math">\(M_{compressions, j}\)</span> as an unknown coefficient vector, and attempt to set these so that the squared error of the fit to <span class="math">\(X_j\)</span> &#8212; using the columns of <span class="math">\(X_{compressed}\)</span> as features &#8212; is minimized. We&#8217;ve worked out the least-squares linear fit solution in <a href="http://efavdb.com/linear-regression/">another post</a> (it&#8217;s also a well-known result). Plugging this result in, we get the optimal <span class="math">\(M_{decompression}\)</span>,<br>
</p>
<div class="math">\begin{eqnarray} \label{A4}  
M_{decompression}^* &amp;=&amp; \left ( X_{compressed}^T X_{compressed} \right)^{-1} X_{compressed}^T X \tag{A4}  
\\  
&amp;=&amp; \left ( M_{compression}^T X^T X M_{compression} \right)^{-1} M_{compression}^T X^T X.  
\end{eqnarray}</div>
<p><br>
To obtain the second line here, we have used (\ref{A1}), the definition of <span class="math">\(X_{compressed}\)</span>.</p>
<p>What happens if we try to compress our approximate matrix a second time? Nothing: The matrix product <span class="math">\(M_{compression} M_{decompression}^*\)</span> is a projection operator. That is, it satisfies the condition<br>
</p>
<div class="math">\begin{eqnarray}  
(M_{compression} M_{decompression}^*)^2 = M_{compression} M_{decompression}^*. \label{A5} \tag{A5}  
\end{eqnarray}</div>
<p><br>
This result is easy enough to confirm using (\ref{A4}). What (\ref{A5}) means geometrically is that our compression operator projects a point in <span class="math">\(n\)</span>-dimensional space onto a subspace of dimension <span class="math">\(k\)</span>. Once a point sits in this subspace, hitting the point with the composite operator has no effect, as the new point already sits in the projected subspace. This is consistent with our 2-d cartoon depicting the effect of <span class="caps">PCA</span> and <code>linselect</code>, above. However, this is also true for general choices of <span class="math">\(M_{compression}\)</span>, provided we use the optimal <span class="math">\(M_{decompression}\)</span> associated with&nbsp;it.</p>
<h2>Footnotes</h2>
<p>[1] For a discussion on how <span class="caps">PCA</span> selects its <span class="math">\(k\)</span> components, see our prior <a href="http://efavdb.com/principal-component-analysis/">post</a> on the topic. To identify good feature subsets, <code>linselect</code> uses the stepwise selection strategy. This is described in its <a href="https://github.com/EFavDB/linselect">readme</a>. Here, we simply use the forward selection approach, but <code>linselect</code> supports fairly general stepwise search&nbsp;protocols.</p>
<p>[2] The tickers included are: <span class="caps">AAPL</span>, <span class="caps">ADBE</span>, <span class="caps">ADP</span>, <span class="caps">ADSK</span>, <span class="caps">AMAT</span>, <span class="caps">AMZN</span>, <span class="caps">ASML</span>, <span class="caps">ATVI</span>, <span class="caps">AVGO</span>, <span class="caps">BABA</span>, <span class="caps">BIDU</span>, <span class="caps">CRM</span>, <span class="caps">CSCO</span>, <span class="caps">CTSH</span>, <span class="caps">EA</span>, <span class="caps">FB</span>, <span class="caps">GOOG</span>, <span class="caps">GPRO</span>, <span class="caps">HPE</span>, <span class="caps">HPQ</span>, <span class="caps">IBM</span>, <span class="caps">INFY</span>, <span class="caps">INTC</span>, <span class="caps">INTU</span>, <span class="caps">ITW</span>, <span class="caps">LRCX</span>, <span class="caps">MSFT</span>, <span class="caps">NFLX</span>, <span class="caps">NOK</span>, <span class="caps">NVDA</span>, <span class="caps">NXPI</span>, <span class="caps">OMC</span>, <span class="caps">ORCL</span>, <span class="caps">PANW</span>, <span class="caps">PYPL</span>, <span class="caps">QCOM</span>, <span class="caps">SAP</span>, <span class="caps">SNAP</span>, <span class="caps">SQ</span>, <span class="caps">SYMC</span>, T, <span class="caps">TSLA</span>, <span class="caps">TSM</span>, <span class="caps">TWTR</span>, <span class="caps">TXN</span>, <span class="caps">VMW</span>, <span class="caps">VZ</span>, <span class="caps">WDAY</span>, <span class="caps">WDC</span>, and <span class="caps">ZNGA</span>.</p>
<p>[3] Workday (<span class="caps">WDAY</span>) is a SaaS company that offers a product to businesses, Paypal (<span class="caps">PYPL</span>) is a company that provides payments infrastructure supporting e-commerce, Amazon (<span class="caps">AMZN</span>) is an e-commerce company, Lam Research (<span class="caps">LRCX</span>) makes chips, and Hewlett-Packard (<span class="caps">HPQ</span>) makes computers. Each of these are representatives of a different&nbsp;sub-sector.</p>
<p>[4] We can also get a sense of the compression error by plotting the compressed traces for one of the stocks. <a href="./wp-content/uploads/2018/06/sq.png"><img alt="sq" src="./wp-content/uploads/2018/06/sq.png"></a> The plot at right does this for Square inc. The ups and downs of <span class="caps">SQ</span> are largely captured by both methods. However, some refined details are lost in the compressions. Similar accuracy levels are seen for each of the other stocks in the full set (not shown&nbsp;here).</p>
<p>[5] The missing twentieth member of the G20 is the <span class="caps">EU</span>. We don&#8217;t consider the <span class="caps">EU</span> here simply because the site we scraped from does not have a page dedicated to&nbsp;it.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


             
 
            
                <hr />
    <div class="author_blurb">
        <a href="" target="_blank" rel="nofollow noopener noreferrer">
            <img src=/wp-content/uploads/2014/12/JonathanLinkedIn.jpg alt="Jonathan Landy Avatar" title="Jonathan Landy">
            <span class="author_name">Jonathan Landy</span>
        </a>
        Jonathan grew up in the midwest and then went to school at Caltech and UCLA. Following this, he did two postdocs, one at UCSB and one at UC Berkeley.  His academic research focused primarily on applications of statistical mechanics, but his professional passion has always been in the mastering, development, and practical application of slick math methods/tools. He worked as a data-scientist at Square for four years and is now working on a quantitative investing startup.
    </div>

            






            <hr/>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="./linselect-demo.html" title="Previous: linselect demo: a tech sector stock analysis">linselect demo: a tech sector stock analysis</a></li>
                <li class="next-article"><a href="./the-speed-of-traffic.html" title="Next: The speed of traffic">The speed of traffic</a> »</li>
            </ul>
            </nav>
            </aside>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2018-08-11T07:30:00-07:00">Aug 11, 2018</time>
            <h4>Category</h4>
            <a class="category-link" href="./categories.html#linselect-ref">linselect</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="./tags.html#linselect-ref">linselect
                    <span>2</span>
</a></li>
                <li><a href="./tags.html#methods-ref">methods
                    <span>3</span>
</a></li>
                <li><a href="./tags.html#python-ref">python
                    <span>2</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="https://twitter.com/efavdb" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Twitter" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1da1f3"/><path fill="#fff" d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>

    <div>
        <span class="site-name">EFAVDB</span> - Everybody's Favorite Data Blog
    </div>



    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>