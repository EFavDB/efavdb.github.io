var tipuesearch = {"pages":[{"title":"Authors","text":"Those interested in hiring us for consulting projects should feel free to contact us on social media — either via private tweet or directly on LinkedIn. We specialize in various aspects of data science and math modeling and are also available for in-office tutorials. Jonathan Landy ( LinkedIn ) grew up in the Midwest and then went to school at Caltech and UCLA . Following this, he did two postdocs, one at UCSB and one at UC Berkeley. His academic research focused primarily on applications of statistical mechanics, but his professional passion has always been in the mastering, development, and practical application of slick math methods/tools. He currently works as a data-scientist at Stitch Fix. Damien Ramunno-Johnson ( LinkedIn , Twitter ) is a highly experienced researcher with a background in clinical and applied research. Like, JSL , he got his PhD at UCLA . He has many years of experience working with imaging, and has a particularly strong background in image segmentation, registration, detection, data analysis, and more recently machine learning. He now works as a data-scientist at Square in San Francisco. Cathy Yeh got a PhD at UC Santa Barbara studying soft-matter/polymer physics. After stints in a translational modeling group at Pfizer in San Diego, Driver (a no longer extant startup trying to match cancer patients to clinical trials) in San Francisco, and Square, she is currently participating in the OpenAI Scholars program. Dustin McIntosh ( LinkedIn ) got a B.S in Engineering Physics from the Colorado School of Mines (Golden, CO ) before moving to UC Santa Barbara for graduate school. There he became interested in Soft Condensed Matter Physics and Polymer Physics, studying the interaction between single DNA molecules and salt ions ( Google Scholar ). After a brief postdoc at UC San Diego studying the physics of bacterial growth, Dustin decided to move into the data science business for good - he is now a Quantitative Analyst at Google in Mountain View.","tags":"pages","url":"https://efavdb.com/pages/authors.html","loc":"https://efavdb.com/pages/authors.html"},{"title":"Spaced repetition can allow for infinite recall","text":"My friend Andrew is an advocate of the \"spaced repetition\" technique for memorization of a great many facts [1]. The ideas behind this are two-fold: When one first \"learns\" a new fact, it needs to be reviewed frequently in order to not forget it. However, with each additional review, the fact can be retained longer before a refresher is needed to maintain it in recall. Because of this, one can maintain a large, growing body of facts in recall through limited, daily review: Each day, one need only review for ten minutes or so, covering a small number of facts. The facts included should be sampled from the full library in a way that prefers newer entries, but that also sprinkles in older cards often enough so that none are ever forgotten. Apps have been written to intelligently take care of the sampling process for us. Taking this framework as correct motivates questioning exactly how far it can be pushed: Would an infinitely-long-lived, but forgetful person be able to recall an infinite number of facts using this method? \\(\\ldots\\) Below, we show that the answer is: YES ! Proof: We first posit that the number of days \\(T\\) that a fact can be retained before it needs to be reviewed grows as a power-law in \\(s\\) , the number of times it's been reviewed so far, \\begin{eqnarray} \\tag{1}\\label{1} T(s) \\sim s&#94;{\\gamma}, \\end{eqnarray} with \\(\\gamma > 0\\) . With this assumption, if \\(N(t)\\) facts are to be recalled from \\(t\\) days ago, one can show that the amount of work needed today to retain these will go like (see appendix for a proof of this line) \\begin{eqnarray}\\tag{2}\\label{2} w(t) \\sim \\frac{N(t)}{t&#94;{\\gamma / (\\gamma + 1)}}. \\end{eqnarray} The total work needed today is then the sum of work needed for each past day's facts, \\begin{eqnarray} \\tag{3} \\label{3} W(total) = \\int_1&#94;{\\infty} \\frac{N(t)}{t&#94;{\\gamma / (\\gamma + 1)}} dt. \\end{eqnarray} Now, each day we only have a finite amount of time to study. However, the above total work integral will diverge at large \\(t\\) unless it decays faster than \\(1/t\\) . To ensure this, we can limit the number of facts retained from from \\(t\\) days ago to go as \\begin{eqnarray} \\tag{4} \\label{4} N(t) \\sim \\frac{1}{t&#94;{\\epsilon}} \\times \\frac{1}{t&#94;{1 / (\\gamma + 1)}}, \\end{eqnarray} where \\(\\epsilon\\) is some small, positive constant. Plugging (\\ref{4}) into (\\ref{3}) shows that we are guaranteed a finite required study time each day. However, after \\(t\\) days of study, the total number of facts retained scales as \\begin{eqnarray} N_{total}(t) &\\sim & \\int_1&#94;{t} N(t) dt \\\\ &\\sim & \\int_0&#94;{\\infty} \\frac{1}{t&#94;{1 / (\\gamma + 1)}} \\\\ &\\sim & t&#94;{ \\gamma / (\\gamma + 1)}. \\tag{5} \\label{5} \\end{eqnarray} Because we assume that \\(\\gamma > 0\\) , this grows without bound over time, eventually allowing for an infinitely large library. We conclude that — though we can't remember a fixed number of facts from each day in the past using spaced repetition — we can ultimately recall an infinite number of facts using this method. To do this only requires that we gradually curate our previously-introduced facts so that the scaling (\\ref{4}) holds at all times. Appendix: Proof of (2) Recall that we assume \\(N(s)\\) facts have been reviewed exactly \\(s\\) times. On a given day, the number of these that need to be reviewed then goes like \\begin{eqnarray} \\tag{A1}\\label{A1} W(s) \\sim \\frac{N(s)}{T(s)}. \\end{eqnarray} where \\(T(s)\\) is given in (\\ref{1}). This holds because each of the \\(N(s)\\) facts that have been studied \\(s\\) times so far must be reviewed within \\(T(s)\\) days, or one will be forgotten. During these \\(T(s)\\) days, each will move to having been reviewed \\(s+1\\) times. Therefore, \\begin{eqnarray} \\tag{A2} \\label{A2} \\frac{ds}{dt} &\\sim & \\frac{1}{T(s)} \\end{eqnarray} Integrating this gives \\(s\\) as a function of \\(t\\) , \\begin{eqnarray} \\tag{A3} \\label{A3} s \\sim t&#94;{1 / (\\gamma + 1)} \\end{eqnarray} Plugging this last line and (1) into (A1), we get (2). References [1] See Andrew's blog post on spaced repetition here . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Case studies","url":"https://efavdb.com/memory recall","loc":"https://efavdb.com/memory recall"},{"title":"Counting the number of ways to make change for a trillion dollars","text":"There are four ways to make change for \\(N=10\\) cents: \\(\\{\\) 10 pennies; 1 nickel and 5 pennies; 2 nickels; 1 dime \\(\\}\\) . How many ways are there to make change for one trillion dollars — using just pennies, nickels, dimes, and quarters? To answer this, we present here a hybrid dynamic programming / analytic strategy that allows us to count the number of ways \\(\\mathbb{Q}(N)\\) to make change for any \\(N\\) . The result for one trillion dollars? \\(\\mathbb{Q}(10&#94;{14}) = 133333333333423333333333351000000000001\\) . Dynamic programming solution We begin by introducing a set of recursion relations that will directly allow for a \"pure\" dynamic programming strategy for counting the number of ways to make change: Let, \\(\\mathbb{Q}(N), \\mathbb{D}(N), \\mathbb{N}(N)\\) and \\(\\mathbb{P}(N)\\) be the number of ways to make change for \\(N\\) cents allowing all four denominations, allowing only up to dimes (i.e., excluding quarters), only up to nickels, and finally only pennies, respectively. Next, we note that \\begin{eqnarray} \\nonumber \\mathbb{Q}(N) &=& \\mathbb{D}(N) + \\mathbb{D}(N-25) + \\mathbb{D}(N - 50) + \\ldots \\\\ &\\equiv & \\mathbb{D}(N) + \\mathbb{Q}(N-25). \\tag{1} \\label{q_iter} \\end{eqnarray} Here, we have decomposed the number of ways to make change in terms of the number of quarters included. For example, the second term in the first line above is the number of ways to make change when we have exactly \\(1\\) quarter: The count in this case is simply the number of ways we can make change for \\(N-25\\) cents, using only dimes, nickels, and pennies — i.e., \\(\\mathbb{D}(N-25)\\) . Similarly, \\begin{eqnarray} \\nonumber \\mathbb{D}(N) &=& \\mathbb{N}(N) +\\mathbb{N}(N-10) + \\mathbb{N}(N-20) + \\ldots \\\\ &\\equiv & \\mathbb{N}(N) + \\mathbb{D}(N-10) \\tag{2} \\label{d_iter} \\end{eqnarray} and \\begin{eqnarray} \\nonumber \\mathbb{N}(N) &=& \\mathbb{P}(N) + \\mathbb{P}(N-5) + \\mathbb{P}(N-10) + \\ldots \\\\ &\\equiv & \\mathbb{P}(N) + \\mathbb{N}(N-5). \\tag{3} \\label{n_iter} \\end{eqnarray} Finally, \\(\\mathbb{P}(N) \\equiv 1\\) for all natural \\(N\\) . It is straightforward to sum the above equations in reverse using dynamic programming: We start by using the fact that \\(\\mathbb{P}(N) = 1\\) all natural \\(N\\) , then evaluate \\([\\mathbb{N}(1), \\ldots, \\mathbb{N}(N)]\\) on a computer using the iterative equation (\\ref{n_iter}). Storing these values in memory, we can then evaluate \\([\\mathbb{D}(1), \\ldots, \\mathbb{D}(N)]\\) using (\\ref{d_iter}), and then finally evaluate \\([\\mathbb{Q}(1), \\ldots, \\mathbb{Q}(N)]\\) using (\\ref{q_iter}). A python program that carries out this strategy is given below. Using this we are able to evaluate that, e.g., the number of ways to make change for one dollar is \\(\\mathbb{Q}(100) = 242\\) . The program below is sufficient for \"practical\" \\(N\\) , but we need to do better to count the number of ways to make change for one trillion dollars : On my laptop, I can evaluate the result for one-hundred thousand dollars in about \\(20\\) seconds. The runtime of our program scales linearly with \\(N\\) though, so it would take it about \\(2 \\cdot 10&#94;{7}\\) seconds to get our target result — too slow. import numpy as np def ways_to_make_change ( N , denominations_list = [ 1 , 5 , 10 , 25 ]): \"\"\" This function calculates the number of ways to make change for N cents, using only the denominations included in the passed `denominations_list`. parameters ---------- N: int The target number of cents we wish to make change for. denominations_list: list A list of denominations that we can use to make change. \"\"\" for index , denom in enumerate ( denominations_list ): if index == 0 : counts = 1 * ( np . arange ( N + 1 ) % denom == 0 ) else : counts_lower_denoms = counts . copy () for i in np . arange ( denom , N + 1 ): counts [ i ] = counts_lower_denoms [ i ] + counts [ i - denom ] return counts print ( ways_to_make_change ( 100 )[ - 1 ]) # output: 242 Geometric interpretation, large \\(N\\) limiting form In this section, we briefly consider the geometry of our problem. The plot above is a 3d visual of all the possible ways to make change for one dollar: Each point here has an \\((x,y,z)\\) position that encodes the number of nickels, dimes, and quarters, respectively, in a particular change solution. To get the full solution for a particular lattice point, we calculate the sum \\(5 x + 10 y + 25 z\\) — subtracting this from \\(N=100\\) gives the number of pennies that must be included in its solution. In this way, we see that we get a valid solution at each non-negative lattice point whose coordinates give a sum that is not larger than \\(100\\) . For general \\(N\\) , we have \\begin{eqnarray} \\mathbb{Q}(N) = \\text{# of lattice points: } 5 x + 10 y + 25 z \\leq N \\tag{4} \\label{4} \\end{eqnarray} where we are again counting all non-negative lattice points on a lattice spaced along \\(x\\) with distance \\(5\\) , etc. We note that the inequality (\\ref{4}) defines a polytope, and that there is a beautiful, pre-existing body of work — based on the use of generating functions — that allows one to count the number of lattice points within an arbitrary polytope \\([1]\\) . One can look up results from that line of work to directly answer the change counting problem we are considering here. However, the hybrid dynamic programming / analytic approach that we present below — based on our recursion relations above — is more tailored to our problem and consequently easier to derive. Before moving on, we point out that we can get a good approximation to the number of lattice points at large \\(N\\) by simply considering the volume of the polytope: The idea is to make use of the fact that the lattice points are evenly spaced. To the find an approximation to the number that sit inside the polytope then, we simply need to divide the polytope's volume ( \\(\\frac{1}{3!} N&#94;3\\) ) by the volume of each lattice cell ( \\(5 \\cdot 10 \\cdot 25\\) ). This gives [2], \\begin{eqnarray} \\mathbb{Q}(N) \\sim \\frac{1}{7500}N&#94;3. \\tag{5} \\label{5} \\end{eqnarray} Eq. (\\ref{5}) is the correct, leading behavior at large \\(N\\) . However, it's not exact because of \"discreteness effects\" that occur at the boundary of our polytope — these determine whether specific, individual lattice points lie inside the polytope or not. These effects result in correction terms at quadratic and lower powers of \\(N\\) . Our calculation below provides one method for capturing the full set of terms. Exact solution To get the exact result for \\(\\mathbb{Q}(N)\\) , we will directly sum the recursions relations (\\ref{q_iter})-(\\ref{n_iter}): Recall from above, that we have \\begin{eqnarray} \\mathbb{P}(N) \\equiv 1 \\tag{6} \\label{6} \\end{eqnarray} for any natural \\(N\\) . Plugging this into (\\ref{n_iter}), we obtain \\begin{eqnarray} \\tag{7} \\label{7} \\mathbb{N}(N) = \\lfloor \\frac{N}{5} \\rfloor + 1 \\end{eqnarray} Notice that if we evaluate this at a series of points separated by \\(\\Delta N = 5\\) , we get back what looks like a simple linear function of \\(N\\) — it increases by \\(1\\) with each increase of \\(5\\) in \\(N\\) . Now, if we plug our last line into (\\ref{d_iter}) we obtain \\begin{eqnarray} \\nonumber \\mathbb{D}(N) &=& \\left ( \\lfloor \\frac{N}{5} \\rfloor + 1 \\right) + \\left( \\lfloor \\frac{N - 10 }{5} \\rfloor + 1\\right) + \\ldots \\\\ &=& \\left ( \\lfloor \\frac{N}{5}\\rfloor + 1 \\right) + \\left ( \\lfloor \\frac{N}{5}\\rfloor - 1 \\right) + \\ldots \\tag{8} \\label{8} \\end{eqnarray} This is an arithmetic sequence that depends on \\(\\lfloor \\frac{N}{5} \\rfloor\\) . Summing this series, we obtain \\begin{eqnarray} \\mathbb{D}(N) = \\begin{cases} \\frac{1}{4} \\left (\\lfloor \\frac{N}{5} \\rfloor + 2 \\right)&#94;2, & \\text{$\\lfloor \\frac{N}{5} \\rfloor$ even} \\\\ \\frac{1}{4} \\left (\\lfloor \\frac{N}{5}\\rfloor + 1 \\right)\\left (\\lfloor \\frac{N}{5}\\rfloor + 3 \\right ), & \\text{$\\lfloor \\frac{N}{5}\\rfloor$ odd} \\tag{9} \\label{9} \\end{cases} \\end{eqnarray} Notice that in this case, if we evaluate the function at a series of points each separated by \\(10\\) , the result will again look like a simple, smooth function — this time a quadratic function of \\(N\\) . However, the particular form of the quadratic depends on whether \\(\\lfloor \\frac{N}{5} \\rfloor\\) is even or odd — that is, the function skips back and forth between two different quadratics. We are now at the point where we can solve for the full number of ways to make change, \\(\\mathbb{Q}(N)\\) . In principle, all we need to do is to plug (\\ref{9}) into (\\ref{q_iter}), and sum as above. This is straightforward for a particular \\(N\\) , but we'd like a general formula. If we work out a few terms for any particular value of \\(N\\) , we see that we will get alternating terms: perhaps starting with a term of the upper type from (\\ref{9}), followed by a term of the lower type, etc. The sums of the upper form can be evaluated analytically (e.g., by using the pyramid summation method), as can the sum of the terms of the bottom form. For any starting point, the result is a cubic in \\(\\lfloor \\frac{N}{5} \\rfloor\\) . However, a small bit of work shows that the precise cubic form that applies depends on the value of \\(\\lfloor \\frac{N}{5} \\rfloor (\\text{mod } 10)\\) , so we have ten separate cubics to solve for. Rather than laboriously evaluate the sums for each of the ten possible remainders of \\(\\lfloor \\frac{N}{5} \\rfloor\\) modulo \\(10\\) , we can simply carry out polynomial fits to data that we get from our python implementation of the dynamic programming approach. For example, plugging the first four values of \\(N\\) that satisfy \\(\\lfloor \\frac{N}{5} \\rfloor \\equiv 0 (\\text{mod } 10)\\) into our program, we get \\begin{eqnarray} \\nonumber \\mathbb{Q}(1) &=& 1 \\\\ \\nonumber \\mathbb{Q}(50) &=& 49 \\\\ \\nonumber \\mathbb{Q}(100) &=& 242 \\\\ \\mathbb{Q}(150) &=& 680 \\tag{10} \\label{10} \\end{eqnarray} Fitting a cubic to this data — which involves solving a system of linear equations for the unknown coefficients of the polynomial — gives \\begin{eqnarray} \\mathbb{Q}(N) = \\frac{\\lfloor \\frac{N}{5}\\rfloor &#94;3}{60}+\\frac{9 \\lfloor \\frac{N}{5}\\rfloor &#94;2}{40}+\\frac{53 \\lfloor \\frac{N}{5}\\rfloor }{60}+1 \\tag{11} \\label{11} \\end{eqnarray} Again, this holds when \\(\\lfloor \\frac{N}{5}\\rfloor \\equiv 0 (\\text{mod } 10)\\) . This is the equation we used to get the result quoted for one trillion dollars at the top of our post. Notice that the leading term here matches what's required by our asymptotic result, (\\ref{5}). It turns out that when we plug in data for each of the other nine possible remainders, the cubic fits returned are all the same — except for their constant terms. Presumably, this is because the coefficients of positive powers of \\(N\\) each correspond to some asymptotic, geometric effects that must all agree when \\(N\\) is large — as we discussed above. At any rate, we have tabulated the constant terms that apply for each possible remainder below, and this completes our solution. \\(\\lfloor \\frac{N}{5} \\rfloor \\mod(10)\\) constant term in \\(\\mathbb{Q}(N)\\) \\(0\\) \\(1\\) \\(1\\) \\(\\frac{7}{8}\\) \\(2\\) \\(\\frac{6}{5}\\) \\(3\\) \\(\\frac{7}{8}\\) \\(4\\) \\(\\frac{4}{5}\\) \\(5\\) \\(\\frac{7}{8}\\) \\(6\\) \\(1\\) \\(7\\) \\(\\frac{43}{40}\\) \\(8\\) \\(1\\) \\(9\\) \\(\\frac{27}{40}\\) Acknowledgements This work in this post was done in collaboration with my father, Steven Landy. References [1] Beck, Matthias, and Sinai Robins. Computing the continuous discretely. Vol. 61. Berlin: Springer Science+ Business Media, LLC , 2007. [2] One way to derive this volume result is to integrate the recursion relations (1)-(3), ignoring discreteness effects — i.e., integrate assuming that each of the variables \\(\\mathbb{P}(N), \\mathbb{N}(N), \\mathbb{D}(N)\\) , and \\(\\mathbb{Q}(N)\\) are smooth functions of \\(N\\) . This will hold in the large \\(N\\) limit to leading order, so will give us the leading order result. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"programming, Case studies","url":"https://efavdb.com/change","loc":"https://efavdb.com/change"},{"title":"Generalized Dollar Cost Averaging","text":"In this note, I consider a generalization of Dollar Cost Averaging — a popular investing strategy that involves gradually building up one's holding in a stock over a pre-specified period of time. The generalization I consider can guarantee better prices paid per share — relative to the standard approach — but its use comes at the cost of requiring one to have a flexible budget. The idea essentially boils down to leaning in more heavily on low price periods. Model system and results In this post, we will assume a scenario where one wants to enter into a stock over a set of \\(K\\) equally spaced periods. These periods could be separated by minutes, hours, or days, etc. The price of the stock at period \\(i\\) will be written as \\(p_i\\) and the number of shares purchased then will be \\begin{eqnarray} N_i = A \\frac{1}{p_i&#94;{\\gamma}}, \\tag{1} \\label{1} \\end{eqnarray} where \\(A\\) and \\(\\gamma\\) are constants that are set before beginning. The total number of shares purchased, total money spent, and price per share realized are then given by, \\begin{eqnarray}\\tag{2} \\label{2} N_{total} &=& \\sum_{i=1}&#94;K N_i \\\\ \\tag{3} \\label{3} \\text{total money spent} &=& \\sum_{i=1}&#94;K p_i N_i \\\\ \\tag{4} \\label{4} \\text{price per share} &=& \\frac{\\sum_{i=1}&#94;K p_i N_i}{\\sum_{i=1}&#94;K N_i}, \\end{eqnarray} respectively. The green curve in the figure below records the price paid per share under our strategy as a function of \\(\\gamma\\) for the example price series shown in black (this has a hidden, separate x-axis). This figure illustrates a number of general points about our strategy: If \\(\\gamma = 0\\) , we purchase a fixed number of shares each day, \\(A\\) . The resulting price paid per share (\\ref{4}) is then the arithmetic mean of the period prices. If \\(\\gamma =1\\) , we spend an equal amount of money each day. This is the standard dollar cost averaging method. Plugging this choice into (\\ref{4}), one finds that the price paid per share in this case is the harmonic mean of the prices over the series. The arithmetic-harmonic means inequality implies that the \\(\\gamma=1\\) necessarily beats the \\(\\gamma=0\\) approach when it comes to price per share paid. This can be understood as follows: The \\(\\gamma=1\\) method results in relatively more shares being purchased on low price periods, which causes these periods to be more heavily weighted in the relevant average. More generally, we prove in an appendix below that increasing \\(\\gamma\\) always causes the price per share to go down. This holds for all possible price series — including that shown in our figure above, where we see the green curve is monotonically decreasing. This result holds because increasing \\(\\gamma\\) causes one lean in even harder on low price periods. As \\(\\gamma \\to \\infty\\) , our purchase strategy is dominated by the lowest price period, and this is the price we end up paying per share on average. As \\(\\gamma \\to -\\infty\\) , our strategy is dominated by the highest price period, and this is the price we pay. This limit is attractive when we want to exit / sell a holding. The downside of our strategy — relative to the standard dollar cost averaging method — is that it requires a flexible budget. However, for \\(\\gamma\\) not too large, we can often get a meaningful reduction in price per share without dramatically increasing the variance in amount spent: e.g., a one percent drop in price per share might be possible if one is willing to have a ten percent flexibility in one's budget. We give numerical examples in the next section. If the stock is drifting over the \\(K\\) periods in question, all dollar cost averaging methods may give a higher price than simply purchasing everything on the first period. For this reason — when a cheaper price per share is the objective — I suggest using these strategies over short time frames only — say, over a day or week. That's because stock price variance typically dominates drift over short periods. We now turn to a quick numerical study, and then conclude with our appendix covering the proof of monotonicity. Numerical study In this section, we provide some helper python code that allows one to simulate price series and check the results of running our strategy (\\ref{1}) on top of them. The following two code blocks (1) generate log normal random walk price series, and (2) carry out our strategy. import numpy as np def simulated_price_series ( initial_price , drift , std , steps ): \"\"\" Returns a simulated price series asserting the log of the price takes a random walk with drift and variance, as passed. \"\"\" drifts = np . arange ( steps ) * drift random_factors = std * np . random . randn ( steps ) random_factors [ 0 ] = 0 returns = drifts + random_factors return initial_price * np . exp ( returns ) def simulated_series_and_purchases ( gamma , initial_price , drift , std , steps ): \"\"\" Simulate price trajectory of length steps and then apply the generalized dollar cost averaging method -- this entails buying N_i shares at step i, with N_i ~ 1 / p_i &#94; gamma. Return the total money spent, shares purchased, and average price per share. \"\"\" price_series = simulated_price_series ( initial_price , drift , std , steps ) shares_series = 100 / (( price_series / price_series [ 0 ]) ** gamma ) money_spent_series = price_series * shares_series # summary stats shares = np . sum ( shares_series ) money_spent = np . sum ( money_spent_series ) return shares , money_spent , money_spent / shares With these in place, I ran 40k simulation with the following parameters for the price series: initial_price = 10.0 steps = 10 drift = 0.0 std = 0.05 In the simulations, I considered \\(\\gamma=5\\) , \\(10\\) , and \\(1\\) — the last being the standard dollar cost averaging method. Histograms of the resulting spends are shown below for each case. The titles give the mean and standard deviation of the amount spent over the traces, and the legends give the mean prices per share, averaging over traces. We can see that we can get a fairly significant improvement in mean price as we lift \\(\\gamma\\) , but doing this requires that we occassionally spend a lot more money. One can mitigate the very large spend events by capping the amount spent per day. For example, if one caps the daily spend at fifty percent above that on the first day, the \\(\\gamma=10\\) case here gives a spend distribution of \\(10194 \\pm 2069\\) , and a mean price per share of \\(9.86\\) . Appendix: Average price per share monotonicity In this appendix, we give a proof that increasing \\(\\gamma\\) in (\\ref{1}), always results in a decreasing average price per share (more precisely, it can't result in an increase). To begin, we recall our definitions: The number of shares purchased at \\(p_i\\) is assumed to be \\begin{eqnarray} \\tag{A1} N_i = A p_i&#94;{-\\gamma}. \\end{eqnarray} The total number of shares purchased is then obtained by summing this \\begin{eqnarray} \\tag{A2} N_{total} = \\sum_{i=1}&#94;k N_i \\end{eqnarray} The average price per share is then \\begin{eqnarray} \\tag{A3} \\label{A3} \\overline{p} = \\sum_{i=1}&#94;k w_i p_i \\end{eqnarray} where \\begin{eqnarray} w_j &\\equiv & \\frac{N_j}{N_{total}} \\\\ &=& \\frac{p_j&#94;{-\\gamma}}{\\sum_{i=1}&#94;k p_i&#94;{-\\gamma}} \\tag{A4} \\end{eqnarray} is the normalized weight applied to price \\(p_j\\) . Plugging this last line into (\\ref{A3}), we get a simple expression for the average price per share \\begin{eqnarray}\\label{A5}\\tag{A5} \\overline{p} = \\frac{\\langle p_i&#94;{-\\gamma + 1} \\rangle}{\\langle p_i&#94;{-\\gamma} \\rangle} \\end{eqnarray} Here, we use brackets for equal weight averages over the price series. This formula shows that the mean paid price per share is related to the ratio of two adjacent moments of the price series. We will now argue that this is monotic in \\(\\gamma\\) [Aside: It looks like one might be able to provide an alternative proof of our result from the ratio of moments expression above, combining that with one of the Chebychev inequalities. However, I haven't been able to get that to work just yet]. To show that (\\ref{A3}) is monotonic in \\(\\gamma\\) , we first note that the relative weight applied to any two prices is given by \\begin{eqnarray}\\label{A6}\\tag{A6} \\frac{w_i}{w_j} = \\left(\\frac{p_i}{p_j}\\right)&#94;{-\\gamma} \\end{eqnarray} If \\(p_i\\) is the minimum price in the series this is necessarily an increasing function of \\(\\gamma\\) for any other \\(p_j\\) . In order for the weights to continue to be normalized to one, this implies that the weight applied to the smallest price must always increase as we increase \\(\\gamma\\) . Next, we note that with the above observation, our result is trivially true when \\(k=1\\) and \\(k=2\\) . Suppose then that the result holds up to \\(k-1\\) periods and the consider the case at \\(k\\) . If we order the \\(p_i\\) values from largest to smallest, we then have \\begin{eqnarray}\\label{A7} \\tag{A7} \\overline{p} = \\left ( 1 - w_k \\right) \\sum_{i=1}&#94;{k-1} \\tilde{w}_i p_i+ w_k p_k \\end{eqnarray} The sum above is the weighted average we'd realize if we only invested on the first \\(k-1\\) ordered prices, and the \\(\\tilde{w}_i\\) are the effective weights we'd apply in that case. By assumption, this inner sum is necessarily non-increasing with \\(\\gamma\\) . Therefore, if we let \\(w&#94;{\\prime}\\) be the weight applied at some \\(\\gamma&#94;{\\prime} > \\gamma\\) , we have \\begin{eqnarray} \\overline{p}&#94;{\\prime} &=& \\left ( 1 - w_k&#94;{\\prime} \\right) \\sum_{i=1}&#94;{k-1} \\tilde{w}_i&#94;{\\prime} p_i + w_k&#94;{\\prime} p_k \\\\ &\\leq & \\left ( 1 - w_k&#94;{\\prime} \\right) \\sum_{i=1}&#94;{k-1} \\tilde{w}_i p_i + w_k&#94;{\\prime} p_k \\label{A8} \\tag{A8} \\end{eqnarray} But this last line is just like the \\(k=2\\) case: We move from one weighted average of two values (namely, \\(p_k\\) and the weighted sum of the first \\(k-1\\) prices with weights fixed as at \\(\\gamma\\) — which is necessarily not smaller than \\(p_k\\) ) to another that applies more weight to the smaller of the two values — ultimately giving a smaller result. Continuing with the last line, then, we have \\begin{eqnarray} \\overline{p}&#94;{\\prime} &\\leq& \\left ( 1 - w_k&#94;{\\prime} \\right) \\sum_{i=1}&#94;{k-1} \\tilde{w}_i p_i + w_k&#94;{\\prime} p_k \\\\ &\\leq& \\left ( 1 - w_k \\right) \\sum_{i=1}&#94;{k-1} \\tilde{w}_i p_i + w_k p_k \\\\ &\\equiv& \\overline{p}. \\label{A9} \\tag{A9} \\end{eqnarray} This completes our proof. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Finance","url":"https://efavdb.com/generalized_dollar_cost_averaging","loc":"https://efavdb.com/generalized_dollar_cost_averaging"},{"title":"Physics-based proof of the duality theorem for linear programs","text":"Textbook proofs of the duality theorem often apply abstract arguments that offer little tangible insight into the relationship between a linear program and its dual. Here, we map the general linear program onto a simple mechanics problem. In this context, the significance of the theorem is relatively clear. The general linear program and its dual The goal of a linear program is to optimize a linear objective function subject to some linear constraints. By introducing slack and other auxiliary variables, the general linear program can be expressed as [1] \\begin{eqnarray} \\tag{1} \\label{primal} \\text{max}_{\\textbf{x} \\in \\mathbb{R}_n} \\textbf{c}&#94;T \\cdot \\textbf{x}, \\text{subject to: } A \\cdot \\textbf{x} \\leq \\textbf{b}. \\end{eqnarray} Here, the optimization is over \\(\\textbf{x} \\in \\mathbb{R}_n\\) , \\(A\\) is a given \\(m \\times n\\) real matrix, \\(\\textbf{b}\\) and \\(\\textbf{c}\\) are given real vectors, and the inequality holds component-wise. Any \\(\\textbf{x}\\) that satisfies the inequality constraints above is called a feasible solution to (\\ref{primal}) and if a point can be found with finite components that optimizes the objective it is called an optimal solution . Associated with (\\ref{primal}) is the dual linear program, \\begin{eqnarray} \\label{dual} \\tag{2} \\text{min}_{\\textbf{w} \\in \\mathbb{R}_m} \\textbf{b}&#94;T \\cdot \\textbf{w} , \\text{subject to: } A&#94;T \\cdot \\textbf{w} = \\textbf{c}, \\textbf{w} \\geq \\textbf{0}. \\end{eqnarray} The optimization here is over \\(\\textbf{w} \\in \\mathbb{R}_m\\) , and \\(A\\) , \\(\\textbf{b}\\) , and \\(\\textbf{c}\\) are the same variables present in (\\ref{primal}). In this post, we apply some simple ideas from classical mechanics to prove the following theorem, one of the central results connecting the two linear programs above: Duality theorem If (\\ref{primal}) has an optimal solution at \\(\\textbf{x}&#94;*\\) , then (\\ref{dual}) will also have an optimal solution at some point \\(\\textbf{w}&#94;*\\) , and these points satisfy \\begin{eqnarray}\\label{strong_law} \\tag{3} \\textbf{c}&#94;T \\cdot \\textbf{x}&#94;*= \\textbf{b}&#94;T \\cdot \\textbf{w}&#94;*. \\end{eqnarray} That is, the optimal objectives of (\\ref{primal}) and (\\ref{dual}) agree. Proof: To begin we assume that \\(\\textbf{x}&#94;*\\) is an optimal solution to (\\ref{primal}). We also assume for simplicity that (i) we have chosen a coordinate system so that \\(\\textbf{x} = \\textbf{0}\\) is a feasible solution of (\\ref{primal}) and that (ii) each row \\(\\hat{\\textbf{A}}_i\\) of \\(A\\) has been normalized to unit length. Next, we introduce a physical system relevant to (\\ref{primal}) and (\\ref{dual}): We consider a mobile point particle that interacts with a set of \\(m\\) fixed walls, all sitting in \\(\\mathbb{R}_n\\) . The particle's coordinate \\(\\textbf{x}_p\\) is initially set to \\(\\textbf{x}_p = \\textbf{0}\\) . The fixed \\(i\\) -th wall sits at those points \\(\\textbf{x}\\) that satisfy \\(\\hat{\\textbf{A}}_i \\cdot \\textbf{x} = b_i\\) . We take the force on the particle from wall \\(i\\) to have two parts: (i) a constant, long range force, \\(w_i \\hat{\\textbf{A}}_i\\) , normal to the wall, and (ii) a \"hard core\" force, \\(-n_i(\\textbf{x}_p) \\hat{\\textbf{A}}_i\\) , also normal to the wall. The hard core force makes the wall impenetrable to the particle, but otherwise allows the particle to move freely: Its magnitude is zero when the particle does not touch the wall, but on contact it scales up to whatever value is needed to prevent the particle from passing through it. To relate the physical system to our linear programs, we'll require \\(\\textbf{x}_p\\) to be a feasible solution to (\\ref{primal}) and the vector of long range force magnitudes \\(\\textbf{w}\\) to be a feasible solution to (\\ref{dual}). A point \\(\\textbf{x}_p\\) is a feasible solution to (\\ref{primal}) if and only if the particle is within the interior space bounded by the set of walls. Further, the equality constraint of (\\ref{dual}) is equivalent to the condition that all feasible \\(\\textbf{w}\\) result in a total long range force acting on the particle of \\(\\sum_i w_i \\hat{\\textbf{A}}_i = \\textbf{c}\\) . The non-negativity condition on feasible \\(\\textbf{w}\\) vectors in (\\ref{dual}) further requires that the long range forces each be either attractive or zero. A simple example of the sort of physical system we've described here is shown in Fig. 1a. Fig. 1: An example system: (a) A particle at \\(\\textbf{x}_p=\\textbf{0}\\) interacts with three walls. The total long range force on the particle is \\(\\sum_i w_i \\hat{\\textbf{A}}_i = \\textbf{c}\\) . (b) The particle sits at its long range potential minimum, \\(\\textbf{x}&#94;*\\) , with walls \\(2\\) and \\(3\\) \" binding\". The hard core normal forces from these walls must point inward and sum to \\(-\\textbf{c}\\) — otherwise, there would be a net force on the particle, and it would continue to move, but that won't happen once it sits at its potential minimum. Wall \\(1\\) is not binding and is now a distance \\(d_1(\\textbf{x}&#94;*)\\) away from the particle. This results in a positive difference between the dual and primal objectives (\\ref{gap}), unless we can find a feasible \\(\\textbf{w}\\) for which \\(w_1=0\\) . Setting \\(\\textbf{w}&#94;* = \\textbf{n}(\\textbf{x}&#94;*)\\) — the vector of hard-core normal force magnitudes at \\(\\textbf{x}&#94;*\\) — provides such a solution. This is non-negative, zero for non-binding walls, and results in a total long range force of \\(\\textbf{c}\\) — a consequence of the point above that the particle must remain at rest at its potential minimum. This choice for \\(\\textbf{w}\\) optimizes (\\ref{dual}) and gives an objective matching that of (\\ref{primal}) at \\(\\textbf{x}&#94;*\\) . When we assert the conditions above, the potential associated with the long range forces in our physical system ends up being related to the objective functions of (\\ref{primal}) and (\\ref{dual}). Up to a constant, the potential of a force \\(\\textbf{f}(\\textbf{x})\\) is defined to be [2] \\begin{eqnarray}\\label{potential} \\tag{4} U(\\textbf{x}) \\equiv -\\int \\textbf{f}(\\textbf{x}) \\cdot d\\textbf{x} \\end{eqnarray} In our case, the long range force between the wall \\(i\\) and the particle is a constant and normal to the wall. Its potential is therefore simply \\begin{eqnarray}\\label{potential_i} \\tag{5} U_i(\\textbf{x}_p) = w_i d_i(\\textbf{x}_p), \\end{eqnarray} where \\begin{eqnarray}\\label{perp_distance} \\tag{6} d_{i}(\\textbf{x}_p) \\equiv b_i - \\hat{\\textbf{A}}_i \\cdot \\textbf{x}_p. \\end{eqnarray} This is the perpendicular distance between the particle and the wall \\(i\\) . The physical significance of (\\ref{potential_i}) is that it is the total energy it takes to separate the particle from wall \\(i\\) by a distance \\(d_i\\) , working against the attractive force \\(w_i\\) . Notice that if we plug (\\ref{perp_distance}) into (\\ref{potential_i}), the total long range potential at \\(\\textbf{x}_p\\) can be written as \\begin{eqnarray}\\nonumber \\sum_i w_i d_i(\\textbf{x}_p)&=& \\sum_i w_i \\left (b_i - \\hat{\\textbf{A}}_i \\cdot \\textbf{x}_p \\right) \\\\ &=& \\textbf{w}&#94;T \\cdot \\textbf{b} - \\textbf{c} \\cdot \\textbf{x}_p. \\tag{7} \\label{potential_as_objective_difference} \\end{eqnarray} Here, we have used one of the feasibility conditions on \\(\\textbf{w}\\) to get the last line. The right side of (\\ref{potential_as_objective_difference}) is the difference between the dual and primal objectives. This will be minimized at the feasible point \\(\\textbf{x}_p\\) that maximizes \\(\\textbf{c}\\cdot \\textbf{x}_p\\) — the primal objective — and at that feasible \\(\\textbf{w}\\) that minimizes \\(\\textbf{w}&#94;T \\cdot \\textbf{b}\\) — the dual objective. In other words, we see that both our programs independently contribute to the common goal of minimizing the particle's long range potential, (\\ref{potential_as_objective_difference}), subject to our system's constraints. The last preperatory remark we must make relates to the fact that we have assumed that \\(\\textbf{x}&#94;*\\) is an optimal solution to (\\ref{primal}) — i.e., it is a point that is as far \"down\" in the \\(\\textbf{c}\\) direction as possible within the feasible set. This must mean that \\(\\textbf{x}&#94;*\\) sits somewhere at the boundary of the primal feasible set, with some of the constraints of (\\ref{primal}) binding — i.e., satisfied as equalities. Further, if we place and release the particle gently at \\(\\textbf{x}&#94;*\\) , it must stay at rest as it can fall no further — just as a particle acted on by gravity stays at rest when it is placed in the bottom of a bucket. To stay at rest, there must be no net force on the particle, which means that the hard core normal forces \\(-n_i \\hat{\\textbf{A}}_i\\) from the binding walls at \\(\\textbf{x}&#94;*\\) must sum to exactly \\(-\\textbf{c}\\) , fully countering the constant long range force. The set of forces acting on the particle at \\(\\textbf{x}&#94;*\\) is illustrated Fig. 1b for our simple example. To complete the argument, we note that the long range potential at \\(\\textbf{x}&#94;*\\) is given from (\\ref{potential_as_objective_difference}) by \\begin{eqnarray} \\label{gap} \\tag{8} \\sum_i w_i d_i(\\textbf{x}&#94;*) &=& \\textbf{w}&#94;T \\cdot \\textbf{b} - \\textbf{c} \\cdot \\textbf{x}&#94;*. \\end{eqnarray} This is non-negative because \\(\\textbf{w} \\geq \\textbf{0}\\) and \\(d_i(\\textbf{x}&#94;*) >0\\) for each non-binding wall. It follows that the dual objective is always bounded from below by the optimal primal objective. Further, the gap between the two — the left side of (\\ref{gap}) — can only be zero if the long range interaction strengths are zero for each non-binding wall at \\(\\textbf{x}&#94;*\\) — i.e., if the particle is not actually attracted to the walls that are not binding at \\(\\textbf{x}&#94;*\\) . The vector \\(\\textbf{n}(\\textbf{x}&#94;*)\\) of hard core normal force magnitudes at \\(\\textbf{x}&#94;*\\) provides such a solution for \\(\\textbf{w}\\) : This is a feasible \\(\\textbf{w}\\) because it is non-negative and results in normal forces that sum to \\(\\textbf{c}\\) . Further, it is non-zero only for the binding constraints. It follows that \\(\\textbf{w}&#94;* = \\textbf{n}(\\textbf{x}&#94;*)\\) gives an optimal solution to the dual, at which point its objective matches that of the optimal primal solution. This argument is summarized in the caption to Fig. 1. References [1] Matousek, J., and Ga ̈rtner, B. Understanding and using linear programming. Springer (2007) [2] Marion, Jerry B. Classical dynamics of particles and systems. Academic Press, (2013). if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"optimization","url":"https://efavdb.com/duality","loc":"https://efavdb.com/duality"},{"title":"To Flourish or to Perish","text":"In this post, I explore some basic math behind the World Wandering Dudes framework previously introduced in this post . To briefly reintroduce the system: imagine a field, a 2-D square lattice of \\(M\\) sites with food distributed randomly at a density \\(\\rho_{food}\\) . Creatures wander across the field taking \\(N\\) steps per day via random walk, gathering any food they come across. At the end of each day, if a creature has no food it dies, if a creature has exactly 1 food it survives, and if a creature has 2 or more food it survives and reproduces. The food will then resprout at random locations. For the purposes of this post, to make the modeling simpler, we introduce the following properties: All food spoils at the end of each day before resprouting - creatures cannot store it and it does not accumulate on the field. All creatures teleport to a random field position at the end of each day, preventing creatures from consistently competing for food with their children. Periodic boundary conditions on the field with \\(M \\gg N\\) such that creatures do not wander \"around the world\" within a single day. First, let's imagine a single creature alone on the field. After taking \\(N\\) steps, the creature will explore \\(N_{unique}\\) unique sites on the lattice, where \\(\\langle N_{unique} \\rangle \\sim N&#94;\\beta\\) , with \\(\\beta \\approx 0.87\\) (see this post for more details). The number of food that this creature will gather by the end of the day is \\(x \\sim \\text{Binom}(N_{unique}, \\rho_{food})\\) . We'll represent the probability the creature gets \\(x\\) food as \\begin{eqnarray} \\tag{1} \\label{binom} p_x = {N_{unique} \\choose x} {\\rho_{food}}&#94;x (1-\\rho_{food})&#94;{N_{unique}-x}. \\\\ \\end{eqnarray} For very small \\(N\\) , \\(p_0\\) is large and \\(p_{2+} = 1 - p_0 - p_1\\) is small - the creature is likely to perish without reproducing. The larger \\(N\\) , the larger \\(N_{unique}\\) , resulting in smaller \\(p_0\\) and larger \\(p_{2+}\\) . There is a specific \\(N\\) for which \\(p_0 = p_{2+}\\) - at this critical point \\(N_c\\) , the creature is equally likely to reproduce and to die. For \\(\\rho_{food} = 0.03\\) , the relevant probabilities along with this critical point are plotted below. A couple of interesting notes: At \\(N_c\\) , the creature is most likely to get exactly 1 food and barely survive, it dies or reproduces with equal probability \\(\\approx 0.315 < 1/3\\) . \\(N_{unique, c} > 1/\\rho_{food}\\) - the creature must be, on average, gathering more than 1 food in order for it to not die off. We can reproduce \\(N_c\\) from the simulation framework as well. We ran numerous simulations, holding \\(\\rho_{food} = 0.03\\) constant and varying \\(N\\) for a few creatures (code here ). For each \\(N\\) , we plot the fraction of simulations that still had creatures after 200 days: For sufficiently large fields, the transition from long-term survival to population collapse is sharp at \\(N_c(\\rho_{food} = 0.03) \\approx 72\\) as predicted by the binomial probabilities above, creatures that walk longer than this survive and multiply, creatures that walk shorter, die off. Smaller fields show propensity for population collapse at even higher \\(N\\) which can be attributed to inherent instability of relatively small populations ( \\(n < 10\\) or so). We can extend this understanding to a population of \\(n\\) creatures: Based on the rules outlined at the beginning of the post, the number of creatures on day \\(i+1\\) , \\(n_{i+1}\\) , will be: \\begin{eqnarray} \\tag{2} \\label{dt} n_{i+1} = n_{i} - n_{i}(x=0) + n_{i}(x \\geq 2), \\\\ \\end{eqnarray} where \\(n_{i}(x=j)\\) is the number of creatures getting \\(j\\) food in the previous day (creatures tomorrow = creatures today - deaths today + births today). In equilibrium, \\(\\langle n_{i+1} \\rangle = \\langle n_i \\rangle\\) , implying \\(\\langle n_{i}(x=0) \\rangle = \\langle n_{i}(x>=2) \\rangle\\) (births=deaths). With well-separated creatures, we'll find \\begin{eqnarray} \\langle n_{i}(x=0) \\rangle &=& \\langle n_{i} \\rangle p_0 \\\\ \\langle n_{i}(x \\geq 2) \\rangle &=& \\langle n_{i} \\rangle p_{2+}. \\tag{3} \\label{birthdeath} \\end{eqnarray} Again, equilibrium is only achieved at \\(p_0 = p_{2+}\\) as discussed above. However, if the creatures are thriving, they will multiply to the point that they are no longer in isolation and start to \"steal\" each other's food. This will then curb the population's growth. To explore this, for simulations with creatures that survived their first 200 days, we look at the average number of creatures on the field from day 100 to 200 (to isolate the equilibrium condition). We plot \\(\\langle n \\rangle / \\rho_{food} M\\) (average creatures as a fraction of the daily food sprout rate): The trend generally makes sense, creatures with longer walks gather more food and can maintain larger populations. Near the critical point \\(N_c\\) , the stable populations are very low relative to the amount of food on the field, but they are stable (for sufficiently large fields). In this case, interactions between creatures are rare occurrences, but there are enough to keep the population from growing at the very slow rate that it would if all its creatures were in isolation ( \\(p_{2+}\\) is only marginally larger here than \\(p_0\\) ). The remainder of this post is a discussion of the theoretical line in the figure above. In order to approximate the impact of creature food \"stealing\" on the equilibrium population level, we must return to equations (\\ref{birthdeath}) and add a term to account for these interactions to each. Some creatures that would have otherwise reproduced will lose this ability due to displaced food. Likewise, some creatures that would have barely survived will die. As an approximation, we'll only model the impact on creatures that would have gotten exactly 1 (or 2) food as these creatures are the most at risk of losing the ability to survive (or reproduce) as a result of food displacement. The probability of the average creature having an interaction with one of the other creatures scales as \\( n_{i} (\\langle N_{unique} \\rangle/M)\\) (the number of other creatures times their relative footprint on the field), So, we can modify equations (\\ref{birthdeath}), reducing births and supplementing deaths: \\begin{eqnarray} \\langle n_{i}(x=0) \\rangle &=& \\langle n_{i} \\rangle (p_0 + p_1 \\frac{A \\langle N_{unique} \\rangle}{M} \\langle n_{i} \\rangle) \\\\ \\langle n_{i}(x \\geq 2) \\rangle &=& \\langle n_{i} \\rangle (p_{2+} - p_2 \\frac{A \\langle N_{unique} \\rangle}{M} \\langle n_{i} \\rangle) , \\tag{4} \\label{birthdeathint} \\end{eqnarray} where \\(A\\) is some constant of proportionality having to do with some of the geometry of the system and the fraction of food displaced as a result of the average interaction. Again, equilibrium will be reached when the two quantities in equations (\\ref{birthdeathint}) are equal, which leads to a unique solution for the average number of creatures at equilibrium: \\begin{eqnarray} \\tag{5} \\label{equilN} \\frac{\\langle n_{i} \\rangle}{\\rho_{food} M} = \\frac{1}{A \\rho_{food}}\\frac{p_{2+}-p_0}{p_1 + p_2}. \\end{eqnarray} The line in the figure above is for \\(A = 3/4\\) , which was a fit by eye. Not shown here, but we have confirmed that this prediction (with \\(A = 3/4\\) ) works reasonbly well for other values of \\(\\rho_{food}\\) as well. This discussed model has some limitations: It considers only creatures with exactly 1(2) food to be at risk of dying (failing to reproduce) as a result of interactions, while actually creatures that get 3+ food without interactions have a chance to have enough stolen to not reproduce or even to not even survive. It fails to consider 3+ -body interactions (a creature with a trajectory overlapping with 2 or more other creatures). It neglects the distribution of \\(N_{unique}\\) , essentially mapping \\(N_{unique} \\rightarrow \\langle N_{unique} \\rangle\\) . The model is thus not correct, but simple and captures the core of the problem. It reminds me vaguely of the Flory approximation in polymer physics (see discussion here ). if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"World Wandering Dudes","url":"https://efavdb.com/to-flourish-or-to-perish","loc":"https://efavdb.com/to-flourish-or-to-perish"},{"title":"Pricing dividend stocks","text":"We review how one can price a dividend-bearing stock by simply discounting its dividend stream using the value suggested by the Capital Asset Pricing Model ( CAPM ). As an example, we consider the price of AT &T common stock. The model result matches the current market price quite well. Varying inputs to the model also allows us to explore how its price might adjust to changes in company performance. Discounted revenue equations In this note, we'll consider the value of a stock that generates a discrete set of dividends, with dividend \\(D_t\\) paid out at time \\(t\\) , for \\(t \\in \\{1, 2, \\ldots\\}\\) . To estimate a fair price \\(V_0\\) for the stock today (assumed to be just after the most recent dividend payout at \\(t=0\\) ), we'll consider how the value evolves over time. If we purchase and hold the stock now, then at period \\(1\\) , we'll still have the stock but also a dividend. The total expected fair value at that time will then sum to \\begin{eqnarray} \\tag{1} \\label{ev_step} \\text{expected value at period 1} \\equiv E(V_1 + D_1). \\end{eqnarray} Of course, because there is some risk in our investment, the expected value above may not be realized. To take this into account — as well as the fact that inflation and other concerns cause us to prefer money now over money later — we will assert that we are only willing to pay a fraction \\(d\\) of (\\ref{ev_step}) above to acquire and hold the stock now. That is, we will define the fair price now as \\begin{eqnarray}\\label{discount} \\tag{2} V_0 = d \\times E(V_1 + D_1). \\end{eqnarray} Note that at this point, we still don't know the values of \\(V_0\\) or \\(V_1\\) in the above equation. However, we can make progress if we assume that the same \\(d\\) factor can be applied between any two periods. In this case, we also have for general \\(i\\) , \\begin{eqnarray} \\tag{3} E(V_i) = d \\times E(V_{i+1} + D_{i+1}). \\end{eqnarray} If we plug this into (\\ref{discount}) we obtain \\begin{eqnarray} \\tag{4} \\label{value} V_0 = \\sum_{i=1}&#94;{\\infty} d&#94;i E(D_i) \\end{eqnarray} This is an expression for the current price that depends only on the future dividends — quantities that we can attempt to forecast via extrapolations of recent dividend values, or through a thorough analysis of the company's prospects. To simplify our result further, we'll assume a steady growth for the dividends at rate \\(g\\) , so that \\begin{eqnarray} \\label{div_growth} \\tag{5} E(D_{t}) = D_0 g&#94;i \\end{eqnarray} Plugging this form into (\\ref{value}), we get \\begin{eqnarray} V_0 &=& \\sum_{i=1}&#94;{\\infty} d&#94;i D_0 g&#94;i \\\\ &=& D_0 \\frac{dg}{1 - dg} \\tag{6} \\label{value_geometric} \\end{eqnarray} Formula (\\ref{value_geometric}) is the result that we will use below to approximately price dividend-bearing stocks. Note that it is a function of only three quantities: \\(D_0\\) — the most recent dividend value, \\(g\\) — the assumed growth rate of the dividends, and \\(d\\) — the \"discount rate\", which in principle could vary person to person. To price a stock, one need only plug in values for these three quantities. To set \\(D_0\\) , we can simply look up the value of the most recent dividend. To set \\(g\\) , we can forecast the business prospects of the company. To set \\(d\\) , we should pick a value that reflects both how risk-averse we are and also how much risk the company in question carries: \\(d\\) should be set smaller for more risk averse individuals and for companies carrying more risk. In the next two sections, we (i) show how one can use the Capital Asset Pricing Model ( CAPM ) to estimate the \"market\" value of \\(d\\) , and (ii) use our results to estimate a fair market price for AT &T's stock. CAPM discount rate The CAPM provides a method for estimating the return one should expect from a stock. Combining this with our analysis above, we can get an estimate for discount rate \\(d\\) the market applies to a stock. Plugging this into (\\ref{value_geometric}) then gives a self-consistent estimate for the stock's fair market price. To make the connection, we first note that (\\ref{discount}) implies that the expected return on holding the stock for one period is given by \\begin{eqnarray} E(r_{stock}) &\\equiv & \\log \\left ( \\frac{E(V_{1} + D_{1})}{V_0}\\right)\\\\ &=& \\log (1/d) \\tag{8}\\label{grow_at_d_inv} \\end{eqnarray} Next, we quote the CAPM 's estimate for the return of a stock \\(&#94;1\\) \\begin{eqnarray} \\tag{9} \\label{capm} E(r_{stock, capm}) = r_{risk free} + \\beta \\times \\left (r_{market} - r_{risk free} \\right). \\end{eqnarray} Here, \\(r_{risk free}\\) is typically taken as the rate of growth of short-term treasury bonds, \\(r_{market}\\) is the mean growth rate of the market as a whole — often the historical growth rate of the S&P500 is used here, and \\(\\beta\\) is the slope in a fit of the stock's past total returns to those of the market. Tests of this prediction have been carried out, and the model has been found to be fairly accurate. We can therefore approximate the market's expectation for the return on the stock to be that given by CAPM . Equating \\(E(r_{stock})\\) and \\(E(r_{stock, capm})\\) from the last two lines gives the CAPM approximation for the market discount rate, \\begin{eqnarray} d_{market} &\\approx& \\exp \\left (-E(r_{stock, capm}) \\right) \\\\ &=& \\exp \\left ( - r_{risk free} - \\beta \\times \\left (r_{market} - r_{risk free} \\right) \\right) \\tag{10} \\label{market_d} \\end{eqnarray} These equations give us a reasonable method for setting \\(d\\) , enabling us to estimate what the market as a whole should think is a fair price for a given stock. We turn now to our application, the pricing of AT &T's common stock. Application: AT &T (ticker T) We are now ready to consider the fair market value of AT &T's common stock. Recall that we need three quantities to price the stock, \\(D_0\\) , \\(d\\) , and \\(g\\) . We consider each of these below. THE MOST RECENT DIVIDEND A quick online search for the dividend history of AT &T's shows that a \\(52\\) cent dividend was payed in October 2020 — link . Therefore, \\begin{eqnarray} D_0 = 0.52 \\tag{10} \\end{eqnarray} This company pays out dividends quarterly. THE DIVIDEND GROWTH RATE The link above also shows that the dividend paid by AT &T in 2015 was \\(47\\) cents per quarter. The historical quarterly growth rate of the dividend over this period has therefore been \\begin{eqnarray} g_{historical} &=& (52 / 47)&#94;{1 / 20} \\\\ &\\approx & 1.005 \\tag{11} \\end{eqnarray} Here, the exponent was one over the number of quarters considered between October 2015 and October 2020 — 20 quarters. It is important to note that the historical growth rate is not necessarily that which we can expect going forward. In fact, AT &T has already declared a dividend for January 2021 of \\(52\\) cents, the same value as was distributed each quarter in 2020. In each of the past five years, the company raised dividends with each new year. The upcoming dividend therefore signals a change in growth rate — at least in the short-term. If a value near \\(g\\approx 1.000\\) better describes the behavior of AT &T in the near term, this will significantly affect its stock price. We estimate prices for a set of \\(g\\) values below to highlight this dependence. THE MARKET DISCOUNT RATE To evaluate the CAPM market discount rate, we need to know a stock's beta. This can be obtained through a regression or it can simply be looked up online. Doing this for AT &T we see a current value quoted near \\(0.7\\) . However, over the last five years the value has varied, going as low as \\(0.3\\) or so in 2016 — link . Since the value has fluctuated over time, we'll quote values below for a few possible forward looking betas, \\(\\{0.3, 0.5, 0.7, 0.9\\}\\) . To convert these to discount rates, we used the capm_d_quarterly method below — python code that implements (\\ref{market_d}). You can see that we assume a risk-free rate of \\(0.07\\%\\) , and a market growth rate of \\(9.8\\%\\) — these are the current short-term treasury rates and historical average S&P500 rates, according to an online search. The discount rates that result for our chosen betas are \\begin{eqnarray} d_{beta = 0.3} &=& 0.993 \\\\ d_{beta = 0.5} &=& 0.988 \\\\ d_{beta = 0.7} &=& 0.983 \\\\ d_{beta = 0.9} &=& 0.978 \\tag{12} \\label{discounting_macys} \\end{eqnarray} FINAL CAPM PRICES We now have everything we need to estimate a fair market price for AT &T. We simply need to plug the values for \\(D_0\\) , \\(g\\) , and \\(d\\) we have obtained above into the pricing equation, (\\ref{value_geometric}). Our code snippet below takes care of this for us and the values that it provides are printed underneath. Reviewing the output, we see a large range of possible price values as we vary our degrees of freedom — the values range from \\(18\\) to over \\(200\\) ! (The NaN value in the table corresponds to a situation where the growth rate is larger than the discount rate, which is not possible long term). These results indicate that there is strong sensitivity in the price of a stock to its growth rate and beta. E.g., we note that if we plug in today's values of \\(\\beta = 0.7\\) and \\(g = 1.000\\) (marked with an * in the table below), we get a value of \\(29.39\\) dollars — amazingly close to the last traded value of \\(29.40\\) on 12/18 (not bad!). However, we note that if the company were able to quickly return to the old steady growth rate of \\(1.005\\) , we'd move to the cell just right of the current value, giving a price of \\(41\\) dollars — an impressive gain. Should we buy the stock? That's for you to decide — hopefully the results here have helped provide a mental framework useful for thinking through that question. import numpy as np import pandas as pd R_RISK_FREE = 0.0007 R_MARKET = 0.098 D_0 = 0.51 def capm_d_quarterly ( beta , r_risk_free = R_RISK_FREE , r_market = R_MARKET ): \"\"\" Return the discount rate from CAPM, on a quaterly basis. \"\"\" r_annual = r_risk_free + beta * ( r_market - r_risk_free ) r_quarterly = r_annual / 4.0 d_quarterly = np . exp ( - r_quarterly ) return d_quarterly def capm_price ( beta , g , D_0 ): \"\"\" Price of stock given its beta, dividend growth rate, and most recent dividend in dollars. \"\"\" d = capm_d_quarterly ( beta ) if ( d * g > 1 ): return None else : return D_0 * d * g / ( 1 - d * g ) results = [] for beta in ( 0.3 , 0.5 , 0.7 , 0.9 ): for g in ( 0.99 , 0.995 , 1.0 , 1.005 , 1.01 ): results . append ({ 'beta' : beta , 'g' : g , 'price' : capm_price ( beta = beta , g = g , D_0 = D_0 )}) df = pd . DataFrame ( results ) print pd . pivot_table ( df , values = 'price' , index = [ 'beta' ], columns = [ 'g' ]) # # output # g 0.990 0.995 1.000 1.005 1.010 # beta # 0.3 28.850623 40.594413 67.995569 204.979922 NaN # 0.5 22.526179 29.140479 41.082910 69.133459 213.387273 # 0.7 18.459807 22.703362 29.392585* 41.497605 70.069095 # 0.9 15.625393 18.579209 22.856848 29.605283 41.834554 Footnotes [1] For a simple derivation of CAPM 's main results, see Derman . As a caveat, we note that one of the primary assumptions of CAPM is that market prices are set by rational market participants. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Finance","url":"https://efavdb.com/pricing-dividend-stocks","loc":"https://efavdb.com/pricing-dividend-stocks"},{"title":"Utility engines","text":"A person's happiness does not depend only on their current lot in life, but also on the rate of change of their lot. This is because a person's prior history informs their expectations. Here, we build a model that highlights this emotional \"path-dependence\" quality of utility. Interestingly, we find that it can be gamed: One can increase net happiness via a process of gradual increased deprivation, followed by sudden jolts in increased consumption, as shown in the cartoon below — this is the best approach. In particular, it beats the steady consumption rate strategy. The utility function model In this post, we assume that the utility realized by a person over a time \\(T\\) is given by \\begin{eqnarray} \\tag{1} \\label{1} U(t) = \\int_0&#94;T \\left (a \\vert x&#94;{\\prime}(t) \\vert + b \\vert x&#94;{\\prime}(t) \\vert&#94;2 \\right) \\text{sign}(x&#94;{\\prime}(t)) dt. \\end{eqnarray} Here, \\(x(t)\\) is a measure of consumption (one's \"lot in life\") at time \\(t\\) . Our model here is a natural Taylor expansion, relevant for small changes in \\(x(t)\\) . It is positive when consumption is going up and negative when consumption is going down. We'll be interested to learn whether varying \\(x(t)\\) subject to a fixed average constraint can result in increased net happiness, relative to the steady state consumption solution. The answer is yes, and this can be understood qualitatively by considering the two terms above: First term, \\(a \\vert x&#94;{\\prime}(t) \\vert \\text{sign}(x&#94;{\\prime}(t))\\) : This term is proportional to the rate of change of consumption. We assume that \\(a > 0\\) , so that as we start to consume less, it is negative, as we go back up it is positive. We will be interested in cycles that repeat here, so will assume that our \\(x(t)\\) is periodic with period \\(T\\) . In this case, the linear term — while possibly acutely felt at each moment — will integrate to an average of zero over the long term. Second term, \\(b \\vert x&#94;{\\prime}(t) \\vert&#94;2 \\text{sign}(x&#94;{\\prime}(t))\\) : This term is non-linear. It is very weak for small rates of change but kicks in strongly whenever we have an abrupt change. We again assume that \\(b>0\\) so that big drops in consumption are very painful, etc. With the above comments in place, we can now see how our figure gives a net gain in utility: On average, only the quadratic term matters and this will effectively only contribute during sudden jumps. The declines in our figure are gradual, and so contribute only weakly in this term. However, the increases are sudden and each give a significant utility \"fix\" as a consequence. For those interested, we walk through the simple mathematics needed to exactly optimize our utility function in an appendix. Concluding comments on the practical application of these ideas are covered next. Practical considerations A few comments: Many people treat themselves on occasion — with chocolates, vacations, etc. — perhaps empirically realizing that varying things improves their long term happiness. It is interesting to consider the possibility of optimizing this effect, which we do with our toy model here: In this model, we do not want to live in a steady state salted with occasional treats: Instead, we want the saw-tooth shape of consumption shown in our figure. A sad fact of life is that progress tends to be gradual, while set backs tend to occur suddenly — e.g., stocks tend to move in patterns like this. This is the worst way things could go, according to our model. True, human utility functions are certainly more complex than what we have considered here. It is interesting to contrast models of utility with conservative physical systems, where the energy of a state is not path dependent, but depends only on the current state. Path dependence means that two identical people in the same current situation can have very different valuations of their lot in life. The appendix below discusses the mathematical optimization of (\\ref{1}). Appendix — optimizing (\\ref{1}) For simplicity, we consider a path that goes down from \\(t=0\\) to \\(t_0\\) — making its way down by \\(\\Delta x\\) , then goes back up to where it started from \\(t_0\\) to \\(T\\) . It is easy to see that the first term integrates to zero in this case, provided we start and end at the same value of \\(x\\) . Now, consider the second term. On the way down, we have \\begin{eqnarray} \\int_0&#94;{t_0} \\vert x&#94;{\\prime} \\vert&#94;2 dt &\\equiv & t_0 \\langle \\vert x&#94;{\\prime} \\vert&#94;2 \\rangle_{t_0} \\\\ &\\geq & t_0 \\langle \\vert x&#94;{\\prime} \\vert \\rangle&#94;2_{t_0} \\\\ &=& t_0 \\left( \\frac{\\Delta x}{t_0} \\right)&#94;2 \\tag{2} \\end{eqnarray} The inequality here is equivalent to the statement that the variance of the rate of change of our consumption is positive. We get equality — and minimal loss from the quadratic term on the way down — if the slope is constant throughout. That is, we want a linear drop in \\(x(t)\\) from \\(0\\) to \\(t_0\\) . With this choice, we get \\begin{eqnarray} \\int_0&#94;{t_0} \\vert x&#94;{\\prime} \\vert&#94;2 dt = \\frac{\\Delta x&#94;2}{t_0}. \\tag{3} \\end{eqnarray} On the way back up, we'd like to max out the inequality analogous to above. This is achieved by having the recovery occur as quickly as possible, say over a window of time \\(t_{r}\\) with \\(r\\) standing for recovery. We can decrease our loss by increasing \\(t_0\\) up to \\(t_0 \\to T\\) . In this case, our integral of the quadratic over all time goes to \\begin{eqnarray} \\text{gain} = b \\Delta x&#94;2 \\left (\\frac{1}{t_r} - \\frac{1}{T} \\right) \\tag{4}. \\end{eqnarray} This gives the optimal lift possible — that realized by the saw-tooth approach shown in our first figure above. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Case studies","url":"https://efavdb.com/utility-engines","loc":"https://efavdb.com/utility-engines"},{"title":"2 + 1 = 4, by quinoa","text":"I was struck the other day by the following: The cooking instructions on my Bob's tri-colored quinoa package said to combine 2 cups of water with 1 cup of dried quinoa, which would ultimately create 4 cups of cooked quinoa. See image above. My first reaction was to believe that some error had been made. However, I then realized that the explanation was packing: When one packs spheres or other awkward solid geometric shapes into a container, they cannot fill the space completely. Little pockets of air sit between the spheres. A quick google search for the packing fraction of spheres gives a value of \\(0.75\\) for a crystalline structure and about \\(0.64\\) for random packings — apparently a universal law. We can get a similar number out from my quinoa instructions: Suppose that before the quinoa is cooked, the water fills its volume completely. However, after cooking, the water is absorbed into the quinoa and forced to share its packing fraction. The quinoa stays at the same packing fraction before and after cooking, so the water must be responsible for the volume growth. This implies it went from 2 cups to 3, or \\begin{eqnarray} \\tag{1} \\label{1} 2 = \\rho \\times 3, \\end{eqnarray} where \\(\\rho\\) is the packing fraction of the quinoa \"spheres\". We conclude that the packing fraction is \\(\\rho = 2/3\\) , very close to the googled value of \\(\\rho = 0.64\\) . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Cooking math","url":"https://efavdb.com/quinoa packing","loc":"https://efavdb.com/quinoa packing"},{"title":"Long term credit assignment with temporal reward transport","text":"[ TOC ] Summary Standard reinforcement learning algorithms struggle with poor sample efficiency in the presence of sparse rewards with long temporal delays between action and effect. To address the long term credit assignment problem, we build on the work of [1] to use \"temporal reward transport\" ( TRT ) to augment the immediate rewards of significant state-action pairs with rewards from the distant future using an attention mechanism to identify candidates for TRT . A series of gridworld experiments show clear improvements in learning when TRT is used in conjunction with a standard advantage actor critic algorithm. Introduction Episodic reinforcement learning ( RL ) models the interaction of an agent with an environment as a Markov Decision Process with a finite number of time steps \\(T\\) . The environment dynamics \\(p(s',r|s, a)\\) are modeled as a joint probability distribution over the next state \\(s'\\) and reward \\(r\\) picked up along the way given the previous state \\(s\\) and action \\(a\\) . In general, the agent does not have access to an exact model of the environment. The agent's goal is to maximize its cumulative rewards, the discounted returns \\(G_t\\) , \\begin{eqnarray}\\label{return} \\tag{1} G_t := R_{t+1} + \\gamma R_{t+2} + … = \\sum_{k=0}&#94;T \\gamma&#94;k R_{t+k+1} \\end{eqnarray} where \\(0 \\leq \\gamma \\leq 1\\) , and \\(R_{t}\\) is the reward at time \\(t\\) . In episodic RL , the discount factor \\(\\gamma\\) is often used to account for uncertainty in the future, to favor rewards now vs. later, and as a variance reduction technique, e.g. in policy gradient methods [2, 3]. Using a discount factor \\(\\gamma < 1\\) introduces a timescale by exponentially suppressing rewards in the future by \\(\\exp(-n/\\tau_{\\gamma})\\) . The number of timesteps it takes for a reward to decay by \\(1/e\\) is \\(\\tau_{\\gamma} = 1/(1-\\gamma)\\) , in units of timesteps, which follows from solving for \\(n\\) after setting the left and right sides of (\\ref{discount-timescale}) to be equal \\begin{align}\\label{discount-timescale}\\tag{2} \\gamma &#94; n \\approx \\frac{1}{e} = \\lim_{n \\rightarrow \\infty} \\left(1 - \\frac{1}{n} \\right)&#94;n \\end{align} The state value function \\(v_{\\pi}(s)\\) is the expected return when starting in state \\(s\\) , following policy \\(\\pi(a|s) := p(a|s)\\) , a function of the current state. \\begin{eqnarray}\\label{state-value} \\tag{3} v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s] \\end{eqnarray} Policy gradient algorithms improve the policy by using gradient ascent along the gradient of the value function. \\begin{eqnarray}\\label{policy-gradient} \\tag{4} \\nabla_{\\theta} v_\\pi(s_0) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}&#94;{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}(A_t | S_t) \\mathcal{R}(\\tau)\\right], \\end{eqnarray} where \\(\\tau \\sim \\pi\\) describes the agent's trajectory following policy \\(\\pi\\) beginning from state \\(s_0\\) , and \\(\\mathcal{R}(\\tau)\\) is a function of the rewards obtained along the trajectory. In practice, policy gradients approximate the expected value in (\\ref{policy-gradient}) by sampling, which results in very high variance estimates of the gradient. Common techniques to reduce the variance of the estimated policy gradient include [2] only assigning credit for rewards (the \"rewards-to-go\") accumulated after a particular action was taken instead of crediting the action for all rewards from the trajectory. subtracting a baseline from the rewards weight that is independent of action. Oftentimes, this baseline is the value function in (\\ref{state-value}). using a large batch size. using the value function (\\ref{state-value}) to bootstrap the returns some number of steps into the future instead of using the full raw discounted return, giving rise to a class of algorithms called actor critics that learn a policy and value function in parallel. For example, one-step bootstrapping would approximate the discounted returns in (\\ref{return}) as \\begin{eqnarray}\\label{bootstrap} \\tag{5} G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma&#94;2 R_{t+3} + \\ldots \\approx R_{t+1} + \\gamma V(S_{t+1}), \\end{eqnarray} where \\(V(S_{t+1})\\) is the estimate of the value of state \\(S_{t+1}\\) (\\ref{state-value}). All of these techniques typically make use of discounting, so an action receives little credit for rewards that happen more than \\(\\tau_{\\gamma}\\) timesteps in the future, making it challenging for standard reinforcement learning algorithms to learn effective policies in situations where action and effect are separated by long temporal delays. Results Temporal reward transport We use temporal reward transport (or TRT ), inspired directly by the Temporal Value Transport algorithm from [1], to mitigate the loss of signal from discounting by splicing temporally delayed future rewards to the immediate rewards following an action that the TRT algorithm determines should receive credit. To assign credit to a specific observation-action pair, we use an attention layer in a neural network binary classifier. The classifier predicts whether the undiscounted returns for an episode are below or above a certain threshold. If a particular observation and its associated action are highly attended to for the classification problem, then that triggers the splicing of future rewards in the episode to that particular observation-action pair. Model training is divided into two parts: Experience collection using the current policy in an advantage actor critic ( A2C ) model. Parameter updates for the A2C model and binary classifier. TRT happens between step 1 and 2; it plays no role in experience collection, but modifies the collected rewards through the splicing mechanism, thereby affecting the advantage and, consequently, the policy gradient in (\\ref{policy-gradient}). \\begin{eqnarray} \\notag R_t \\rightarrow R_t + [\\text{distal rewards} (t' > t + \\tau_\\gamma)] \\end{eqnarray} Environment for experiments We created a gym gridworld environment to specifically study long term credit assignment. The environment is a simplified version of the 3-d DeepMind Lab experiments laid out in [1]. As in [1], we structure the environment to comprise three phases. In the first phase, the agent must take an action that yields no immediate reward. In the second phase, the agent engages with distractions that yield immediate rewards. In the final phase, the agent can acquire a distal reward, depending on the action it took in phase 1. Concretely, the gridworld environment consists of: (1) Empty grid with key: agent can pick up the key but receives no immediate reward for picking it up. (2) Distractor phase: Agent engages with distractors, gifts that yield immediate rewards. (3) Delayed reward phase: Agent should move to a green goal grid cell. If the agent is carrying the key when it reaches the goal, it is rewarded extra points. The agent remains in each phase for a fixed period of time, regardless of how quickly it finishes the intermediate task, and then teleports to the next phase. At the end of each episode, the environment resets with a different random seed that randomizes the placement of the key in phase 1 and distractor objects in phase 2. Experiments In all experiments, we fix the time spent in phase 1 and phase 3, the number of distractor gifts in phase 2, as well as the distal reward in phase 3. In phase 3, the agent receives 5 points for reaching the goal without a key and 20 points for reaching the goal carrying a key (with a small penalty proportional to step count to encourage moving quickly to the goal). Our evaluation metric for each experiment is the the distal reward obtained in phase 3, which focuses on whether the agent learns to pick up the key in phase 1 in order to acquire the distal reward, although we verify that the agent is also learning to open the gifts in phase 2 by plotting the overall returns (see \"Data and code availability\" section). Each experiment varies a particular parameter in the second phase, namely, the time delay, distractor reward size, and distractor reward variance, and compares the performance of the baseline A2C algorithm with A2C supplemented with TRT ( A2C + TRT ). Time delay in distractor phase We vary the time spent in the distractor phase, \\(T_2\\) , as a multiple of the discount factor timescale. We used a discount factor of \\(\\gamma=0.99\\) , which corresponds to a timescale of ~100 steps according to (\\ref{discount-timescale}). We ran experiments for \\(T_2 = (0, 0.5, 1, 2) * \\tau_{\\gamma}\\) . The distractor reward is 3 points per gift. Fig 1. Returns in phase 3 for time delays in phase 2 of 0.5 \\(\\tau_{\\gamma}\\) , \\(\\tau_{\\gamma}\\) , and 2 \\(\\tau_{\\gamma}\\) . As the environment becomes more challenging from left to right with increasing time delay, we see that A2C plateaus around 5 points in phase 3, corresponding to reaching the goal without the key, whereas A2C + TRT increasingly learns to pick up the key over the training period. Distractor reward size We vary the size of the distractor rewards, 4 gifts for the agent to toggle open, in phase 2. We run experiments for a reward of 0, 1, 5, and 8, resulting in maximum possible rewards in phase 2 of 0, 4, 20, and 32. In comparison, the maximum possible reward in phase 3 is 20. Fig 2. Returns in phase 3 for distractor rewards of size 0, 5, and 8. Like the time delay experiments, we see that A2C + TRT shows progress learning to pick up the key, whereas A2C does not over the training period with increasing distractor sizes. Distractor reward variance We fix the mean reward size of the gifts in phase 2 at 5, but change the variance of the rewards by drawing each reward from a uniform distribution centered at 5, with minimum and maximum ranges of [5, 5], [3, 7], and [0, 10], corresponding to variances of 0, 1.33, and 8.33, respectively. Fig 3. Returns in phase 3 for distractor reward variance of size 0, 1.33, and 8.33. The signal-to-noise ratio of the policy gradient, defined as the ratio of the squared magnitude of the expected gradient to the variance of the gradient estimate, was shown to be approximately inversely proportional to the variance of the distractor rewards in phase 2 in [1] for \\(\\gamma = 1\\) . The poor performance of A2C in the highest variance (low signal-to-noise ratio) case is consistent with this observation, with a small standard deviation in performance around the plateau value of 5 compared to the experiments on time delay and distractor reward size. Discussion Like temporal value transport introduced in [1], TRT is a heuristic. Nevertheless, coupling this heuristic with A2C has been shown to improve performance on several tasks characterized by delayed rewards that are a challenge for standard deep RL . Our contribution is a simplified, modular implementation of core ideas in [1], namely, splicing additional rewards from the distant future to state-action pairs identified as significant through a self-attention mechanism. Unlike [1], we implement the self-attention mechanism in a completely separate model and splice the rewards-to-go instead of an estimated value. In addition to the modularity that comes splitting out the attention mechanism for TRT into a separate model, another advantage of decoupling the models is that we can increase the learning rate of the classifier without destabilizing the learning of the main actor critic model if the classification problem is comparatively easy. Related work Other works also draw on the idea of using hindsight to reduce the variance estimates of the policy gradient, and hence increase sample efficiency. \"Hindsight credit assignment\" proposed in [7] similarly learns discriminative models in hindsight that give rise to a modified form of the value function, evaluated using tabular models in a few toy environments (not focused specifically on the long term credit assignment problem). RUDDER [8] is more similar in spirit to [1] and TRT in the focus on redistributing rewards to significant state-action pairs, but identified using saliency analysis on an LSTM instead of an attention mechanism. Future work The robustness of the TRT algorithm should be further assessed on a wider variety of environments, including e.g. Atari Bowling, which is another environment with a delayed reward task used for evaluations by [1] and [8]. It remains to be seen whether the attention mechanism and TRT can handle more complex scenarios, in particular scenarios where a sequence of actions must be taken. Just as it is difficult to extract interpretable features from a linear model in the presence of multicollinearity, it is possible that the attention-based classifier may encounter similar problems identifying important state-action pairs when a sequence of actions is required, as our model has no mechanism for causal reasoning. Although our experiments only evaluated TRT on A2C , coupling it with any policy gradient method based on sampling action space should yield similar benefits, which could be straightforwardly tested with our modular implementation. A benefit of using self-attention is the temporal granularity over an LSTM . However, a downside is that our approach relies on having the full context of the episode for the attention mechanism ([1] similarly relies on full episodes), in contrast to other methods that can handle commonly used truncation windows with a bootstrapped final value for non-terminal states. Holding full episodes in memory can become untenable for very long episodes, but we have not yet worked out a way to handle this situation in the current setup. Our first pass implementation transported the raw rewards-to-go instead of the value estimate used in [1], but it is unclear whether transporting the rewards-to-go (essentially re-introducing a portion of the undiscounted Monte Carlo returns) for a subset of important state-action pairs provides a strong enough signal to outweigh the advantages of using a boostrapped estimate intended for variance reduction; the answer may depend on the particular task/environment and is of course contingent on the quality of the value estimate. The classifier model itself has a lot of room for experimentation. The idea of using a classifier was motivated by a wish to easily extract state-action pairs with high returns from the attention layer, although we have yet to explore whether this provides a clear benefit over a regression model like [1]. The binary classifier is trained to predict whether the rewards-to-go of each subsequence of an episode exceeds a moving average of maximum returns. On the one hand, this is less intuitive than only making the prediction for undiscounted returns of the full episode and introduces highly non-iid inputs for the classifier, which can make make training less stable. On the other hand, one can interpret the current format as a form of data augmentation that results in more instances of the positive class (high return episodes) that benefits the classifier. If the classifier were modified to only make a single prediction per episode, it may be necessary to create a buffer of recent experiences to shift the distribution of data towards more positive samples for the classifier to draw from in addition to episodes generated from the most recent policy (with the untested assumption that the classifier would be less sensitive to training on such off-policy data than the main actor critic model while benefiting from the higher incidence of the positive class). Finally, the TRT algorithm introduces additional hyperparameters that could benefit from additional tuning, including the constant factor multiplying the transported rewards and the attention score threshold to trigger TRT . Methods Environment The agent receives a partial observation of the environment, the 7x7 grid in front of it, with each grid cell encoding 3 input values, resulting in 7x7x3 values total (not pixels). The gridworld environment supports 7 actions: left, right, forward, pickup, drop, toggle, done. The environment consists of three phases: Phase 1 \"key\": 6x6 grid cells, time spent = 30 steps Phase 2 \"gifts\": 10x10 grid cells, time spent = 50 steps (except for the time delay experiment, which varies the time spent) Phase 3 \"goal\": 7x7 grid cells, time spent = 70. If the agent picks up the key in phase 1, it is initialized carrying the key in phase 3, but not phase 2. The carrying state is visible to the agent in phase 1 and 3. Except for the time delay experiment, each episode is 150 timesteps. Distractor rewards in phase 2: 4 distractor objects, gifts that the agent can toggle open, that yield immediate rewards Each opened gift yields a mean reward of 3 points (except in the reward size experiment) Gift rewards have a variance of 0 (except in the reward variance experiment) Distal rewards in phase 3: 5 points for reaching the goal without a key and 20 points for reaching the goal carrying a key. There is a small penalty of -0.9 * step_count / max_steps=70 to encourage moving quickly to the goal. For convenience of parallelizing experience collection of complete episodes, the time in the final phase is fixed, even if the agent finishes the task of navigating to the green goal earlier. Furthermore, for convenience of tracking rewards acquired in the final phase, the agent only receives the reward for task completion in the last step of the final phase, even though this last reward reflects the time and state in which the agent initially reached the goal. Note, unlike the Reconstructive Memory Agent in [1], our agent does not have the ability to encode and reconstruct memories, and our environment is not set up to test for that ability. Agent model The agent's model is an actor critic consisting of 3 components: an image encoder convolutional net ( CNN ), an recurrent neural net layer providing memory, and dual heads outputting the policy and value. We used an open-sourced model that has been extensively tested for gym-minigrid environments from [4]. Fig 4. A2C model with three convolutional layers, LSTM , and dual policy and value function heads. The image encoder consists of three convolutional layers interleaved with rectified linear (ReLU) activation functions. A max pooling layer also immediately precedes the second convolutional layer. The encoded image is followed by a single Long Short Term Memory ( LSTM ) layer. The LSTM outputs a hidden state which feeds into the dual heads of the actor critic. Both heads consist of two fully connected linear layers sandwiching a tanh activation layer. The output of the actor, the policy, is the same size as the action space in the environment. The output of the critic is a scalar corresponding to the estimated value. Binary classifier with self-attention The inputs to the binary classifier are the sequence of image embeddings output by the actor critic model's CNN (not the hidden state of the LSTM ) and one-hot encoded actions taken in that state. The action passes through a linear layer with 32 hidden units before concatenation with the image embedding. Next, the concatenated vector \\(\\mathbf{x}_i\\) undergoes three separate linear transformations, playing the role of \"query\", \"key\" and \"value\" (see [6] for an excellent explanation upon which we based our implementation of attention). Each transformation projects the vector to a space of size equal to the length of the episode. \\begin{eqnarray}\\label{key-query} \\tag{6} \\mathbf{q}_i &=& \\mathbf{W}_q \\mathbf{x}_i \\\\ \\mathbf{k}_i &=& \\mathbf{W}_k \\mathbf{x}_i \\\\ \\mathbf{v}_i &=& \\mathbf{W}_v \\mathbf{x}_i \\\\ \\end{eqnarray} The self-attention layer outputs a weighted average over the value vectors, where the weight is not a parameter of the neural net, but the dot product of the query and key vectors. \\begin{eqnarray}\\label{self-attention} \\tag{7} w'_{ij} &=& \\mathbf{q}_i&#94;\\top \\mathbf{k}_j \\\\ w_{ij} &=& \\text{softmax}(w'_{ij}) \\\\ \\mathbf{y_i} &=& \\sum_j w_{ij} \\mathbf{v_j} \\end{eqnarray} The dot product in (\\ref{self-attention}) is between embeddings of different frames in an episode. We apply masking to the weight matrix before the softmax in (\\ref{self-attention}) to ensure that observations from different episodes do not pay attention to each other, in addition to future masking (observations can only attend past observations in the same episode). The output of the attention layer then passes through a fully connected layer with 64 hidden units, followed by a ReLU activation, and the final output is a scalar, the logit predicting whether the rewards-to-go from a given observation are below or above a threshold. The threshold itself is a moving average over the maximum undiscounted returns seen across network updates, where the averaging window is a hyperparameter that should balance updating the threshold in response to higher returns due to an improving policy (in general, increasing, although monotonicity is not enforced) with not increasing so quickly such that there are too few episodes in the positive (high returns) class in a given batch of collected experiences. Fig 5. Binary classifier model with attention, accepting sequences as input. Temporal reward transport After collecting a batch of experiences by following the A2C model's policy, we calculate the attention scores \\(w_{ij}\\) from (\\ref{self-attention}) using observations from the full episode as context. Fig 6. Attention scores for a single episode with future masking (of the upper right triangle). The bright vertical stripes correspond to two highly attended state-action pairs. We calculate the importance, defined as the average weight of observation \\(i\\) , ignoring masked regions in the attention matrix as \\begin{eqnarray}\\label{importance} \\tag{8} \\text{importance}_i = \\frac{1}{T - i} \\sum_{j \\geq i}&#94;{T} w_{ij} \\end{eqnarray} Fig 7. Importances for a batch of collected experiences (16 processes x 600 frames = 9600 frames), with frame or step number on the horizontal axis and process number on the vertical axis. Observations with an importance score above a threshold (between 0 and 1) hyperparameter are eligible for TRT . After identifying the candidates for TRT , we add the distal rewards-to-go, weighted by the importance and another hyperparameter for tuning the impact of the TRT rewards \\(\\alpha_{TRT}\\) to the original reward \\(r_i\\) obtained during experience collection: \\begin{eqnarray}\\label{trt} \\tag{9} r_i &\\rightarrow& r_i + \\text{TRT-reward}_i \\\\ \\text{TRT-reward}_i &\\equiv& \\alpha_{TRT} * \\text{importance}_i * \\text{rewards-to-go}_i \\end{eqnarray} We define the distal rewards-to-go in (\\ref{trt}) as the total undiscounted returns from observation \\(i\\) , excluding rewards accumulated in an immediate time window of size equal to the discount factor timescale \\(\\tau_\\gamma\\) defined in (\\ref{discount-timescale}). This temporal exclusion zone helps prevent overcounting rewards. We calculate the advantage after TRT using the generalized advantage estimation algorithm GAE - \\(\\lambda\\) [3] with \\(\\lambda=0.95\\) , which, analogous to TD - \\(\\lambda\\) [2], calculates the advantage from an exponentially weighted average over 1- to n-step bootstrapped estimates of the \\(Q\\) value. One of the benefits of using GAE - \\(\\lambda\\) is the spillover effect that enables the TRT -reinforced rewards to directly affect neighboring state-action pairs in addition to the triggering state-action pair. Training For experience collection, we used 16 parallel processes, with 600 frames collected per process for a batch size of 9600 frames between parameter updates. The A2C model loss per time step is \\begin{eqnarray} \\label{a2c-loss} \\tag{10} \\mathcal{L}_{A2C} \\equiv \\mathcal{L}_{policy} + \\alpha_{value} \\mathcal{L}_{value} - \\alpha_{entropy} \\text{entropy}, \\end{eqnarray} where \\begin{eqnarray} \\notag \\mathcal{L}_{policy} &=& - \\log p(a_t | o_t, h_t), \\\\ \\mathcal{L}_{value} &=& \\left\\Vert \\hat{V}(o_t, h_t) - R_t \\right\\Vert&#94;2, \\end{eqnarray} and \\(o_t\\) and \\(h_t\\) are the observation and hidden state from the LSTM at time \\(t\\) , respectively. We accumulate the losses defined in (\\ref{a2c-loss}) by iterating over batches of consecutive time steps equal to the size of the LSTM memory of 10, i.e. truncated backpropagation in time for 10 timesteps. The classifier with attention has a binary cross entropy loss , where the contribution to the loss from positive examples is weighted by a factor of 2. We clip both gradient norms according to a hyperparameter \\(\\text{max_grad_norm}=0.5\\) , and we optimize both models using RMSprop with learning rates of 0.01, RMSprop \\(\\alpha=0.99\\) , and RMSprop \\(\\epsilon=1e&#94;{-8}\\) . Data and code availability Environment Code for the components of the 3 phase environment is in our fork of gym-minigrid . The base environment for running the experiments is defined in https://github.com/frangipane/rl-credit/ . Each experiment script subclasses that base environment, varying some parameter in the distractor phase. Experiments The parameters and results of the experiments are documented in the following publicly available reports on Weights and Biases: Distractor phase time delays Distractor phase reward size Distractor phase variance of rewards Code for running the experiments is at https://github.com/frangipane/rl-credit in the examples/ submodule. Acknowledgements Thank you to OpenAI, my OpenAI mentor J. Tworek, Microsoft for the cloud computing credits, Square for supporting my participation in the program, and my 2020 cohort of Scholars: A. Carrera, P. Mishkin, K. Ndousse, J. Orbay, A. Power (especially for the tip about future masking in transformers), and K. Slama. References [1] Hung C, Lillicrap T, Abramson J, et al. 2019. Optimizing agent behavior over long time scales by transporting value . Nat Commun 10, 5223. [2] Sutton R, Barto A. 2018. Reinforcement Learning: An Introduction (2nd Edition) . Cambridge ( MA ): MIT Press. [3] Schulman J, Moritz P, Levine S, et al. 2016. High-Dimensional Continuous Control Using Generalized Advantage Estimation . ICLR . [4] Willems L. RL Starter Files and Torch AC . GitHub. [5] Chevalier-Boisvert M, Willems L, Pal S. 2018. Minimalistic Gridworld Environment for OpenAI Gym . GitHub. [6] Bloem P. 2019. Transformers from Scratch [blog]. [accessed 2020 May 1]. http://www.peterbloem.nl/blog/transformers. [7] Harutyunyan A, Dabney W, Mesnard T. 2019. Hindsight Credit Assignment . Advances in Neural Information Processing Systems 32: 12488—12497. [8] Arjona-Medina J, Gillhofer M, Widrich M, et al. 2019. RUDDER : Return Decomposition for Delayed Rewards . Advances in Neural Information Processing Systems 32: 13566—13577. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Machine learning","url":"https://efavdb.com/ltca","loc":"https://efavdb.com/ltca"},{"title":"Visualizing an actor critic algorithm in real time","text":"Deep reinforcement learning algorithms can be hard to debug, so it helps to visualize as much as possible in the absence of a stack trace [1]. How do we know if the learned policy and value functions make sense? Seeing these quantities plotted in real time as an agent is interacting with an environment can help us answer that question. Here's an example of an agent wandering around a custom gridworld environment. When the agent executes the toggle action in front of an unopened red gift, it receives a reward of 1 point, and the gift turns grey/inactive. The model is an actor critic, a type of policy gradient algorithm (for a nice introduction, see Jonathan's battleship post or [2]) that uses a neural network to parametrize its policy and value functions. This agent barely \"meets expectations\" — notably getting stuck at an opened gift between frames 5-35 — but the values and policy mostly make sense. For example, we tend to see spikes in value when the agent is immediately in front of an unopened gift while the policy simultaneously outputs a much higher probability of taking the appropriate toggle action in front of the unopened gift. (We'd achieve better performance by incorporating some memory into the model in the form of an LSTM ). We're sharing a little helper code to generate the matplotlib plots of the value and policy functions that are shown in the video. Comments Training of the model is not included. You'll need to load a trained actor critic model, along with access to its policy and value functions for plotting. Here, the trained model has been loaded into agent with a get_action method that returns the action to take, along with a numpy array of policy probabilities and a scalar value for the observation at the current time step. The minigridworld environment conforms to the OpenAI gym API , and the for loop is a standard implementation for interacting with the environment. The gridworld environment already has a built in method for rendering the environment in iteractive mode env.render('human') . Matplotlib's autoscale_view and relim functions are used to make updates to the figures at each step. In particular, this allows us to show what appears to be a sliding window over time of the value function line plot. When running the script, the plots pop up as three separate figures. References [1] Berkeley Deep RL bootcamp - Core Lecture 6 Nuts and Bolts of Deep RL Experimentation — John Schulman ( video | slides ) - great advice on the debugging process, things to plot [2] OpenAI Spinning Up: Intro to policy optimization","tags":"Programming","url":"https://efavdb.com/visualize-actor-critic","loc":"https://efavdb.com/visualize-actor-critic"},{"title":"2-D random walks are special","text":"Here, we examine the statistics behind discrete random walks on square lattices in \\(M\\) dimensions, with focus on two metrics (see figure below for an example in 2-D): 1. \\(R\\) , the final distance traveled from origin (measured by the Euclidean norm) and 2. \\(N_{unique}\\) , the number of unique locations visited on the lattice. We envision a single random walker on an \\(M\\) -D lattice and allow it to wander randomly throughout the lattice, taking \\(N\\) steps. We'll examine how the distributions of \\(R\\) and \\(N_{unique}\\) vary with \\(M\\) and \\(N\\) ; we'll show that their averages, \\(\\langle R \\rangle\\) and \\(\\langle N_{unique} \\rangle\\) , and their standard deviations, \\(\\sigma_R\\) and \\(\\sigma_{N_{unique}}\\) , scale as power laws with \\(N\\) . The dependence of the exponents and scaling factors on \\(M\\) is interesting and can be only partially reconciled with theory. A simple simulation of random walks is easy to write in python for arbitrary dimensions (see this colab notebook , github ). Here's a look at the distribution of our two metrics for \\(N = 1000\\) for a few different dimensionalities: Let's make some high-level sense of these results: \\(\\langle R \\rangle\\) depends only weakly on \\(M\\) while \\(\\langle N_{unique} \\rangle\\) clearly increases with \\(M\\) . Both of these results make sense: In 1-D, the walker always has an equal chance to step further away from the origin or closer to it. It also always has at least a 50% chance of backtracking to a position it has already visited. As you add dimensions, it becomes less likely to step immediately closer or further from the origin and more likely to wander in an orthogonal direction, increasing the distance from the origin by roughly the same amount independent of which orthogonal direction, while also visiting completely new parts of the lattice. In 1-D, if you take an even number of steps, \\(R\\) is always an even integer, so the distribution of \\(R\\) appears \"stripe-y\" above. \\(R(M=1)\\) can be understood as something like a folded normal distribution for large \\(N\\) . As \\(M\\) gets larger, the distribution of \\(R\\) tightens up. We are essentially taking \\(M\\) 1-D random walks, each with \\(\\sim N/M\\) steps and adding the results in quadrature. The result of this is that our walker is less likely to be very close to the origin and simultaneously less likely to wander too far afield as it is more likely to have traveled a little bit in many orthogonal dimensions. The width of the \\(N_{unique}\\) distribution, \\(\\sigma_{N_{unique}}\\) , depends in an interesting way on \\(M\\) , increasing from 1-D to 2-D and thereafter decreasing. This seems because the distribution is centered most distant from the extremes of 0 or \\(N\\) . Let's take a look at the means of our two metrics as a function of \\(N\\) for various \\(M\\) (apologies for using a gif here, but it helps with 1. keeping this post concise and 2. comparing the plots as \\(M\\) changes incrementally. If you prefer static images, I've included them at github ): Note, this is a log-log plot ; power laws show up as straight lines. One of the first things you'll notice is that the variation in dependence on \\(N\\) across different \\(M\\) is fairly banal for \\(R\\) , but much more interesting for \\(N_{unique}\\) . In fact, it is a well-known result (see, e.g., here ) that \\(\\langle R \\rangle \\sim N&#94;\\alpha\\) with \\(\\alpha = 0.5\\) for all \\(M\\) . \\(\\langle N_{unique} \\rangle\\) , on the other hand, seems similar to \\(\\langle R \\rangle\\) in 1-D, but the dependence on \\(N\\) increases dramatically thereafter and approaches \\(\\langle N_{unique} \\rangle \\approx N&#94;\\beta\\) with \\(\\beta=1\\) at higher dimensions. If you look closely, you'll note that 2-D is particularly special (more on that shortly). We can also look at the widths (standard deviations) of the two distributions (again, you'll note that 2-D is particularly interesting): First, note that all 4 of the quantities describing the distributions of \\(R\\) and \\(N_{unique}\\) plotted above are well-described by power laws. Thus, we can extract eight parameters to describe them (four exponents and four scaling factors) as a function of \\(M\\) : \\begin{eqnarray} \\langle R \\rangle &\\approx& R_0 * N&#94;\\alpha \\\\ \\langle N_{unique} \\rangle &\\approx& N_0 * N&#94;\\beta \\\\ \\sigma_R &\\approx& \\sigma_{R,0} * N&#94;\\gamma \\\\ \\sigma_{N_{unique}} &\\approx& \\sigma_{N_{unique},0} * N&#94;\\delta \\\\ \\end{eqnarray} I've plotted all the theoretical results that I could find for these parameters as lines above: As mentioned, \\(\\alpha = 0.5\\) for all \\(M\\) . (blue line top plot) \\(R_0\\) gives \\(\\langle R \\rangle\\) its weak dependence on \\(M\\) , varying according to an elegant ratio of Gamma functions as described here . (blue dash-dot line bottom plot) It has been shown that \\(\\beta = 0.5\\) for \\(M=1\\) and \\(\\beta = 1\\) for all \\(M \\geq 3\\) . (orange dash and line in the top plot) I found these theoretical results in this wonderful paper , which you should really glance at, if not read, just to see an interesting piece of history - it's by George H. Vineyard in 1963 (no so long ago for such a fundamental math problem!) at Brookhaven National Lab for the US Atomic Energy Commission. Written on a typewriter, here is a sample equation, complete with hand-written scribbles to indicate the vectors and a clearly corrected typo on the first cosine: In the same paper, Vineyard derives that \\(N_{unique, 0} = \\sqrt{8/\\pi}\\) in 1-D and \\(N_{unique, 0} \\approx 0.659462670\\) (yes with all those sig. figs.) in 3-D as derived from evaluating Watson's Integrals (which, I take it, is what the integrals in the image above are called). (orange dashes in bottom plot) The most interesting thing about all this, to me, is that there is no known theoretical result for \\(\\beta\\) in 2-D. From our data, we get \\(\\beta = 0.87 \\pm 0.02\\) . An interesting phenomenological argument for \\(\\beta\\) 's dependence on \\(M\\) : The random walker spends most of it's time roaming around within a distance of \\(\\langle R \\rangle \\sim N&#94;{\\alpha}\\) from the origin (where \\(\\alpha=1/2\\) , independent of \\(M\\) ), as that is where it is going to end up. In 1-D, there are only \\(\\sim 2 N&#94;{1/2}\\) sites within this distance, so the number of unique sites visited scales with \\(N&#94;{1/2}\\) . In 3-D, there are \\(\\sim \\frac{4}{3} \\pi N&#94;{3/2}\\) sites in the sphere of radius \\(N&#94;{1/2}\\) , which is \\(\\gg N\\) , so the walker explores \\(\\sim N\\) sites (as it can visit no more than it walks), similar for larger \\(M\\) . This is what makes 2-D random walks special : the number of sites within \\(N&#94;{1/2}\\) distance of the origin is \\(\\sim N\\) , scaling as the number of steps taken - so, the scaling arguments above no longer apply, leaving us somewhere between \\(1/2 < \\beta < 1\\) . A few final notes on the exponents and scaling factors that I don't understand in terms of theory: We don't get exactly \\(\\beta=1\\) from the data in 3-D above, but that may be because some of our \\(N\\) are not quite large enough. I have not seen any theory for \\(\\sigma_{R,0}\\) , \\(\\sigma_{N_{unique},0}\\) , \\(\\gamma\\) , or \\(\\delta\\) . \\(\\gamma\\) appears to also be independent of \\(M\\) at 0.5. Surely this isn't hard to derive? \\(\\delta\\) increases dramatically from 1-D to 2-D (again, 2-D appears the most interesting case) and slowly settles back down to 0.5 at large \\(M\\) . \\(\\sigma_{N_{unique},0}\\) has a strange effect for \\(M=2\\) also, dipping before going back up and then finally decaying towards zero for \\(M>4\\) . For large \\(M\\) , the data indicate that \\(\\sigma_{R,0} \\approx \\sigma_{N_{unique},0}\\) and \\(\\gamma \\approx \\delta \\approx 0.5\\) , indicating that \\(\\sigma_{R} \\approx \\sigma_{N_{unique}}\\) . Further, not shown here, but in github , \\(\\sigma_{R,0} \\approx \\sigma_{N_{unique},0} \\sim \\sqrt{1/M}\\) . Or, in total, \\(\\sigma_{R} \\approx \\sigma_{N_{unique}} \\sim \\sqrt{N/M}\\) for large \\(M\\) and \\(N\\) . I really enjoyed working on this post from end-to-end: writing the random walker class, finding a great python jackknife function from astropy (see the colab notebook ), hacking my way through matplotlib, discovering the amazing Vineyard paper, and just generally exploring this incredibly rich problem that is so easy to state yet difficult to theoretically solve. I'll also note that the 2-D results are of particular interest to me given their applicability to the World Wandering Dudes project that I have been working on. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"https://efavdb.com/random-walk-scaling","loc":"https://efavdb.com/random-walk-scaling"},{"title":"Q-learning and DQN","text":"[ TOC ] Q-learning is a reinforcement learning ( RL ) algorithm that is the basis for deep Q networks ( DQN ), the algorithm by Google DeepMind that achieved human-level performance for a range of Atari games and kicked off the deep RL revolution starting in 2013-2015. We begin with some historical context, then provide an overview of value function methods / Q-learning, and conclude with a discussion of DQN . If you want to skip straight to code, the implementation of DQN that we used to train the agent playing Atari Breakout below is available here . If you watch the video long enough, you'll see the agent has learned a strategy that favors breaking bricks at the edges so the ball \"breaks out\" to the upper side, resulting in a cascade of points. Historical context The theories that underpin today's reinforcement learning algorithms were developed decades ago. For example, Watkins developed Q-learning, a value function method, in 1989 , and Williams proposed the REINFORCE policy gradient method in 1987 . So why the recent surge of interest in deep RL ? Representational power from Neural Networks Until 2013, most applications of RL relied on hand engineered inputs for value function and policy representations, which drastically limited the scope of applicability to the real world. Mnih et. al [1] made use of advances in computational power and neural network ( NN ) architectures to use a deep NN for value function approximation , showing that NNs can learn a useful representation from raw pixel inputs in Atari games. Variations on a theme: vanilla RL algorithms don't work well out-of-the-box The basic RL algorithms that were developed decades ago do not work well in practice without modifications. For example, REINFORCE relies on Monte Carlo estimates of the performance gradient; such estimates of the performance gradient are high variance, resulting in unstable or impractically slow learning (poor sample efficiency). The original Q-learning algorithm also suffers from instability due to correlated sequential training data and parameter updates affecting both the estimator and target, creating a \"moving target\" and hence divergences. We can think of these original RL algorithms as the Wright Brothers plane. The foundational shape is there and recognizable in newer models. However, the enhancements of newer algorithms aren't just bells and whistles — they have enabled the move from toy problems into more functional territory. Q-learning Background RL models the sequential decision-making problem as a Markov Decision Process ( MDP ): transitions from state to state involve both environment dynamics and an agent whose actions affect both the probability of transitioning to the next state and the reward received. The goal is to find a policy, a mapping from state to actions, that will maximize the agent's expected returns, i.e. their cumulative future rewards. Q-learning is an algorithm for learning the eponymous \\(Q(s,a)\\) action-value function, defined as the expected returns for each state-action \\((s,a)\\) pair, corresponding to following the optimal policy. Goal: solve the Bellman optimality equation Recall that \\(q_*\\) is described by a self-consistent, recursive relation, the Bellman optimality equation, that falls out from the Markov property [6, 7] of MDPs \\begin{eqnarray}\\label{action-value-bellman-optimality} \\tag{1} q_*(s, a) &=& \\mathbb{E}_{\\pi*} [R_{t+1} + \\gamma \\max_{a'} q_*(S_{t+1}', a') | S_t = s, A_t = a] \\\\ &=& \\sum_{s', r} p(s', r | s, a) [r + \\gamma \\max_{a'} q_*(s', a') ] \\end{eqnarray} where \\(0 \\leq \\gamma \\leq 1\\) is the discount rate which characterizes how much we weight rewards now vs. later, \\(R_{t+1}\\) is the reward at timestep \\(t+1\\) , and \\(p(s', r | s, a)\\) is the environment transition dynamics. Our introduction to RL provides more background on the Bellman equations in case (\\ref{action-value-bellman-optimality}) looks unfamiliar. The Q-learning approach to solving the Bellman equation We use capitalized \\(Q\\) to denote an estimate and lowercase \\(q\\) to denote the real action-value function. The Q-learning algorithm makes the following update: \\begin{eqnarray}\\label{q-learning} \\tag{2} Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t)] \\end{eqnarray} The quantity in square brackets in (\\ref{q-learning}) is exactly 0 for the optimal action-value, \\(q*\\) , based on (\\ref{action-value-bellman-optimality}). We can think of it as an error term, \"the Bellman error\", that describes how far off the target quantity \\(R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a)\\) is from our current estimate \\(Q(S_t, A_t)\\) . The goal with Q-learning is to iteratively calculate (\\ref{q-learning}), updating our estimate of \\(Q\\) to reduce the Bellman error, until we have converged on a solution. Q-learning makes two approximations: I. It replaces the expectation value in (\\ref{action-value-bellman-optimality}) with sampled estimates, similar to Monte Carlo estimates. Unlike the dynamic programming approach we described in an earlier post , sampling is necessary since we don't have access to the model of the environment, i.e. the environment transition dynamics. II . It replaces the target \\(R_{t+1} + \\max_a \\gamma q_*(s',a')\\) in (\\ref{action-value-bellman-optimality}), which contains the true action-value function \\(q_*\\) , with the one-step temporal difference, TD (0), target \\(R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a)\\) . The TD (0) target is an example of bootstrapping because it makes use of the current estimate of the action-value function, instead of, say the cumulative rewards from an entire episode, which would be a Monte Carlo target. Temporal difference methods reduce variance that comes from sampling a single trajectory like Monte Carlo at the cost of introducing bias from using an approximate function in the target for updates. Figure 8.11 of [7] nicely summarizes the types of approximations and their limits in the following diagram: Deep Q-Networks ( DQN ) Key contributions to Q-learning The DQN authors made two key enhancements to the original Q-learning algorithm to actually make it work: Experience replay buffer : to reduce the instability caused by training on highly correlated sequential data, store samples (transition tuples \\((s, a, s', r)\\) ) in an \"experience replay buffer\". Cut down correlations by randomly sampling the buffer for minibatches of training data. The idea of experience replay was introduced by Lin in 1992 . Freeze the target network : to address the instability caused by chasing a moving target, freeze the target network and only update it periodically with the latest parameters from the trained estimator. These modifications enabled [1] to successfully train a deep Q-network, an action-value function approximated by a convolutional neural net, on the high dimensional visual inputs of a variety of Atari games. The authors also employed a number of tweaks / data preprocessing on top of the aforementioned key enhancements. One preprocessing trick of note was the concatenation of the four most recent frames as input into the Q-network in order to provide some sense of velocity or trajectory, e.g. the trajectory of a ball in games such as Pong or Breakout. This preprocessing decision helps uphold the assumption that the problem is a Markov Decision Process, which underlies the Bellman optimality equations and Q-learning algorithms; otherwise, the assumption is violated if the agent only observes some fraction of the state of the environment, turning the problem into a partially observable MDP . DQN implementation in code We've implemented DQN here , tested for (1) the Cartpole toy problem, which uses a multilayer perceptron MLPCritic as the Q-function approximator for non-visual input data, and (2) Atari Breakout, which uses a convolutional neural network CNNCritic as the Q-function approximator for the (visual) Atari pixel data. The Cartpole problem is trainable on the average modern laptop CPU , but we recommend using a beefier setup with GPUs and lots of memory to do Q-learning on Atari. Thanks to the OpenAI Scholars program and Microsoft, we were able to train DQN on Breakout using an Azure Standard_NC24 consisting of 224 GiB RAM and 2 K80 GPUs. The values from the \\(Q\\) estimator and frozen target network are fed into the Huber loss that is used to update the parameters of the Q-function in this code snippet: def compute_loss_q ( data ): o , a , r , o2 , d = data [ 'obs' ], data [ 'act' ], data [ 'rew' ], data [ 'obs2' ], data [ 'done' ] # Pick out q-values associated with / indexed by the action that was taken # for that observation q = torch . gather ( ac . q ( o ), dim = 1 , index = a . view ( - 1 , 1 ) . long ()) # Bellman backup for Q function with torch . no_grad (): # Targets come from frozen target Q-network q_target = torch . max ( target_q_network ( o2 ), dim = 1 ) . values backup = r + ( 1 - d ) * gamma * q_target loss_q = F . smooth_l1_loss ( q [:, 0 ], backup ) . mean () return loss_q The experience replay buffer was taken from OpenAI's Spinning Up in RL [6] code tutorials for the problem: class ReplayBuffer : \"\"\" A simple FIFO experience replay buffer for DDPG agents. Copied from: https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py#L11, modified action buffer for discrete action space. \"\"\" def __init__ ( self , obs_dim , act_dim , size ): ... def store ( self , obs , act , rew , next_obs , done ): self . obs_buf [ self . ptr ] = obs self . obs2_buf [ self . ptr ] = next_obs self . act_buf [ self . ptr ] = act self . rew_buf [ self . ptr ] = rew self . done_buf [ self . ptr ] = done self . ptr = ( self . ptr + 1 ) % self . max_size self . size = min ( self . size + 1 , self . max_size ) def sample_batch ( self , batch_size = 32 ): idxs = np . random . choice ( self . size , size = batch_size , replace = False ) batch = dict ( obs = self . obs_buf [ idxs ], obs2 = self . obs2_buf [ idxs ], act = self . act_buf [ idxs ], rew = self . rew_buf [ idxs ], done = self . done_buf [ idxs ]) return { k : torch . as_tensor ( v , dtype = torch . int32 , device = device ) if k == 'act' else torch . as_tensor ( v , dtype = torch . float32 , device = device ) for k , v in batch . items ()} Finally, we used OpenAI's baselines Atari wrappers to handle the rather involved data preprocessing steps. You can see logs and plots like this plot of the mean raw returns per step in the environment for the Atari DQN training run in our wandb dashboard . Conclusion From a pedagogical point of view, Q-learning is a good study for someone getting off the ground with RL since it pulls together many core RL concepts, namely: Model the sequential decision making process as an MDP where environment dynamics are unknown. Frame the problem as finding action-value functions that satisfy the Bellman equations. Iteratively solve the Bellman equations using bootstrapped estimates from samples of an agent's interactions with an environment. Use neural networks to approximate value functions to handle the more realistic situation of an observation space being too high-dimensional to be stored in table. DQN on top of vanilla Q-learning itself is noteworthy because the modifications — experience replay and frozen target networks — are what make Q-learning actually work, demonstrating that the devil is in the details. Furthermore, the DQN tricks have been incorporated in many other RL algorithms, e.g. see [6] for more examples. The tricks aren't necessarily \"pretty\", but they come from understanding/intuition about shortcomings of the basic algorithms. References Papers [1] Mnih et al 2015 - Human-level control through deep reinforcement learning Video lectures [2] David Silver - RL lecture 6 Value Function Approximation ( video , slides ) [3] Sergey Levine's lecture ( CS285 ) on value function methods ( video , slides ) [4] Sergey Levine's lecture ( CS285 ) on deep RL with Q-functions ( video , slides ) [5] Vlad Mnih - Berkeley Deep RL Bootcamp 2017 - Core Lecture 3 DQN + Variants ( video , slides ) Books / tutorials [6] OpenAI - Spinning Up: The Optimal Q-Function and the Optimal Action [7] Sutton and Barto - Reinforcement Learning: An Introduction (2nd Edition) , section 6.5 \"Q-learning: Off-policy TD Control\", section 16.5 \"Human-level Video Game Play\" if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Machine learning","url":"https://efavdb.com/dqn","loc":"https://efavdb.com/dqn"},{"title":"Sample pooling to reduce needed disease screening test counts","text":"Pooling of test samples can be used to reduce the mean number of test counts required to determine who in a set of subjects carries a disease. E.g., if the blood samples of a set of office workers are combined and tested, and the test comes back negative, then the full office can be ruled out as disease carriers using just a single test (whereas the naive approach would require testing each separately). However, if the test comes back positive, then a refined search through the workers must be carried out to decide which have the disease and which do not. Here, we consider two methods for refined search when a group is flagged positive, and provide python code that can be used to find the optimal pooling strategy. This depends on the frequency of disease within the testing population, \\(p\\) . Impact summary of pooling concept: If \\(p = O(1)\\) , so that many people have the illness, pooling doesn't help. If \\(p = 0.1\\) , perhaps typical of people being screened with symptoms, we can reduce the test count needed by about \\(\\sim 0.6\\) using pooling, and the two refined search methods we consider perform similarly here. If \\(p = 0.001\\) , so that positive cases are rare — perhaps useful for screening an office of workers expected to be healthy, then we can cut the mean test count by a factor of \\(50\\) , and the bisection method for refined search performs best here (details below). Code for this analysis can be found at our github ( link ). COVID19 background, strategies considered here The idea of pooling is an old one, but I happened on the idea when an article was posted about it to the statistics subreddit this past week ( link ). There the question was posed what the optimal pooling count would be, motivating this post. I imagine pooling may be useful for COVID19 under two conditions: (1) situations where testing capactity is the limiting factor (as opposed to speed of diagnosis, say), and (2) Situations where a great many people need to be screened and it is unlikely that any of them have it — e.g., daily tests within a large office buiding. We consider two pooling methods here: (1) A simple method where if the test on the group comes back positive, we immediately screen each individual. (2) A bisection method, where if a group comes back positive, we split it in two and run the test on each subgroup, repeating from there recursively. E.g., in a group of size 16 with one positive, the recursive approach generates the following set of test subsets (see notebook on our github linked above for code) seq = generate_random_seq () test_counts_needed . append ( test_count ( seq )) total size = 16 [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ] [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] [ 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ] [ 0 , 0 , 0 , 0 ] [ 1 , 0 , 0 , 0 ] [ 1 , 0 ] [ 1 ] [ 0 ] [ 0 , 0 ] Here, the 13th individual had the disease, and the bisection method required a total of 9 tests (one for each row above) to determine the full set of diagnoses. Note that 9 is less than 16, the number needed when we screen everyone from the start. Our purpose is to provide code and equations that can be used to select from these two methods should anyone want to apply this idea. Caveat: We currently ignore any possibility of error in the tests. This may make the approach invalid for some or all of the current covid19 tests. Error rates should be studied next where appropriate. Model and results We posit that we have a pool of \\begin{eqnarray} N = 2&#94;{\\mathbb{K}} \\tag{1} \\label{count_pop} \\end{eqnarray} people to be tested. In the first round, we pool all their samples and test the group. If the group comes back positive, we then run one of the refined methods to figure out which people exactly have the illness. Each person is supposed to have a probability \\(p\\) of having the disease. Below, we ask how to set \\(\\mathbb{K}\\) — which determines the pooling size — so as to minimize the mean number of tests needed divided by \\(N\\) , which can be considered the pooling reduction factor. The mean number of tests needed from the simple strategy is \\begin{eqnarray}\\tag{2} \\label{simple_result} \\overline{N}_{simple} = (1 - p)&#94;N\\times 1 + \\left [1 - (1-p)&#94;N \\right] \\times (1 + N) \\end{eqnarray} The mean number needed in the bisection strategy is \\begin{eqnarray} \\tag{3} \\label{bisection_result} \\overline{N}_{bisection} = 1 + 2 \\sum_{k=0}&#94;{\\mathbb{K}} 2&#94;k \\left (1 - (1 -p)&#94;{2&#94;{\\mathbb{K}-k}} \\right) \\end{eqnarray} The proof of (\\ref{simple_result}) is straightforward and we give an argument for (\\ref{bisection_result}) in an appendix. A cell of our notebook checks this and confirms its accuracy. Using the above results, our code produces plots of the mean number of tests needed to screen a population vs \\(\\mathbb{K}\\) . This then finds the optimal number for each type. The plots below give the results for the three \\(p\\) values noted in the abstract. Case 1: \\(p = 0.5\\) , large fraction of disease carriers. Main result: The pooling strategies both cause the mean number of tests to be larger than if we just screened each individual from the start (seen here because the y-axis values are always bigger than 1). The approach is not useful here. Case 2: \\(p = 0.1\\) , modest fraction of disease carriers. Main result: The two methods both give comparable benefits. It is optimal to pool using \\(\\mathbb{K}=2\\) , which gives groups of \\(N = 4\\) patients. This cuts the number of needed tests by a factor of \\(0.6\\) . Case 3: \\(p = 0.001\\) , small fraction of disease carriers. Main result: Bisection wins, the optimal \\(\\mathbb{K} = 9\\) here, which gives a pooling group of size \\(512\\) . We cut the test count needed by a factor of \\(50\\) . Note: We also show here a histogram showing the number of tests needed when we run a simulated system like this. We see that we often only need one test, and there is another peak around \\(20\\) tests, with a long tail after that. The code to generate the optimal \\(\\mathbb{K}\\) plots above is given below. This can be used to generate generalized plots like those above for any \\(p\\) . The histogram plot is contained in our github repo, linked in our abstract. Our appendix follows. import numpy as np % pylab inline K = 5 P_POSITIVE = 0.05 def theory_bisection ( p = P_POSITIVE , K = K ): count = 1 + 2 * np . sum ([ 2 ** k * ( 1 - ( 1 - p ) ** ( 2 ** ( K - k ))) for k in range ( K )] ) return count / 2 ** K def theory_simple ( p = P_POSITIVE , K = K ): n = 2 ** K p0 = ( 1 - p ) ** n count = 1 * p0 + ( 1 + n ) * ( 1 - p0 ) return count / n print 'Bisection: fraction of full testing: %2.2f ' % ( theory_bisection ()) print 'Simple: fraction of full testing: %2.2f ' % ( theory_simple ()) p = 0.1 data = [ theory_bisection ( p , k ) for k in range ( 15 )] min_index = np . argmin ( data ) plot ( data , 'o--' , label = 'bisection (min = %2.2f )' % data [ min_index ], alpha = 0.5 ) plot ( min_index , data [ min_index ], 'ro' , alpha = 0.5 ) data = [ theory_simple ( p , k ) for k in range ( 15 )] min_index = np . argmin ( data ) plot ( data , 'o--' , label = 'simple (min = %2.2f )' % data [ min_index ], alpha = 0.5 ) plot ( min_index , data [ min_index ], 'go' , alpha = 0.5 ) plt . legend () plt . title ( 'Test count reduction vs log_2 pooling size, p = %0.3f ' % p ) plt . xlabel ( 'log_2 pooling size' ) plt . ylabel ( 'mean tests / pooling size' ) Appendix: Derivation of (\\ref{bisection_result}) Consider a binary tree with the root node being the initial test. Each node has two children that correspond to the tests of the two subgroups for a given test. We must test these if the parent is positive. Level \\(0\\) is the initial test and \\(k\\) rows down we call the level \\(k\\) of tests. There are total of \\(2&#94;k\\) posible tests to run at this level, and there are a total of \\(\\mathbb{K}\\) levels. The number of tests that need to be run at level \\(k\\) is set by the number of positive tests at level \\(k-1\\) . We have \\begin{eqnarray} \\text{Number of tests} = 1 + \\sum_{k=0}&#94;{\\mathbb{K} - 1} \\text{number positive level k} \\end{eqnarray} Averaging this equation gives \\begin{eqnarray} \\overline{\\text{Number of tests}} &=& 1 + \\sum_{k=0}&#94;{\\mathbb{K} - 1} 2&#94;k \\times prob(\\text{test at level k positive}) \\\\ &=& 1 + \\sum_{k=0}&#94;{\\mathbb{K} - 1} 2&#94;k \\times [ 1- (1 - p)&#94;{2&#94;{\\mathbb{K} - k}}]. \\end{eqnarray} The inner factor here is the probability that a given test of the size being considered comes back positive — this has \\(N / 2&#94;k = 2&#94;{\\mathbb{K} - k}\\) people in it. This is the result shown above in (\\ref{bisection_result}). if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"https://efavdb.com/pooling","loc":"https://efavdb.com/pooling"},{"title":"Dynamic programming in reinforcement learning","text":"Background We discuss how to use dynamic programming ( DP ) to solve reinforcement learning ( RL ) problems where we have a perfect model of the environment. DP is a general approach to solving problems by breaking them into subproblems that can be solved separately, cached, then combined to solve the overall problem. We'll use a toy model, taken from [1], of a student transitioning between five states in college, which we also used in our introduction to RL : The model (dynamics) of the environment describe the probabilities of receiving a reward \\(r\\) in the next state \\(s'\\) given the current state \\(s\\) and action \\(a\\) taken, \\(p(s', r | s, a)\\) . We can read these dynamics off the diagram of the student Markov Decision Process ( MDP ), for example: \\(p(s'=\\text{CLASS2}, r=-2 | s=\\text{CLASS1}, a=\\text{study}) = 1.0\\) \\(p(s'=\\text{CLASS2}, r=1 | s=\\text{CLASS3}, a=\\text{pub}) = 0.4\\) If you'd like to jump straight to code, see this jupyter notebook . The role of value functions in RL The agent's (student's) policy maps states to actions, \\(\\pi(a|s) := p(a|s)\\) . The goal is to find the optimal policy \\(\\pi_*\\) that will maximize the expected cumulative rewards, the discounted return \\(G_t\\) , in each state \\(s\\) . The value functions, \\(v_{\\pi}(s)\\) and \\(q_{\\pi}(s, a)\\) , in MDPs formalize this goal. \\begin{eqnarray} v_{\\pi}(s) &=& \\mathbb{E}_{\\pi}[G_t | S_t = s] \\\\ q_{\\pi}(s, a) &=& \\mathbb{E}_{\\pi}[G_t | S_t = s, A_t = a] \\end{eqnarray} We want to be able to calculate the value function for an arbitrary policy, i.e. prediction , as well as use the value functions to find an optimal policy, i.e. the control problem. Policy evaluation Policy evaluation deals with the problem of calculating the value function for some arbitrary policy. In our introduction to RL post , we showed that the value functions obey self-consistent, recursive relations, that make them amenable to DP approaches given a model of the environment. These recursive relations are the Bellman expectation equations, which write the value of each state in terms of an average over the values of its successor / neighboring states, along with the expected reward along the way. The Bellman expectation equation for \\(v_{\\pi}(s)\\) is \\begin{eqnarray}\\label{state-value-bellman} \\tag{1} v_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_{\\pi}(s') ], \\end{eqnarray} where \\(\\gamma\\) is the discount factor \\(0 \\leq \\gamma \\leq 1\\) that weights the importance of future vs. current returns. DP turns (\\ref{state-value-bellman}) into an update rule (\\ref{policy-evaluation}), \\(\\{v_k(s')\\} \\rightarrow v_{k+1}(s)\\) , which iteratively converges towards the solution, \\(v_\\pi(s)\\) , for (\\ref{state-value-bellman}): \\begin{eqnarray}\\label{policy-evaluation} \\tag{2} v_{k+1}(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_k(s') ] \\end{eqnarray} Applying policy evaluation to our student model for an agent with a random policy, we arrive at the following state value function (see jupyter notebook for implementation): Finding the optimal value functions and policy Policy iteration We can evaluate the value functions for a given policy by turning the Bellman expectation equation (\\ref{state-value-bellman}) into an update equation with the iterative policy evaluation algorithm. But how do we use value functions to achieve our end goal of finding an optimal policy that corresponds to the optimal value functions? Imagine we know the value function for a policy. If taking the greedy action, corresponding to taking \\(\\text{arg} \\max_a q_{\\pi}(s,a)\\) , from any state in that policy is not consistent with that policy, or, equivalently, \\(\\max_a q_{\\pi}(s,a) > v_\\pi(s)\\) , then the policy is not optimal since we can improve the policy by taking the greedy action in that state and then onwards following the original policy. The policy iteration algorithm involves taking turns calculating the value function for a policy (policy evaluation) and improving on the policy (policy improvement) by taking the greedy action in each state for that value function until converging to \\(\\pi_*\\) and \\(v_*\\) (see [2] for pseudocode for policy iteration). Value iteration Unlike policy iteration, the value iteration algorithm does not require complete convergence of policy evaluation before policy improvement, and, in fact, makes use of just a single iteration of policy evaluation. Just as policy evaluation could be viewed as turning the Bellman expectation equation into an update, value iteration turns the Bellman optimality equation into an update. In our previous post introducing RL using the student example, we saw that the optimal value functions are the solutions to the Bellman optimality equation, e.g. for the optimal state-value function: \\begin{eqnarray}\\label{state-value-bellman-optimality} \\tag{3} v_*(s) &=& \\max_a q_{\\pi*}(s, a) \\\\ &=& \\max_a \\mathbb{E} [R_{t+1} + \\gamma v_*(S_{t+1}) | S_t = s, A_t = a] \\\\ &=& \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_*(s') ] \\end{eqnarray} As a DP update equation, (\\ref{state-value-bellman-optimality}) becomes: \\begin{eqnarray}\\label{value-iteration} \\tag{4} v_{k+1}(s) = \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_k(s') ] \\end{eqnarray} Value iteration combines (truncated) policy evaluation with policy improvement in a single step; the state-value functions are updated with the averages of the value functions of the neighbor states that can occur from a greedy action, i.e. the action that maximizes the right hand side of (\\ref{value-iteration}). Applying value iteration to our student model, we arrive at the following optimal state value function, with the optimal policy delineated by red arrows (see jupyter notebook ): Summary We've discussed how to solve for (a) the value functions of an arbitrary policy, (b) the optimal value functions and optimal policy. Solving for (a) involves turning the Bellman expectation equations into an update, whereas (b) involves turning the Bellman optimality equations into an update. These algorithms are guaranteed to converge (see [1] for notes on how the contraction mapping theorem guarantees convergence). You can see the application of both policy evaluation and value iteration to the student model problem in this jupyter notebook . References [1] David Silver's RL Course Lecture 3 - Planning by Dynamic Programming ( video , slides ) [2] Sutton and Barto - Reinforcement Learning: An Introduction - Chapter 4: Dynamic Programming [3] Denny Britz's notes on RL and DP , including crisp implementations in code of policy evaluation, policy iteration, and value iteration for the gridworld example discussed in [2]. [4] Deep RL Bootcamp Lecture 1: Motivation + Overview + Exact Solution Methods, by Pieter Abbeel ( video , slides ) - a very compressed intro. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Machine learning","url":"https://efavdb.com/reinforcement-learning-dynamic-programming","loc":"https://efavdb.com/reinforcement-learning-dynamic-programming"},{"title":"Introduction to reinforcement learning by example","text":"We take a top-down approach to introducing reinforcement learning ( RL ) by starting with a toy example: a student going through college. In order to frame the problem from the RL point-of-view, we'll walk through the following steps: Setting up a model of the problem as a Markov Decision Process, the framework that underpins the RL approach to sequential decision-making problems Deciding on an objective : maximize rewards Writing down an equation whose solution is our objective : Bellman equations David Silver walks through this example in his lecture notes on RL , but as far as we can tell, does not provide code, so we're sharing our implementation, comprising: the student's college environment using the OpenAI gym package. a jupyter notebook sampling from the model Student in toy college We model the student as an agent in a college environment who can move between five states: CLASS 1, 2, 3, the FACEBOOK state, and SLEEP state. The states are represented by the four circles and square. The SLEEP state — the square with no outward bound arrows — is a terminal state, i.e. once a student reaches that state, her journey is finished. Actions that a student can take in her current state are labeled in red (facebook/quit/study/sleep/pub) and influence which state she'll find herself in next. In this model, most state transitions are deterministic functions of the action in the current state, e.g. if she decides to study in CLASS 1, then she'll definitely advance to CLASS 2. The single non-deterministic state transition is if she goes pubbing while in CLASS 3, where the pubbing action is indicated by a solid dot; she can end up in CLASS 1, 2 or back in 3 with probability 0.2, 0.4, or 0.4, respectively, depending on how reckless the pubbing was. The model also specifies the reward \\(R\\) associated with acting in one state and ending up in the next. In this example, the dynamics \\(p(s',r|s,a)\\) , are given to us, i.e. we have a full model of the environment, and, hopefully, the rewards have been designed to capture the actual end goal of the student. Markov Decision Process Formally, we've modeled the student's college experience as a finite Markov Decision Process ( MDP ). The dynamics are Markov because the probability of ending up in the next state depends only on the current state and action, not on any history leading up to the current state. The Markov property is integral to the simplification of the equations that describe the model, which we'll see in a bit. The components of an MDP are: \\(S\\) - the set of possible states \\(R\\) - the set of (scalar) rewards \\(A\\) - the set of possible actions in each state The dynamics of the system are described by the probabilities of receiving a reward in the next state given the current state and action taken, \\(p(s',r|s,a)\\) . In this example, the MDP is finite because there are a finite number of states, rewards, and actions. The student's agency in this environment comes from how she decides to act in each state. The mapping of a state to actions is the policy , \\(\\pi(a|s) := p(a|s)\\) , and can be a deterministic or stochastic function of her state. Suppose we have an indifferent student who always chooses actions randomly. We can sample from the MDP to get some example trajectories the student might experience with this policy. In the sample trajectories below, the states are enclosed in parentheses (STATE) , and actions enclosed in square brackets [action] . Sample trajectories : ( CLASS1 ) --[facebook]-->(FACEBOOK)--[facebook]-->(FACEBOOK)--[facebook]-->(FACEBOOK)--[facebook]-->(FACEBOOK)--[quit]-->(CLASS1)--[facebook]-->(FACEBOOK)--[quit]-->(CLASS1)--[study]-->(CLASS2)--[sleep]-->(SLEEP) ( FACEBOOK ) --[quit]-->(CLASS1)--[study]-->(CLASS2)--[study]-->(CLASS3)--[study]-->(SLEEP) ( SLEEP ) ( CLASS1 ) --[facebook]-->(FACEBOOK)--[quit]-->(CLASS1)--[study]-->(CLASS2)--[sleep]-->(SLEEP) ( FACEBOOK ) --[facebook]-->(FACEBOOK)--[facebook]-->(FACEBOOK)--[facebook]-->(FACEBOOK)--[facebook]-->(FACEBOOK)--[quit]-->(CLASS1)--[facebook]-->(FACEBOOK)--[quit]-->(CLASS1)--[study]-->(CLASS2)--[study]-->(CLASS3)--[pub]-->(CLASS2)--[study]-->(CLASS3)--[study]-->(SLEEP) Rewards following a random policy : Under this random policy, what total reward would the student expect when starting from any of the states? We can estimate the expected rewards by summing up the rewards per trajectory and plotting the distributions of total rewards per starting state: Maximizing rewards: discounted return and value functions We've just seen how we can estimate rewards starting from each state given a random policy. Next, we'll formalize our goal in terms of maximizing returns. Returns We simply summed the rewards from the sample trajectories above, but the quantity we often want to maximize in practice is the discounted return \\(G_t\\) , which is a sum of the weighted rewards: \\begin{eqnarray}\\label{return} \\tag{1} G_t := R_{t+1} + \\gamma R_{t+2} + … = \\sum_{k=0}&#94;\\infty \\gamma&#94;k R_{t+k+1} \\end{eqnarray} where \\(0 \\leq \\gamma \\leq 1\\) . \\(\\gamma\\) is the discount rate which characterizes how much we weight rewards now vs. later. Discounting is mathematically useful for avoiding infinite returns in MDPs without a terminal state and allows us to account for uncertainty in the future when we don't have a perfect model of the environment. Aside The discount factor introduces a time scale since it says that we don't care about rewards that are far in the future. The half-life (actually, the \\(1/e\\) life) of a reward in units of time steps is \\(1/(1-\\gamma)\\) , which comes from a definition of \\(1/e\\) : \\begin{align} \\frac{1}{e} = \\lim_{n \\rightarrow \\infty} \\left(1 - \\frac{1}{n} \\right)&#94;n \\end{align} \\(\\gamma = 0.99\\) is often used in practice, which corresponds to a half-life of 100 timesteps since \\(0.99&#94;{100} = (1 - 1/100)&#94;{100} \\approx 1/e\\) . Value functions Earlier, we were able to estimate the expected undiscounted returns starting from each state by sampling from the MDP under a random policy. Value functions formalize this notion of the \"goodness\" of being in a state. State value function \\(v\\) The state value function \\(v_{\\pi}(s)\\) is the expected return when starting in state \\(s\\) , following policy \\(\\pi\\) . \\begin{eqnarray}\\label{state-value} \\tag{2} v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s] \\end{eqnarray} The state value function can be written as a recursive relationship, the Bellman expectation equation, expressing the value of a state in terms of the values of its neighors by making use of the Markov property. \\begin{eqnarray}\\label{state-value-bellman} \\tag{3} v_{\\pi}(s) &=& \\mathbb{E}_{\\pi}[G_t | S_t = s] \\\\ &=& \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+2} | S_t = s] \\\\ &=& \\sum_{a} \\pi(a|s) \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_{\\pi}(s') ] \\end{eqnarray} This equation expresses the value of a state as an average over the discounted value of its neighbor / successor states, plus the expected reward transitioning from \\(s\\) to \\(s'\\) , and \\(v_{\\pi}\\) is the unique * solution. The distribution of rewards depends on the student's policy since her actions influence her future rewards. Note on terminology : Policy evaluation uses the Bellman expectation equation to solve for the value function given a policy \\(\\pi\\) and environment dynamics \\(p(s', r | s, a)\\) . This is different from policy iteration and value iteration, which are concerned with finding an optimal policy. We can solve the Bellman equation for the value function as an alternative to the sampling we did earlier for the student toy example. Since the problem has a small number of states and actions, and we have full knowledge of the environment, an exact solution is feasible by directly solving the system of linear equations or iteratively using dynamic programming. Here is the solution to (\\ref{state-value-bellman}) for \\(v\\) under a random policy in the student example (compare to the sample means in the histogram of returns): We can verify that the solution is self-consistent by spot checking the value of a state in terms of the values of its neighboring states according to the Bellman equation, e.g. the CLASS1 state with \\(v_{\\pi}(\\text{CLASS1}) = -1.3\\) : $$ v_{\\pi}(\\text{CLASS1}) = 0.5 [-2 + 2.7] + 0.5 [-1 + -2.3] = -1.3 $$ Action value function \\(q\\) Another value function is the action value function \\(q_{\\pi}(s, a)\\) , which is the expected return from a state \\(s\\) if we follow a policy \\(\\pi\\) after taking an action \\(a\\) : \\begin{eqnarray}\\label{action-value} \\tag{4} q_{\\pi}(s, a) := \\mathbb{E}_{\\pi} [ G_t | S_t = s, A = a ] \\end{eqnarray} We can also write \\(v\\) and \\(q\\) in terms of each other. For example, the state value function can be viewed as an average over the action value functions for that state, weighted by the probability of taking each action, \\(\\pi\\) , from that state: \\begin{eqnarray}\\label{state-value-one-step-backup} \\tag{5} v_{\\pi}(s) = \\sum_{a} \\pi(a|s) q_{\\pi}(s, a) \\end{eqnarray} Rewriting \\(v\\) in terms of \\(q\\) in (\\ref{state-value-one-step-backup}) is useful later for thinking about the \"advantage\", \\(A(s,a)\\) , of taking an action in a state, namely how much better is an action in that state than the average? \\begin{align} A(s,a) \\equiv q(s,a) - v(s) \\end{align} Why \\(q\\) in addition to \\(v\\) ? Looking ahead, we almost never have access to the environment dynamics in real world problems, but solving for \\(q\\) instead of \\(v\\) lets us get around this problem; we can figure out the best action to take in a state solely using \\(q\\) (we further expand on this in our discussion below on the Bellman optimality equation for \\(q_*\\) . A concrete example of using \\(q\\) is provided in our post on multiarmed bandits (an example of a simple single-state MDP ), which discusses agents/algorithms that don't have access to the true environment dynamics. The strategy amounts to estimating the action value function of the slot machine and using those estimates to inform which slot machine arms to pull in order to maximize rewards. Optimal value and policy The crux of the RL problem is finding a policy that maximizes the expected return. A policy \\(\\pi\\) is defined to be better than another policy \\(\\pi'\\) if \\(v_{\\pi}(s) > v_{\\pi'}(s)\\) for all states. We are guaranteed * an optimal state value function \\(v_*\\) which corresponds to one or more optimal policies \\(\\pi*\\) . Recall that the value function for an arbitrary policy can be written in terms of an average over the action values for that state (\\ref{state-value-one-step-backup}). In contrast, the optimal value function \\(v_*\\) must be consistent with following a policy that selects the action that maximizes the action value functions from a state, i.e. taking a \\(\\max\\) (\\ref{state-value-bellman-optimality}) instead of an average (\\ref{state-value-one-step-backup}) over \\(q\\) , leading to the Bellman optimality equation for \\(v_*\\) : \\begin{eqnarray}\\label{state-value-bellman-optimality} \\tag{6} v_*(s) &=& \\max_a q_{\\pi*}(s, a) \\\\ &=& \\max_a \\mathbb{E}_{\\pi*} [R_{t+1} + \\gamma v_*(S_{t+1}) | S_t = s, A_t = a] \\\\ &=& \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_*(s') ] \\end{eqnarray} The optimal policy immediately follows: take the action in a state that maximizes the right hand side of (\\ref{state-value-bellman-optimality}). The principle of optimality , which applies to the Bellman optimality equation, means that this greedy policy actually corresponds to the optimal policy! Note: Unlike the Bellman expectation equations, the Bellman optimality equations are a nonlinear system of equations due to taking the max. The Bellman optimality equation for the action value function \\(q_*(s,a)\\) is: \\begin{eqnarray}\\label{action-value-bellman-optimality} \\tag{7} q_*(s, a) &=& \\mathbb{E}_{\\pi*} [R_{t+1} + \\gamma \\max_{a'} q_*(S_{t+1}', a') | S_t = s, A_t = a] \\\\ &=& \\sum_{s', r} p(s', r | s, a) [r + \\gamma \\max_{a'} q_*(s', a') ] \\end{eqnarray} Looking ahead: In practice, without a knowledge of the environment dynamics, RL algorithms based on solving value functions can approximate the expectation in (\\ref{action-value-bellman-optimality}) by sampling, i.e. interacting with the environment, and iteratively selecting the action that corresponds to maximizing \\(q\\) in each state that the agent lands in along its trajectory, which is possible since the maximum occurs inside the summation in (\\ref{action-value-bellman-optimality}). In contrast, this sampling approach doesn't work for (\\ref{state-value-bellman-optimality}) because of the maximum outside the summation in…that's why action value functions are so useful when we lack a model of the environment! Here is the optimal state value function and policy for the student example, which we solve for in a later post: Comparing the values per state under the optimal policy vs the random policy, the value in every state under the optimal policy exceeds the value under the random policy. Summary We've discussed how the problem of sequential decision making can be framed as an MDP using the student toy MDP as an example. The goal in RL is to figure out a policy — what actions to take in each state — that maximizes our returns. MDPs provide a framework for approaching the problem by defining the value of each state, the value functions, and using the value functions to define what a \"best policy\" means. The value functions are unique solutions to the Bellman equations, and the MDP is \"solved\" when we know the optimal value function. Much of reinforcement learning centers around trying to solve these equations under different conditions, e.g. unknown environment dynamics and large — possibly continuous — states and/or action spaces that require approximations to the value functions. We'll discuss how we arrived at the solutions for this toy problem in a future post! Example code Code for sampling from the student environment under a random policy in order to generate the trajectories and histograms of returns is available in this jupyter notebook . The code for the student environment creates an environment with an API that is compatible with OpenAI gym — specifically, it is derived from the gym.envs.toy_text.DiscreteEnv environment. The uniqueness of the solution to the Bellman equations for finite MDPs is stated without proof in Ref [2], but Ref [1] motivates it briefly via the contraction mapping theorem*. References [1] David Silver's RL Course Lecture 2 - ( video , slides ) [2] Sutton and Barto - Reinforcement Learning: An Introduction - Chapter 3: Finite Markov Decision Processes if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Machine learning","url":"https://efavdb.com/intro-rl-toy-example","loc":"https://efavdb.com/intro-rl-toy-example"},{"title":"A Framework for Studying Population Dynamics","text":"In this post, I want to briefly introduce a new side project for the blog with applications to understanding population dynamics, natural selection, game theory, and probably more. World Wandering Dudes is a simulation framework in which you initiate a \"world\" which consists of a \"field\" and a set of \"creatures\" (dudes). The field has food on it. Each day, the creatures run around gathering the food which they need to survive and reproduce. Example Here's an example of a few days passing in a world where food randomly sprouts each day, never spoiling, initiated with a single creature (particular note: after day 1 passes and there are two creatures, one of them doesn't store enough food to reproduce at the end of the day): Taking a snapshot of the world at the end of each day for the first 20 or so days you can see the creatures take over the full field before coming to some general equilibrium state. How the world works A. Each day consists of a number of discrete time steps. During each time step, the creatures move around the field randomly, if they find food they grab it from the field and store it. B. At the end of the day, a few things happen: Each creature must eat some food. If they don't have enough stored, they die. If they have enough food after eating, they may also reproduce. Offspring may have mutated properties (e.g., they may move a little faster each time step - speedy creatures - or they may eat less food - efficient creatures) The food may spoil throughout the world (or not) and new food may sprout on the field. Examining the historical record You can also look at this historical recordfor the field and examine some metrics including total creature count, birth/death rate, the mutation composition of the creatures, amount of stored food, amount of ungathered food, and more: Some phenomenological notes on this particular case (more details on the math behind some of this in future posts): The dynamics of the world are stochastic. For example, sometimes the first creature doesn't find any food and dies immediately. The population initially grows roughly exponentially as food becomes plentiful across the map. With the accumulated food on the field from the initial low-population days, the creatures grow in numbers beyond a sustainable population and a period of starvation and population culling follows. :( The population reaches an equilibrium at which the number of creatures is nearly the same as the amount of food sprouted each day (it's not exactly equal!). At equilibrium, the rate at which creatures are being born is equal to the rate at which they die (on average) and both appear to be about a third of the total population (it's not a third!). As mentioned above, upon reproduction the creatures will mutate and the fitter creatures may take over the world. In this particular case, efficient creatures come about first and quickly take over the population. The world can actually sustain a higher population of efficient vs normal/speedy creatures, so the total population increases accordingly. Shortly thereafter, a few speedy creatures start to show up and they, slowly, take over the world, out-competing the efficient creatures and slowly suppressing the overall population. More to come on extensions of this project and understanding the math behind it in the future. Check it out yourself The github repository is here . You'll need a bunch of the usual python packages for data science. git clone https://github.com/dustinmcintosh/world_wandering_dudes cd world_wandering_dudes Update the directory for saving figures in SET_ME.py if you'd like to store them somewhere special. Run the sample code: python scripts/basic_simulation.py You can recycle the same world again using: python scripts/basic_simulation.py -wp my_world.pkl","tags":"World Wandering Dudes","url":"https://efavdb.com/world-wandering-dudes","loc":"https://efavdb.com/world-wandering-dudes"},{"title":"Multiarmed bandits in the context of reinforcement learning","text":"Reinforcement Learning: An Introduction by Sutton and Barto[1] is a book that is universally recommended to beginners in their RL studies. The first chapter is an extended text-heavy introduction. The second chapter deals with multiarmed bandits, i.e. slot machines with multiple arms, and is the subject of today's post. Before getting into the what and how of bandits, I'd like to address the why , since the \"why\" can guard against getting lost in the details / not seeing the forest for the trees. Why discuss multiarmed bandits? RL treats the problem of trying to achieve a goal in an environment where an agent is not instructed about which actions to take to achieve that goal, in contrast to supervised learning problems. Learning the best actions to take is a complicated problem, since the best actions depend on what state an agent is in, e.g. an agent trying to get to a goalpost east of its current location as quickly as possible may find that moving east is a generally good policy, but not if there is a fire-breathing dragon in the way, in which case, it might make sense to move up or down to navigate around the obstacle. Multiarmed bandits are simpler problem: a single state system. No matter which action an agent takes, i.e. which slot machine arm the agent pulls, the agent ends up back in the same state; the distribution of rewards as a consequence of the agent's action remains the same, assuming a stationary distribution of rewards, and actions have no effect on subsequent states or rewards. This simple case study is useful for building intuition and introducing RL concepts that will be expanded on in later chapters of [1]. Key RL concepts introduced by the multiarmed bandit problem The nature of the problem Agent has a goal : In RL and multiarmed bandit problems, we want to figure out the strategy, or \"policy\" in RL lingo, that will maximize our rewards. For the simple bandit problem, this goal is equivalent to maximizing the reward — literally, money! — for each arm pull. Unlike supervised learning, no ground truth is supplied : Each slot has a different distribution of rewards, but the agent playing the machine does not know that distribution. Instead, the agent has to try different actions and evaluate how good the actions are. The goodness of an action is straightforwardly determined by its immediate reward in the bandit case. Exploration vs. exploitation : Based on a few trials, one arm may appear to yield the highest rewards, but the agent may decide to try others occasionally to improve its estimates of the rewards, an example of balancing exploration and exploitation. The various algorithms handle exploration vs. exploitation differently, but this example introduces one method that is simple but widely-used in practice: the epsilon-greedy algorithm, which takes greedy actions most of the time (exploits) but takes random actions (explores) a fraction epsilon of the time. Different approaches to learning a policy model-free : All the strategies discussed in [1] for solving the bandit problem are \"model-free\" strategies. In real world applications, a model of the world is rarely available, and the agent has to figure out how to act based on sampled experience, and the same applies to the bandit case; even though bandits are a simpler single state system (we don't have to model transitions from state to state), an agent still does not know the model that generates the probability of a reward \\(r\\) given an action \\(a\\) , \\(P(r|a)\\) and has to figure that out from trial and error. There are model-based algorithms that attempt to model the environment's transition dynamics from data, but many popular algorithms today are model-free because of the difficulty of modeling those dynamics. Learning action-values The bandit problem introduces the idea of estimating the expected value associated with each action, namely the action-value function in RL terms. The concept is very intuitive — as an agent pulls on different bandit arms, it will accumulate rewards associated with each arm. A simple way to estimate the expected value per arm is just to average the rewards generated by pulling on each slot. The policy that follows is then implicit, namely, take the action / pull on the arm with the highest estimated action-value! Historically, RL formalism has dealt with estimating value functions and using them to figure out a policy, which includes the Q-Learning (\"Q\" stands for action-value!) approach we mentioned in our earlier post . Learning policies directly [1] also use the bandit problem to introduce a type of algorithm that approaches the problem, not indirectly by learning a value function and deriving the policy from those value functions, but by parameterizing the policy directly and learning the parameters that optimize the rewards. This class of algorithm is a \"policy gradient method\" and is very popular today for its nice convergence properties. After the foreshadowing in the bandit problem, policy gradients only reappear very late in [1] — chapter 13! We now provide code for concreteness. Ground truth is hidden in our multiarmed bandit The Bandit class initializes a multiarmed bandit. The distribution of rewards per arm follows a Gaussian distribution with some mean dollar amount. class Bandit : \"\"\"N-armed bandit with stationary distribution of rewards per arm. Each arm (action) is identified by an integer. \"\"\" def __init__ ( self , n_arms : int , mu : float , sigma : float ): self . n_arms = n_arms self . std = sigma # a dict of the mean action_value per arm, w/ each action_value sampled from a Gaussian self . action_values = { k : s for k , s in enumerate ( np . random . normal ( mu , sigma , n_arms ))} self . actions = list ( self . action_values . keys ()) # arms of the bandit def __call__ ( self , action : int ) -> float : \"\"\"Get reward from bandit for action\"\"\" return np . random . normal ( self . action_values [ action ], self . std ) Implementation detail: the means per arm, stored in self.action_values , are drawn from a Gaussian distribution upon initialization). The agent doesn't know the true mean rewards per arm — it only sees a sample reward when he takes the action of pulling on a particular bandit arm ( __call__ ). Action, reward, update strategy For every action the agent takes, it gets a reward. With each additional interaction with the bandit, the agent has a new data point it can use to update its strategy (whether indirectly, via an updated action-value estimate, or directly in the policy gradient). class BaseBanditAlgo ( ABC ): \"\"\"Base class for algorithms to maximize the rewards for the multiarmed bandit problem\"\"\" def __init__ ( self , bandit : Bandit ): self . bandit = bandit self . timestep = 0 self . rewards = [] @abstractmethod def _select_action ( self ) -> int : pass @abstractmethod def _update_for_action_and_reward ( self , action : int , reward : float ): pass def run ( self ) -> float : action = self . _select_action () reward = self . bandit ( action ) self . _update_for_action_and_reward ( action , reward ) return reward def __call__ ( self , n_timesteps : int ): for i in range ( n_timesteps ): self . timestep += 1 self . rewards . append ( self . run ()) Two types of strategies: value based and policy based value based - agents try to directly estimate the value of each action (and whose policies, i.e. probability of selecting an action, are therefore implicit, since the agent will want to choose the action that has the highest value) policy based - agents don't try to directly estimate the value of an action and instead directly store the policy, i.e. the probability of taking each action. An example of a value based strategy / action-value method for the bandit problem is the EpsilonGreedy approach, which selects the action associated with the highest estimated action-value with probability \\(1-\\epsilon\\) , but chooses a random arm a fraction \\(\\epsilon\\) of the time as part of its exploration strategy. class EpsilonGreedy ( BaseEstimateActionValueAlgo ): \"\"\"Greedy algorithm that explores/samples from the non-greedy action some fraction, epsilon, of the time. - For a basic greedy algorithm, set epsilon = 0. - For optimistic intialization, set q_init > mu, the mean of the Gaussian from which the real values per bandit arm are sampled (default is 0). \"\"\" def __init__ ( self , bandit : Bandit , epsilon : float , ** kwargs ): super () . __init__ ( bandit , ** kwargs ) self . epsilon = epsilon def _select_action ( self ) -> int : if np . random . sample () < self . epsilon : # take random action a = np . random . choice ( self . bandit . actions ) else : # take greedy action a = max ( self . est_action_values , key = lambda key : self . est_action_values [ key ]) return a (See end of post for additional action-value methods.) An example of a policy based strategy is the GradientBandit method, which stores its policy, the probability per action in self.preferences . It learns these preferences by doing stochastic gradient ascent along the preferences in the gradient of the expected reward in _update_for_action_and_reward (see [1] for derivation). class GradientBandit ( BaseBanditAlgo ): \"\"\"Algorithm that does not try to estimate action values directly and, instead, tries to learn a preference for each action (equivalent to stochastic gradient ascent along gradient in expected reward over preferences). \"\"\" def __init__ ( self , bandit : Bandit , alpha : float ): super () . __init__ ( bandit ) self . alpha = alpha # step-size self . reward_baseline_avg = 0 self . preferences = { action : 0 for action in bandit . actions } self . _calc_probs_from_preferences () def _calc_probs_from_preferences ( self ): \"\"\"Probabilities per action follow a Boltzmann distribution over the preferences \"\"\" exp_preferences_for_action = { action : np . exp ( v ) for action , v in self . preferences . items ()} partition_fxn = sum ( exp_preferences_for_action . values ()) self . probabilities_for_action = OrderedDict ({ action : v / partition_fxn for action , v in exp_preferences_for_action . items ()}) def _select_action ( self ) -> int : return np . random . choice ( list ( self . probabilities_for_action . keys ()), p = list ( self . probabilities_for_action . values ())) def _update_for_action_and_reward ( self , action , reward ): \"\"\"Update preferences\"\"\" reward_diff = reward - self . reward_baseline_avg # can we combine these updates into single expression using kronecker delta? self . preferences [ action ] += self . alpha * reward_diff * ( 1 - self . probabilities_for_action [ action ]) for a in self . bandit . actions : if a == action : continue else : self . preferences [ a ] -= self . alpha * reward_diff * self . probabilities_for_action [ a ] self . reward_baseline_avg += 1 / self . timestep * reward_diff self . _calc_probs_from_preferences () Extra: Total rewards for different bandit algorithms We have discussed a bunch of different bandit algorithms, but haven't seen what rewards they yield in practice! In this Jupyter notebook , we run the algorithms through a range of values for their parameters to compare their cumulative rewards across 1000 timesteps (also averaged across many trials of different bandits to smooth things out). In the end, we arrive at a plot of the parameter study, that reproduces Figure 2.6 in [1]. References [1] Sutton and Barto - Reinforcement Learning: An Introduction (2nd Edition) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Machine learning","url":"https://efavdb.com/multiarmed-bandits","loc":"https://efavdb.com/multiarmed-bandits"},{"title":"Introduction to OpenAI Scholars 2020","text":"Two weeks ago, I started at the OpenAI Scholars program, which provides the opportunity to study and work full time on a project in an area of deep learning over 4 months. I'm having a blast! It's been a joy focusing 100% on learning and challenging myself in an atmosphere full of friendly intellectual energy and drive. My mentor is Jerry Tworek, an OpenAI research scientist who works on reinforcement learning ( RL ) in robotics, and I've also chosen to focus on RL during the program. I constructed a syllabus that will definitely evolve over time, but I'll try to keep it up-to-date to serve as a useful record for myself and a guide for others who might be interested in a similar course of study. Some casual notes from the last two weeks: (1) There are manifold benefits to working on a topic that is in my mentor's area of expertise. For example, I've already benefited from Jerry's intuition around hyperparameter tuning and debugging RL -specific problems, as well as his guidance on major concepts I should focus on in my first month, namely, model-free RL divided broadly into Q-Learning and Policy Gradients. (2) Weights & Biases at wandb.com is a fantastic free tool for tracking machine learning experiments that many people use at OpenAI. It was staggeringly simple to integrate wandb with my training script — both for local runs and in the cloud! Just ~4 extra lines of code, and logged metrics automagically appear in my wandb dashboard, with auto-generated plots grouped by experiment name, saved artifacts, etc. Here's an example of a dashboard tracking experiments for my first attempt at implementing a deep RL algorithm from scratch ( DQN , or Deep Q learning). The script that is generating the experiments is still a work in progress, but you can see how few lines were required to integrate with wandb here . Stay tuned for a blog post about DQN itself in the future! (3) I've found it very helpful to parallelize reading Sutton and Barto's Reinforcement Learning: An Introduction , the classic text on RL , with watching David Silver's pedagogical online lectures . Silver's lectures follow the book closely for the first few chapters, then start condensing several chapters per lecture beginning around lecture 4 or 5 — helpful since I'm aiming to ramp up on RL over a short period of time! Silver also supplements with insightful explanations and material that aren't covered in the book, e.g. insights about the convergence properties of some RL algorithms. Note, Silver contributed to the work on Deep Q Learning applied to Atari that generated a lot of interest in deep RL beginning in 2013, leading to a publication in Nature in 2015, so his lecture 6 on Value Function Approximation ( video , slides ) is a perfect accompaniment to reading the paper.","tags":"Misc","url":"https://efavdb.com/openai-scholars-intro","loc":"https://efavdb.com/openai-scholars-intro"},{"title":"Universal limiting mean return of CPPI investment portfolios","text":"CPPI * is a risk management tactic that can be applied to any investment portfolio. The approach entails banking a percentage of profits whenever a new all time high wealth is achieved, thereby ensuring that a portfolio's drawdown never goes below some maximum percentage. Here, I review CPPI and then consider the mean growth rate of a CPPI portfolio. I find that in a certain, common limit, this mean growth is given by a universal formula, (\\ref{cppi_asymptotic_growth}) below. This universal result does not depend in detail on the statistics of the investment in question, but instead only on its mean return and standard deviation. I illustrate the formula's accuracy with a simulation in python. * CPPI = \"Constant Proportion Portfolio Insurance\" Introduction The drawdown of an investment portfolio at a given date is equal to the amount of money lost relative to its maximum held capital up to that date. This is illustrated in the figure at right — a portfolio that once held $100 now holds only $90, so the drawdown is currently $10. CPPI is a method that can be applied to guarantee that the maximum fractional drawdown is never more than some predetermined value — the idea is to simply squirrel away an appropriate portion of earnings whenever we hit a new maximum account value, and only risk what's left over from that point on. For example, to cap the max loss at 50%, one should only risk 50% of the initial capital and then continue to bank 50% of any additional earnings whenever a new all time high is reached. According to Wikipedia, the first person to study CPPI was Perold, who derived the statistical properties of a CPPI portfolio's value at time \\(t\\) , assuming the underlying stochastic investment follows a Wiener process. I was introduced to the CPPI concept by the book \"Algorithmic Trading\" by Ernest Chan. This book implicitly poses the question of what the mean return is for general, discrete investment strategies. Here, I show that a universal formula applies in this case, valid at low to modest leverages and small unit investment Sharpe ratios — this result is given in equation (\\ref{cppi_asymptotic_growth}) below. The post proceeds as follows: In the next section, I define some notation and then write down the limiting result. In the following section, I give a numerical example in python. Finally, an appendix contains a derivation of the main result. This makes use of the universal limiting drawdown distribution result from my prior post . CPPI formulation and universal mean growth rate In this section, I review the CPPI strategy and give the limiting mean return result. Consider a portfolio that at time \\(t\\) has value, $$ \\begin{align}\\tag{1} W_t = S_t + \\Gamma_t \\end{align} $$ where \\(W\\) is our total wealth, \\(S\\) is the banked (safe) portion, and \\(\\Gamma\\) is the portion we are willing to bet or gamble. The savings is set so that each time we reach a new all time high wealth, \\(S\\) is adjusted to be equal to a fraction \\(\\Pi\\) of the net wealth. When this is done, the value of \\(\\Gamma\\) must also be adjusted downwards — some of the investment money is moved to savings. Before adjustment, the result of a bet moves \\(\\Gamma\\) to \\begin{align}\\label{gamble_eom_cppi} \\tag{2} \\tilde{\\Gamma}_{t+1} \\equiv \\left (1 + f (g_{t} - 1)\\right) \\Gamma_t. \\end{align} Here, the tilde at left indicates this is the value before any reallocation is applied — in case we have reached a new high, \\(g_t\\) is a stochastic investment outcome variable, and \\(f\\) is our \"leverage\" — a constant that encodes how heavily we bet on the game. I will assume all \\(g_i\\) are independent random variables that are identically distributed with distribution \\(p(g)\\) . I will assume that \\(f\\) is a fixed value throughout time, and will write \\begin{align} \\tag{3} \\label{3} f = \\frac{1}{\\phi} \\frac{ \\langle g -1 \\rangle }{ \\text{var}(g)} \\end{align} This re-parameterization helps to make the math work out more nicely below. It is also motivated by the Kelly formula, which specifies the gambling leverage that maximizes wealth at long times (here, setting \\(\\phi \\to 1\\) gives the Kelly exposure). The equations above define the CPPI scheme. The main result of this post is that in the limit where the leverage is not too high, and the stochastic \\(g\\) has small Sharpe ratio (mean return over standard deviation), the mean log wealth at time \\(t\\) satisfies \\begin{align} \\label{cppi_asymptotic_growth} \\tag{4} \\frac{1}{t}\\langle \\log W_{t} - \\log W_{0} \\rangle \\sim \\frac{ \\langle g -1 \\rangle&#94;2 }{ \\text{var}(g)} (1-\\Pi) \\frac{(2 \\phi -1)}{2 \\phi &#94;2}. \\end{align} This result can be used to estimate the mean growth of a portfolio to which CPPI is applied. Before illustrating its accuracy via a simulation below, I highlight a few points about the result: The fact that (\\ref{cppi_asymptotic_growth}) is universal makes it very practical to apply to a real world investment: Rather than having to estimate the full distribution for \\(g\\) , we need only estimate its mean and variance to get an estimate for the portfolio's long-time mean return. The mean return (\\ref{cppi_asymptotic_growth}) is equivalent to that found by Perold for Gaussian processes — as expected, since the result is \"universal\". The maximum return at fixed \\(\\Pi\\) is again obtained at \\(\\phi = 1\\) , the Kelly maximum. If \\(\\phi = 1/2\\) , we are at twice Kelly exposure and the mean return is zero — this a well known result. At \\(\\phi < 1/2\\) , we are above twice Kelly and the return is negative. The mean return is reduced by a factor \\((1-\\Pi)\\) — the fraction of new high wealths we are exposing to loss. It is interesting that the result is not suppressed faster than this as we hold out more wealth, given that we have lower exposure after a loss than we would otherwise. One can ask what \\(\\phi\\) gives us the same mean return using CPPI as we would obtain at full exposure using \\(\\phi_0\\) . E.g., consider the case of \\(\\Pi = 1/2\\) , which corresponds to insuring that half of our wealth is protected from loss. Equating the mean gains of these two gives \\begin{align} \\frac{1}{2} \\frac{(2 \\phi -1)}{2 \\phi &#94;2} = \\frac{(2 (2)-1)}{2 (2)&#94;2} = \\frac{3}{8}. \\end{align} The two roots for \\(\\phi\\) are plotted versus \\(\\phi_0\\) below. Notice that we can't find solutions for all \\(\\phi_0\\) — it's not possible to match the mean return for high leverages at full exposure when we force protection of some of our assets. We now turn to a simulation example. Python CPPI simulation In the code below, we consider a system where in each step we either \"win\" or \"lose\": If we win, the money we risk grows by a factor of \\(f \\times 1.02\\) , and if we lose, it goes down by a factor of \\(f \\times 1 / 1.02\\) . We take the probability of winning to be \\(0.65\\) . This game has a Sharpe ratio of \\(0.32\\) , small enough that our approximation should work well. The code below carries out a simulated repeated investment game over 100 trials — we hope that it is clear what is happening at each step. import numpy as np # investment definitions -- a random walk LIFT_ON_WIN = 1.02 LIFT_ON_LOSS = 1 / LIFT_ON_WIN P_WIN = 0.65 P_LOSS = 1 - P_WIN g_minus_1_bar = P_WIN * ( LIFT_ON_WIN - 1 ) + P_LOSS * ( LIFT_ON_LOSS - 1 ) var_g = P_WIN * ( LIFT_ON_WIN - 1 ) ** 2 + P_LOSS * ( LIFT_ON_LOSS - 1 ) ** 2 - ( g_minus_1_bar ) ** 2 full_kelly = g_minus_1_bar / var_g sharpe = g_minus_1_bar / np . sqrt ( var_g ) print 'Sharpe ratio of g unit bet: %.4f ' % sharpe def simulate_once ( phi , pi , steps ): initial_value = 1.0 f = full_kelly / phi current_nav = initial_value current_max = current_nav for _ in range ( steps ): # update current max current_max = max ( current_nav , current_max ) # calculate current effective nav current_drawdown = current_max - current_nav gamma = current_max * ( 1 - pi ) - current_drawdown # play round of investment game dice_roll = np . random . rand () win = ( dice_roll < P_WIN ) loss = 1 - win g = LIFT_ON_WIN * win + LIFT_ON_LOSS * loss nav_change = gamma * f * ( g - 1 ) # update wealth current_nav += nav_change return current_nav end_results = [] # kelly and cppi properties PHI = 10.0 PI = . 75 STEPS = 1000 TRIALS = 100 # simulation loop for trial in range ( TRIALS ): end_nav = simulate_once ( phi = PHI , pi = PI , steps = STEPS ) end_results . append ( end_nav ) theory = ( g_minus_1_bar ** 2 / var_g ) * ( 1 - PI ) * ( 2 * PHI - 1 ) / ( 2 * PHI ** 2 ) print 'Experiment: %2.5f ' % ( np . mean ( np . log ( end_results )) / STEPS ) print 'Theory: %2.5f ' % theory # OUTPUT: # Sharpe ratio of g unit bet: 0.3249 # Experiment: 0.00251 # Theory: 0.00251 The last lines above show the output of our print statements. In particular, the last two lines show the mean growth rate observed over the 100 trials and the theoretical value (\\ref{cppi_asymptotic_growth}) — these agree to three decimal places. Using a loop over \\(\\phi\\) values, I used the code above to obtain the plot below of mean returns vs \\(\\phi\\) . This shows that the limiting result works quite well over most \\(\\phi\\) — though there is some systematic, modest discrepancy at small \\(\\phi\\) . This is expected as the quadratic expansion for log wealth used below starts to break down at high exposures. Nevertheless, the fit is qualitatively quite good at all \\(\\phi\\) . This suggests that our result (\\ref{cppi_asymptotic_growth}) can be used for quick mean return forecasts for most practical, applied cases of CPPI . Appendix: Derivation of mean return We give a rough sketch here of a proof of (\\ref{cppi_asymptotic_growth}). Our aim is to quickly get the universal form in a relatively clean way. Note that these results may be new, or perhaps well known to finance theorists — I'm not sure. To begin let us define \\begin{align} \\Gamma&#94;*_t = \\max_{t&#94;{\\prime} \\leq t} \\Gamma_t. \\end{align} This is the maximum \\(\\Gamma\\) seen to date at time \\(t\\) . Necessarily, this is the value of \\(\\Gamma\\) as of the most recent all time high preceeding \\(t\\) . If \\(\\Gamma_t < \\Gamma&#94;*_t\\) , we say that we are in drawdown by value \\(\\Gamma&#94;*_t - \\Gamma_t\\) . At all times, we have \\begin{align}\\nonumber S_t &= \\frac{\\Pi}{1 - \\Pi} \\Gamma&#94;*_t \\\\ &\\equiv \\rho \\Gamma&#94;*_t. \\end{align} This result holds because the saved portion is \\(\\Pi\\) times the net wealth when we reach a new high and \\(\\Gamma&#94;*\\) is what's left over, \\((1 - \\Pi)\\) times the net wealth at that time. From the above definitions, our net wealth after a step is given by \\begin{align}\\nonumber W_{t+1} &= W_{t} \\left ( 1 + f(g_t -1) \\frac{\\Gamma_t}{W_{t}} \\right ) \\\\ \\nonumber &= W_{t} \\left ( 1 + f(g_t -1) \\frac{\\Gamma_t}{S_t + \\Gamma_t} \\right ) \\\\ &= W_{t} \\left ( 1 + f(g_t -1) \\frac{\\frac{\\Gamma_t}{\\Gamma&#94;*_t}}{\\rho+ \\frac{\\Gamma_t}{\\Gamma&#94;*_t}} \\right ) \\end{align} Iterating and taking the logarithm we obtain \\begin{align}\\nonumber \\tag{A1} \\label{A1} \\log W_{t} &= \\log W_{0} + \\sum_{i=0}&#94;{t-1} \\log \\left ( 1 + f(g_i -1) \\frac{\\frac{\\Gamma_i}{\\Gamma&#94;*_i}}{\\rho+ \\frac{\\Gamma_i}{\\Gamma&#94;*_i}} \\right ) \\\\ &\\approx \\log W_{0} + \\sum_{i=0}&#94;{t-1} f (g_i -1) \\frac{\\frac{\\Gamma_i}{\\Gamma&#94;*_i}}{\\rho+ \\frac{\\Gamma_i}{\\Gamma&#94;*_i}} - \\frac{f&#94;2}{2} \\left ((g_i -1) \\frac{\\frac{\\Gamma_i}{\\Gamma&#94;*_i}}{\\rho+ \\frac{\\Gamma_i}{\\Gamma&#94;*_i}}\\right)&#94;2 + \\ldots \\end{align} The series expansion in the second line can be shown to converge quickly provided we have selected a leverage \\(f\\) that always results in a small percentage change in our net wealth each step. Note that it is the breakdown of this expansion that causes the slight divergence at low \\(\\phi\\) in our last plot above. Our aim is to evaluate the average of the last equation. The first key point needed to do this is to note that at step \\(i\\) , we have \\(g_i\\) and \\(\\Gamma_i / \\Gamma_i&#94;*\\) independent (the outcome of the unit bet doesn't depend on how much we have to wager at this time). This allows us to factor the averages above into one over \\(g\\) and over the \\(\\Gamma_i / \\Gamma_i&#94;*\\) distribution. The former is relatively easy to write down if we assume some properties for \\(f\\) and \\(g\\) . To proceed on the latter, we note that \\begin{align}\\nonumber \\log \\frac{\\Gamma_i}{\\Gamma&#94;*_i} = \\log \\Gamma_i - \\log \\Gamma&#94;*_i \\end{align} is the drawdown of a random walk, with steps defined by (\\ref{gamble_eom_cppi}). We have argued in our last post that the tail of the distribution of this drawdown distribution is such that \\begin{align}\\nonumber p( \\log \\Gamma_i - \\log \\Gamma&#94;*_i = -k) \\propto \\exp(\\alpha k). \\end{align} where \\(\\alpha\\) is given in that post as an implicit function of the statistics of \\(g\\) . This tail form will hold almost everywhere when the Sharpe ratio is small. We will assume this here. Using the change of variables rule, we get \\begin{align}\\nonumber p\\left(\\frac{\\Gamma_i}{\\Gamma&#94;*_i} = k\\right) \\sim \\begin{cases} \\alpha k&#94;{\\alpha - 1} & \\text{if } x \\in (0, 1) \\\\ 0 & \\text{else} \\end{cases} \\end{align} Again, this is an approximation that assumes we spend relatively little time within one jump from the current all time high — a result that will hold in the small Sharpe ratio limit. With this result, we obtain \\begin{align}\\nonumber \\tag{A2} \\label{A2} \\left \\langle \\frac{\\frac{\\Gamma_i}{\\Gamma&#94;*_i}}{\\rho+ \\frac{\\Gamma_i}{\\Gamma&#94;*_i}} \\right \\rangle &\\equiv \\int_0&#94;1 \\frac{x}{\\rho + x} \\alpha x&#94;{\\alpha - 1} dx \\\\ &= \\frac{\\alpha \\, _2F_1\\left(1,\\alpha +1;\\alpha +2;-\\frac{1}{\\rho }\\right)}{\\rho (\\alpha +1) } \\end{align} Here, \\(_2F_1\\) is the hypergeometric function. Similarly, \\begin{align}\\nonumber \\tag{A3} \\label{A3} \\left \\langle \\left( \\frac{\\frac{\\Gamma_i}{\\Gamma&#94;*_i}}{\\rho+ \\frac{\\Gamma_i}{\\Gamma&#94;*_i}} \\right)&#94;2 \\right \\rangle &\\equiv \\int_0&#94;1 \\left( \\frac{x}{\\rho + x} \\right)&#94;2 \\alpha x&#94;{\\alpha - 1} dx \\\\ &= \\frac{\\alpha \\left(\\frac{\\rho }{\\rho +1}-\\frac{(\\alpha +1) \\, _2F_1\\left(1,\\alpha +2;\\alpha +3;-\\frac{1}{\\rho }\\right)}{\\alpha +2}\\right)}{\\rho &#94;2} \\end{align} Note that both of the last two lines go to one as \\(\\rho \\to 0\\) , the limit where we invest our entire net worth and protect nothing. In this case, the growth equations are just those for a fully invested account. If you plug the last two results into the second line of (\\ref{A1}), you get an expression for the mean return. To get the above results, we assumed a small Sharpe ratio. Therefore, to simplify things, we can use the value of \\(\\alpha\\) that we derived in our last post that is valid in this limit. This was given by \\(\\alpha \\sim 2 \\mu / \\sigma&#94;2\\) , where \\(\\mu\\) and \\(\\sigma\\) and mean and standard deviation of the random walk. We now evaluate these to get an expression for \\(\\alpha\\) in terms of the statistics of \\(g\\) . First, we note that the mean drift of \\(\\log \\Gamma\\) is given by \\begin{align}\\nonumber \\mu &\\equiv \\langle \\log(1 + f (g-1)) \\rangle \\ \\nonumber &\\sim f \\langle g -1 \\rangle - \\frac{f&#94;2}{2} \\langle (g-1)&#94;2 \\rangle \\\\ &\\sim f \\langle g -1 \\rangle - \\frac{f&#94;2}{2} \\text{var}(g). \\end{align} The last line follows from the assumption that the Sharpe ratio for \\(g -1\\) is small, so that \\begin{align}\\nonumber \\langle (g-1)&#94;2 \\rangle &= \\left ( \\langle (g-1)&#94;2 \\rangle - \\langle (g-1) \\rangle&#94;2 \\right) + \\langle (g-1) \\rangle&#94;2 \\\\ &= \\text{var}(g) \\left ( 1 + \\frac{ \\langle (g-1) \\rangle&#94;2}{\\text{var}(g)} \\right ). \\end{align} Similarly, one can show that \\begin{align}\\nonumber \\sigma&#94;2 &\\equiv \\text{var} \\log(1 + f (g-1)) \\\\ &\\sim f&#94;2 \\text{var}(g). \\end{align} This gives \\begin{align}\\nonumber \\alpha &\\sim 2 \\frac{\\mu}{\\sigma&#94;2} \\\\ \\nonumber &\\sim 2 \\frac{ f \\langle g -1 \\rangle - \\frac{f&#94;2}{2} \\text{var}(g)}{ f&#94;2 \\text{var}(g)} \\\\ &= \\frac{2}{f} \\frac{ \\langle g -1 \\rangle }{ \\text{var}(g)} - 1 \\end{align} The second term in the first line can be neglected because we require the change in value to be a small fraction of our net wealth. We anticipate applying the algorithm to values of \\(f\\) of order \\(f \\sim O( \\frac{ \\langle g -1 \\rangle }{ \\text{var}(g)})\\) , so the above is \\(O(1)\\) . If we plug these results into the limiting form for \\(\\alpha\\) and use (\\ref{3}) for \\(f\\) , we get \\begin{align} \\tag{A4} \\label{A4} \\alpha \\sim 2 \\phi -1. \\end{align} We won't show the details, but if you plug (\\ref{A2}), (\\ref{A3}), and (\\ref{A4}) into (\\ref{A1}), this gives (\\ref{cppi_asymptotic_growth}). To get there, you need to show that a big collapse of terms occurs in the two hypergeometric functions. This collapse can be derived using the series expansion for \\(_2F_1\\) in about one page. The collapse of terms only occurs in the small Sharpe ratio limit, where \\(\\alpha\\) is given as above. We note that for some walks, an exponential form holds everywhere. In this case, the more general expression using \\(_2F_1\\) applies even at high Sharpe ratios — though we still require \\(f\\) small. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Finance","url":"https://efavdb.com/universal-mean-return-formula-of-the-cppi-investment-strategy","loc":"https://efavdb.com/universal-mean-return-formula-of-the-cppi-investment-strategy"},{"title":"Universal drawdown statistics in investing","text":"We consider the equilibrium drawdown distribution for a biased random walk — in the context of a repeated investment game, the drawdown at a given time is how much has been lost relative to the maximum capital held up to that time. We show that in the tail, this is exponential. Further, when mean drift is small, this has an exponent that is universal in form, depending only on the mean and standard deviation of the step distribution. We give simulation examples in python consistent with the results. Introduction and main results In this post, we consider a topic of high interest to investors and gamblers alike — the statistics of drawdown. This is the amount of money the investor has lost relative to their maximum held capital to date. For example, if an investor once held $100, but now holds only $90, his drawdown is currently $10. We will provide some results that characterize how unlikely it is for the investor to have a large drawdown of $\\(k\\), given knowledge of the statistics of his bets. We will take as our model system a biased random walk. The probability that at step \\(t\\) the investment goes from \\(k&#94;{\\prime}\\) to \\(k\\) will be taken to be independent of time and given by \\begin{eqnarray}\\tag{1} \\label{step_distribution} p(k&#94;{\\prime} \\to k) = \\tau(k - k&#94;{\\prime}). \\end{eqnarray} We will assume that this has a positive bias \\(\\mu\\) , so that on average the investor makes money. With this assumption, we show below that for \\(\\vert k \\vert\\) more than a few step sizes, the drawdown distribution has an exponential form, \\begin{eqnarray}\\tag{2} \\label{exponential} p(k) \\propto \\exp\\left( - \\alpha \\vert k \\vert \\right) \\end{eqnarray} where the decay constant \\(\\alpha\\) satisfies \\begin{eqnarray}\\tag{3} \\label{dd_decay_eqn} 1 = \\int_{-\\infty}&#94;{\\infty} \\exp\\left( \\alpha j \\right) \\tau(-j) dj. \\end{eqnarray} The form (\\ref{exponential}) holds for general distributions and (\\ref{dd_decay_eqn}) provides the formula for obtaining \\(\\alpha\\) in this case. However, in the limit where the mean drift \\(\\mu\\) in \\(\\tau\\) is small relative to its standard deviation, \\(\\sigma\\) , we show that the solution to (\\ref{dd_decay_eqn}) has a universal form, giving \\begin{eqnarray}\\tag{4} \\label{exponential_universal} p(k) \\propto \\exp\\left( - 2 \\frac{\\mu}{\\sigma&#94;2} \\vert k \\vert \\right). \\end{eqnarray} Because it is difficult to find very high drift investments, this simple form should hold for most real world investments (under the assumption of a Markov process). It can be used to give one a sense of how much time they can expect to sit at a particular drawdown, given estimates for \\(\\mu\\) and \\(\\sigma\\) . The results (\\ref{exponential} - \\ref{exponential_universal}) are the main results of this post. These may be new, but could also be well-known to finance theorists — we are not sure. We illustrate their accuracy in the following section using a numerical example, and provide derivations in an appendix. Numerical examples in python Here, we will consider two different kinds of random walk — one where the steps are always the same size, but there is bias in the forward direction, and the other where the steps are taken from a Gaussian or normal distribution. The code below carries out a simulated investing scenario over one million steps. import numpy as np def binary ( mu ): \"\"\" Return either mu - 1 or mu + 1 with equal probability. Note unit std. \"\"\" return np . random . choice ([ - 1 , 1 ]) + mu def normal_random_step ( mu ): \"\"\" Return a random unit normal with unit std. \"\"\" return np . random . randn () + mu # CONSTANTS TIME_STEPS = 10 ** 6 MU = 0.1 # BINARY WALK position = 0 max_position_to_date = 0 drawdowns_binary = [] for time in range ( TIME_STEPS ): position += STEP_FUNC ( MU ) max_position_to_date = max ( max_position_to_date , position ) drawdowns_binary . append ( max_position_to_date - position ) # GAUSSIAN / NORMAL WALK STEP_FUNC = normal_random_step position = 0 max_position_to_date = 0 drawdowns_normal = [] for time in range ( TIME_STEPS ): position += STEP_FUNC ( MU ) max_position_to_date = max ( max_position_to_date , position ) drawdowns_normal . append ( max_position_to_date - position ) You can see in the code that we have a loop over steps. At each step, we append to a list of observed drawdown values. A plot of the histogram of these values for the Normal case at \\(\\mu = 0.1\\) is shown at right. To check whether our theoretical forms are accurate, it is useful to plot the cumulative distribution functions vs the theoretical forms — the latter will again be exponential with the same \\(\\alpha\\) values as the probability distribution functions. It turns out that the exponent \\(\\alpha\\) that solves (\\ref{dd_decay_eqn}) is always given by the universal form for a Gaussian. However, for the binary walker, we need to solve for this numerically in general. The following code snippet does this. from scipy.optimize import fsolve # Solving numerically for binary case. binary_alpha_func = lambda x : 1 - np . exp ( x * MU ) * np . cosh ( x ) alpha_initial_guess = - 4 alpha_solution = fsolve ( binary_alpha_func , alpha_initial_guess ) A plot of the function above and the solution when \\(\\mu = 0.85\\) is shown below. Note that there is always an unphysical solution at \\(\\alpha =0\\) — this should be ignored. Using the above results, I have plotted the empirical cdfs versus \\(k\\) for both walk distributions. The values are shown below for \\(\\mu = 0.1\\) (left) and \\(\\mu = 0.85\\) (right). The slopes of the theoretical and numerical results are what should be compared as these give the value of \\(\\alpha\\) . Note that \\(\\mu = 0.1\\) is a small drift relative to the standard deviation ( \\(\\sigma = 1\\) , here), but \\(\\mu = 0.85\\) is not. This is why at left the universal form gives us a good fit to the decay rates for both systems, but at right we need our numerical solution to (\\ref{dd_decay_eqn}) to get the binary decay rate. In conclusion, we have found that the exponential form of drawdown works quite well in these examples, with the theoretical results (\\ref{exponential} - \\ref{exponential_universal}) providing methods for identifying the exponents. In particular, the plot at left above illustrates the universality of form (\\ref{exponential_universal}) — it holds for all walks, provided we are in the small bias limit. Appendix: Derivations To derive the exponential form, we consider an integral equation for the drawdown probability \\(p\\) . At equilibrium, we have \\begin{eqnarray}\\tag{A1} p(k) = \\int_{-\\infty}&#94;{0} p(k&#94;{\\prime}) T(k&#94;{\\prime}, k) dk&#94;{\\prime}. \\end{eqnarray} where \\(T\\) is the transition function for the drawdown process. In the tail, we can ignore the boundary at zero and this goes to \\begin{eqnarray}\\tag{A2} p(k) = \\int_{-\\infty}&#94;{\\infty} p(k&#94;{\\prime}) \\tau(k - k&#94;{\\prime}) dk&#94;{\\prime}, \\end{eqnarray} where we have taken the upper limit to infinity, assuming that the transition function has a finite length so that this is acceptable. We can solve this by positing an exponential solution of form \\begin{eqnarray}\\tag{A3} p(k) \\equiv A \\exp\\left(\\alpha k \\right). \\end{eqnarray} Plugging this into the above gives \\begin{eqnarray} \\nonumber A \\exp\\left(\\alpha k \\right) &=& \\int_{-\\infty}&#94;{\\infty} A \\exp\\left(\\alpha k&#94;{\\prime} \\right) \\tau(k - k&#94;{\\prime}) dk&#94;{\\prime}\\ \\tag{A4} &=& A \\exp\\left(\\alpha k \\right) \\int_{-\\infty}&#94;{\\infty} \\exp\\left( \\alpha j \\right) \\tau(-j) dj \\end{eqnarray} Simplifying this gives (\\ref{exponential}). Now, to get the universal form, we make use of the cumulant expansion, writing \\begin{eqnarray} \\nonumber 1 &=& \\int_{-\\infty}&#94;{\\infty} \\exp\\left( \\alpha j \\right) \\tau(-j) dj \\ &\\equiv & \\exp \\left ( - \\mu \\alpha + \\sigma&#94;2 \\frac{\\alpha&#94;2}{2} + \\ldots \\right) \\tag{A5} \\end{eqnarray} Provided the expansion converges quickly, we obtain \\begin{eqnarray} - \\mu \\alpha + \\sigma&#94;2 \\frac{\\alpha&#94;2}{2} + \\ldots = 0 \\tag{A6} \\end{eqnarray} giving \\begin{eqnarray} \\label{cppi_alpha_asymptotic} \\tag{A7} \\alpha \\sim 2 \\frac{\\mu}{\\sigma&#94;2} \\end{eqnarray} With this solution, the \\(k\\) -th term in the cumulant expansion goes like \\begin{eqnarray} \\tag{A8} \\frac{2&#94;k}{k!} \\left( \\frac{\\mu}{\\sigma&#94;2} \\right)&#94;k O(\\overline{x&#94;k}) \\sim \\frac{2&#94;k}{k!} \\left( \\frac{\\mu}{\\sigma} \\right)&#94;k \\end{eqnarray} assuming the the jumps are constrained over some length scale proportional to \\(\\sigma\\) . We see that provided the drift to standard deviation is small, the series converges quickly and our approximation is universally good. Unless you're cursed with an unusually large drift ratio for a given move, this form should work well. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Finance","url":"https://efavdb.com/universal-drawdown-statistics-in-investing","loc":"https://efavdb.com/universal-drawdown-statistics-in-investing"},{"title":"TimeMarker class for python","text":"We give a simple class for marking the time at different points in a code block and then printing out the time gaps between adjacent marked points. This is useful for identifying slow spots in code. The TimeMarker class In the past, whenever I needed to speed up a block of python code, the first thing I would do was import the time package, then manually insert a set of lines of the form t1 = time.time() , t2 = time.time() , etc. Then at the end, print t2 - t1, t3 -t2, ... , etc. This works reasonably well, but I found it annoying and time consuming to have to save each time point to a different variable name. In particular, this prevented quick copy and paste of the time marker line. I finally thought to fix it this evening: Behold the TimeMarker class, which solves this problem for me: import time class TimeMarker (): def __init__ ( self ): self . markers = [] def mark ( self ): self . markers . append ( time . time ()) def print_markers ( self ): for pair in zip ( self . markers , self . markers [ 1 :]): print pair [ 1 ] - pair [ 0 ] Here is a simple code example: tm = TimeMarker () tm . mark () sum ( range ( 10 ** 2 )) tm . mark () sum ( range ( 10 ** 6 )) tm . mark () tm . print_markers () # (output...) # 7.9870223999e-05 # 0.0279731750488 The key here is that I can quickly paste the tm.mark() repeatedly throughout my code and quickly check where the slow part sits.","tags":"Programming","url":"https://efavdb.com/timemarker-class-for-python","loc":"https://efavdb.com/timemarker-class-for-python"},{"title":"Backpropagation in neural networks","text":"Overview We give a short introduction to neural networks and the backpropagation algorithm for training neural networks. Our overview is brief because we assume familiarity with partial derivatives, the chain rule, and matrix multiplication. We also hope this post will be a quick reference for those already familiar with the notation used by Andrew Ng in his course on \"Neural Networks and Deep Learning\" , the first in the deeplearning.ai series on Coursera. That course provides but doesn't derive the vectorized form of the backpropagation equations, so we hope to fill in that small gap while using the same notation. Introduction: neural networks A single neuron acting on a single training example The basic building block of a neural network is the composition of a nonlinear function (like a sigmoid , tanh , or ReLU ) \\(g(z)\\) \\begin{eqnarray} \\nonumber a&#94;{[l]} = g(z&#94;{[l]}) \\end{eqnarray} with a linear function acting on a (multidimensional) input, \\(a\\) . \\begin{eqnarray} \\nonumber z&#94;{[l]} = w&#94;{[l]T} a&#94;{[l-1]} + b&#94;{[l]} \\end{eqnarray} These building blocks, i.e. \"nodes\" or \"neurons\" of the neural network, are arranged in layers, with the layer denoted by superscript square brackets, e.g. \\([l]\\) for the \\(l\\) th layer. \\(n_l\\) denotes the number of neurons in layer \\(l\\) . Forward propagation Forward propagation is the computation of the multiple linear and nonlinear transformations of the neural network on the input data. We can rewrite the above equations in vectorized form to handle multiple training examples and neurons per layer as \\begin{eqnarray} \\tag{1} \\label{1} A&#94;{[l]} = g(Z&#94;{[l]}) \\end{eqnarray} with a linear function acting on a (multidimensional) input, \\(A\\) . \\begin{eqnarray} \\tag{2} \\label{2} Z&#94;{[l]} = W&#94;{[l]} A&#94;{[l-1]} + b&#94;{[l]} \\end{eqnarray} The outputs or activations, \\(A&#94;{[l-1]}\\) , of the previous layer serve as inputs for the linear functions, \\(z&#94;{[l]}\\) . If \\(n_l\\) denotes the number of neurons in layer \\(l\\) , and \\(m\\) denotes the number of training examples in one (mini)batch pass through the neural network, then the dimensions of these matrices are: Variable Dimensions \\(A&#94;{[l]}\\) ( \\(n_l\\) , \\(m\\) ) \\(Z&#94;{[l]}\\) ( \\(n_l\\) , \\(m\\) ) \\(W&#94;{[l]}\\) ( \\(n_l\\) , \\(n_{l-1}\\) ) \\(b&#94;{[l]}\\) ( \\(n_l\\) , 1) For example, this neural network consists of only a single hidden layer with 3 neurons in layer 1. The matrix \\(W&#94;{[1]}\\) has dimensions (3, 2) because there are 3 neurons in layer 1 and 2 inputs from the previous layer (in this example, the inputs are the raw data, \\(\\vec{x} = (x_1, x_2)\\) ). Each row of \\(W&#94;{[1]}\\) corresponds to a vector of weights for a neuron in layer 1. The final output of the neural network is a prediction in the last layer \\(L\\) , and the closeness of the prediction \\(A&#94;{[L](i)}\\) to the true label \\(y&#94;{(i)}\\) for training example \\(i\\) is quantified by a loss function \\(\\mathcal{L}(y&#94;{(i)}, A&#94;{[L](i)})\\) , where superscript \\((i)\\) denotes the \\(i\\) th training example. For classification, the typical choice for \\(\\mathcal{L}\\) is the cross-entropy loss (log loss). The cost \\(J\\) is the average loss over all \\(m\\) training examples in the dataset. \\begin{eqnarray} \\tag{3} \\label{3} J = \\frac{1}{m} \\sum_{i=1}&#94;m \\mathcal{L}(y&#94;{(i)}, A&#94;{[L](i)}) \\end{eqnarray} Minimizing the cost with gradient descent The task of training a neural network is to find the set of parameters \\(W\\) and \\(b\\) (with different \\(W\\) and \\(b\\) for different nodes in the network) that will give us the best predictions, i.e. minimize the cost (\\ref{3}). Gradient descent is the workhorse that we employ for this optimization problem. We randomly initialize the parameters \\(W\\) and \\(b\\) for each node, then iteratively update the parameters by moving them in the direction that is opposite to the gradient of the cost. \\begin{eqnarray} \\nonumber W_\\text{new} &=& W_\\text{previous} - \\alpha \\frac{\\partial J}{\\partial W} \\\\ b_\\text{new} &=& b_\\text{previous} - \\alpha \\frac{\\partial J}{\\partial b} \\end{eqnarray} \\(\\alpha\\) is the learning rate, a hyperparameter that needs to be tuned during the training process. The gradient of the cost is calculated by the backpropagation algorithm. Backpropagation equations These are the vectorized backpropagation ( BP ) equations which we wish to derive: \\begin{eqnarray} \\nonumber dW&#94;{[l]} &\\equiv& \\frac{\\partial J}{\\partial W&#94;{[l]}} = \\frac{1}{m} dZ&#94;{[l]}A&#94;{[l-1]T} \\tag{BP1} \\label{BP1} \\\\ db&#94;{[l]} &\\equiv& \\frac{\\partial J}{\\partial b&#94;{[l]}} = \\frac{1}{m} \\sum_{i=1}&#94;m dZ&#94;{[l](i)} \\tag{BP2} \\label{BP2} \\\\ dA&#94;{[l-1]} &\\equiv& \\frac{\\partial \\mathcal{L}}{\\partial A&#94;{[l-1]}} = W&#94;{[l]T}dZ&#94;{[l]} \\tag{BP3} \\label{BP3} \\\\ dZ&#94;{[l]} &\\equiv& \\frac{\\partial \\mathcal{L}}{\\partial Z&#94;{[l]}} = dA&#94;{[l]} * g'(Z&#94;{[l]}) \\tag{BP4} \\label{BP4} \\end{eqnarray} The \\(*\\) in the last line denotes element-wise multiplication. \\(W\\) and \\(b\\) are the parameters we want to learn (update), but the BP equations include two additional expressions for the partial derivative of the loss in terms of linear and nonlinear activations per training example since they are intermediate terms that appear in the calculation of \\(dW\\) and \\(db\\) . Chain rule We'll need the chain rule for total derivatives , which describes how the change in a function \\(f\\) with respect to a variable \\(x\\) can be calculated as a sum over the contributions from intermediate functions \\(u_i\\) that depend on \\(x\\) : \\begin{eqnarray} \\nonumber \\frac{\\partial f(u_1, u_2, ..., u_k)}{\\partial x} = \\sum_{i}&#94;k \\frac{\\partial f}{\\partial u_i} \\frac{\\partial u_i}{\\partial x} \\end{eqnarray} where the \\(u_i\\) are functions of \\(x\\) . This expression reduces to the single variable chain rule when only one \\(u_i\\) is a function of \\(x\\) . The gradients for every node can be calculated in a single backward pass through the network, starting with the last layer and working backwards, towards the input layer. As we work backwards, we cache the values of \\(dZ\\) and \\(dA\\) from previous calculations, which are then used to compute the derivative for variables that are further upstream in the computation graph. The dependency of the derivatives of upstream variables on downstream variables, i.e. cached derivatives, is manifested in the \\(\\frac{\\partial f}{\\partial u_i}\\) term in the chain rule. (Backpropagation is a dynamic programming algorithm!) The chain rule applied to backpropagation In this section, we apply the chain rule to derive the vectorized form of equations BP (1-4). Without loss of generality, we'll index an element of the matrix or vector on the left hand side of BP (1-4); the notation for applying the chain rule is therefore straightforward because the derivatives are just with respect to scalars. BP1 The partial derivative of the cost with respect to the \\(s\\) th component (corresponding to the \\(s\\) th input) of \\(\\vec{w}\\) in the \\(r\\) th node in layer \\(l\\) is: \\begin{eqnarray} dW&#94;{[l]}_{rs} &\\equiv& \\frac{\\partial J}{\\partial W&#94;{[l]}_{rs}} \\\\ &=& \\frac{1}{m} \\sum_{i}&#94;m \\frac{\\partial \\mathcal{L}}{\\partial W&#94;{[l]}_{rs}} \\\\ &=& \\frac{1}{m} \\sum_{i}&#94;m \\frac{\\partial \\mathcal{L}}{\\partial z&#94;{[l]}_{ri}} \\frac{\\partial z&#94;{[l]}_{ri}}{\\partial W&#94;{[l]}_{rs}} \\tag{4} \\label{4} \\end{eqnarray} The last line is due to the chain rule. The first term in (\\ref{4}) is \\(dZ&#94;{[l]}_{ri}\\) by definition (\\ref{ BP4 }). We can simplify the second term of (\\ref{4}) using the definition of the linear function (\\ref{2}), which we rewrite below explicitly for the \\(i\\) th training example in the \\(r\\) th node in the \\(l\\) th layer in order to be able to more easily keep track of indices when we take derivatives of the linear function: \\begin{eqnarray} \\tag{5} \\label{5} Z&#94;{[l]}_{ri} = \\sum_j&#94;{n_{l-1}} W&#94;{[l]}_{rj} A&#94;{[l-1]}_{ji} + b&#94;{[l]}_r \\end{eqnarray} where \\(n_{l-1}\\) denotes the number of nodes in layer \\(l-1\\) . Therefore, \\begin{eqnarray} dW&#94;{[l]}_{rs} &=& \\frac{1}{m} \\sum_{i}&#94;m dZ&#94;{[l]}_{ri} A&#94;{[l-1]}_{si} \\\\ &=& \\frac{1}{m} \\sum_{i}&#94;m dZ&#94;{[l]}_{ri} A&#94;{[l-1]T}_{is} \\\\ &=& \\frac{1}{m} \\left( dZ&#94;{[l]} A&#94;{[l-1]T} \\right)_{rs} \\end{eqnarray} BP2 The partial derivative of the cost with respect to \\(b\\) in the \\(r\\) th node in layer \\(l\\) is: \\begin{eqnarray} db&#94;{[l]}_r &\\equiv& \\frac{\\partial J}{\\partial b&#94;{[l]}_r} \\\\ &=& \\frac{1}{m} \\sum_{i}&#94;m \\frac{\\partial \\mathcal{L}}{\\partial b&#94;{[l]}_r} \\\\ &=& \\frac{1}{m} \\sum_{i}&#94;m \\frac{\\partial \\mathcal{L}}{\\partial z&#94;{[l]}_{ri}} \\frac{\\partial z&#94;{[l]}_{ri}}{\\partial b&#94;{[l]}_r} \\tag{6} \\label{6} \\\\ &=& \\frac{1}{m} \\sum_{i}&#94;m dZ&#94;{[l]}_{ri} \\end{eqnarray} (\\ref{6}) is due to the chain rule. The first term in (\\ref{6}) is \\(dZ&#94;{[l]}_{ri}\\) by definition (\\ref{ BP4 }). The second term of (\\ref{6}) simplifies to \\(\\partial z&#94;{[l]}_{ri} / \\partial b&#94;{[l]}_r = 1\\) from (\\ref{5}). BP3 The partial derivative of the loss for the \\(i\\) th example with respect to the nonlinear activation in the \\(r\\) th node in layer \\(l-1\\) is: \\begin{eqnarray} dA&#94;{[l-1]}_{ri} &\\equiv& \\frac{\\partial \\mathcal{L}}{\\partial A&#94;{[l-1]}_{ri}} \\\\ &=& \\sum_{k=1}&#94;{n_l} \\frac{\\partial \\mathcal{L}}{\\partial Z&#94;{[l]}_{ki}} \\frac{\\partial Z&#94;{[l]}_{ki}}{\\partial A&#94;{[l-1]}_{ri}} \\tag{7} \\label{7} \\\\ &=& \\sum_{k=1}&#94;{n_l} dZ&#94;{[l]}_{ki} W&#94;{[l]}_{kr} \\tag{8} \\label{8} \\\\ &=& \\sum_{k=1}&#94;{n_l} W&#94;{[l]T}_{rk} dZ&#94;{[l]}_{ki} \\\\ &=& \\left( W&#94;{[l]T} dZ&#94;{[l]} \\right)_{ri} \\end{eqnarray} The application of the chain rule (\\ref{7}) includes a sum over the nodes in layer \\(l\\) whose linear functions take \\(A&#94;{[l-1]}_{ri}\\) as an input, assuming the nodes between layers \\(l-1\\) and \\(l\\) are fully-connected. The first term in (\\ref{8}) is by definition \\(dZ\\) (\\ref{ BP4 }); from (\\ref{5}), the second term in (\\ref{8}) evaluates to \\(\\partial Z&#94;{[l]}_{ki} / \\partial A&#94;{[l-1]}_{ri} = W&#94;{[l]}_{kr}\\) . BP4 The partial derivative of the loss for the \\(i\\) th example with respect to the linear activation in the \\(r\\) th node in layer \\(l\\) is: \\begin{eqnarray} dZ&#94;{[l]}_{ri} &\\equiv& \\frac{\\partial \\mathcal{L}}{\\partial Z&#94;{[l]}_{ri}} \\\\ &=& \\frac{\\partial \\mathcal{L}}{\\partial A&#94;{[l]}_{ri}} \\frac{\\partial A&#94;{[l]}_{ri}}{\\partial Z&#94;{[l]}_{ri}} \\\\ &=& dA&#94;{[l]}_{ri} * g'(Z&#94;{[l]}_{ri}) \\end{eqnarray} The second line is by the application of the chain rule (single variable since only a single nonlinear activation depends on directly on \\(Z&#94;{[l]}_{ri}\\) ). \\(g'(Z)\\) is the derivative of the nonlinear activation function with respect to its input, which depends on the nonlinear activation function that is assigned to that particular node, e.g. sigmoid vs. tanh vs. ReLU. Conclusion Backpropagation efficiently executes gradient descent for updating the parameters of a neural network by ordering and caching the calculations of the gradient of the cost with respect to the parameters in the nodes. This post is a little heavy on notation since the focus is on deriving the vectorized formulas for backpropagation, but we hope it complements the lectures in Week 3 of Andrew Ng's \"Neural Networks and Deep Learning\" course as well as the excellent, but even more notation-heavy, resources on matrix calculus for backpropagation that are linked below. More resources on vectorized backpropagation The matrix calculus you need for deep learning - from explained.ai How the backpropagation algorithm works - Chapter 2 of the Neural Networks and Deep Learning free online text if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Theory","url":"https://efavdb.com/backpropagation-in-neural-networks","loc":"https://efavdb.com/backpropagation-in-neural-networks"},{"title":"An orientational integral","text":"We evaluate an integral having to do with vector averages over all orientations in an n-dimensional space. Problem definition Let \\(\\hat{v}\\) be a unit vector in \\(n\\) -dimensions and consider the orientation average of \\begin{eqnarray} \\tag{1} \\label{1} J \\equiv \\langle \\hat{v} \\cdot \\vec{a}_1 \\hat{v} \\cdot \\vec{a}_2 \\ldots \\hat{v} \\cdot \\vec{a}_k \\rangle \\end{eqnarray} where \\(\\vec{a}_1, \\ldots, \\vec{a}_k\\) are some given fixed vectors. For example, if all \\(\\vec{a}_i\\) are equal to \\(\\hat{x}\\) , we want the orientation average of \\(v_x&#94;k\\) . Solution We'll evaluate our integral using parameter differentiation of the multivariate Gaussian integral. Let \\begin{eqnarray} \\nonumber I &=& \\frac{1}{(2 \\pi)&#94;{n/2}} \\int e&#94;{- \\frac{\\vert \\vec{v} \\vert&#94;2}{2} + \\sum_{i=1}&#94;k \\alpha_i \\vec{v} \\cdot \\vec{a}_i} d&#94;nv \\\\ \\tag{2} \\label{2} &=& \\exp \\left [- \\frac{1}{2} \\vert \\sum_{i=1}&#94;k \\alpha_i \\vec{a}_i \\vert&#94;2 \\right] \\end{eqnarray} The expression in the second line follows from completing the square in the exponent in the first — for review, see our post on the normal distribution, here . Now, we consider a particular derivative of \\(I\\) with respect to the \\(\\alpha\\) parameters. From the first line of (\\ref{2}), we have \\begin{eqnarray} \\tag{3} \\label{3} \\partial_{\\alpha_1}\\ldots \\partial_{\\alpha_k}I \\vert_{\\vec{\\alpha}=0} &=& \\frac{1}{(2 \\pi)&#94;{n/2}} \\int e&#94;{- \\frac{\\vert \\vec{v} \\vert&#94;2}{2}} \\prod_{i=1}&#94;k \\vec{v} \\cdot \\vec{a}_i d&#94;n v \\\\ &\\equiv & \\frac{1}{(2 \\pi)&#94;{n/2}} \\int_0&#94;{\\infty} e&#94;{- \\frac{\\vert \\vec{v} \\vert&#94;2}{2}} v&#94;{n + k -1} dv \\int \\prod_{i=1}&#94;k \\hat{v} \\cdot \\vec{a}_i d \\Omega_v \\\\ &=& \\frac{2&#94;{k/2 - 1}}{\\pi&#94;{n/2}} \\Gamma(\\frac{n+k}{2}) \\times \\int \\prod_{i=1}&#94;k \\hat{v} \\cdot \\vec{a}_i d \\Omega_v \\end{eqnarray} The second factor above is almost our desired orientation average \\(J\\) — the only thing it's missing is the normalization, which we can get by evaluating this integral without any \\(\\vec{a}\\) ‘ s. Next, we evaluate the parameter derivative considered above in a second way, using the second line of (\\ref{2}). This gives, \\begin{eqnarray} \\tag{4} \\label{4} \\partial_{\\alpha_1}\\ldots \\partial_{\\alpha_k}I \\vert_{\\vec{\\alpha}=0} &=& \\partial_{\\alpha_1}\\ldots \\partial_{\\alpha_k} \\exp \\left [- \\frac{1}{2} \\vert \\sum_{i=1}&#94;k \\alpha_i \\vec{a}_i \\vert&#94;2 \\right] \\vert_{\\vec{\\alpha}=0} \\\\ &=& \\sum_{\\text{pairings}} (\\vec{a}_{i_1} \\cdot \\vec{a}_{i_2}) (\\vec{a}_{i_3} \\cdot \\vec{a}_{i_4})\\ldots (\\vec{a}_{i_{k-1}} \\cdot \\vec{a}_{i_k}) \\end{eqnarray} The sum here is over all possible, unique pairings of the indices. You can see this is correct by carrying out the differentiation one parameter at a time. To complete the calculation, we equate (\\ref{3}) and (\\ref{4}). This gives \\begin{eqnarray} \\tag{5}\\label{5} \\int \\prod_{i=1}&#94;k \\hat{v} \\cdot \\vec{a}_i d \\Omega_v = \\frac{\\pi&#94;{n/2}} {2&#94;{k/2 - 1}\\Gamma(\\frac{n+k}{2})}\\sum_{\\text{pairings}} (\\vec{a}_{i_1} \\cdot \\vec{a}_{i_2}) (\\vec{a}_{i_3} \\cdot \\vec{a}_{i_4})\\ldots (\\vec{a}_{i_{k-1}} \\cdot \\vec{a}_{i_k}) \\end{eqnarray} Again, to get the desired average, we need to divide the above by the normalization factor. This is given by the value of the integral (\\ref{5}) when \\(k = 0\\) . This gives, \\begin{eqnarray}\\tag{6}\\label{6} J = \\frac{1}{2&#94;{k/2}}\\frac{\\Gamma(n/2)}{\\Gamma(\\frac{n+k}{2})} \\sum_{\\text{pairings}} (\\vec{a}_{i_1} \\cdot \\vec{a}_{i_2}) (\\vec{a}_{i_3} \\cdot \\vec{a}_{i_4})\\ldots (\\vec{a}_{i_{k-1}} \\cdot \\vec{a}_{i_k}) \\end{eqnarray} Example Consider the case where \\(k=2\\) and \\(\\vec{a}_1 = \\vec{a}_2 = \\hat{x}\\) . In this case, we note that the average of \\(\\hat{v}_x&#94;2\\) is equal to the average along any other orientation. This means we have \\begin{eqnarray}\\nonumber \\tag{7} \\label{7} \\langle \\hat{v}_x&#94;2 \\rangle &=& \\frac{1}{n} \\sum_{i=1}&#94;n \\langle \\hat{v}_x&#94;2 + \\hat{v}_y&#94;2 + \\ldots \\rangle \\\\ &=& \\frac{1}{n} \\end{eqnarray} We get this same result from our more general formula: Plugging in \\(k=2\\) and \\(\\vec{a}_1 = \\vec{a}_2 = \\hat{x}\\) into (\\ref{6}), we obtain \\begin{eqnarray}\\nonumber \\tag{8} \\label{8} \\langle \\hat{v}_x&#94;2 \\rangle &=& \\frac{1}{2}\\frac{\\Gamma(n/2)}{\\Gamma(\\frac{n}{2} + 1)} \\\\ &=& \\frac{1}{n} \\end{eqnarray} The two results agree. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Theory","url":"https://efavdb.com/an-orientational-integral","loc":"https://efavdb.com/an-orientational-integral"},{"title":"Compounding benefits of tax protected accounts","text":"Here, we highlight one of the most important benefits of tax protected accounts (eg Traditional and Roth IRAs and 401ks). Specifically, we review the fact that not having to pay taxes on any investment growth that occurs while the money is held in the account results in compounding / exponential growth with a larger exponent than would be obtained in a traditional account. The growth equations Here, we consider three types of investment account: A standard bank account without tax protection, a traditional tax protected account, and a Roth tax protected account. We'll consider an idealized situation where we earn regular income of \\(D_0&#94;{\\prime}\\) at time \\(0\\) and then place this wealth (taxed, as appropriate for each case) into an investment that always returns a growth factor of \\(g\\) . For simplicity, we'll assume that our tax rate never changes and is given by \\(t\\) . In the next three sections, we calculate expressions for the final wealth at time \\(T\\) that results from each account. Following that, we compare the results. Standard account In the standard account, the initial income must be taxed before it can be invested. Again, we define \\(t\\) as the tax rate per year, so that the money left after tax at the start is \\begin{eqnarray} \\tag{1} \\label{1} D_0 = D_0&#94;{\\prime} (1 - t). \\end{eqnarray} We place this money into an idealized investment that always returns a growth of \\(g\\) . Therefore, after one year, the net wealth before tax is \\begin{eqnarray}\\tag{2} \\label{2} D_1&#94;{\\prime} = D_0 (1 + g). \\end{eqnarray} The portion \\(D_0 g\\) is new income that must be taxed, so after tax we have \\begin{eqnarray}\\tag{3} \\label{3} D_1 = D_0 + D_0 g (1 - t) = D_0[1 + g(1-t)]. \\end{eqnarray} If we iterate this expression up to time \\(T\\) , we obtain \\begin{eqnarray}\\nonumber D_T &=& D_0[1 + g(1-t)]&#94;T \\ &\\equiv & D_0&#94;{\\prime} (1 - t)[1 + g(1-t)]&#94;T \\tag{4} \\label{4} \\end{eqnarray} This is our equation for the final, post-tax wealth obtained from the standard account. Traditional tax protected account In the traditional account, we do not need to pay tax at time \\(0\\) on our initial \\(D_0&#94;{\\prime}\\) dollars. Instead, this wealth is immediately put into our growth investment for \\(T\\) years. This gives a pretax wealth at time \\(T\\) of \\begin{eqnarray}\\tag{5} \\label{5} D_T&#94;{\\prime} = D_0 [1 + g]&#94;T. \\end{eqnarray} However, when this money is taken out at time \\(T\\) it must be taxed. This gives \\begin{eqnarray}\\tag{6} \\label{6} D_T = D_0&#94;{\\prime} (1-t) [1 + g]&#94;T. \\end{eqnarray} This is the equation that describes the net wealth generated by the traditional tax protected account. Roth tax protected account In the Roth account, we do pay taxes on the initial \\(D_0&#94;{\\prime}\\) at time \\(0\\) . However, once this is done, we never need to pay taxes again, even when taking the money out at expiration. Therefore, the net wealth at time \\(T\\) is \\begin{eqnarray}\\tag{7} \\label{7} D_T = D_0&#94;{\\prime} (1-t) (1 + g)&#94;T \\end{eqnarray} Notice that this expression is identical to that for the traditional tax protected account. Comparison Now that we have derived expressions for the final wealth in the three types of account, we can easily compare them. First, note that (\\ref{4}), (\\ref{6}), and (\\ref{7}) all share the common factor of \\(D_0&#94;{\\prime} (1-t) \\equiv D_0\\) , which can be considered the initial post-tax wealth. This means that the only difference between the standard and tax protected accounts is the effective growth rate: The growth rate term for the standard account is \\begin{eqnarray} \\text{growth factor (standard account)} = [1 + g(1-t)]&#94;T \\tag{8} \\label{8} \\end{eqnarray} while that for the two tax protected accounts is \\begin{eqnarray} \\text{growth factor (tax protected)}=[1 + g]&#94;T \\tag{9}\\label{9} \\end{eqnarray} These two factors may look similar, but they represent exponential growth with different exponents. Consequently, for large \\(T\\) , the growth from (\\ref{9}) can be much larger than that from (\\ref{8}). To illustrate this point, we tabulate the two functions assuming \\(7\\) percent growth for \\(30\\) years at a few representative tax rates below. Notice that the growth rates are similar when that tax rates are lower — which makes sense because taxation does not have much of an effect in this limit. However, the tax protected account has a much larger value in the opposite limit — 7.61 vs 2.81 for the standard account! def standard ( T , g , t ): return ( 1 + g * ( 1 - t )) ** T def tax_protected ( T , g , t ): return ( 1 + g ) ** T taxes = [ 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ] standard_values = [ standard ( 30 , 0.07 , t ) for t in taxes ] protected_values = [ tax_protected ( 30 , 0.07 , t ) for t in taxes ] # output: TAX RATE : 0.1 , 0.2 , 0.3 , 0.4 , 0.5 STANDARD : 6.25 , 5.13 , 4.20 , 3.44 , 2.81 PROTECTED : 7.61 , 7.61 , 7.61 , 7.61 , 7.61 Final comments We've considered an idealized situation here in order to highlight the important point that tax protected accounts enjoy much larger compounding / exponential growth rates than do standard accounts. This can have a very big effect when taxation is high. However, it's important to point out that there are other important characteristics not highlighted by our simplified model system. One important case is that the benefits of the traditional and roth accounts can differ if one's tax rate changes over time. If interested, you should look into this elsewhere. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Finance","url":"https://efavdb.com/compounding-benefits-of-tax-protected-accounts","loc":"https://efavdb.com/compounding-benefits-of-tax-protected-accounts"},{"title":"Utility functions and immigration","text":"We consider how the GDP or utility output of a city depends on the number of people living within it. From this, we derive some interesting consequences that can inform both government and individual attitudes towards newcomers. The utility function and benefit per person In this post, we will consider an idealized town whose net output \\(U\\) (the GDP ) scales as a power law with the number of people \\(N\\) living within it. That is, we'll assume, \\begin{eqnarray} \\tag{1} \\label{1} U(N) = a N&#94;{\\gamma}. \\end{eqnarray} We'll assume that the average benefit captured per person is their share of this utility, \\begin{eqnarray} \\tag{2} \\label{2} BPP(N) = U(N) / N = a N&#94;{\\gamma -1}. \\end{eqnarray} What can we say about the above \\(a\\) and \\(\\gamma\\) ? Well, we must have \\(a> 0\\) if the society is productive. Further, because we know that cities allow for more complex economies as the number of occupants grow, we must have \\(\\gamma > 1\\) . These are the only assumptions we will make here. Below, we'll see that these assumptions imply some interesting consequences. Marginal benefits When a new person immigrates to a city, its \\(N\\) value goes up by one. Here, we consider how the utility and benefit per person changes when this occurs. The increase in net utility is simply \\begin{eqnarray}\\tag{3} \\label{3} \\partial_N U(N) = a \\gamma N&#94;{\\gamma -1}. \\end{eqnarray} Notice that because we have \\(\\gamma > 1\\) , (\\ref{3}) is a function that increases with \\(N\\) . That is, cities with larger populations benefit more (as a collective) per immigrant newcomer than those cities with smaller \\(N\\) would. This implies that the governments of large cities should be more enthusiastic about welcoming of newcomers than those of smaller cities. Now consider the marginal benefit per person when one new person moves to this city. This is simply \\begin{eqnarray}\\tag{4} \\label{4} \\partial_N BPP(N) = a (\\gamma - 1) N&#94;{\\gamma -2}. \\end{eqnarray} Notice that this is different from the form (\\ref{3}) that describes the marginal increase in total city utility. In particular, while (\\ref{4}) is positive, it is not necessarily increasing with \\(N\\) : If \\(\\gamma < 2\\) , (\\ref{4}) decreases with \\(N\\) . Cities having \\(\\gamma\\) values like this are such that the net new wealth captured per existing citizen — thanks to each new immigrant — quickly decays to zero. The consequence is that city governments and existing citizens can have a conflict of interest when it comes to immigration. Equilibration In a local population that has freedom of movement, we can expect the migration of people to push the benefit per person to be equal across cities. In cases like this, we should then have \\begin{eqnarray}\\tag{5} \\label{5} a_i N&#94;{\\gamma_i -1} \\approx a_j N&#94;{\\gamma_j -1}, \\end{eqnarray} for each city \\(i\\) and \\(j\\) for which there is low mutual migration costs. We point out that this is not the same result required to maximize the net, global output. This latter score is likely that which an authoritarian government might try to maximize. To maximize net utility, we need to have the marginal utility per city equal across cities, which means \\begin{eqnarray}\\tag{6} \\label{6} \\partial_N U_i(N) = \\partial_N U_j(N) \\end{eqnarray} or, \\begin{eqnarray}\\tag{7} \\label{7} a_i \\gamma_i N&#94;{\\gamma_i -1} = a_j \\gamma_j N&#94;{\\gamma_j -1}. \\end{eqnarray} We see that (\\ref{5}) and (\\ref{7}) differ in that there are \\(\\gamma\\) factors in (\\ref{7}) that are not present in (\\ref{5}). This implies that as long as the \\(\\gamma\\) values differ across cities, there will be a conflict of interest between the migrants and the government. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Case studies","url":"https://efavdb.com/utility-functions-and-immigration","loc":"https://efavdb.com/utility-functions-and-immigration"},{"title":"The speed of traffic","text":"We use a simple argument to estimate the speed of traffic on a highway as a function of the density of cars. The idea is to simply calculate the maximum speed that traffic could go without supporting a growing traffic jam. Jam dissipation argument To estimate the speed of traffic as a function of density, we'll calculate an upper bound and argue that actual traffic speeds must be described by an equation similar to that obtained. To derive our upper bound, we'll consider what happens when a small traffic jam forms. If the speed of cars is such that the rate of exit from the jam is larger than the rate at which new cars enter the jam, then the jam will dissipate. On the other hand, if this doesn't hold, the jam will grow, causing the speed to drop until a speed is obtained that allows the jam to dissipate. This sets the bound. Although we consider a jam to make the argument simple, what we really have in mind is any other sort of modest slow-down that may occur. To begin, we introduce some definitions. (1) Let \\(\\lambda\\) be the density of cars in units of \\([cars / mile]\\) . (2) Next we consider the rate of exit from a jam: Note that when traffic is stopped, a car cannot move until the car in front of it does. Because a human is driving the car, there is a slight delay between the time that one car moves and the car behind it moves. Let \\(T\\) be this delay time in \\([hours]\\) . (3) Let \\(v\\) be the speed of traffic outside the jam in units of \\([miles / hour]\\) . With the above definitions, we now consider the rate at which cars exit a jam. This is the number of cars that can exit the jam per hour, which is simply \\begin{eqnarray} \\tag{1} \\label{1} r_{out} = \\frac{1}{T}. \\end{eqnarray} Next, the rate at which cars enter the jam is given by \\begin{eqnarray} \\tag{2} \\label{2} r_{in} = \\lambda v. \\end{eqnarray} Requiring that \\(r_{out} > r_{in}\\) we get \\begin{eqnarray} \\label{3} \\tag{3} v < \\frac{1}{\\lambda T}. \\end{eqnarray} This is our bound and estimate for the speed of traffic. We note that this form for \\(v\\) follows from dimensional analysis, so the actual rate of traffic must have the same algebraic form as our upper bound (\\ref{3}) — it can differ by a constant factor in front, but should have the same \\(\\lambda\\) and \\(T\\) dependence. Plugging in numbers I estimate \\(T\\) , the delay time between car movements to be about one second, which in hours is \\begin{eqnarray} \\tag{4} \\label{4} T \\approx 0.00028\\ [hour]. \\end{eqnarray} Next for \\(\\lambda\\) , note that a typical car is about 10 feet long and a mile is around 5000 feet, so the maximum for \\(\\lambda\\) is around \\( \\lambda \\lesssim 500 [cars / mile]\\) . Consider a case where there is a car every 10 car lengths or so. In this case, the density will go down from the maximum by a factor of 10, or \\begin{eqnarray}\\tag{5} \\label{5} \\lambda \\approx 50 \\ [cars / mile]. \\end{eqnarray} Plugging (\\ref{4}) and (\\ref{5}) into (\\ref{3}), we obtain \\begin{eqnarray} \\tag{6} v \\lesssim \\frac{1}{0.00028 * 50} \\approx 70\\ [mile / hour], \\end{eqnarray} quite close to our typical highway traffic speeds (and speed limits). Final comments The above bound clearly depends on what values you plug in — I picked numbers that seemed reasonable, but admit I adjusted them a bit till I got the final number I wanted for \\(v\\) . Anecdotally, I've found the result to work well at other densities: For example, when traffic is slow on the highway near my house, if I see that there is a car every 5 car lengths, the speed tends to be about \\(30 [miles / hour]\\) — so scaling rule seems to work. The last thing I should note is that wikipedia has an article outlining some of the extensive research literature that's been done on traffic flows — you can see that here . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Case studies","url":"https://efavdb.com/the-speed-of-traffic","loc":"https://efavdb.com/the-speed-of-traffic"},{"title":"Linear compression in python: PCA vs unsupervised feature selection","text":"We illustrate the application of two linear compression algorithms in python: Principal component analysis ( PCA ) and least-squares feature selection. Both can be used to compress a passed array, and they both work by stripping out redundant columns from the array. The two differ in that PCA operates in a particular rotated frame, while the feature selection solution operates directly on the original columns. As we illustrate below, PCA always gives a stronger compression. However, the feature selection solution is often comparably strong, and its output has the benefit of being relatively easy to interpret — a virtue that is important for many applications. We use our python package linselect to carry out efficient feature selection-based compression below — this is available on pypi ( pip install linselect ) and GitHub . Linear compression algorithms To compress a data array having \\(n\\) columns, linear compression algorithms begin by fitting a \\(k\\) -dimensional line, or hyperplane , to the data (with \\(k < n\\) ). Any point in the hyperplane can be uniquely identified using a basis of \\(k\\) components. Marking down each point's projected location in the hyperplane using these components then gives a \\(k\\) -column, compressed representation of the data. This idea is illustrated in Fig. 1 at right, where a line is fit to some two-component data. Projecting the points onto the line and then marking down how far along the line each projected point sits, we obtain a one-column compression. Carrying out this process can be useful if storage space is at a premium or if any operations need to be applied to the array (usually operations will run much faster on the compressed format). Further, compressed data is often easier to interpret and visualize, thanks to its reduced dimension. In this post, we consider two automated linear compression algorithms: principal component analysis ( PCA ) and least-squares unsupervised feature selection. These differ because they are obtained from different hyperplane fitting strategies: The PCA approach is obtained from the \\(k\\) -dimensional hyperplane fit that minimizes the data's total squared-projection error. In general, the independent variables of this fit — i.e., the \\(k\\) components specifying locations in the fit plane — end up being some linear combinations of the original \\(x_i\\) ‘ s. In contrast, the feature selection strategy intelligently picks a subset of the original array columns as predictors and then applies the usual least-squares fit to the others for compression [1]. These approaches are illustrated in the left and right panels of Fig. 2 below. The two fit lines there look very similar, but the encodings returned by these strategies differ qualitatively: The 1-d compression returned by PCA is how far along the \\(PCA_1\\) direction a point sits (this is some linear combination of \\(x_1\\) and \\(x_2\\) — see figure), while the feature selection solution simply returns each point's \\(x_1\\) value. One of our goals here is to explain why this difference can favor the feature selection approach in certain applications. Our post proceeds as follows: In the next section, we consider two representative applications in python: (1) The compression of a data set of tech-sector stock price quotes, and (2) the visualization of some economic summary statistics on the G20 nations. Working through these applications, we are able to familiarize ourselves with the output of the two algorithms, and also through contrast to highlight their relative virtues. The discussion section summarizes what we learn. Finally, a short appendix covers some of the formal mathematics of compression. There, we prove that linear compression-decompression operators are always projections. Fig. 2 . A cartoon illustrating the projection that results when applying PCA (left) and unsupervised feature selection — via linselect (right): The original 2-d big dots are replaced by their small dot, effectively-1-d approximations — a projection. Applications Both data sets explored below are available on our Github, here . Stock prices Loading and compressing the data In this section, we apply our algorithms to a prepared data set of one year's worth of daily percentage price lifts on 50 individual tech stocks [2]. We expect these stocks to each be governed by a common set of market forces, motivating the idea that a substantial compression might be possible. This is true, and the compressed arrays that result may be more efficiently operated on, as noted above. In addition, we'll see below that we can learn something about the full data set by examining the compression outputs. The code below loads our data, smooths it over a running 30 day window (to remove idiosyncratic noise that is not of much interest), prints out the first three rows, compresses the data using our two methods, and then finally prints out the first five PCA components and the top five selected stocks. import pandas as pd import numpy as np from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from linselect import FwdSelect # CONSTANTS KEEP = 5 # compression dimension WINDOW_SIZE = 30 # smoothing window size # LOAD AND SMOOTH THE DATA df = pd . read_csv ( 'stocks.csv' ) df = df . rolling ( WINDOW_SIZE ) . mean () . iloc [ WINDOW_SIZE :] print df . iloc [: 3 ] TICKERS = df . iloc [:, 1 :] . columns . values X = df . iloc [:, 1 :] . values # PCA COMPRESSION s = StandardScalar () pca = PCA ( n_components = KEEP ) pca . fit ( s . fit_transform ( X )) X_compressed_pca = pca . transform ( s . fit_transform ( X )) # FEATURE SELECTION COMPRESSION selector = FwdSelect () selector . fit ( X ) X_compressed_linselect = X [:, selector . ordered_features [: KEEP ]] # PRINT OUT FIRST FIVE PCA COMPONENTs, TOP FIVE STOCKS print pca . components_ [: KEEP ] print TICKERS [ selector . ordered_features ][: KEEP ] The output of the above print statements: # The first three rows of the data frame: date AAPL ADBE ADP ADSK AMAT AMZN \\ 30 2017 - 05 - 31 0.002821 0.002994 0.000248 0.009001 0.006451 0.003237 31 2017 - 06 - 01 0.003035 0.002776 0.000522 0.008790 0.005487 0.003450 32 2017 - 06 - 02 0.003112 0.002964 - 0.000560 0.008573 0.005523 0.003705 ASML ATVI AVGO ... T TSLA TSM \\ 30 0.000755 0.005933 0.003988 ... - 0.001419 0.004500 0.003590 31 0.002174 0.006369 0.003225 ... - 0.001125 0.003852 0.004279 32 0.001566 0.006014 0.005343 ... - 0.001216 0.004130 0.004358 TWTR TXN VMW VZ WDAY WDC ZNGA 30 0.008292 0.001467 0.001984 - 0.001741 0.006103 0.002916 0.007811 31 0.008443 0.001164 0.002026 - 0.001644 0.006303 0.003510 0.008379 32 0.007796 0.000637 0.001310 - 0.001333 0.006721 0.002836 0.008844 # PCA top components: [[ 0.10548148 , 0.20601986 , - 0.0126039 , 0.20139121 , ... ], [ - 0.11739195 , 0.02536787 , - 0.2044143 , 0.08462741 , ... ], [ 0.03251305 , 0.10796197 , - 0.00463919 , - 0.17564998 , ... ], [ 0.08678107 , 0.1931497 , - 0.16850867 , 0.16260134 , ... ], [ - 0.0174396 , 0.01174769 , - 0.11617622 , - 0.01036602 , ... ]] # Feature selector output: [ 'WDAY' , 'PYPL' , 'AMZN' , 'LRCX' , 'HPQ' ] Lines 22 and 27 in the first code block above are the two compressed versions of the original data array, line 16. For each row, the first compression stores the amplitude of that date's stock changes along each of the first five PCA components (printed below line 17 of second code block), while the second compression is simply equal to the five columns of the original array corresponding to the stocks picked out by the selector (printed below line 24 of the second code block). Exploring the encodings Working with the compressed arrays obtained above provides some immediate operational benefits: Manipulations of the compressed arrays can be carried out more quickly and they require less memory for storage. Here, we review how valuable insight can also obtained from our compressions — via study of the compression components. First, we consider the PCA components. It turns out that these components are the eigenvectors of the correlation matrix of our data set ( \\(X&#94;T \\cdot X\\) ) — that is, they are the collective, fluctuation modes present in the data set (for those who have studied classical mechanics, you can imagine the system as one where the different stocks are masses that are connected by springs, and these eigenvectors are the modes of the system). Using this fact, one can show that the components evolve in an uncorrelated manner. Further, one can show that projecting the data set down onto the top \\(k\\) modes gives the minimum squared projection error of all possible \\(k\\) -component projections. The first component then describes the largest amplitude fluctuation pattern exhibited in the data. From line 18 above, this is \\([ 0.105, 0.206, -0.012, 0.201, ... ]\\) . These coefficients tell us that when the first stock ( AAPL ) goes up by some amount, the second ( ADBE ) typically goes up by about twice as much (this follows from fact that 0.206 is about twice as big as 0.105), etc. This isn't the full story of course, because each day's movements are a superposition (sum) of the amplitudes along each of PCA components. Including more of these components in a compression allows one to capture more of the detailed correlation patterns exhibited in the data. However, each additional PCA component provides progressively less value as one moves down the ranking — it is this fact that allows a good compression to be obtained using only a minority of these modes. Whereas the PCA components directly encode the collective, correlated fluctuations exhibited in our data, the feature selection solution attempts to identify a minimally-redundant subset of the original array's columns — one that is representative of the full set. This strategy is best understood in the limit where the original columns fall into a set of discreet clusters (in our example, we might expect the businesses operating in a particular sub-sector to fall into a single cluster). In such cases, a good compression is obtained by selecting one representative column from each cluster: Once the representatives are selected, each of the other members of a given cluster can be approximately reconstructed using its selected representative as a predictor. In the above, we see that our automated feature selector has worked well, in that the companies selected (‘ WDAY ', ‘ PYPL ', ‘ AMZN ', ‘ LRCX ', and ‘ HPQ ') each operate in a different part of the tech landscape [3]. In general, we can expect the feature selector to attempt to mimic the PCA approach, in that it will seek columns that fluctuate in a nearly orthogonal manner. However, whereas the PCA components highlight which columns fluctuate together, the feature selector attempts to throw out all but one of the columns that fluctuate together — a sort-of dual approach. Compression strength To decide how many compression components are needed for a given application, one need only consider the variance explained as a function of the compression dimension — this is equal to one minus the average squared error of the projections that result from the compressions (see footnote [4] for a visualization of the error that results from compression here). In the two python packages we're using, one can access these values as follows: >> print np . cumsum ( pca . explained_variance_ratio_ ) [ 0.223 0.367 0.493 0.598 0.696 ] >> print [ var / 50.0 for var in selector . ordered_cods [: KEEP ]] [ 0.169 0.316 0.428 0.530 0.612 ] The printed lines above show that both algorithms capture more than \\(50%\\) of the variance exhibited in the data using only 4 of the 50 stocks. The PCA compressions are stronger in each dimension because PCA is unconstrained — it can use any linear combination of the initial features for compression components, whereas the feature selector is constrained to use a subset of the original features. A plot of the values above across all compression dimensions is shown in Fig. 3 below. Looking at this plot, we see an elbow somewhere between \\(5\\) and \\(10\\) retained components. This implies that our \\(50\\) -dimensional data set mostly lies within a subspace of dimension \\(k \\in (5, 10)\\) . Using any \\(k\\) in that interval will provide a decent compression, and a satisfying large dimensional reduction — a typical result of applying these algorithms to large, raw data sets. Again, this is useful because it allows one to stop tracking redundant columns that offers little incremental value. Fig. 3 . Plots of the compression strength (coefficient of determination or \\(r&#94;2\\) ) for our two compression algorithms versus compression dimension. We see two things: (1) PCA gives a slightly stronger compression at each dimension, and (2) The full data set spans 50 dimensions, but the elbow in the plots suggests the data largely sits in a subspace having dimension between 5 to 10. G20 economic summary stats Loading and compressing the data In this section, we explore economic summary statistics on the 19 individual countries belonging to the G20 [5]. We scraped this data from data.un.org — for example, the link used for the United States can be found here . Our aim here will be to illustrate how compression algorithms can be used to aid in the visualization of a data set: Plotting the rows of a data set allows one to quickly get a sense for the relationship between them (here, the different G20 countries). Because we cannot plot in more than two or three dimensions, compression is a necessary first step in this process. A sample row from our data set is given below — the values for Argentina. GDP growth rate(annual %, const. 2005 prices) 2.40 GDP per capita(current US$) 14564 Economy: Agriculture(% of GVA) 6 Economy: Industry(% of GVA) 27.8 Economy: Services and other activity(% of GVA) 66.2 Employment: Agriculture(% of employed) 2 Employment: Industry(% of employed) 24.8 Employment: Services(% of employed) 73.1 Unemployment(% of labour force) 6.5 CPI: Consumer Price Index(2000=100) 332 Agricultural production index(2004-2006=100) 119 Food production index(2004-2006=100) 119 International trade: Exports(million US$) / GPV 0.091 International trade: Imports(million US$) / GPV 0.088 Balance of payments, current account / GPV -0.025 Labour force participation(female) pop. %) 48.6 Labour force participation(male) pop. %) 74.4 Comparing each of the 19 countries across these 17 fields would be a complicated task. However, by considering a plot like Fig. 3 for this data set, we learned that many of these fields are highly correlated (plot not shown). This means that we can indeed get a reasonable, approximate understanding of the relationship between these economies by compressing down to two dimensions and plotting the result. The code to obtain these compressions follows: from linselect import FwdSelect from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler import pandas as pd # LOADING THE DATA df = pd . read_csv ( 'g20.csv' , index_col = 0 ) X = df . values countries = df . index . values # FEATURE SELECTION selector = FwdSelect () selector . fit ( X ) x1 , y1 = X [:, selector . ordered_features [: 2 ]] . T # PRINCIPAL COMPONENT ANALYSIS pca = PCA () s = StandardScaler () x2 , y2 = pca . fit_transform ( s . fit_transform ( X )) . T [: 2 ] The plots of the \\((x_1, y_1)\\) and \\((x_2, y_2)\\) compressions obtained above are given in Fig. 4. Visualizing and interpreting the compressed data The first thing to note about Fig. 4 is that the geometries of the upper (feature selection) and lower ( PCA ) plots are very similar — the neighbors of each country are the same in the two plots. As we know from our discussion above, the first two PCA components must give a stronger compressed representation of the data than is obtained from the feature selection solution. However, given that similar country relationships are suggested by the two plots, the upper, feature selection view might be preferred. This is because its axes retain their original meaning and are relatively easy to interpret : The y-axis is a measure of the relative scale of international trade within each of the individual economies and the x-axis is a measure of the internal makeup of the economies. Examining the upper, feature selection plot of Fig. 4, a number of interesting insights can be found. One timely observation: International trade exports are a lower percentage of GDP for the US than for any other country considered (for imports, it is third, just after Argentina and Brazil). This observation might be related to the US administration's recent willingness to engage in trading tariff increases with other countries. Nations in the same quadrant include Great Britain (gb), Japan (jp), and Australia (au) — each relatively industrialized and geographically isolated nations. In the opposite limits, we have Germany (de) and India (in). The former is relatively industrial and not isolated, while the latter's economy weights agriculture relatively highly. Summary In this section, we illustrated a general analysis method that allows one to quickly gain insight into a data set: Visual study of the compressed data via a plot. Using this approach, we first found here that the G20 nations are best differentiated economically by considering how important international trade is to their economies and also the makeup of their economies (agricultural or other) — i.e., these are the two features that best explain the full data set of 17 columns that we started with. Plotting the data across these two variables and considering the commonalities of neighboring countries, we were able to identify some natural hypotheses influencing the individual economies. Specifically, geography appears to inform at least one of their key characteristics: more isolated countries often trade less. This is an interesting insight, and one that is quickly arrived at through the compression / plotting strategy. Fig. 4 . Plots of the compressed economic summary statistics on the G20 nations, taken from data.un.org: linselect unsupervised feature selection (upper) and PCA (lower). Discussion In this post, we have seen that carrying out compressions on a data set can provide insight into the original data. By examining the PCA components, we gain access to the collective fluctuations present within the data. The feature selection solution returns a minimal subset of the original features that captures the broad stroke information contained in the original full set — in cases where clusters are present, the minimal set contains a representative from each. Both methods allow one to determine the effective dimension of a given data set — when applied to raw data sets, this is often much lower than the apparent dimension due to heavy redundancy. In general, compressing a data set down into lower dimensions will make the data easier to interpret. We saw in this in the second, G20 economic example above, where a feature set was originally provided that had many columns. Compressing this down into two-dimensions quickly gave us a sense of the relationships between the different economies. The PCA and feature selection solutions gave similar plots there, but the feature selection solution had the extra benefit of providing easily interpreted axes. When one's goal is to use compression for operational efficiency gains, the appropriate dimension can be identified by plotting the variance explained versus compression dimension. Because PCA is unconstrained, it will give a stronger compression at any dimension. However, the feature selection approach has its own operational advantages: Once a representative subset of features has been identified, one can often simply stop tracking the others. Doing this can result in a huge cost savings for large data pipelines. A similar savings is not possible for PCA , because evaluation of the PCA components requires one to first evaluate each of the original feature / column values for a given data point. A similar consideration is also important in some applications: For example, when developing a stock portfolio, transaction costs may make it prohibitively expensive to purchase all of the stocks present in a given sector. By purchasing only a representative subset, a minimal portfolio can be constructed without incurring a substantial transaction cost burden. In summary, the two compression methods we have considered here are very similar, but subtly different. Appreciating these differences allows one to choose the best approach for a given application. Appendix: Compression as projection We can see that the composite linear compression-decompression operator is a projection operator as follows: If \\(X\\) is our data array, the general equations describing compression and decompression are, \\begin{eqnarray} \\label{A1} \\tag{A1} X_{compressed} &=& X \\cdot M_{compression} \\\\ \\label{A2} \\tag{A2} X_{approx} &=& X_{compressed} \\cdot M_{decompression}. \\end{eqnarray} Here, \\(M_{compression}\\) is an \\(n \\times k\\) matrix and \\(M_{decompression}\\) is a \\(k \\times n\\) matrix. The squared error of the approximation is, \\begin{eqnarray} \\Lambda &=& \\sum_{i,j} \\left (X_{ij} - X_{approx, ij}\\right)&#94;2 \\\\ &=& \\sum_j \\Vert X_j - X_{compressed} \\cdot M_{decompression, j} \\Vert&#94;2. \\label{A3} \\tag{A3} \\end{eqnarray} This second line here shows that we can minimize the entire squared error by minimizing each of the column squared errors independently. Further, each of the column level minimizations is equivalent to a least-squares linear regression problem: We treat the column vector \\(M_{compressions, j}\\) as an unknown coefficient vector, and attempt to set these so that the squared error of the fit to \\(X_j\\) — using the columns of \\(X_{compressed}\\) as features — is minimized. We've worked out the least-squares linear fit solution in another post (it's also a well-known result). Plugging this result in, we get the optimal \\(M_{decompression}\\) , \\begin{eqnarray} \\label{A4} M_{decompression}&#94;* &=& \\left ( X_{compressed}&#94;T X_{compressed} \\right)&#94;{-1} X_{compressed}&#94;T X \\tag{A4} \\\\ &=& \\left ( M_{compression}&#94;T X&#94;T X M_{compression} \\right)&#94;{-1} M_{compression}&#94;T X&#94;T X. \\end{eqnarray} To obtain the second line here, we have used (\\ref{A1}), the definition of \\(X_{compressed}\\) . What happens if we try to compress our approximate matrix a second time? Nothing: The matrix product \\(M_{compression} M_{decompression}&#94;*\\) is a projection operator. That is, it satisfies the condition \\begin{eqnarray} (M_{compression} M_{decompression}&#94;*)&#94;2 = M_{compression} M_{decompression}&#94;*. \\label{A5} \\tag{A5} \\end{eqnarray} This result is easy enough to confirm using (\\ref{A4}). What (\\ref{A5}) means geometrically is that our compression operator projects a point in \\(n\\) -dimensional space onto a subspace of dimension \\(k\\) . Once a point sits in this subspace, hitting the point with the composite operator has no effect, as the new point already sits in the projected subspace. This is consistent with our 2-d cartoon depicting the effect of PCA and linselect , above. However, this is also true for general choices of \\(M_{compression}\\) , provided we use the optimal \\(M_{decompression}\\) associated with it. Footnotes [1] For a discussion on how PCA selects its \\(k\\) components, see our prior post on the topic. To identify good feature subsets, linselect uses the stepwise selection strategy. This is described in its readme . Here, we simply use the forward selection approach, but linselect supports fairly general stepwise search protocols. [2] The tickers included are: AAPL , ADBE , ADP , ADSK , AMAT , AMZN , ASML , ATVI , AVGO , BABA , BIDU , CRM , CSCO , CTSH , EA , FB , GOOG , GPRO , HPE , HPQ , IBM , INFY , INTC , INTU , ITW , LRCX , MSFT , NFLX , NOK , NVDA , NXPI , OMC , ORCL , PANW , PYPL , QCOM , SAP , SNAP , SQ , SYMC , T, TSLA , TSM , TWTR , TXN , VMW , VZ , WDAY , WDC , and ZNGA . [3] Workday ( WDAY ) is a SaaS company that offers a product to businesses, Paypal ( PYPL ) is a company that provides payments infrastructure supporting e-commerce, Amazon ( AMZN ) is an e-commerce company, Lam Research ( LRCX ) makes chips, and Hewlett-Packard ( HPQ ) makes computers. Each of these are representatives of a different sub-sector. [4] We can also get a sense of the compression error by plotting the compressed traces for one of the stocks. The plot at right does this for Square inc. The ups and downs of SQ are largely captured by both methods. However, some refined details are lost in the compressions. Similar accuracy levels are seen for each of the other stocks in the full set (not shown here). [5] The missing twentieth member of the G20 is the EU . We don't consider the EU here simply because the site we scraped from does not have a page dedicated to it. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"linselect","url":"https://efavdb.com/unsupervised-feature-selection-in-python-with-linselect","loc":"https://efavdb.com/unsupervised-feature-selection-in-python-with-linselect"},{"title":"linselect demo: a tech sector stock analysis","text":"This is a tutorial post relating to our python feature selection package, linselect . The package allows one to easily identify minimal, informative feature subsets within a given data set. Here, we demonstrate linselect ‘ s basic API by exploring the relationship between the daily percentage lifts of 50 tech stocks over one trading year. We will be interested in identifying minimal stock subsets that can be used to predict the lifts of the others. This is a demonstration walkthrough, with commentary and interpretation throughout. See the package docs folder for docstrings that succinctly detail the API . Contents: Load the data and examine some stock traces FwdSelect, RevSelect; supervised, single target FwdSelect, RevSelect; supervised, multiple targets FwdSelect, RevSelect; unsupervised GenSelect The data and a Jupyter notebook containing the code for this demo are available on our github, here . The linselect package can be found on our github, here . 1 - Load the data and examine some stock traces In this tutorial, we will explore using linselect to carry out various feature selection tasks on a prepared data set of daily percentage lifts for 50 of the largest tech stocks. This covers data from 2017-04-18 to 2018-04-13. In this section, we load the data and take a look at a couple of the stock traces that we will be studying. Load data The code snippet below loads the data and shows a small sample. # load packages from linselect import FwdSelect , RevSelect , GenSelect import matplotlib.pyplot as plt import numpy as np import pandas as pd # load the data, print out a sample df = pd . read_csv ( 'stocks.csv' ) print df . iloc [: 3 , : 5 ] print df . shape # date AAPL ADBE ADP ADSK # 0 2017-04-18 -0.004442 -0.001385 0.000687 0.004884 # 1 2017-04-19 -0.003683 0.003158 0.001374 0.017591 # 2 2017-04-20 0.012511 0.009215 0.009503 0.005459 # (248, 51) The last line here shows that there were 248 trading days in the range considered. Plot some stock traces The plot below shows Apple's and Google's daily lifts on top of each other, over our full date range (the code for the plot can be found in our notebook). Visually, it's clear that the two are highly correlated — when one goes up or down, the other tends to as well. This suggests that it should be possible to get a good fit to any one of the stocks using the changes in each of the other stocks. In general, a stock's daily price change should be a function of the market at large, the behavior of its market segment(s) and sub-segment(s), and some idiosyncratic behavior special to the company in question. Given this intuition, it seems reasonable to expect one to be able to fit a given stock given the lifts from just a small subset of the other stocks — stocks representative of the sectors relevant to the stock in question. Adding multiple stocks from each segment shouldn't provide much additional value since these should be redundant. We'll confirm this intuition below and use linselect to identify these optimal subsets. Lesson : The fluctuations of related stocks are often highly correlated. Below, we will be using linselect to find minimal subsets of the 50 stocks that we can use to develop good linear fits to one, multiple, or all of the others. 2 - FwdSelect and RevSelect; supervised, single target Goal: Demonstrate how to identify subsets of the stocks that can be used to fit a given target stock well. First we carry out a FwdSelect fit to identify good choices. Next, we compare the FwdSelect and RevSelect results Forward selection applied to AAPL The code snippet below uses our forward selection class, FwdSelect to seek the best feature subsets to fit AAPL 's performance. # Define X, y variables def get_feature_tickers ( targets ): all_tickers = df . iloc [:, 1 :] . columns return list ( c for c in all_tickers if c not in targets ) TARGET_TICKERS = [ 'AAPL' ] FEATURE_TICKERS = get_feature_tickers ( TARGET_TICKERS ) X = df [ FEATURE_TICKERS ] . values y = df [ TARGET_TICKERS ] . values # Forward step-wise selection selector = FwdSelect () selector . fit ( X , y ) # Print out main results of selection process (ordered feature indices, CODs) print selector . ordered_features [: 3 ] print selector . ordered_cods [: 3 ] # [25, 7, 41] # [0.43813848, 0.54534304, 0.58577418] The last two lines above print out the main outputs of FwdSelect : The ordered_features list provides the indices of the features, ranked by the algorithm. The first index shown provides the best possible single feature fit to AAPL , the second index provides the next best addition, etc. Note that we can get the tickers corresponding to these indices using: python print [FEATURE_TICKERS[i] for i in selector.ordered_features[:3]] # ['MSFT' 'AVGO' 'TSM'] A little thought plus a Google search rationalizes why these might be the top three predictors for AAPL : First, Microsoft is probably a good representative of the large-scale tech sector, and second the latter two companies work closely with Apple. AVGO (Qualcomm) made Apple's modem chips until very recently, while TSM (Taiwan semi-conductor) makes the processors for iphones and ipads — and may perhaps soon also provide the CPUs for all Apple computers. Apparently, we can predict APPL performance using only a combination of (a) a read on the tech sector at large, plus (b) a bit of idiosyncratic information also present in APPL 's partner stocks. The ordered_cods list records the coefficient of determination ( COD or R&#94;2) of the fits in question — the first number gives the COD obtained with just MSFT , the second with MSFT and AVGO , etc. A plot of the values in ordered_cods versus feature count is given below. Here, we have labeled the x-axis with the tickers corresponding to the elements of our selector.ordered_features . We see that the top three features almost fit AAPL 's performance as well as the full set! Lesson : We can often use linselect to significantly reduce the dimension of a given feature set, with minimal cost in performance. This can be used to compress a data set and can also improve our understanding of the problem considered. Lesson : To get a feel for the effective number of useful features we have at hand, we can plot the output ordered_cods versus feature count. Compare forward and reverse selection applied to TSLA The code snippet below applies both FwdSelect and RevSelect to seek minimal subsets that fit Tesla's daily lifts well. The outputs are plotted below this. This shows that FwdSelect performs slightly better when two or fewer features are included here, but that RevSelect finds better subsets after that. Lesson : In general, we expect forward selection to work better when looking for small subsets and reverse selection to perform better at large subsets. # Define X, y variables TARGET_TICKERS = [ 'TSLA' ] FEATURE_TICKERS = get_feature_tickers ( TARGET_TICKERS ) X = df [ FEATURE_TICKERS ] . values y = df [ TARGET_TICKERS ] . values # Forward step-wise selection selector = FwdSelect () selector . fit ( X , y ) # Reverse step-wise selection selector2 = RevSelect () selector2 . fit ( X , y ) 3 - FwdSelect and RevSelect; supervised, multiple targets In the code below, we seek feature subsets that perform well when fitting multiple targets simultaneously. Lesson : linselect can be used to find minimal feature subsets useful for fitting multiple targets. The optimal, \"perfect score\" COD in this case is equal to number of targets (three in our example). # Define X, y variables TARGET_TICKERS = [ 'TSLA' , 'ADP' , 'NFLX' ] FEATURE_TICKERS = get_feature_tickers ( TARGET_TICKERS ) X = df [ FEATURE_TICKERS ] . values y = df [ TARGET_TICKERS ] . values # Forward step-wise selection selector = FwdSelect () selector . fit ( X , y ) # Reverse step-wise selection selector2 = RevSelect () selector2 . fit ( X , y ) 4 - FwdSelect and RevSelect; unsupervised Here, we seek those features that give us a best fit to / linear representation of the whole set. This goal is analogous to that addressed by PCA , but is a feature selection variant: Whereas PCA returns a set of linear combinations of the original features, the approach here will return a subset of the original features. This has the benefit of leaving one with a feature subset that is interpretable. (Note: See [1] for more examples like this. There, I show that if you try to fit smoothed versions of the stock performances, very good, small subsets can be found. Without smoothing, noise obscures this point). Lesson : Unsupervised selection seeks to find those features that best describe the full data set — a feature selection analog of PCA . Lesson : Again, a perfect COD score is equal to the number of targets. In the unsupervised case, this is also the number of features (50 in our example). # Set X equal to full data set. ALL_TICKERS = list ( df . iloc [:, 1 :] . columns ) X = df [ ALL_TICKERS ] . values # Stepwise regressions selector = FwdSelect () selector . fit ( X ) selector2 = RevSelect () selector2 . fit ( X ) 5 - GenSelect GenSelect ‘ s API is designed to expose the full flexibility of the efficient linear stepwise algorithm. Because of this, its API is somewhat more complex than that of FwdSelect and RevSelect . Here, our aim is to quickly demo this API . The Essential ingredients: We pass only a single data matrix X , and must specify which columns are the predictors and which are targets. Because we might sweep up and down, we cannot define an ordered_features list as in FwdSelect and RevSelect (the best subset of size three now may not contain the features in the best subset of size two). Instead, GenSelect maintains a dictionary best_results that stores information on the best results seen so far for each possible feature count. The keys of this dictionary correspond to the possible feature set sizes. The values are also dictionaries, each having two keys: s and cod . These specify the best feature subset seen so far with size equal to the outer key, and the corresponding COD , respectively. We can move back and forth, adding features to or removing them from the predictor set. We can specify the search protocol for doing this. We can reposition our search to any predictor set location and continue the search from there. We can access the costs of each possible move from our current location, without stepping. If an \\(m \\times n\\) data matrix X is passed to GenSelect , three Boolean arrays define the state of the search. s — This array specifies which of the columns are currently being used as predictors. targets — This specifies which of the columns are the target variables. mobile — This specifies which of the columns are locked into or out of our fit — those that are not mobile are marked False . Note: We usually want the targets to not be mobile — though this is not the case in unsupervised applications. One might sometimes also want to lock certain features into the predictor set, and the mobile parameter can be used to accomplish this. Use GenSelect to carry out a forward sweep for TSLA The code below carries out a single forward sweep for TSLA . Note that the protocol argument of search is set to (1, 0) , which gives a forward search (see docstrings). For this reason, our results match those of FwdSelect at this point. Lesson : Setting up a basic GenSelect call requires defining a few input parameters. # Define X X = df [ ALL_TICKERS ] . values # Define targets and mobile Boolean arrays TARGET_TICKERS = [ 'TSLA' ] FEATURE_TICKERS = get_feature_tickers ( TARGET_TICKERS ) targets = np . in1d ( ALL_TICKERS , TARGET_TICKERS ) mobile = np . in1d ( ALL_TICKERS , FEATURE_TICKERS ) # Set up search with an initial \\`position\\`. Then search. selector = GenSelect () selector . position ( X , mobile = mobile , targets = targets ) selector . search ( protocol = ( 1 , 0 ), steps = X . shape [ 1 ]) # Review best 3 feature set found print np . array ( ALL_TICKERS )[ selector . best_results [ 3 ][ 's' ]], selector . best_results [ 3 ][ 'cod' ] # ['ATVI' 'AVGO' 'CTSH'] 0.225758 Continue the search above A GenSelect instance always retains a summary of the best results it has seen so far. This means that we can continue a search where we left off after a search call completes. Below, we reposition our search and sweep back and forth to better explore a particular region. Note that this slightly improves our result. Lesson : We can carry out general search protocols using GenSelect ‘ s position and search methods. # Reposition back to the best fit of size 3 seen above. s = selector . best_results [ 3 ][ 's' ] selector . position ( s = s ) # Now sweep back and forth around there a few times. STEPS = 10 SWEEPS = 3 selector . search ( protocol = ( 0 , 1 ), steps = STEPS ) selector . search ( protocol = ( 2 * STEPS , 2 * STEPS ), steps = SWEEPS * 4 * STEPS ) # Review best results found now with exactly N_RETAINED features (different from first pass in cell above?) print np . array ( ALL_TICKERS )[ selector . best_results [ 3 ][ 's' ]], selector . best_results [ 3 ][ 'cod' ] # ['AMZN' 'NVDA' 'ZNGA'] 0.229958 Compare to forward and reverse search results Below, we compare the COD values of our three classes. Lesson : GenSelect can be used to do a more thorough search than FwdSelect and RevSelect , and so can sometimes find better feature subsets. # Get the best COD values seen for each feature set size from GenSelect search gen_select_cods = [] for i in range ( 1 , X . shape [ 1 ]): if i not in selector . best_results : break gen_select_cods . append ( selector . best_results [ i ][ 'cod' ]) # Plot cod versus feature set size. fig , ax = plt . subplots ( figsize = ( 10 , 5 )) plt . plot ( gen_select_cods , label = 'GenSelect' ) # FwdSelect again to get corresponding results. selector2 = FwdSelect () selector2 . fit ( X [:, mobile ], X [:, targets ]) plt . plot ( selector2 . ordered_cods , '--' , label = 'FwdSelect' ) # RevSelect again to get corresponding results. selector3 = RevSelect () selector3 . fit ( X [:, mobile ], X [:, targets ]) plt . plot ( selector3 . ordered_cods , '-.' , label = 'RevSelect' ) plt . title ( 'Coefficient of Determination (COD or R&#94;2) for {target} vs features retained' . format ( target = ', ' . join ( TARGET_TICKERS ))) plt . legend () plt . show () Examine the cost of removing a feature from the predictor set Below, we reposition to the best feature set of size 10 seen so far. We then apply the method reverse_cods to expose the cost of removing each of these individuals from the predictor set at this point. Were we to take a reverse step, the feature with the least cost would be the one taken (looks like FB from the plot). Lesson : We can easily access the costs associated with removing individual features from our current location. We can also access the COD gains associated with adding in new features by calling the forward_cods method. # Reposition s = selector . best_results [ 10 ][ 's' ] selector . position ( s = s ) # Get costs to remove a feature (see also \\`forward_cods\\` method) costs = selector . reverse_cods ()[ s ] TICKERS = np . array ( ALL_TICKERS )[ selector . best_results [ 10 ][ 's' ]] # Plot costs to remove each feature given current position fig , ax = plt . subplots ( figsize = ( 10 , 5 )) plt . plot ( costs ) plt . xticks ( np . arange ( 0 , len ( TICKERS )), rotation = 90 ) ax . set_xticklabels ( TICKERS ) plt . show () Final comments In this tutorial, we've illustrated many of the basic API calls available in linselect . In a future tutorial post, we plan to illustrate some interesting use cases of some of these API calls — e.g., how to use GenSelect ‘ s arguments to explore the value of supplemental features, added to an already existing data set. References [1] J. Landy. Stepwise regression for unsupervised learning, 2017. arxiv.1706.03265 . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"linselect","url":"https://efavdb.com/linselect-demo","loc":"https://efavdb.com/linselect-demo"},{"title":"Making AI Interpretable with Generative Adversarial Networks","text":"It has been quite awhile since I have posted, largely because soon after I started my job at Square I had a child! I hope to have some newer blog post soon. But along those lines I want to share a blog post I did with a coworker ( Juan Hernandez ) for Square that gives a taste of some of the cool data science work we have been up to. This post covers work we did to create a framework for making models interpretable.","tags":"Misc","url":"https://efavdb.com/gans","loc":"https://efavdb.com/gans"},{"title":"Integration method to map model scores to conversion rates from example data","text":"This note addresses the typical applied problem of estimating from data how a target \"conversion rate\" function varies with some available scalar score function — e.g., estimating conversion rates from some marketing campaign as a function of a targeting model score. The idea centers around estimating the integral of the rate function; differentiating this gives the rate function. The method is a variation on a standard technique for estimating pdfs via fits to empirical cdfs. Problem definition and naive binning solution Here, we are interested in estimating a rate function, \\(p \\equiv p(x)\\) , representing the probability of some \"conversion\" event as a function of \\(x\\) , some scalar model score. To do this, we assume we have access to a finite set of score-outcome data of the form \\(\\{(x_i, n_i), i= 1, \\ldots ,k\\}\\) . Here, \\(x_i\\) is the score for example \\(i\\) and \\(n_i \\in \\{0,1\\}\\) is its conversion indicator. There are a number of standard methods for estimating rate functions. For example, if the score \\(x\\) is a prior estimate for the conversion rate, a trivial mapping \\(p(x) = x\\) may work. This won't work if the score function in question is not an estimate for \\(p\\) . A more general approach is to bin together example data points that have similar scores: The observed conversion rate within each bin can then be used as an estimate for the true conversion rate in the bin's score range. An example output of this approach is shown in Fig. 1. Another option is to create a moving average, analogous to the binned solution. The simple binning approach introduces two inefficiencies: (1) Binning coarsens a data set, resulting in a loss of information. (2) The data in one bin does not affect the data in the other bins, precluding exploitation of any global smoothness constraints that could be placed on \\(p\\) as a function of \\(x\\) . The running average approach is also subject to these issues. The method we discuss below alleviates both inefficiencies. Fig. 1. Binned probability estimate approach: All data with scores in a given range are grouped together, and the outcomes from those data points are used to estimate the conversion rate in each bin. Here, the x-axis represents score range, data was grouped into six bins, and mean and standard deviation of the outcome probabilities were estimated from the observed outcomes within each bin. Efficient estimates by integration It can be difficult to directly fit a rate function p(x) using score-outcome data because data of this type does not lie on a continuous curve (the y-values alternate between 0 and 1, depending on the outcome for each example). However, if we consider the empirical integral of the available data, we obtain a smooth, increasing function that is much easier to fit. To evaluate the empirical integral, we assume the samples are first sorted by \\(x\\) and define $$ \\tag{1} \\label{1} \\delta x_i \\equiv x_i - x_{i-1}. $$ Next, the empirical integral is taken as $$ \\tag{2} \\label{2} \\hat{J}(x_j) \\equiv \\sum_{i=0}&#94;{j} n_i \\delta x_i, $$ which approximates the integral $$\\tag{3} \\label{3} J(x) \\equiv \\int_{x_0}&#94;{x_j} p(x) dx. $$ We can think of (\\ref{3}) as the number of expected conversions given density- \\(1\\) sampling over the \\(x\\) range noted. Taking a fit to the \\(\\{(x_i, \\hat{J}(x_i))\\}\\) values gives a smooth estimate for (\\ref{3}). Differentiating with respect to \\(x\\) the gives an estimate for \\(p(x)\\) . Fig. 2 illustrates the approach. Here, I fit the available data to a quadratic, capturing the growth in \\(p\\) with \\(x\\) . The example in Fig. 2 has no error bar shown. One way to obtain error bars would be to work with a particular fit form. The uncertainty in the fit coefficients could then be used to estimate uncertainties in the values at each point. Fig. 2. (Left) A plot of the empirical integral of the data used to generate Fig. 1 is in blue. A quadratic fit is shown in red. (Right) The derivative of the red fit function at left is shown, an estimate for the rate function in question, \\(p\\equiv p(x)\\) . Example python code The code snippet below carries out the procedure described above on a simple example. One example output is shown in Fig. 3 at the bottom of the section. Running the code multiple times gives one a sense of the error that is present in the predictions. In practical applications, this can't be done so carrying out the error analysis procedure suggested above should be done to get a better sense of the error involved. % pylab inline import numpy as np from scipy.optimize import curve_fit def p_given_x ( x ): return x ** 2 def outcome_given_p ( p ): return np . random . binomial ( 1 , p ) # Generate some random data x = np . sort ( np . random . rand ( 200 )) p = p_given_x ( x ) y = outcome_given_p ( p ) # Calculate delta x, get weighted outcomes delta_x = x [ 1 :] - x [: - 1 ] weighted_y = y [: - 1 ] * delta_x # Integrate and fit j = np . cumsum ( weighted_y ) def fit_func ( x , a , b , c , d ): return a * x ** 3 + b * x ** 2 popt , pcov = curve_fit ( fit_func , x [: - 1 ], j ) j_fit = fit_func ( x [: - 1 ], * popt ) # Finally, differentiate and compare to actual p p_fit = ( j_fit [ 1 :] - j_fit [: - 1 ]) / delta_x [: - 1 ] # Plots plt . figure ( figsize = ( 10 , 3 )) plt . subplot ( 1 , 2 , 1 ) plot ( x [: - 1 ], j , '*' , label = 'empirical integral' ) plot ( x [: - 1 ], j_fit , 'r' , label = 'fit to integral' ) plt . legend () plt . subplot ( 1 , 2 , 2 ) plot ( x [: - 2 ], p_fit , 'g' , label = 'fit to p versus x' ) plot ( x , p , 'k--' , label = 'actual p versus x' ) plt . legend () Fig. 3. The result of one run of the algorithm on a data set where \\(p(x) \\equiv x&#94;2\\) , given 200 random samples of \\(x \\in (0, 1)\\) . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods, Statistics, Theory","url":"https://efavdb.com/integration-method-to-map-model-scores-to-conversion-rates-from-example-data","loc":"https://efavdb.com/integration-method-to-map-model-scores-to-conversion-rates-from-example-data"},{"title":"Gaussian Processes","text":"We review the math and code needed to fit a Gaussian Process ( GP ) regressor to data. We conclude with a demo of a popular application, fast function minimization through GP -guided search. The gif below illustrates this approach in action — the red points are samples from the hidden red curve. Using these samples, we attempt to leverage GPs to find the curve's minimum as fast as possible. Appendices contain quick reviews on (i) the GP regressor posterior derivation, (ii) SKLearn's GP implementation, and (iii) GP classifiers. Introduction Gaussian Processes (GPs) provide a tool for treating the following general problem: A function \\(f(x)\\) is sampled at \\(n\\) points, resulting in a set of noisy \\(&#94;1\\) function measurements, \\(\\{f(x_i) = y_i \\pm \\sigma_i, i = 1, \\ldots, n\\}\\) . Given these available samples, can we estimate the probability that \\(f = \\hat{f}\\) , where \\(\\hat{f}\\) is some candidate function? To decompose and isolate the ambiguity associated with the above challenge, we begin by applying Bayes's rule, \\begin{eqnarray} \\label{Bayes} \\tag{1} p(\\hat{f} \\vert \\{y\\}) = \\frac{p(\\{y\\} \\vert \\hat{f} ) p(\\hat{f})}{p(\\{y\\}) }. \\end{eqnarray} The quantity at left above is shorthand for the probability we seek — the probability that \\(f = \\hat{f}\\) , given our knowledge of the sampled function values \\(\\{y\\}\\) . To evaluate this, one can define and then evaluate the quantities at right. Defining the first in the numerator requires some assumption about the source of error in our measurement process. The second function in the numerator is the prior — it is here where the greatest assumptions must be taken. For example, we'll see below that the prior effectively dictates the probability of a given smoothness for the \\(f\\) function in question. In the GP approach, both quantities in the numerator at right above are taken to be multivariate Normals / Gaussians. The specific parameters of this Gaussian can be selected to ensure that the resulting fit is good — but the Normality requirement is essential for the mathematics to work out. Taking this approach, we can write down the posterior analytically, which then allows for some useful applications. For example, we used this approach to obtain the curves shown in the top figure of this post — these were obtained through random sampling from the posterior of a fitted GP , pinned to equal measured values at the two pinched points shown. Posterior samples are useful for visualization and also for taking Monte Carlo averages. In this post, we (i) review the math needed to calculate the posterior above, (ii) discuss numerical evaluations and fit some example data using GPs, and (iii) review how a fitted GP can help to quickly minimize a cost function — eg a machine learning cross-validation score. Appendices cover the derivation of the GP regressor posterior, SKLearn's GP implementation, and GP Classifiers. Our minimal python class SimpleGP used below is available on our GitHub, here . Note: To understand the mathematical details covered in this post, one should be familiar with multivariate normal distributions — these are reviewed in our prior post, here . These details can be skipped by those primarily interested in applications. Analytic evaluation of the posterior To evaluate the left side of (\\ref{Bayes}), we will evaluate the right. Only the terms in the numerator need to be considered, because the denominator does not depend on \\(\\hat{f}\\) . This means that the denominator must equate to a normalization factor, common to all candidate functions. In this section, we will first write down the assumed forms for the two terms in the numerator and then consider the posterior that results. The first assumption that we will make is that if the true function is \\(\\hat{f}\\) , then our \\(y\\) -measurements are independent and Gaussian-distributed about \\(\\hat{f}(x)\\) . This assumption implies that the first term on the right of (\\ref{Bayes}) is \\begin{eqnarray} \\tag{2} \\label{prob} p(\\{y\\} \\vert \\hat{f} ) \\equiv \\prod_{i=1}&#94;n \\frac{1}{\\sqrt{2 \\pi \\sigma_i&#94;2}} \\exp \\left ( - \\frac{(y_i - \\hat{f}(x_i) )&#94;2}{2 \\sigma_i&#94;2} \\right). \\end{eqnarray} The \\(y_i\\) above are the actual measurements made at our sample points, and the \\(\\sigma_i&#94;2\\) are their variance uncertainties. The second thing we must do is assume a form for \\(p(\\hat{f})\\) , our prior. We restrict attention to a set of points \\(\\{x_i: i = 1, \\ldots, N\\}\\) , where the first \\(n\\) points are the points that have been sampled, and the remaining \\((N-n)\\) are test points at other locations — points where we would like to estimate the joint statistics \\(&#94;2\\) of \\(f\\) . To progress, we simply assume a multi-variate Normal distribution for \\(f\\) at these points, governed by a covariance matrix \\(\\Sigma\\) . This gives \\begin{eqnarray} \\label{prior} \\tag{3} &&p(f(x_1), \\ldots, f(x_N) ) \\sim \\ && \\frac{1}{\\sqrt{ (2 \\pi)&#94;{N} \\vert \\Sigma \\vert }} \\exp \\left ( - \\frac{1}{2} \\sum_{ij=1}&#94;N f_i \\Sigma&#94;{-1}_{ij} f_j \\right). \\end{eqnarray} Here, we have introduced the shorthand, \\(f_i \\equiv f(x_i)\\) . Notice that we have implicitly assumed that the mean of our normal distribution is zero above. This is done for simplicity: If a non-zero mean is appropriate, this can be added in to the analysis, or subtracted from the underlying \\(f\\) to obtain a new one with zero mean. The particular form of \\(\\Sigma\\) is where all of the modeler's insight and ingenuity must be placed when working with GPs. Researchers who know their topic very well can assert well-motivated, complex priors — often taking the form of a sum of terms, each capturing some physically-relevant contribution to the statistics of their problem at hand. In this post, we'll assume the simple form \\begin{eqnarray} \\tag{4} \\label{covariance} \\Sigma_{ij} \\equiv \\sigma&#94;2 \\exp \\left( - \\frac{(x_i - x_j)&#94;2}{2 l&#94;2}\\right). \\end{eqnarray} Notice that with this assumed form, if \\(x_i\\) and \\(x_j\\) are close together, the exponential will be nearly equal to one. This ensures that nearby points are highly correlated, forcing all high-probability functions to be smooth. The rate at which (\\ref{covariance}) dies down as two test points move away from each another is controlled by the length-scale parameter \\(l.\\) If this is large (small), the curve will be smooth over a long (short) distance. We illustrate these points in the next section, and also explain how an appropriate length scale can be inferred from the sample data at hand in the section after that. Now, if we combine (\\ref{prob}) and (\\ref{prior}) and plug this into (\\ref{Bayes}), we obtain an expression for the posterior, \\(p(f \\vert \\{y\\})\\) . This function is an exponential whose argument is a quadratic in the \\(f_i\\) . In other words, like the prior, the posterior is a multi-variate normal. With a little work, one can derive explicit expressions for the mean and covariance of this distribution: Using block notation, with \\(0\\) corresponding to the sample points and \\(1\\) to the test points, the marginal distribution at the test points is \\begin{eqnarray} \\tag{5} \\label{posterior} && p(\\textbf{f}_1 \\vert \\{y\\}) =\\ && N\\left ( \\Sigma_{10} \\frac{1}{\\sigma&#94;2 I_{00} + \\Sigma_{00}} \\cdot \\textbf{y}, \\Sigma_{11} - \\Sigma_{10} \\frac{1}{\\sigma&#94;2 I_{00} + \\Sigma_{00}} \\Sigma_{01} \\right). \\end{eqnarray} Here, \\begin{eqnarray} \\tag{6} \\label{sigma_mat} \\sigma&#94;2 I_{00} \\equiv \\left( \\begin{array}{cccc} \\sigma_1&#94;2 & 0 & \\ldots &0 \\\\ 0 & \\sigma_2&#94;2 & \\ldots &0 \\\\ \\ldots & & & \\\\ 0 & 0 & \\ldots & \\sigma_n&#94;2 \\end{array} \\right), \\end{eqnarray} and \\(\\textbf{y}\\) is the length- \\(n\\) vector of measurements, \\begin{eqnarray}\\tag{7} \\label{y_vec} \\textbf{y}&#94;T \\equiv (y_1, \\ldots, y_n). \\end{eqnarray} Equation (\\ref{posterior}) is one of the main results for Gaussian Process regressors — this result is all one needs to evaluate the posterior. Notice that the mean at all points is linear in the sampled values \\(\\textbf{y}\\) and that the variance at each point is reduced near the measured values. Those interested in a careful derivation of this result can consult our appendix — we actually provide two derivations there. However, in the remainder of the body of the post, we will simply explore applications of this formula. Numerical evaluations of the posterior In this section, we will demonstrate how two typical applications of (\\ref{posterior}) can be carried out: (i) Evaluation of the mean and standard deviation of the posterior distribution at a test point \\(x\\) , and (ii) Sampling functions \\(\\hat{f}\\) directly from the posterior. The former is useful in that it can be used to obtain confidence intervals for \\(f\\) at all locations, and the latter is useful both for visualization and also for obtaining general Monte Carlo averages over the posterior. Both concepts are illustrated in the header image for this post: In this picture, we fit a GP to a one-d function that had been measured at two locations. The blue shaded region represents a one-sigma confidence interval for the function value at each location, and the colored curves are posterior samples. The code for our SimpleGP fitter class is available on our GitHub . We'll explain a bit how this works below, but those interested in the details should examine the code — it's a short script and should be largely self-explanatory. Intervals The code snippet below initializes our SimpleGP class, defines some sample locations, values, and uncertainties, then evaluates the mean and standard deviation of the posterior at a set of test points. Briefly, this carried out as follows: The fit method evaluates the inverse matrix \\(\\left [ \\sigma&#94;2 I_{00} + \\Sigma_{00} \\right]&#94;{-1}\\) that appears in (\\ref{posterior}) and saves the result for later use — this allows us to avoid reevaluation of this inverse at each test point. Next, (\\ref{posterior}) is evaluated once for each test point through the call to the interval method. # Initialize fitter -- set covariance parameters WIDTH_SCALE = 1.0 LENGTH_SCALE = 1.0 model = SimpleGP ( WIDTH_SCALE , LENGTH_SCALE , noise = 0 ) # Insert observed sample data here, fit sample_x = [ - 0.5 , 2.5 ] sample_y = [ . 5 , 0 ] sample_s = [ 0.01 , 0.25 ] model . fit ( sample_x , sample_y , sample_s ) # Get the mean and std at each point in x_test test_x = np . arange ( - 5 , 5 , . 05 ) means , stds = model . interval ( test_x ) In the above, WIDTH_SCALE and LENGTH_SCALE are needed to specify the covariance matrix (\\ref{covariance}). The former corresponds to \\(\\sigma\\) and the latter to \\(l\\) in that equation. Increasing WIDTH_SCALE corresponds to asserting less certainty as to the magnitude of unknown function and increasing LENGTH_SCALE corresponds to increasing how smooth we expect the function to be. The figure below illustrates these points: Here, the blue intervals were obtained by setting WIDTH_SCALE = LENGTH_SCALE = 1 and the orange intervals were obtained by setting WIDTH_SCALE = 0.5 and LENGTH_SCALE = 2 . The result is that the orange posterior estimate is tighter and smoother than the blue posterior. In both plots, the solid curve is a plot of the mean of the posterior distribution, and the vertical bars are one sigma confidence intervals. Posterior samples To sample actual functions from the posterior, we will simply evaluate the mean and covariance matrix in (\\ref{posterior}) again, this time passing in the multiple test point locations at which we would like to know the resulting sampled functions. Once we have the mean and covariance matrix of the posterior at these test points, we can pull samples from (\\ref{posterior}) using an external library for multivariate normal sampling — for this purpose, we used the python package numpy. The last step in the code snippet below carries out these steps. # Insert observed sample data here. sample_x = [ - 1.5 , - 0.5 , 0.7 , 1.4 , 2.5 , 3.0 ] sample_y = [ 1 , 2 , 2 , . 5 , 0 , 0.5 ] sample_s = [ 0.01 , 0.25 , 0.5 , 0.01 , 0.3 , 0.01 ] # Initialize fitter -- set covariance parameters WIDTH_SCALE = 1.0 LENGTH_SCALE = 1.0 model = SimpleGP ( WIDTH_SCALE , LENGTH_SCALE , noise = 0 ) model . fit ( sample_x , sample_y , sample_s ) # Get the mean and std at each point in test_x test_x = np . arange ( - 5 , 5 , . 05 ) means , stds = model . interval ( test_x ) # Sample here SAMPLES = 10 samples = model . sample ( test_x , SAMPLES ) Notice that in lines 2-4 here, we've added in a few additional function sample locations (for fun). The resulting intervals and posterior samples are shown in the figure below. Notice that near the sampled points, the posterior is fairly well localized. However, on the left side of the plot, the posterior approaches the prior once we have moved a distance \\(\\geq 1\\) , the length scale chosen for the covariance matrix (\\ref{covariance}). Selecting the covariance hyper-parameters In the above, we demonstrated that the length scale of our covariance form dramatically affects the posterior — the shape of the intervals and also of the samples from the posterior. Appropriately setting these parameters is a general problem that can make working with GPs a challenge. Here, we describe two methods that can be used to intelligently set such hyper-parameters, given some sampled data. Cross-validation A standard method for setting hyper-parameters is to make use of a cross-validation scheme. This entails splitting the available sample data into a training set and a test set. One fits the GP to the training set using one set of hyper-parameters, then evaluates the accuracy of the model on the held out test set. One then repeats this process across many hyper-parameter choices, and selects that set which resulted in the best test set performance. Marginal Likelihood Maximization Often, one is interested in applying GPs in limits where evaluation of samples is expensive. This means that one often works with GPs in limits where only a small number of samples are available. In cases like this, the optimal hyper-parameters can vary quickly as the number of training points is increased. This means that the optimal selections obtained from a cross-validation schema may be far from the optimal set that applies when one trains on the full sample set \\(&#94;3\\) . An alternative general approach for setting the hyper-parameters is to maximize the marginal likelihood. That is, we try to maximize the likelihood of seeing the samples we have seen — optimizing over the choice of available hyper-parameters. Formally, the marginal likelihood is evaluated by integrating out the unknown \\(\\hat{f}&#94;4\\) , \\begin{eqnarray} \\tag{8} p(\\{y\\} \\vert \\Sigma) \\equiv \\int p(\\{y\\} \\vert f) p(f \\vert \\Sigma) df. \\end{eqnarray} Carrying out the integral directly can be done just as we have evaluated the posterior distribution in our appendix. However, a faster method is to note that after integrating out the \\(f\\) , the \\(y\\) values must be normally distributed as \\begin{eqnarray}\\tag{9} p(\\{y\\} \\vert \\Sigma) \\sim N(0, \\Sigma + \\sigma&#94;2 I_{00}), \\end{eqnarray} where \\(\\sigma&#94;2 I_{00}\\) is defined as in (\\ref{sigma_mat}). This gives \\begin{eqnarray} \\tag{10} \\label{marginallikelihood} \\log p(\\{y\\}) \\sim - \\log \\vert \\Sigma + \\sigma&#94;2 I_{00} \\vert - \\textbf{y} \\cdot ( \\Sigma + \\sigma&#94;2 I_{00} )&#94;{-1} \\cdot \\textbf{y}. \\end{eqnarray} The two terms above compete: The second term is reduced by finding the covariance matrix that maximizes the exponent. Maximizing this alone would tend to result in an overfitting of the data. However, this term is counteracted by the first, which is the normalization for a Gaussian integral. This term becomes larger given short decay lengths and low diagonal variances. It acts as regularization term that suppresses overly complex fits. In practice, to maximize (\\ref{marginallikelihood}), one typically makes use of gradient descent, using analytical expressions for the gradient. This is the approach taken by SKLearn. Being able to optimize the hyper-parameters of a GP is one of this model's virtures. Unfortunately, (\\ref{marginallikelihood}) is not guaranteed to be convex and multiple local minima often exist. To obtain a good minimum, one can attempt to initialize at some well-motivated point. Alternatively, one can reinitialize the gradient descent repeatedly at random points, finally selecting the best option at the end. Function minimum search and machine learning We're now ready to introduce one of the popular application of GPs: fast, guided function minimum search. In this problem, one is able to iteratively obtain noisy samples of a function, and the aim is to identify as quickly as possible the global minimum of the function. Gradient descent could be applied in cases like this, but this approach generally requires repeated sampling if the function is not convex. To reduce the number of steps / samples required, one can attempt to apply a more general, explore-exploit type strategy — one balancing the desire to optimize about the current best known minimum with the goal of seeking out new local minima that are potentially even better. GP posteriors provide a natural starting point for developing such strategies. The idea behind the GP -guided search approach is to develop a score function on top of the GP posterior. This score function should be chosen to encode some opinion of the value of searching a given point — preferably one that takes an explore-exploit flavor. Once each point is scored, the point with the largest (or smallest, as appropriate) score is sampled. The process is then repeated iteratively until one is satisfied. Many score functions are possible. We discuss four possible choices below, then give an example. Gaussian Lower Confidence Bound ( GLCB ) . The GLCB scores each point \\(x\\) as \\begin{eqnarray}\\tag{11} s_{\\kappa}(x) = \\mu(x) - \\kappa \\sigma(x). \\end{eqnarray} Here, \\(\\mu\\) and \\(\\sigma\\) are the GP posterior estimates for the mean and standard deviation for the function at \\(x\\) and \\(\\kappa\\) is a control parameter. Notice that the first \\(\\mu(x)\\) term encourages exploitation around the best known local minimum. Similarly, the second \\(\\kappa \\sigma\\) term encourages exploration — search at points where the GP is currently most unsure of the true function value. Gaussian Probability of Improvement ( GPI ) . If the smallest value seen so far is \\(y\\) , we can score each point using the probability that the true function value at that point is less than \\(y\\) . That is, we can write \\begin{eqnarray}\\tag{12} s(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma&#94;2}} \\int_{-\\infty}&#94;y e&#94;{-(v - \\mu)&#94;2 / (2 \\sigma&#94;2)} dv. \\end{eqnarray} Gaussian Expected Improvement ( EI ) . A popular variant of the above is the so-called expected improvement. This is defined as \\begin{eqnarray} \\tag{13} s(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma&#94;2}} \\int_{-\\infty}&#94;y e&#94;{-(v - \\mu)&#94;2 / (2 \\sigma&#94;2)} (y - v) dv. \\end{eqnarray} This score function tends to encourage more exploration than the probability of improvement, since it values uncertainty more highly. Probability is minimum . A final score function of interest is simply the probability that the point in question is the minimum. One way to obtain this score is to sample from the posterior many times. For each sample, we mark its global minimum, then take a majority vote for where to sample next. The gif at the top of this page (copied below) illustrates an actual GP -guided search, carried out in python using the package skopt \\(&#94;5\\) . The red curve at left is the (hidden) curve \\(f\\) whose global minimum is being sought. The red points are the samples that have been obtained so far, and the green shaded curve is the GP posterior confidence interval for each point — this gradually improves as more samples are obtained. At right is the Expected Improvement ( EI ) score function at each point that results from analysis on top of the GP posterior — the score function used to guide search in this example. The process is initialized with five random samples, followed by guided search. Notice that as the process evolves, the first few samples focus on exploitation of known local minima. However, after a handful of iterations, the diminishing returns of continuing to sample these locations loses out to the desire to explore the middle points — where the actual global minimum sits and is found. Discussion In this post we've overviewed much of the math of GPs: The math needed to get to the posterior, how to sample from the posterior, and finally how to make practical use of the posterior. In principle, GPs represent a powerful tool that can be used to fit any function. In practice, the challenge in wielding this tool seems to sit mainly with selection of appropriate hyper-parameters — the search for appropriate parameters often gets stuck in local minima, causing fits to go off the rails. Nevertheless, when done correctly, application of GPs can provide some valuable performance gains — and they are always fun to visualize. Some additional topics relating to GPs are contained in our appendices. For those interested in even more detail, we can recommend the free online text by Rasmussen and Williams \\(&#94;6\\) . Appendix A: Derivation of posterior In this appendix, we present two methods to derive the posterior (\\ref{posterior}). Method 1 We will begin by completing the square. Combining (\\ref{prob}) and (\\ref{prior}), a little algebra gives \\begin{align} \\tag{A1} \\label{square_complete} p(f_1, \\ldots, f_N \\vert \\{y\\}) &\\sim \\exp \\left (-\\sum_{i=1}&#94;n \\frac{(y_i - f_i)&#94;2}{2 \\sigma&#94;2_i} - \\frac{1}{2} \\sum_{ij=1}&#94;N f_i \\Sigma&#94;{-1}_{ij} f_j \\right) \\\\ &\\sim N\\left ( \\frac{1}{\\Sigma&#94;{-1} + \\frac{1}{\\sigma&#94;2} I } \\cdot \\frac{1}{\\sigma&#94;2} I \\cdot \\textbf{y}, \\frac{1}{\\Sigma&#94;{-1} + \\frac{1}{\\sigma&#94;2} I } \\right). \\end{align} Here, \\(\\frac{1}{\\sigma&#94;2} I\\) is defined as in (\\ref{sigma_mat}), but has zeros in all rows outside of the sample set. To obtain the expression (\\ref{posterior}), we must identify the block structure of the inverse matrix that appears above. To start, we write \\begin{align} \\tag{A2} \\label{matrix_to_invert} \\frac{1}{\\Sigma&#94;{-1} + \\frac{1}{\\sigma&#94;2}I } &= \\Sigma \\frac{1}{I + \\frac{1}{\\sigma&#94;2}I \\Sigma} \\\\ &= \\Sigma \\left( \\begin{matrix} I_{00} + \\frac{1}{\\sigma&#94;2}I_{00} \\Sigma_{00} & \\frac{1}{\\sigma&#94;2}I_{00} \\Sigma_{01}\\\\ 0 & I_{11} \\end{matrix} \\right)&#94;{-1}, \\end{align} where we are using block notation. To evaluate the inverse that appears above, we will make use of the block matrix inversion formula, \\begin{align} &\\left( \\begin{matrix} A & B\\\\ C & D \\end{matrix} \\right)&#94;{-1} = \\\\ &\\left( \\begin{matrix} (A - B D&#94;{-1} C)&#94;{-1} & - (A - B D&#94;{-1} C)&#94;{-1} B D&#94;{-1} \\\\ -D&#94;{-1} C (A - B D&#94;{-1} C)&#94;{-1} & D&#94;{-1} + D&#94;{-1} C (A - B D&#94;{-1} C) B D&#94;{-1} \\end{matrix} \\right). \\end{align} The matrix (\\ref{matrix_to_invert}) has blocks \\(C = 0\\) and \\(D=I\\) , which simplifies the above significantly. Plugging in, we obtain \\begin{align} \\label{shifted_cov} \\tag{A3} \\frac{1}{\\Sigma&#94;{-1} + \\frac{1}{\\sigma&#94;2}I } = \\Sigma \\left( \\begin{matrix} \\frac{1}{I_{00} + \\frac{1}{\\sigma&#94;2}I \\Sigma_{00}} & - \\frac{1}{I_{00} + \\frac{1}{\\sigma&#94;2}I \\Sigma_{00}} \\Sigma_{01}\\\\ 0 & I_{11} \\end{matrix} \\right) \\end{align} With this result and (\\ref{square_complete}), we can read off the mean of the test set as \\begin{align} \\tag{A4} \\label{mean_test} & \\left [ [ \\Sigma&#94;{-1} + \\frac{1}{\\sigma&#94;2} I_{00} ]&#94;{-1} \\cdot \\frac{1}{\\sigma&#94;2} I_{00} \\cdot \\textbf{y} \\right ]_1 \\\\ &= \\Sigma_{10} \\frac{1}{I_{00} + \\frac{1}{\\sigma&#94;2}I_{00} \\Sigma_{00}} \\frac{1}{\\sigma&#94;2} I_{00} \\cdot \\textbf{y} \\\\ &= \\Sigma_{10} \\frac{1}{\\sigma&#94;2 I_{00} + \\Sigma_{00}} \\cdot \\textbf{y}, \\end{align} where we have multiplied the numerator and denominator by the inverse of \\(\\frac{1}{\\sigma&#94;2}I_{00}\\) in the second line. Similarly, the covariance of the test set is given by the lower right block of (\\ref{shifted_cov}). This is, \\begin{align}\\tag{A5} \\label{covariance_test} \\Sigma_{11} - \\Sigma_{10} \\cdot \\frac{1}{\\sigma&#94;2 I_{00} + \\Sigma_{00}} \\cdot \\Sigma_{01}. \\end{align} The results (\\ref{mean_test}) and (\\ref{covariance_test}) give (\\ref{posterior}). Method 2 In this second method, we consider the joint distribution of a set of test points \\(\\textbf{f}_1\\) and the set of observed samples \\(\\textbf{f}_0\\) . Again, we assume that the function density has mean zero. The joint probability density for the two is then \\begin{align}\\tag{A6} p(\\textbf{f}_0, \\textbf{f}_1) \\sim N \\left ( \\left ( \\begin{matrix} 0 \\\\ 0 \\end{matrix} \\right), \\left ( \\begin{matrix}{cc} \\Sigma_{0,0} & \\Sigma_{0,1} \\\\ \\Sigma_{1,0} & \\Sigma_{11} \\end{matrix} \\right ) \\right ) \\end{align} Now, we use the result \\begin{align} \\tag{A7} p( \\textbf{f}_1 \\vert \\textbf{f}_0) &=& \\frac{p( \\textbf{f}_0, \\textbf{f}_1)}{p( \\textbf{f}_0)}. \\end{align} The last two expressions are all that are needed to derive (\\ref{posterior}). The main challenge involves completing the square, and this can be done with the block matrix inversion formula, as in the previous derivation. Appendix B: SKLearn implementation and other kernels SKLearn provides contains the GaussianProcessRegressor class. This allows one to carry out fits and sampling in any dimension — i.e., it is more general than our minimal class in that it can fit feature vectors in more than one dimension. In addition, the fit method of the SKLearn class attempts to find an optimal set of hyper-parameters for a given set of data. This is done through maximization of the marginal likelihood, as described above. Here, we provide some basic notes on this class and the built in kernels that one can use to define the covariance matrix \\(\\Sigma\\) in (\\ref{prior}). We also include a simple code snippet illustrating calls. Pre-defined Kernels Radial-basis function ( RBF ): This is the default — equivalent to our (\\ref{covariance}). The RBF is characterized by a scale parameter, \\(l\\) . In more than one dimension, this can be a vector, allowing for anisotropic correlation lengths. White kernel : The White Kernel is used for noise estimation — docs suggest useful for estimating the global noise level, but not pointwise. Matern: This is a generalized exponential decay, where the exponents is a powerlaw in separation distance. Special limits include the RBF and also an absolute distance exponential decay. Some special parameter choices allow for existence of single or double derivatives. Rational quadratic: This is \\((1 + (d / l)&#94;2)&#94;{\\alpha}\\) . Exp-Sine-Squared: This allows one to model periodic functions. This is just like the RBF , but the distance that gets plugged in is the sine of the actual distance. A periodicity parameter exists, as well as a \"variance\" — the scale of the Gaussian suppression. Dot product kernel : This takes form \\(1 + x_i \\cdot x_j\\) . It's not stationary, in the sense that the result changes if a constant translation is added in. They state that you get this result from linear regression analysis if you place \\(N(0,1)\\) priors on the coefficients. Kernels as objects : The kernels are objects, but support binary operations between them to create more complicated kernels, eg addition, multiplication, and exponentiation (latter simply raises initial kernel to a power). They all support analytic gradient evaluation. You can access all of the parameters in a kernel that you define via some helper functions — eg, kernel.get_params() . kernel.hyperparameters is a list of all the hyper-parameters. Parameters n_restarts_optimizer : This is the number of times to restart the fit — useful for exploration of multiple local minima. The default is zero. alpha : This optional argument allows one to pass in uncertainties for each measurement. normalize_y : This is used to indicate that the mean of the \\(y\\) -values we're looking for is not necessarily zero. Example call The code snippet below carries out a simple fit. The result is the plot shown at the top of this section. from sklearn.gaussian_process.kernels import RBF , ConstantKernel as C from sklearn.gaussian_process import GaussianProcessRegressor import numpy as np # Build a model kernel = C ( 1.0 , ( 1e-3 , 1e3 )) * RBF ( 10 , ( 0.5 , 2 )) gp = GaussianProcessRegressor ( kernel = kernel , n_restarts_optimizer = 9 ) # Some data xobs = np . array ([[ 1 ], [ 1.5 ], [ - 3 ]]) yobs = np . array ([ 3 , 0 , 1 ]) # Fit the model to the data (optimize hyper parameters) gp . fit ( xobs , yobs ) # Plot points and predictions x_set = np . arange ( - 6 , 6 , 0.1 ) x_set = np . array ([[ i ] for i in x_set ]) means , sigmas = gp . predict ( x_set , return_std = True ) plt . figure ( figsize = ( 8 , 5 )) plt . errorbar ( x_set , means , yerr = sigmas , alpha = 0.5 ) plt . plot ( x_set , means , 'g' , linewidth = 4 ) colors = [ 'g' , 'r' , 'b' , 'k' ] for c in colors : y_set = gp . sample_y ( x_set , random_state = np . random . randint ( 1000 )) plt . plot ( x_set , y_set , c + '--' , alpha = 0.5 ) More details on the sklearn implementation can be found here . Appendix C: GP Classifiers Here, we describe how GPs are often used to fit binary classification data — data where the response variable \\(y\\) can take on values of either \\(0\\) or \\(1\\) . The mathematics for GP Classifiers does not work out as cleanly as it does for GP Regressors. The reason is that the \\(0 / 1\\) response is not Gaussian-distributed, which means that the posterior is not either. To make use of the program, one approximates the posterior as normal, via the Laplace approximation. The starting point is to write down a form for the probability of seeing a given \\(y\\) value at \\(x\\) . This, ones takes as the form, \\begin{align} \\tag{A8} \\label{classifier} p(y \\vert f(x)) = \\frac{1}{1 + \\exp\\left (- y \\times f(x)\\right)}. \\end{align} This form is a natural non-linear generalization of logistic regression — see our post on this topic, here . To proceed, the prior for \\(f\\) is taken to once again have form (\\ref{prior}). Using this and (\\ref{classifier}), we obtain the posterior for \\(f\\) \\begin{align} p(f \\vert y) &\\sim \\frac{1}{1 + \\exp\\left (- y \\times f(x)\\right)} \\exp \\left ( - \\frac{1}{2} \\sum_{ij=1}&#94;N f_i \\Sigma&#94;{-1}_{ij} f_j \\right) \\\\ &\\approx N(\\mu, \\Sigma&#94;{\\prime}) \\tag{A9} \\end{align} Here, the last line is the Laplace / Normal approximation to the line above it. Using this form, one can easily obtain confidence intervals and samples from the approximate posterior, as was done for regressors. Footnotes [1] The size of the \\(\\sigma_i\\) determines how precisely we know the function value at each of the \\(x_i\\) points sampled — if they are all \\(0\\) , we know the function exactly at these points, but not anywhere else. [2] One might wonder whether introducing more points to the analysis would change the posterior statistics for the original \\(N\\) points in question. It turns out that this is not the case for GPs: If one is interested only in the joint-statistics of these \\(N\\) points, all others integrate out. For example, consider the goal of identifying the posterior distribution of \\(f\\) at only a single test point \\(x\\) . In this case, the posterior for the \\(N = n+1\\) points follows from Bayes's rule, \\begin{align} \\tag{f1} p(f(x_1), \\ldots, f(x_n), f(x_{n+1}) \\vert \\{y\\}) = \\frac{p(\\{y\\} \\vert f) p(f)}{p(\\{y\\})}. \\end{align} Now, by assumption, \\(p(\\{y\\} \\vert f)\\) depends only on \\(f(x_1),\\ldots, f(x_n)\\) — the values of \\(f\\) where \\(y\\) was sampled. Integrating over all points except the sample set and test point \\(x\\) gives \\begin{align} \\tag{f2} &p(f(x_1), \\ldots, f(x_{n+1}) \\vert \\{y\\}) =\\\\ & \\frac{p(\\{y\\} \\vert f(x_1),\\ldots,f(x_n))}{p(\\{y\\})} \\int p(f) \\prod_{i \\not \\in \\{x_1, \\ldots, x_N\\}} df_i \\end{align} The result of the integral above is a Normal distribution — one with covariance given by the original covariance function evaluated only at the points \\(\\{x_1, \\ldots, x_{N} \\}\\) . This fact is proven in our post on Normal distributions — see equation (22) of that post, here . The result implies that we can get the correct sampling statistics on any set of test points, simply by analyzing these alongside the sampled points. This fact is what allows us to tractably treat the formally-infinite number of degrees of freedom associated with GPs. [3] We have a prior post illustrating this point — see here . [4] The marginal likelihood is equal to the denominator of (\\ref{Bayes}), which we previously ignored. [5] We made this gif through adapting the skopt tutorial code, here . [6] For the free text by Rasmussen and Williams, see here . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods","url":"https://efavdb.com/gaussian-processes","loc":"https://efavdb.com/gaussian-processes"},{"title":"Martingales","text":"Here, I give a quick review of the concept of a Martingale. A Martingale is a sequence of random variables satisfying a specific expectation conservation law. If one can identify a Martingale relating to some other sequence of random variables, its use can sometimes make quick work of certain expectation value evaluations. This note is adapted from Chapter 2 of Stochastic Calculus and Financial Applications, by Steele. Definition Often in random processes, one is interested in characterizing a sequence of random variables \\(\\{X_i\\}\\) . The example we will keep in mind is a set of variables \\(X_i \\in \\{-1, 1\\}\\) corresponding to the steps of an unbiased random walk in one-dimension. A Martingale process \\(M_i = f(X_1, X_2, \\ldots X_i)\\) is a derived random variable on top of the \\(X_i\\) variables satisfying the following conservation law \\begin{align} \\tag{1} E(M_i | X_1, \\ldots X_{i-1}) = M_{i-1}. \\end{align} For example, in the unbiased random walk example, if we take \\(S_n = \\sum_{i=1}&#94;n X_i\\) , then \\(E(S_n) = S_{n-1}\\) , so \\(S_n\\) is a Martingale. If we can develop or identify a Martingale for a given \\(\\{X_i\\}\\) process, it can often help us to quickly evaluate certain expectation values relating to the underlying process. Three useful Martingales follow. Again, the sum \\(S_n = \\sum_{i=1}&#94;n X_i\\) is a Martingale, provided \\(E(X_i) = 0\\) for all \\(i\\) . The expression \\(S_n&#94;2 - n \\sigma&#94;2\\) is a Martingale, provided \\(E(X_i) = 0\\) and \\(E(X_i&#94;2) = \\sigma&#94;2\\) for all \\(i\\) . Proof: \\begin{align} \\tag{2} E(S_n&#94;2 | X_1, \\ldots X_{n-1}) &= \\sigma&#94;2 + 2 E(X_n) S_{n-1} + S_{n-1}&#94;2 - n \\sigma&#94;2\\\\ &= S_{n-1}&#94;2 - (n-1) \\sigma&#94;2. \\end{align} The product \\(P_n = \\prod_{i=1}&#94;n X_i\\) is a Martingale, provided \\(E(X_i) = 1\\) for all \\(i\\) . One example of interest is \\begin{align} \\tag{3} P_n = \\frac{\\exp \\left ( \\lambda \\sum_{i=1}&#94;n X_i\\right)}{E(\\exp \\left ( \\lambda X \\right))&#94;n}. \\end{align} Here, \\(\\lambda\\) is a free tuning parameter. If we choose a \\(\\lambda\\) such that \\(E(\\exp(\\lambda X)) = 1\\) for our process, we can get a particularly simple form. Stopped processes In some games, we may want to setup rules that say we will stop the game at time \\(\\tau\\) if some condition is met at index \\(\\tau\\) . For example, we may stop a random walk (initialized at zero) if the walker gets to either position \\(A\\) or \\(-B\\) (wins \\(A\\) or loses \\(B\\) ). This motivates defining the stopped Martingale as, \\begin{align} M_{n \\wedge \\tau} = \\begin{cases} M_n &\\text{if } \\tau \\geq n \\\\ M_{\\tau} &\\text{else}. \\tag{4} \\end{cases} \\end{align} Here, we prove that if \\(M_n\\) is a Martingale, then so is $M_{n \\wedge \\tau} $. This is useful because it will tell us that the stopped Martingale has the same conservation law as the unstopped version. First, we note that if \\(A_i \\equiv f_2(X_1, \\ldots X_{i-1})\\) is some function of the observations so far, then the transformed process \\begin{align} \\tag{5} \\tilde{M}_n \\equiv M_0 + \\sum_{i=1}&#94;n A_i (M_i - M_{i-1}) \\end{align} is also a Martingale. Proof: \\begin{align} \\tag{6} E(\\tilde{M}_n | X_1, \\ldots X_{n-1}) = A_n \\left ( E(M_n) - M_{n-1} \\right) + \\tilde{M}_{n-1} = \\tilde{M}_{n-1}. \\end{align} With this result we can prove the stopped Martingale is also a Martingale. We can do that by writing \\(A_i = 1(\\tau \\geq i)\\) — where \\(1\\) is the indicator function. Plugging this into the above, we get the transformed Martingale, \\begin{align} \\nonumber \\tag{7} \\tilde{M}_n &= M_0 + \\sum_{i=1}&#94;n 1(\\tau \\geq i) (M_i - M_{i-1}) \\\\ &= \\begin{cases} M_n & \\text{if } \\tau \\geq n \\ M_{\\tau} & \\text{else}. \\end{cases} \\end{align} This is the stopped Martingale — indeed a Martingale, by the above. Example applications Problem 1 Consider an unbiased random walker that takes steps of size \\(1\\) . If we stop the walk as soon as he reaches either \\(A\\) or \\(-B\\) , what is the probability that he is at \\(A\\) when the game stops? Solution: Let \\(\\tau\\) be the stopping time and let \\(S_n = \\sum_{i=1}&#94;n X_i\\) be the walker's position at time \\(n\\) . We know that \\(S_n\\) is a Martingale. By the above, so then is \\(S_{n \\wedge \\tau}\\) , the stopped process Martingale. By the Martingale property \\begin{align} \\tag{8} E(S_{n \\wedge \\tau}) = E(S_{i \\wedge \\tau}) \\end{align} for all \\(i\\) . In particular, plugging in \\(i = 0\\) gives \\(E(S_{n \\wedge \\tau}) = 0\\) . If we take \\(n \\to \\infty\\) , then \\begin{align} \\tag{9} \\lim_{n \\to \\infty} E(S_{n \\wedge \\tau}) \\to E(S_{\\tau}) = 0. \\end{align} But we also have \\begin{align} \\tag{10} E(S_{\\tau}) = P(A) * A - (1 - P(A)) B. \\end{align} Equating (9) and (10) gives \\begin{equation} \\tag{11} P(A) = \\frac{B}{A + B} \\end{equation} Problem 2 In the game above, what is the expected stopping time? Solution: Use the stopped version of the Martingale \\(S_n&#94;2 - n \\sigma&#94;2\\) . Problem 3 In a biased version of the random walk game, what is the probability of stopping at \\(A\\) ? Solution: Use the stopped Martingale of form \\(P_n = \\frac{\\exp \\left ( \\lambda \\sum_{i=1}&#94;n X_i\\right)}{E(\\exp \\left ( \\lambda X \\right))&#94;n}\\) , with \\(\\exp[\\lambda] = q/p\\) , where \\(p = 1-q\\) is the probability of step to the right. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Finance","url":"https://efavdb.com/martingales","loc":"https://efavdb.com/martingales"},{"title":"Logistic Regression","text":"We review binary logistic regression. In particular, we derive a) the equations needed to fit the algorithm via gradient descent, b) the maximum likelihood fit's asymptotic coefficient covariance matrix, and c) expressions for model test point class membership probability confidence intervals. We also provide python code implementing a minimal \"LogisticRegressionWithError\" class whose \"predict_proba\" method returns prediction confidence intervals alongside its point estimates. Our python code can be downloaded from our github page, here . Its use requires the jupyter, numpy, sklearn, and matplotlib packages. Introduction The logistic regression model is a linear classification model that can be used to fit binary data — data where the label one wishes to predict can take on one of two values — e.g., \\(0\\) or \\(1\\) . Its linear form makes it a convenient choice of model for fits that are required to be interpretable. Another of its virtues is that it can — with relative ease — be set up to return both point estimates and also confidence intervals for test point class membership probabilities. The availability of confidence intervals allows one to flag test points where the model prediction is not precise, which can be useful for some applications — eg fraud detection. In this note, we derive the expressions needed to fit the logistic model to a training data set. We assume the training data consists of a set of \\(n\\) feature vector- label pairs, \\(\\{(\\vec{x}_i, y_i)\\) , for \\(i = 1, 2, \\ldots, n\\}\\) , where the feature vectors \\(\\vec{x}_i\\) belong to some \\(m\\) -dimensional space and the labels are binary, \\(y_i \\in \\{0, 1\\}.\\) The logistic model states that the probability of belonging to class \\(1\\) is given by \\begin{eqnarray}\\tag{1} \\label{model1} p(y=1 \\vert \\vec{x}) \\equiv \\frac{1}{1 + e&#94;{- \\vec{\\beta} \\cdot \\vec{x} } }, \\end{eqnarray} where \\(\\vec{\\beta}\\) is a coefficient vector characterizing the model. Note that with this choice of sign in the exponent, predictor vectors \\(\\vec{x}\\) having a large, positive component along \\(\\vec{\\beta}\\) will be predicted to have a large probability of being in class \\(1\\) . The probability of class \\(0\\) is given by the complement, \\begin{eqnarray}\\tag{2} \\label{model2} p(y=0 \\vert \\vec{x}) \\equiv 1 - p(y=1 \\vert \\vec{x}) = \\frac{1}{1 + e&#94;{ \\vec{\\beta} \\cdot \\vec{x} } }. \\end{eqnarray} The latter equality above follows from simplifying algebra, after plugging in (\\ref{model1}) for \\(p(y=1 \\vert \\vec{x}).\\) To fit the Logistic model to a training set — i.e., to find a good choice for the fit parameter vector \\(\\vec{\\beta}\\) — we consider here only the maximum-likelihood solution. This is that \\(\\vec{\\beta}&#94;*\\) that maximizes the conditional probability of observing the training data. The essential results we review below are 1) a proof that the maximum likelihood solution can be found by gradient descent, and 2) a derivation for the asymptotic covariance matrix of \\(\\vec{\\beta}\\) . This latter result provides the basis for returning point estimate confidence intervals. On our GitHub page , we provide a Jupyter notebook that contains some minimal code extending the SKLearn LogisticRegression class. This extension makes use of the results presented here and allows for class probability confidence intervals to be returned for individual test points. In the notebook, we apply the algorithm to the SKLearn Iris dataset. The figure at right illustrates the output of the algorithm along a particular cut through the Iris data set parameter space. The y-axis represents the probability of a given test point belong to Iris class \\(1\\) . The error bars in the plot provide insight that is completely missed when considering the point estimates only. For example, notice that the error bars are quite large for each of the far right points, despite the fact that the point estimates there are each near \\(1\\) . Without the error bars, the high probability of these point estimates might easily be misinterpreted as implying high model confidence. Our derivations below rely on some prerequisites: Properties of covariance matrices, the multivariate Cramer-Rao theorem, and properties of maximum likelihood estimators. These concepts are covered in two of our prior posts [ \\(1\\) , \\(2\\) ]. Optimization by gradient descent In this section, we derive expressions for the gradient of the negative-log likelihood loss function and also demonstrate that this loss is everywhere convex. The latter result is important because it implies that gradient descent can be used to find the maximum likelihood solution. Again, to fit the logistic model to a training set, our aim is to find — and also to set the parameter vector to — the maximum likelihood value. Assuming the training set samples are independent, the likelihood of observing the training set labels is given by \\begin{eqnarray} L &\\equiv& \\prod_i p(y_i \\vert \\vec{x}_i) \\\\ &=& \\prod_{i: y_i = 1} \\frac{1}{1 + e&#94;{-\\vec{\\beta} \\cdot \\vec{x}_i}} \\prod_{i: y_i = 0} \\frac{1}{1 + e&#94;{\\vec{\\beta} \\cdot \\vec{x}_i}}. \\tag{3} \\label{likelihood} \\end{eqnarray} Maximizing this is equivalent to minimizing its negative logarithm — a cost function that is somewhat easier to work with, \\begin{eqnarray} J &\\equiv& -\\log L \\\\ &=& \\sum_{\\{i: y_i = 1 \\}} \\log \\left (1 + e&#94;{- \\vec{\\beta} \\cdot \\vec{x}_i } \\right ) + \\sum_{\\{i: y_i = 0 \\}} \\log \\left (1 + e&#94;{\\vec{\\beta} \\cdot \\vec{x}_i } \\right ). \\tag{4} \\label{costfunction} \\end{eqnarray} The maximum-likelihood solution, \\(\\vec{\\beta}&#94;*\\) , is that coefficient vector that minimizes the above. Note that \\(\\vec{\\beta}&#94;*\\) will be a function of the random sample, and so will itself be a random variable — characterized by a distribution having some mean value, covariance, etc. Given enough samples, a theorem on maximum-likelihood asymptotics (Cramer-Rao) guarantees that this distribution will be unbiased — i.e., it will have mean value given by the correct parameter values — and will also be of minimal covariance [ \\(1\\) ]. This theorem is one of the main results motivating use of the maximum-likelihood solution. Because \\(J\\) is convex (demonstrated below), the logistic regression maximum-likelihood solution can always be found by gradient descent. That is, one need only iteratively update \\(\\vec{\\beta}\\) in the direction of the negative \\(\\vec{\\beta}\\) -gradient of \\(J\\) , which is \\begin{eqnarray} - \\nabla_{\\vec{\\beta}} J &=& \\sum_{\\{i: y_i = 1 \\}}\\vec{x}_i \\frac{ e&#94;{- \\vec{\\beta} \\cdot \\vec{x}_i } }{1 + e&#94;{- \\vec{\\beta} \\cdot \\vec{x}_i }} - \\sum_{\\{i: y_i = 0 \\}} \\vec{x}_i \\frac{ e&#94;{\\vec{\\beta} \\cdot \\vec{x}_i }}{1 + e&#94;{\\vec{\\beta} \\cdot \\vec{x}_i } } \\\\ &\\equiv& \\sum_{\\{i: y_i = 1 \\}}\\vec{x}_i p(y=0 \\vert \\vec{x}_i) -\\sum_{\\{i: y_i = 0 \\}} \\vec{x}_i p(y= 1 \\vert \\vec{x}_i). \\tag{5} \\label{gradient} \\end{eqnarray} Notice that the terms that contribute the most here are those that are most strongly misclassified — i.e., those where the model's predicted probability for the observed class is very low. For example, a point with true label \\(y=1\\) but large model \\(p(y=0 \\vert \\vec{x})\\) will contribute a significant push on \\(\\vec{\\beta}\\) in the direction of \\(\\vec{x}\\) — so that the model will be more likely to predict \\(y=1\\) at this point going forward. Notice that the contribution of a term above is also proportional to the length of its feature vector — training points further from the origin have a stronger impact on the optimization process than those near the origin (at fixed classification difficulty). The Hessian (second partial derivative) matrix of the cost function follows from taking a second gradient of the above. With a little algebra, one can show that this has \\(i-j\\) component given by, \\begin{eqnarray} H(J)_{ij} &\\equiv& -\\partial_{\\beta_j} \\partial_{\\beta_i} \\log L \\\\ &=& \\sum_k x_{k; i} x_{k; j} p(y= 0 \\vert \\vec{x}_k) p(y= 1 \\vert \\vec{x}_k). \\tag{6} \\label{Hessian} \\end{eqnarray} We can prove that this is positive semi-definite using the fact that a matrix \\(M\\) is necessarily positive semi-definite if \\(\\vec{s}&#94;T \\cdot M \\cdot \\vec{s} \\geq 0\\) for all real \\(\\vec{s}\\) [ \\(2\\) ]. Dotting our Hessian above on both sides by an arbitrary vector \\(\\vec{s}\\) , we obtain \\begin{eqnarray} \\vec{s}&#94;T \\cdot H \\cdot \\vec{s} &\\equiv& \\sum_k \\sum_{ij} s_i x_{k; i} x_{k; j} s_j p(y= 0 \\vert \\vec{x}_k) p(y= 1 \\vert \\vec{x}_k) \\\\ &=& \\sum_k \\vert \\vec{s} \\cdot \\vec{x}_k \\vert&#94;2 p(y= 0 \\vert \\vec{x}_k) p(y= 1 \\vert \\vec{x}_k) \\geq 0. \\tag{7} \\label{convex} \\end{eqnarray} The last form follows from the fact that both \\(p(y= 0 \\vert \\vec{x}_k)\\) and \\(p(y= 1 \\vert \\vec{x}_k)\\) are non-negative. This holds for any \\(\\vec{\\beta}\\) and any \\(\\vec{s}\\) , which implies that our Hessian is everywhere positive semi-definite. Because of this, convex optimization strategies — e.g., gradient descent — can always be applied to find the global maximum-likelihood solution. Coefficient uncertainty and significance tests The solution \\(\\vec{\\beta}&#94;*\\) that minimizes \\(J\\) — which can be found by gradient descent — is a maximum likelihood estimate. In the asymptotic limit of a large number of samples, maximum-likelihood parameter estimates satisfy the Cramer-Rao lower bound [ \\(2\\) ]. That is, the parameter covariance matrix satisfies [ \\(3\\) ], \\begin{eqnarray} \\text{cov}(\\vec{\\beta}&#94;*, \\vec{\\beta}&#94;*) &\\sim& H(J)&#94;{-1} \\\\ &\\approx& \\frac{1}{\\sum_k \\vec{x}_{k} \\vec{x}_{k}&#94;T p(y= 0 \\vert \\vec{x}_k) p(y= 1 \\vert \\vec{x}_k)}. \\tag{8} \\label{covariance} \\end{eqnarray} Notice that the covariance matrix will be small if the denominator above is large. Along a given direction, this requires that the training set contains samples over a wide range of values in that direction (we discuss this at some length in the analogous section of our post on Linear Regression [ \\(4\\) ]). For a term to contribute in the denominator, the model must also have some confusion about its values: If there are no difficult-to-classify training examples, this means that there are no examples near the decision boundary. When this occurs, there will necessarily be a lot of flexibility in where the decision boundary is placed, resulting in large parameter variances. Although the form above only holds in the asymptotic limit, we can always use it to approximate the true covariance matrix — keeping in mind that the accuracy of the approximation will degrade when working with small training sets. For example, using (\\ref{covariance}), the asymptotic variance for a single parameter can be approximated by \\begin{eqnarray} \\tag{9} \\label{single_cov} \\sigma&#94;2_{\\beta&#94;*_i} = \\text{cov}(\\vec{\\beta}&#94;*, \\vec{\\beta}&#94;*)_{ii}. \\end{eqnarray} In the asymptotic limit, the maximum-likelihood parameters will be Normally-distributed [ \\(1\\) ], so we can provide confidence intervals for the parameters as \\begin{eqnarray} \\tag{10} \\label{parameter_interval} \\beta_i \\in \\left ( \\beta&#94;*_i - z \\sigma_{\\beta&#94;*_i}, \\beta_i&#94;* + z \\sigma_{\\beta&#94;*_i} \\right), \\end{eqnarray} where the value of \\(z\\) sets the size of the interval. For example, choosing \\(z = 2\\) gives an interval construction procedure that will cover the true value approximately \\(95%\\) of the time — a result of Normal statistics [ \\(5\\) ]. Checking which intervals do not cross zero provides a method for identifying which features contribute significantly to a given fit. Prediction confidence intervals The probability of class \\(1\\) for a test point \\(\\vec{x}\\) is given by (\\ref{model1}). Notice that this depends on \\(\\vec{x}\\) and \\(\\vec{\\beta}\\) only through the dot product \\(\\vec{x} \\cdot \\vec{\\beta}\\) . At fixed \\(\\vec{x}\\) , the variance (uncertainty) in this dot product follows from the coefficient covariance matrix above: We have [ \\(2\\) ], \\begin{eqnarray} \\tag{11} \\label{logit_var} \\sigma&#94;2_{\\vec{x} \\cdot \\vec{\\beta}} \\equiv \\vec{x}&#94;T \\cdot \\text{cov}(\\vec{\\beta}&#94;*, \\vec{\\beta}&#94;*) \\cdot \\vec{x}. \\end{eqnarray} With this result, we can obtain an expression for the confidence interval for the dot product, or equivalently a confidence interval for the class probability. For example, the asymptotic interval for class \\(1\\) probability is given by \\begin{eqnarray} \\tag{12} \\label{prob_interval} p(y=1 \\vert \\vec{x}) \\in \\left ( \\frac{1}{1 + e&#94;{- \\vec{x} \\cdot \\vec{\\beta}&#94;* + z \\sigma_{\\vec{x} \\cdot \\vec{\\beta}&#94;*}}}, \\frac{1}{1 + e&#94;{- \\vec{x} \\cdot \\vec{\\beta}&#94;* - z \\sigma_{\\vec{x} \\cdot \\vec{\\beta}&#94;*}}} \\right), \\end{eqnarray} where \\(z\\) again sets the size of the interval as above ( \\(z=2\\) gives a \\(95%\\) confidence interval, etc. [ \\(5\\) ]), and \\(\\sigma_{\\vec{x} \\cdot \\vec{\\beta}&#94;*}\\) is obtained from (\\ref{covariance}) and (\\ref{logit_var}). The results (\\ref{covariance}), (\\ref{logit_var}), and (\\ref{prob_interval}) are used in our Jupyter notebook. There we provide code for a minimal Logistic Regression class implementation that returns both point estimates and prediction confidence intervals for each test point. We used this code to generate the plot shown in the post introduction. Again, the code can be downloaded here if you are interested in trying it out. Summary In this note, we have 1) reviewed how to fit a logistic regression model to a binary data set for classification purposes, and 2) have derived the expressions needed to return class membership probability confidence intervals for test points. Confidence intervals are typically not available for many out-of-the-box machine learning models, despite the fact that intervals can often provide significant utility. The fact that logistic regression allows for meaningful error bars to be returned with relative ease is therefore a notable, advantageous property. Footnotes [ \\(1\\) ] Our notes on the maximum-likelihood estimators can be found here . [ \\(2\\) ] Our notes on covariance matrices and the multivariate Cramer-Rao theorem can be found here . [ \\(3\\) ] The Cramer-Rao identity [ \\(2\\) ] states that covariance matrix of the maximum-likelihood estimators approaches the Hessian matrix of the log-likelihood, evaluated at their true values. Here, we approximate this by evaluating the Hessian at the maximum-likelihood point estimate. [ \\(4\\) ] Our notes on linear regression can be found here . [ \\(5\\) ] Our notes on Normal distributions can be found here . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods, Statistics, Theory","url":"https://efavdb.com/logistic-regression","loc":"https://efavdb.com/logistic-regression"},{"title":"Normal Distributions","text":"I review — and provide derivations for — some basic properties of Normal distributions. Topics currently covered: (i) Their normalization, (ii) Samples from a univariate Normal, (iii) Multivariate Normal distributions, (iv) Central limit theorem. Introduction This post contains a running list of properties (with derivations) relating to Normal (Gaussian) distributions. Normal distributions are important for two principal reasons: Their significance a la the central limit theorem and their appearance in saddle point approximations to more general integrals. As usual, the results here assume familiarity with calculus and linear algebra. Pictured at right is an image of Gauss — \"Few, but ripe.\" Normalization Consider the integral \\begin{align} \\tag{1} I = \\int_{-\\infty}&#94;{\\infty} e&#94;{-x&#94;2} dx. \\end{align} To evaluate, consider the value of \\(I&#94;2\\) . This is \\begin{align}\\tag{2} I&#94;2 &= \\int_{-\\infty}&#94;{\\infty} e&#94;{-x&#94;2} dx \\int_{-\\infty}&#94;{\\infty} e&#94;{-y&#94;2} dy \\\\ &= \\int_0&#94;{\\infty} e&#94;{-r&#94;2} 2 \\pi r dr = -\\pi e&#94;{-r&#94;2} \\vert_0&#94;{\\infty} = \\pi. \\end{align} Here, I have used the usual trick of transforming the integral over the plane to one over polar \\((r, \\theta)\\) coordinates. The result above gives the normalization for the Normal distribution. Samples from a univariate normal Suppose \\(N\\) independent samples are taken from a Normal distribution. The sample mean is defined as \\(\\hat{\\mu} = \\frac{1}{N}\\sum x_i\\) and the sample variance as \\(\\hat{S}&#94;2 \\equiv \\frac{1}{N-1} \\sum (x_i - \\hat{\\mu})&#94;2\\) . These two statistics are independent. Further, the former is Normal distributed with variance \\(\\sigma&#94;2/N\\) and the latter is proportional to a \\(\\chi_{N-1}&#94;2.\\) Proof: Let the sample be \\(\\textbf{x} = (x_1, x_2, \\ldots, x_N)\\) . Then the mean can be written as \\(\\textbf{x} \\cdot \\textbf{1}/N\\) , the projection of \\(\\textbf{x}\\) along \\(\\textbf{1}/N\\) . Similarly, the sample variance can be expressed as the squared length of \\(\\textbf{x} - (\\textbf{x} \\cdot \\textbf{1} / N)\\textbf{1} = \\textbf{x} - (\\textbf{x} \\cdot \\textbf{1} / \\sqrt{N})\\textbf{1}/\\sqrt{N}\\) , which is the squared length of \\(\\textbf{x}\\) projected into the space orthogonal to \\(\\textbf{1}\\) . The independence of the \\(\\{x_i\\}\\) implies that these two variables are themselves independent, the former Normal and the latter \\(\\chi&#94;2_{N-1}.\\) The result above implies that the weight for sample \\(\\textbf{x}\\) can be written as \\begin{align} \\tag{3} p(\\textbf{x} \\vert \\mu, \\sigma&#94;2) = \\frac{1}{(2 \\pi \\sigma&#94;2)&#94;{N/2}} e&#94;{\\left (N (\\hat{\\mu} - \\mu)&#94;2 + (N-1)S&#94;2\\right)/(2 \\sigma&#94;2) }. \\end{align} Aside on sample variance: Given independent samples from any distribution, dividing by \\(N-1\\) gives an unbiased estimate for the population variance. However, if the samples are not independent (eg, direct trace from MCMC ), this factor is not appropriate: We have \\begin{align} \\nonumber (N-1)E(S&#94;2) &= E(\\sum (x_i - \\overline{x})&#94;2) \\\\ &= E(\\sum (x_i - \\mu)&#94;2 - N ( \\overline{x} - \\mu)&#94;2 ) \\\\ \\tag{4} &= N [\\sigma&#94;2 - \\text{var}(\\overline{x})] \\label{sample_var} \\end{align} If the samples are independent, the above gives \\((N-1) \\sigma&#94;2\\) . However, if the samples are all the same, \\(\\text{var}(\\overline{x}) = \\sigma&#94;2\\) , giving \\(S&#94;2=0\\) . In general, the relationship between the samples determines whether \\(S&#94;2\\) is biased or not. From the results above, the quantity \\begin{align} \\label{t-var} \\tag{5} (\\hat{\\mu}- \\mu)/(S/\\sqrt(N)) \\end{align} is the ratio of two independent variables — the numerator a Normal and the denominator the square root of an independent \\(\\chi&#94;2_{N-1}\\) variable. This quantity follows a universal distribution called the \\(t\\) -distribution. One can write down closed-form expressions for the \\(t\\) . For example, when \\(N=2\\) , you get a Cauchy variable: the ratio of one Normal over the absolute value of another, independent Normal (see above). In general, \\(t\\) -distributions have power law tails. A key point is that we cannot evaluate (\\ref{t-var}) numerically if we do not know \\(\\mu\\) . Nevertheless, we can use the known distribution of the above to specify its likely range. Using this, we can then construct a confidence interval for \\(\\mu\\) . Consider now a situation where you have two separate Normal distributions. To compare their variances you can take samples from the two and then construct the quantity \\begin{align}\\label{f-var} \\tag{6} \\frac{S_x / \\sigma_x}{ S_y/ \\sigma_y}. \\end{align} This is the ratio of two independent \\(\\chi&#94;2\\) variables, resulting in what is referred to as an \\(F\\) -distributed variable. Like (\\ref{t-var}), we often cannot evaluate (\\ref{f-var}) numerically. Instead, we use a tabulated cdf of the \\(F\\) -distribution to derive confidence intervals for the ratio of the two underlying variances. Aside: The \\(F\\) -distribution arises in the analysis of both ANOVA and linear regression. Note also that the square of a \\(t\\) -distributed variable (Normal over the square root of a \\(\\chi&#94;2\\) variable) is \\(F\\) -distributed. Multivariate Normals Consider a set of jointly-distributed variables \\(x\\) having normal distribution \\begin{align} \\tag{7} p(x) = \\sqrt{\\frac{ \\text{det}(M)} {2 \\pi}} \\exp \\left [- \\frac{1}{2} x&#94;T \\cdot M \\cdot x \\right ], \\end{align} with \\(M\\) a real, symmetric matrix. The correlation of two components is given by \\begin{align}\\tag{8} \\langle x_i x_j \\rangle = M&#94;{-1}_{ij}. \\end{align} Proof: Let \\begin{align}\\tag{9} I = \\int dx \\exp \\left [- \\frac{1}{2} x&#94;T \\cdot M \\cdot x \\right ]. \\end{align} Then, \\begin{align}\\tag{10} \\partial_{M_{ij}} \\log I = -\\frac{1}{2} \\langle x_i x_j \\rangle. \\end{align} We can also evaluate this using the normalization of the integral as \\begin{align} \\nonumber \\partial_{M_{ij}} \\log I &= - \\frac{1}{2} \\sum_{\\alpha} \\frac{1}{\\lambda_{\\alpha}} \\partial_{M_{ij}} \\lambda_{\\alpha} \\\\ \\nonumber &= - \\frac{1}{2} \\sum_{\\alpha} \\frac{1}{\\lambda_{\\alpha}} v_{\\alpha i } v_{\\alpha j} \\\\ &= - \\frac{1}{2} M&#94;{-1}_{ij}. \\tag{11} \\end{align} Here, I've used the result \\( \\partial_{M_{ij}} \\lambda_{\\alpha} = v_{\\alpha i } v_{\\alpha j}\\) . I give a proof of this next. The last line follows by expressing \\(M\\) in terms of its eigenbasis. Comparing the last two lines above gives the result. Consider a matrix \\(M\\) having eigenvalues \\(\\{\\lambda_{\\alpha}\\}\\) . The first derivative of \\(\\lambda_{\\alpha}\\) with respect to \\(M_{ij}\\) is given by \\(v_{\\alpha, i} v_{\\alpha, j}\\) , where \\(v_{\\alpha}\\) is the unit eigenvector corresponding to the eigenvalue \\(\\lambda_{\\alpha}\\) . Proof: The eigenvalue in question is given by \\begin{align} \\tag{12} \\lambda_{\\alpha} = \\sum_{ij} v_{\\alpha i} M_{ij} v_{\\alpha j}. \\end{align} If we differentiate with respect to \\(M_{ab}\\) , say, we obtain \\begin{align} \\nonumber \\partial_{M_{ab}} \\lambda_{\\alpha} &= \\sum_{ij} \\delta_{ia} \\delta_{jb} v_{\\alpha i} v_{\\alpha j} + 2 v_{\\alpha i} M_{ij} \\partial_{M_{ab}} v_{\\alpha j} \\\\ &= v_{\\alpha a} v_{\\alpha b} + 2 \\lambda_{\\alpha} v_{\\alpha } \\cdot \\partial_{M_{ab}} v_{\\alpha } \\tag{13}. \\end{align} The last term above must be zero since the length of \\(v_{\\alpha }\\) is fixed at \\(1\\) . The conditional distribution. Let \\(x\\) be a vector of jointly distributed variables of mean zero and covariance matrix \\(\\Sigma\\) . If we segment the variables into two sets, \\(x_0\\) and \\(x_1\\) , the distribution of \\(x_1\\) at fixed \\(x_0\\) is also normal. Here, we find the mean and covariance. We have \\begin{align} \\label{multivargaucond} \\tag{14} p(x) = \\mathcal{N} e&#94;{-\\frac{1}{2} x_0&#94;T \\Sigma&#94;{-1}_{00} x_0} e&#94;{ -\\frac{1}{2} \\left \\{ x_1&#94;T \\Sigma&#94;{-1}_{11} x_1 + 2 x_1&#94;T \\Sigma&#94;{-1}_{10} x_0 \\right \\} } \\end{align} Here, \\(\\Sigma&#94;{-1}_{ij}\\) refers to the \\(i-j\\) block of the inverse. To complete the square, we write \\begin{align} \\tag{15} x_1&#94;T \\Sigma&#94;{-1}_{11} x_1 + 2 x_1&#94;T \\Sigma&#94;{-1}_{10} x_0 + c = (x_1&#94;T + a) \\Sigma&#94;{-1}_{11} ( x_1 + a). \\end{align} Comparing both sides, we find \\begin{align} \\tag{16} x_1&#94;T \\Sigma&#94;{-1}_{10} x_0 = x_1&#94;T \\Sigma&#94;{-1}_{11} a \\end{align} This holds for any value of \\(x_1&#94;T\\) , so we must have \\begin{align}\\tag{17} a = \\left( \\Sigma&#94;{-1}_{11} \\right)&#94;{-1} \\Sigma&#94;{-1}_{10} x_0 . \\end{align} Plugging the last few results into (\\ref{multivargaucond}), we obtain \\begin{align} \\nonumber p(x) = \\mathcal{N} e&#94;{-\\frac{1}{2} x_0&#94;T \\left( \\Sigma&#94;{-1}_{00} - \\Sigma&#94;{-1}_{01} \\left( \\Sigma&#94;{-1}_{11} \\right)&#94;{-1} \\Sigma&#94;{-1}_{10} \\right) x_0}\\times \\\\ e&#94;{ -\\frac{1}{2} \\left (x_1 + \\left( \\Sigma&#94;{-1}_{11} \\right)&#94;{-1} \\Sigma&#94;{-1}_{10} x_0 \\right) \\Sigma&#94;{-1}_{11} \\left (x_1 + \\left( \\Sigma&#94;{-1}_{11} \\right)&#94;{-1} \\Sigma&#94;{-1}_{10} x_0 \\right) } \\tag{18} \\label{multivargaucondfix} \\end{align} This shows that \\(x_0\\) and \\(x_1 + \\left( \\Sigma&#94;{-1}_{11} \\right)&#94;{-1} \\Sigma&#94;{-1}_{10} x_0\\) are independent. This formula also shows that the average value of \\(x_1\\) shifts at fixed \\(x_0\\) , \\begin{align}\\tag{19} \\langle x_1 \\rangle = \\langle x_1 \\rangle_0 - \\left( \\Sigma&#94;{-1}_{11} \\right)&#94;{-1} \\Sigma&#94;{-1}_{10} x_0. \\end{align} With some work, we can rewrite this as \\begin{align} \\tag{20} \\langle x_1 \\rangle = \\langle x_1 \\rangle_0 + \\Sigma_{10} \\frac{1}{\\Sigma_{00}}x_0. \\end{align} There are two ways to prove this equivalent form holds. One is to make use of the expression for the inverse of a block matrix. The second is to note that the above is simply the linear response to a shift in \\(x_0\\) — see post on linear regression. If we integrate over \\(x_1\\) in (\\ref{multivargaucondfix}), we obtain the distribution for \\(x_0\\) . This is \\begin{align} \\tag{21} p(x_0) = \\mathcal{N} e&#94; {-\\frac{1}{2} x_0&#94;T \\left( \\Sigma&#94;{-1}_{00} - \\Sigma&#94;{-1}_{01} \\left( \\Sigma&#94;{-1}_{11} \\right)&#94;{-1} \\Sigma&#94;{-1}_{10} \\right) x_0} \\end{align} The block-diagonal inverse theorem can be used to show that this is equivalent to \\begin{align} \\tag{22} p(x_0) = \\mathcal{N} e&#94;{ -\\frac{1}{2} x_0&#94;T \\left( \\Sigma_{00} \\right)&#94;{-1} x_0} \\end{align} Another way to see this is correct is to make use of the fact that the coefficient matrix in the normal is the inverse of the correlation matrix. We know that after integrating out the values of \\(x_1\\) , we remain normal, and the covariance matrix will simply be given by that for \\(x_0\\) . The covariance of the CDF transform in multivariate case — a result needed for fitting Gaussian Copulas to data: Let \\(x_1, x_2\\) be jointly distributed Normal variables with covariance matrix \\begin{align} C = \\left( \\begin{array}{cc} 1 & \\rho \\\\ \\rho & 1 \\end{array} \\right) \\end{align} The CDF transform of \\(x_i\\) is defined as \\begin{align} X_i \\equiv \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}&#94;{x_i} \\exp\\left( -\\frac{\\tilde{x}_i&#94;2}{2} \\right)d\\tilde{x}_i. \\end{align} Here, we'll calculate the covariance of \\(X_1\\) and \\(X_2\\) . Up to a constant that does not depend on \\(\\rho\\) , this is given by the integral \\begin{align} J \\equiv &\\frac{1}{\\sqrt{(2 \\pi)&#94;2 \\text{det} C}} \\int d\\vec{x} e&#94;{-\\frac{1}{2} \\vec{x} \\cdot C&#94;{-1} \\cdot \\vec{x}} \\times \\\\ &\\frac{1}{2 \\pi} \\int_{-\\infty}&#94;{x_1}\\int_{-\\infty}&#94;{x_2} e&#94;{-\\frac{\\tilde{x}_1&#94;2}{2} -\\frac{\\tilde{x}_2&#94;2}{2}}d\\tilde{x}_1 d\\tilde{x}_2. \\end{align} To progress, we first write \\begin{align} \\exp\\left( -\\frac{\\tilde{x}_i&#94;2}{2} \\right ) = \\frac{1}{\\sqrt{2\\pi }}\\int \\exp \\left (- \\frac{1}{2} k_i&#94;2 + i k \\tilde{x}_i \\right ) \\end{align} We will substitute this equation into the prior line and then integrate over the \\(\\tilde{x}_i\\) using the result \\begin{align} \\int_{-\\infty}&#94;{x_i} \\exp \\left ( i k \\tilde{x}_i \\right ) d \\tilde{x}_i = \\frac{e&#94;{i k_i x_i}}{i k_i}. \\end{align} This gives \\begin{align} J = &\\frac{-1}{(2 \\pi)&#94;3 \\sqrt{\\text{det} C} } \\int_{k_1} \\int_{k_2} \\frac{e&#94;{-\\frac{1}{2} (k_1&#94;2 + k_2&#94;2)}}{k_1 k_2} \\times \\\\ &\\int d\\vec{x} e&#94;{-\\frac{1}{2} \\vec{x} \\cdot C&#94;{-1} \\cdot \\vec{x} + i \\vec{k} \\cdot \\vec{x}} \\end{align} The integral on \\(\\vec{x}\\) can now be carried out by completing the square. This gives \\begin{align} J = \\frac{1}{(2 \\pi)&#94;2} \\int_{k_1} \\int_{k_2} \\frac{1}{k_1 k_2} \\exp\\left( -\\frac{1}{2} \\vec{k} \\cdot (C + I) \\cdot \\vec{k} \\right) \\end{align} We now differentiate with respect to \\(\\rho\\) to get rid of the \\(k_1 k_2\\) in the denominator. This gives \\begin{align} \\nonumber \\partial_{\\rho} J &= \\frac{1}{(2 \\pi)&#94;2} \\int_{k_1} \\int_{k_2} \\exp\\left( -\\frac{1}{2} \\vec{k} \\cdot (C + I) \\cdot \\vec{k} \\right) \\\\ \\nonumber &= \\frac{1}{2 \\pi } \\frac{1}{\\sqrt{\\text{det}(C + I)}} \\\\ &= \\frac{1}{4 \\pi } \\frac{1}{\\sqrt{1 - \\frac{\\rho&#94;2}{4}}}. \\end{align} The last step is to integrate with respect to \\(\\rho\\) , but we will now switch back to the original goal of calculating the covariance of the two CDF transforms, \\(P\\) , rather than \\(J\\) itself. At \\(\\rho = 0\\) , we must have \\(P(\\rho=0) = 0\\) , since the transforms will also be uncorrelated in this limit. This gives \\begin{align} \\nonumber P &= \\int_0&#94;{\\rho} \\frac{1}{4 \\pi } \\frac{1}{\\sqrt{1 - \\frac{\\rho&#94;2}{4}}} d \\rho \\\\ &= \\frac{1}{2 \\pi } \\sin&#94;{-1} \\left( \\frac{\\rho}{2} \\right). \\tag{23} \\end{align} Using a similar calculation, we find that the diagonal terms of the CDF covariance matrix are \\(1/12\\) . Central Limit Theorem Let \\(x_1, x_2, \\ldots, x_N\\) be IID random variables with an mgf that exists near \\(0\\) . Let \\(E(x_i) = \\mu\\) and \\(\\text{var}(x_i) = \\sigma&#94;2\\) . Then the variable \\(\\frac{\\overline{x} - \\mu}{\\sigma / \\sqrt{N}}\\) approaches standard normal as \\(N \\to \\infty\\) . Proof: Let \\(y_i =\\frac{x_i - \\mu}{\\sigma}\\) . Then, \\begin{align}\\tag{24} \\tilde{y} \\equiv \\frac{\\overline{x} - \\mu}{\\sigma / \\sqrt{N}} = \\frac{1}{\\sqrt{N}} \\sum_i y_i. \\end{align} Using the fact that the mgf of a sum of independent variables is given by the product of their mgfs, the quantity at left is \\begin{align} \\tag{25} m_{\\tilde{y}}(t) = \\left [ m_{y}\\left (\\frac{t}{\\sqrt{N}} \\right) \\right]&#94;n. \\end{align} We now expand the term in brackets using a Taylor series, obtaining \\begin{align} \\tag{26} m_{\\tilde{y}}(t) &= \\left [1 + \\frac{t&#94;2}{2 N } + O\\left (\\frac{t&#94;3}{ N&#94;{3/2}} \\right) \\right]&#94;N \\\\ &\\to \\exp\\left ( \\frac{t&#94;2}{2} \\right), \\end{align} where the latter form is the fixed \\(t\\) limit as \\(N \\to \\infty\\) . This is the mgf for a \\(N(0,1)\\) variable, proving the result. One can get a sense of the accuracy of the normal approximation at fixed \\(N\\) through consideration of higher moments. For example, if we have an even distribution with mgf \\(1 + x&#94;2 /2 + (1 + \\kappa&#94;{\\prime}) x&#94;4 / 8 + \\ldots\\) . Then the mgf for the scaled average above will be \\begin{align}\\nonumber m_{\\tilde{y}} &= \\left [1 + \\frac{t&#94;2}{2 N } + \\frac{(1 + \\kappa&#94;{\\prime}) t&#94;4}{8 N&#94;2 } + \\ldots \\right]&#94;N \\\\ &= 1 + \\frac{t&#94;2}{2} + \\left (1 + \\frac{\\kappa&#94;{\\prime}}{ N } \\right) \\frac{t&#94;4}{8} + \\ldots \\tag{27} \\end{align} This shows that the deviation in the kurtosis away from its \\(N(0,1)\\) value decays like \\(1/N\\) . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"https://efavdb.com/normal-distributions","loc":"https://efavdb.com/normal-distributions"},{"title":"Model AUC depends on test set difficulty","text":"The AUC score is a popular summary statistic that is often used to communicate the performance of a classifier. However, we illustrate here that this score depends not only on the quality of the model in question, but also on the difficulty of the test set considered: If samples are added to a test set that are easily classified, the AUC will go up — even if the model studied has not improved. In general, this behavior implies that isolated, single AUC scores cannot be used to meaningfully qualify a model's performance. Instead, the AUC should be considered a score that is primarily useful for comparing and ranking multiple models — each at a common test set difficulty. Introduction An important challenge associated with building good classification algorithms centers around their optimization: If an adjustment is made to an algorithm, we need a score that will enable us to decide whether or not the change made was an improvement. Many scores are available for this purpose. A sort-of all-purpose score that is quite popular for characterizing binary classifiers is the model AUC score (defined below). The purpose of this post is to illustrate a subtlety associated with the AUC that is not always appreciated: The score depends strongly on the difficulty of the test set used to measure model performance. In particular, if any soft-balls are added to a test set that are easily classified (i.e., are far from any decision boundary), the AUC will increase. This increase does not imply a model improvement. Two key take-aways follow: The AUC is an inappropriate score for comparing models validated on test sets having differing sampling distributions. Therefore, comparing the AUCs of models trained on samples having differing distributions requires care: The training sets can have different distributions, but the test sets must not. A single AUC measure cannot typically be used to meaningfully communicate the quality of a single model (though single model AUC scores are often reported!) The primary utility of the AUC is that it allows one to compare multiple models at fixed test set difficulty: If a model change results in an increase in the AUC at fixed test set distribution, it can often be considered an improvement. We review the definition of the AUC below and then demonstrate the issues alluded to above. The AUC score, reviewed Here, we quickly review the definition of the AUC . This is a score that can be used to quantify the accuracy of a binary classification algorithm on a given test set \\(\\mathcal{S}\\) . The test set consists of a set of feature vector-label pairs of the form \\begin{eqnarray}\\tag{1} \\mathcal{S} = \\{(\\textbf{x}_i, y_i) \\}. \\end{eqnarray} Here, \\(\\textbf{x}_i\\) is the set of features, or predictor variables, for example \\(i\\) and \\(y_i \\in \\{0,1 \\}\\) is the label for example \\(i\\) . A classifier function \\(\\hat{p}_1(\\textbf{x})\\) is one that attempts to guess the value of \\(y_i\\) given only the feature vector \\(\\textbf{x}_i\\) . In particular, the output of the function \\(\\hat{p}_1(\\textbf{x}_i)\\) is an estimate for the probability that the label \\(y_i\\) is equal to \\(1\\) . If the algorithm is confident that the class is \\(1\\) ( \\(0\\) ), the probability returned will be large (small). To characterize model performance, we can set a threshold value of \\(p&#94;*\\) and mark all examples in the test set with \\(\\hat{p}(\\textbf{x}_i) > p&#94;*\\) as being candidates for class one. The fraction of the truly positive examples in \\(\\mathcal{S}\\) marked in this way is referred to as the true-positive rate ( TPR ) at threshold \\(p&#94;*\\) . Similarly, the fraction of negative examples in \\(\\mathcal{S}\\) marked is referred to as the false-positive rate ( FPR ) at threshold \\(p&#94;*\\) . Plotting the TPR against the FPR across all thresholds gives the model's so-called receiver operating characteristic ( ROC ) curve. A hypothetical example is shown at right in blue. The dashed line is just the \\(y=x\\) line, which corresponds to the ROC curve of a random classifier (one returning a uniform random \\(p\\) value each time). Notice that if the threshold is set to \\(p&#94;* = 1\\) , no positive or negative examples will typically be marked as candidates, as this would require one-hundred percent confidence of class \\(1\\) . This means that we can expect an ROC curve to always go through the point \\((0,0)\\) . Similarly, with \\(p&#94;*\\) set to \\(0\\) , all examples should be marked as candidates for class \\(1\\) — and so an ROC curve should also always go through the point \\((1,1)\\) . In between, we hope to see a curve that increases in the TPR direction more quickly than in the FPR direction — since this would imply that the examples the model is most confident about tend to actually be class \\(1\\) examples. In general, the larger the Area Under the ( ROC ) Curve — again, blue at right — the better. We call this area the \" AUC score for the model\" — the topic of this post. AUC sensitivity to test set difficulty To illustrate the sensitivity of the AUC score to test set difficulty, we now consider a toy classification problem: In particular, we consider a set of unit-variance normal distributions, each having a different mean \\(\\mu_i\\) . From each distribution, we will take a single sample \\(x_i\\) . From this, we will attempt to estimate whether or not the corresponding mean satisfies \\(\\mu_i > 0\\) . That is, our training set will take the form \\(\\mathcal{S} = \\{(x_i, \\mu_i)\\}\\) , where \\(x_i \\sim N(\\mu_i, 1)\\) . For different \\(\\mathcal{S}\\) , we will study the AUC of the classifier function, \\begin{eqnarray} \\label{classifier} \\tag{2} \\hat{p}(x) = \\frac{1}{2} (1 + \\text{tanh}(x)) \\end{eqnarray} A plot of this function is shown below. You can see that if any test sample \\(x_i\\) is far to the right (left) of \\(x=0\\) , the model will classify the sample as positive (negative) with high certainty. At intermediate values near the boundary, the estimated probability of being in the positive class lifts in a reasonable way. Notice that if a test example has a mean very close to zero, it will be difficult to classify that example as positive or negative. This is because both positive and negative \\(x\\) samples are equally likely in this case. This means that the model cannot do much better than a random guess for such \\(\\mu\\) . On the other hand, if an example \\(\\mu\\) is selected that is very far from the origin, a single sample \\(x\\) from \\(N(\\mu, 1)\\) will be sufficient to make a very good guess as to whether \\(\\mu > 0\\) . Such examples are hard to get wrong, soft-balls. The impact of adding soft-balls to the test set on the AUC for model (\\ref{classifier}) can be studied by changing the sampling distribution of \\(\\mathcal{S}\\) . The following python snippet takes samples \\(\\mu_i\\) from three distributions — one tight about \\(0\\) (resulting in a very difficult test set), one that is very wide containing many soft-balls that are easily classified, and one that is intermediate. The ROC curves that result from these three cases are shown following the code. The three curves are very different, with the AUC of the soft-ball set very large and that of the tight set close to that of the random classifier. Yet, in each case the model considered was the same — (\\ref{classifier}). How could the AUC have improved?! import numpy as np from sklearn import metrics fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 12 , 3.5 )) SAMPLES = 1000 means_std = 0.1 for means_std in [ 3 , 0.5 , . 001 ]: means = means_std * np . random . randn ( SAMPLES ) x_set = np . random . randn ( samples ) + means predictions = [ classifier ( item ) for item in x_set ] fpr , tpr , thresholds = metrics . roc_curve ( 1 * ( means > 0 ), predictions ) ax1 . plot ( fpr , tpr , label = means_std ) ax1 . plot ( fpr , fpr , 'k--' ) ax2 . plot ( means , 0 * means , '*' , label = means_std ) ax1 . legend ( loc = 'lower right' , shadow = True ) ax2 . legend ( loc = 'lower right' , shadow = True ) ax1 . set_title ( 'TPR versus FPR -- The ROC curve' ) ax2 . set_title ( 'Means sampled for each case' ) The explanation for the differing AUC values above is clear: Consider, for example, the effect of adding soft-ball negatives to \\(\\mathcal{S}\\) . In this case, the model (\\ref{classifier}) will be able to correctly identify almost all true positive examples at a much higher threshold than that where it begins to mis-classify the introduced negative softballs. This means that the ROC curve will now hit a TPR value of \\(1\\) well-before the FPR does (which requires all negatives — including the soft-balls to be mis-classified). Similarly, if many soft-ball positives are added in, these will be easily identified as such well-before any negative examples are mis-classified. This again results in a raising of the ROC curve, and an increase in AUC — all without any improvement in the actual model quality, which we have held fixed. Discussion The toy example considered above illustrates the general point the AUC of a model is really a function of both the model and the test set it is being applied to. Keeping this in mind will help to prevent incorrect interpretations of the AUC . A special case to watch out for in practice is the situation where the AUC changes upon adjustment of the training and testing protocol applied (which can result, for example, from changes to how training examples are collected for the model). If you see such a change occur in your work, be careful to consider whether or not it is possible that the difficulty of the test set has changed in the process. If so, the change in the AUC may not indicate a change in model quality. Because the AUC score of a model can depend highly on the difficulty of the test set, reporting this score alone will generally not provide much insight into the accuracy of the model — which really depends only on performance near the true decision boundary and not on soft-ball performance. Because of this, it may be a good practice to always report AUC scores for optimized models next to those of some fixed baseline model. Comparing the differences of the two AUC scores provides an approximate method for removing the effect of test set difficulty. If you come across an isolated, high AUC score in the wild, remember that this does not imply a good model! A special situation exists where reporting an isolated AUC score for a single model can provide value: The case where the test set employed shares the same distribution as that of the application set (the space where the model will be employed). In this case, performance within the test set directly relates to expected performance during application. However, applying the AUC to situations such as this is not always useful. For example, if the positive class sits within only a small subset of feature space, samples taken from much of the rest of the space will be \"soft-balls\" — examples easily classified as not being in the positive class. Measuring the AUC on test sets over the full feature space in this context will always result in AUC values near one — leaving it difficult to register improvements in the model near the decision boundary through measurement of the AUC . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods, Theory","url":"https://efavdb.com/model-auc-depends-on-test-set-difficulty","loc":"https://efavdb.com/model-auc-depends-on-test-set-difficulty"},{"title":"Simple python to LaTeX parser","text":"We demo a script that converts python numerical commands to LaTeX format. A notebook available on our GitHub page will take this and pretty print the result. Introduction Here, we provide a simple script that accepts numerical python commands in string format and converts them into LaTeX markup. An example input / output follows: s = 'f(x_123, 2) / (2 + 3/(1 + z(np.sqrt((x + 3)/3)))) + np.sqrt(2 ** w) * np.tanh(2 * math.pi* x)' print command_to_latex ( s ) ## output: \\ frac { f \\ left ({ x } _ { 123 } , 2 \\ right )}{ 2 + \\ frac { 3 }{ 1 + z \\ left ( \\ sqrt { \\ frac { x + 3 }{ 3 }} \\ right )}} + \\ sqrt {{ 2 } &#94; { w }} \\ cdot \\ tanh \\ left ( 2 \\ cdot \\ pi \\ cdot x \\ right ) If the output shown here is plugged into a LaTeX editor, we get the following result: \\begin{eqnarray}\\tag{1} \\frac{f \\left ({x}_{123} , 2 \\right )}{2 + \\frac{3}{1 + z \\left ( \\sqrt{\\frac{x + 3}{3}} \\right )}} + \\sqrt{{2}&#94;{w}} \\cdot \\tanh \\left (2 \\cdot \\pi \\cdot x \\right ) \\end{eqnarray} Our Jupyter notebook automatically pretty prints to this form. We provide the script here as it may be useful for two sorts of applications: 1) facilitating write-ups of completed projects, and 2) visualizing typed-up formulas to aid checks of their accuracy. The latter is particularly helpful for lengthy commands, which are often hard to read in python format. We note that the python package sympy also provides a simple command-to-latex parser. However, I have had trouble getting it to output results if any functions appear that have not been defined — we illustrate this issue in the notebook. As usual, our code can be downloaded from our github page here . Code The main code segment follows. The method command_to_latex recursively computes the LaTeX for any combinations of variables grouped together via parentheses. The base case occurs when there are no parentheses left, at which point the method parse_simple_eqn is called, which converts simple commands to LaTeX. The results are then recombined within the recursive method. Additional replacements can be easily added in the appropriate lines below. def parse_simple_eqn ( q ): \"\"\" Return TeX equivalent of a command without parentheses. \"\"\" # Define replacement rules. simple_replacements = [ [ ' ' , '' ], [ '**' , '&#94;' ], [ '*' , ' \\cdot ' ], [ 'math.' , '' ], [ 'np.' , '' ], [ 'pi' , '\\pi' ], [ 'tan' , ' \\t an' ], [ 'cos' , '\\cos' ], [ 'sin' , '\\sin' ], [ 'sec' , '\\sec' ], [ 'csc' , '\\csc' ], ] complex_replacements = [ [ '&#94;' , '{{ {i1} }}&#94;{{ {i2} }}' ], [ '_' , '{{ {i1} }}_{{ {i2} }}' ], [ '/' , ' \\f rac{{ {i1} }}{{ {i2} }}' ], [ 'sqrt' , '\\sqrt{{ {i2} }}' ], ] # Carry out simple replacements for pair in simple_replacements : q = q . replace ( pair [ 0 ], pair [ 1 ]) # Now complex replacements for item in [ '*' , '/' , '+' , '-' , '&#94;' , '_' , ',' , 'sqrt' ]: q = q . replace ( item , ' ' + item + ' ' ) q_split = q . split () for index , item in enumerate ( q_split ): for pair in complex_replacements : if item == pair [ 0 ]: if item == 'sqrt' : match_str = \" \" . join ( q_split [ index : index + 2 ]) else : match_str = \" \" . join ( q_split [ index - 1 : index + 2 ]) q = q . replace ( match_str , pair [ 1 ] . format ( i1 = q_split [ index - 1 ], i2 = q_split [ index + 1 ])) return q def command_to_latex ( q , index = 0 ): \"\"\" Recursively eliminate parentheses, then apply parse_simple_eqn.\"\"\" open_index , close_index = - 1 , - 1 for q_index , i in enumerate ( q ): if i == '(' : open_index = q_index elif i == ')' : close_index = q_index break if open_index != - 1 : o = q [: open_index ] + '@' + str ( index ) + q [ close_index + 1 :] m = q [ open_index + 1 : close_index ] o_tex = command_to_latex ( o , index + 1 ) m_tex = command_to_latex ( m , index + 1 ) # Clean up redundant parentheses at recombination r_index = o_tex . find ( '@' + str ( index )) if o_tex [ r_index - 1 ] == '{' : return o_tex . replace ( '@' + str ( index ), m_tex ) else : return o_tex . replace ( '@' + str ( index ), ' \\\\ left (' + m_tex + ' \\\\ right )' ) else : return parse_simple_eqn ( q ) That's it! if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Programming","url":"https://efavdb.com/simple-python-to-latex-parser","loc":"https://efavdb.com/simple-python-to-latex-parser"},{"title":"Deep reinforcement learning, battleship","text":"Here, we provide a brief introduction to reinforcement learning ( RL ) — a general technique for training programs to play games efficiently. Our aim is to explain its practical implementation: We cover some basic theory and then walk through a minimal python program that trains a neural network to play the game battleship. Introduction Reinforcement learning ( RL ) techniques are methods that can be used to teach algorithms to play games efficiently. Like supervised machine-learning ( ML ) methods, RL algorithms learn from data — in this case, past game play data. However, whereas supervised-learning algorithms train only on data that is already available, RL addresses the challenge of performing well while still in the process of collecting data. In particular, we seek design principles that Allow programs to identify good strategies from past examples, Enable fast learning of new strategies through continued game play. The reason we particularly want our algorithms to learn fast here is that RL is most fruitfully applied in contexts where training data is limited — or where the space of strategies is so large that it would be difficult to explore exhaustively. It is in these regimes that supervised techniques have trouble and RL methods shine. In this post, we review one general RL training procedure: The policy-gradient, deep-learning scheme. We review the theory behind this approach in the next section. Following that, we walk through a simple python implementation that trains a neural network to play the game battleship. Our python code can be downloaded from our github page, here . It requires the jupyter, tensorflow, numpy, and matplotlib packages. Policy-gradient, deep RL Policy-gradient, deep RL algorithms consist of two main components: A policy network and a rewards function. We detail these two below and then describe how they work together to train good models. The policy network The policy for a given deep RL algorithm is a neural network that maps state values \\(s\\) to probabilities for given game actions \\(a\\) . In other words, the input layer of the network accepts a numerical encoding of the environment — the state of the game at a particular moment. When this input is fed through the network, the values at the output layer correspond to the log probabilities that each of the actions available to us is optimal — one output node is present for each possible action that we can choose. Note that if we knew with certainty which move we should take, only one output node would have a finite probability. However, if our network is uncertain which action is optimal, more than one output node will have finite weight. To illustrate the above, we present a diagram of the network used in our battleship program below. (For a review of the rules of battleship, see footnote [1].) For simplicity, we work with a 1-d battleship grid. We then encode our current knowledge of the environment using one input neuron for each of our opponent's grid positions. In particular, we use the following encoding for each neuron / index: \\begin{align} \\label{input} \\tag{1} x_{0,i} = \\begin{cases} -1 & \\text{Have not yet bombed $i$} \\\\ \\ 0 & \\text{Have bombed $i$, no ship} \\\\ +1 & \\text{Have bombed $i$, ship present}. \\end{cases} \\end{align} In our example figure below, we have five input neurons, so the board is of size five. The first three neurons have value \\(-1\\) implying we have not yet bombed those grid points. Finally, the last two are \\(+1\\) and \\(0\\) , respectively, implying that a ship does sit at the fourth site, but not at the fifth. Note that in the output layer of the policy network shown, the first three values are labeled with log probabilities. These values correspond to the probabilities that we should next bomb each of these indices, respectively. We cannot re-bomb the fourth and fifth grid points, so although the network may output some values to these neurons, we'll ignore them. Before moving on, we note that the reason we use a neural network for our policy is to allow for efficient generalization: For games like Go that have a very large number of states, it is not feasible to collect data on every possible board position. This is exactly the context where ML algorithms excel — generalizing from past observations to make good predictions for new situations. In order to keep our focus on RL , we won't review how ML algorithms work in this post (however, you can check out our archives section for relevant primers). Instead we simply note that — utilizing these tools — we can get good performance by training only on a representative subset of games — allowing us to avoid study of the full set, which can be much larger. The rewards function To train an RL algorithm, we must carry out an iterative game play / scoring process: We play games according to our current policy, selecting moves with frequencies proportional to the probabilities output by the network. If the actions taken resulted in good outcomes, we want to strengthen the probability of those actions going forward. The rewards function is the tool we use to formally score our outcomes in past games — we will encourage our algorithm to try to maximize this quantity during game play. In effect, it is a hyper-parameter for the RL algorithm: many different functions could be used, each resulting in different learning characteristics. For our battleship program, we have used the function \\begin{align} \\label{rewards} \\tag{2} r(a;t_0) = \\sum_{t \\geq t_0} \\left ( h(t) - \\overline{h(t)} \\right) (0.5)&#94;{t-t0} \\end{align} Given a completed game log, this function looks at the action \\(a\\) taken at time \\(t_0\\) and returns a weighted sum of hit values \\(h(t)\\) for this and all future steps in the game. Here, \\(h(t)\\) is \\(1\\) if we had a hit at step \\(t\\) and is \\(0\\) otherwise. In arriving at (\\ref{rewards}), we admit that we did not carry out a careful search over the set of all possible rewards functions. However, we have confirmed that this choice results in good game play, and it is well-motivated: In particular, we note that the weighting term \\((0.5)&#94;{t-t0}\\) serves to strongly incentivize a hit on the current move (we get a reward of \\(1\\) for a hit at \\(t_0\\) ), but a hit at \\((t_0 + 1)\\) also rewards the action at \\(t_0\\) — with value \\(0.5\\) . Similarly, a hit at \\((t_0 + 2)\\) rewards \\(0.25\\) , etc. This weighted look-ahead aspect of (\\ref{rewards}) serves to encourage efficient exploration of the board: It forces the program to care about moves that will enable future hits. The other ingredient of note present in (\\ref{rewards}) is the subtraction of \\(\\overline{h(t)}\\) . This is the expected rewards that a random network would obtain. By pulling this out, we only reward our network if it is outperforming random choices — this results in a net speed-up of the learning process. Stochastic gradient descent In order to train our algorithm to maximize captured rewards during game play, we apply gradient descent. To carry this out, we imagine allowing our network parameters \\(\\theta\\) to vary at some particular step in the game. Averaging over all possible actions, the gradient of the expected rewards is then formally, \\begin{align} \\nonumber \\partial_{\\theta} \\langle r(a \\vert s) \\rangle &\\equiv & \\partial_{\\theta} \\int p(a \\vert \\theta, s) r(a \\vert s) da \\\\ \\nonumber &=& \\int p(a \\vert \\theta, s) r(a \\vert s) \\partial_{\\theta} \\log \\left ( p(a \\vert \\theta, s) \\right) da \\\\ &\\equiv & \\langle r(a \\vert s) \\partial_{\\theta} \\log \\left ( p(a \\vert \\theta, s) \\right) \\rangle. \\tag{3} \\label{formal_ev} \\end{align} Here, the \\(p(a)\\) values are the action probability outputs of our network. Unfortunately, we usually can't evaluate the last line above. However, what we can do is approximate it using a sampled value: We simply play a game with our current network, then replace the expected value above by the reward actually captured on the \\(i\\) -th move, \\begin{align} \\hat{g}_i = r(a_i) \\nabla_{\\theta} \\log p(a_i \\vert s_i, \\theta). \\tag{4} \\label{estimator} \\end{align} Here, \\(a_i\\) is the action that was taken, \\(r(a_i)\\) is reward that was captured, and the derivative of the logarithm shown can be evaluated via back-propagation (aside for those experienced with neural networks: this is the derivative of the cross-entropy loss function that would apply if you treated the event like a supervised-learning training example — with the selected action \\(a_i\\) taken as the label). The function \\(\\hat{g}_i\\) provides a noisy estimate of the desired gradient, but taking many steps will result in a \"stochastic\" gradient descent, on average pushing us towards correct rewards maximization. Summary of the training process In summary, then, RL training proceeds iteratively: To initialize an iterative step, we first play a game with our current policy network, selecting moves stochastically according to the network's output. After the game is complete, we then score our outcome by evaluating the rewards captured on each move — for example, in the battleship game we use (\\ref{rewards}). Once this is done, we then estimate the gradient of the rewards function using (\\ref{estimator}). Finally, we update the network parameters, moving \\(\\theta \\to \\theta + \\alpha \\sum \\hat{g}_i\\) , with \\(\\alpha\\) a small step size parameter. To continue, we then play a new game with the updated network, etc. To see that this process does, in fact, encourage actions that have resulted in good outcomes during training, note that (\\ref{estimator}) is proportional to the rewards captured at the step \\(i\\) . Consequently, when we adjust our parameters in the direction of (\\ref{estimator}), we will strongly encourage those actions that have resulted in large rewards outcomes. Further, those moves with negative rewards are actually suppressed. In this way, over time, the network will learn to examine the system and suggest those moves that will likely produce the best outcomes. That's it for the basics of deep, policy-gradient RL . We now turn to our python example, battleship. Python code walkthrough — battleship RL Load the needed packages. import tensorflow as tf import numpy as np % matplotlib inline import pylab Define our network — a fully connected, three layer system. The code below is mostly tensorflow boilerplate that can be picked up by going through their first tutorials. The one unusual thing is that we have our learning rate in (26) set to the placeholder value (9). This will allow us to vary our step sizes with observed rewards captured below. BOARD_SIZE = 10 SHIP_SIZE = 3 hidden_units = BOARD_SIZE output_units = BOARD_SIZE input_positions = tf . placeholder ( tf . float32 , shape = ( 1 , BOARD_SIZE )) labels = tf . placeholder ( tf . int64 ) learning_rate = tf . placeholder ( tf . float32 , shape = []) # Generate hidden layer W1 = tf . Variable ( tf . truncated_normal ([ BOARD_SIZE , hidden_units ], stddev = 0.1 / np . sqrt ( float ( BOARD_SIZE )))) b1 = tf . Variable ( tf . zeros ([ 1 , hidden_units ])) h1 = tf . tanh ( tf . matmul ( input_positions , W1 ) + b1 ) # Second layer -- linear classifier for action logits W2 = tf . Variable ( tf . truncated_normal ([ hidden_units , output_units ], stddev = 0.1 / np . sqrt ( float ( hidden_units )))) b2 = tf . Variable ( tf . zeros ([ 1 , output_units ])) logits = tf . matmul ( h1 , W2 ) + b2 probabilities = tf . nn . softmax ( logits ) init = tf . initialize_all_variables () cross_entropy = tf . nn . sparse_softmax_cross_entropy_with_logits ( logits , labels , name = 'xentropy' ) train_step = tf . train . GradientDescentOptimizer ( learning_rate = learning_rate ) . minimize ( cross_entropy ) # Start TF session sess = tf . Session () sess . run ( init ) Next, we define a method that will allow us to play a game using our network. The TRAINING variable specifies whether or not to take the optimal moves or to select moves stochastically. Note that the method returns a set of logs that record the game proceedings. These are needed for training. TRAINING = True def play_game ( training = TRAINING ): \"\"\" Play game of battleship using network.\"\"\" # Select random location for ship ship_left = np . random . randint ( BOARD_SIZE - SHIP_SIZE + 1 ) ship_positions = set ( range ( ship_left , ship_left + SHIP_SIZE )) # Initialize logs for game board_position_log = [] action_log = [] hit_log = [] # Play through game current_board = [[ - 1 for i in range ( BOARD_SIZE )]] while sum ( hit_log ) < SHIP_SIZE : board_position_log . append ([[ i for i in current_board [ 0 ]]]) probs = sess . run ([ probabilities ], feed_dict = { input_positions : current_board })[ 0 ][ 0 ] probs = [ p * ( index not in action_log ) for index , p in enumerate ( probs )] probs = [ p / sum ( probs ) for p in probs ] if training == True : bomb_index = np . random . choice ( BOARD_SIZE , p = probs ) else : bomb_index = np . argmax ( probs ) # update board, logs hit_log . append ( 1 * ( bomb_index in ship_positions )) current_board [ 0 ][ bomb_index ] = 1 * ( bomb_index in ship_positions ) action_log . append ( bomb_index ) return board_position_log , action_log , hit_log Our implementation of the rewards function (\\ref{rewards}): def rewards_calculator ( hit_log , gamma = 0.5 ): \"\"\" Discounted sum of future hits over trajectory\"\"\" hit_log_weighted = [( item - float ( SHIP_SIZE - sum ( hit_log [: index ])) / float ( BOARD_SIZE - index )) * ( gamma ** index ) for index , item in enumerate ( hit_log )] return [(( gamma ) ** ( - i )) * sum ( hit_log_weighted [ i :]) for i in range ( len ( hit_log ))] Finally, our training loop. Here, we iteratively play through many games, scoring after each game, then adjusting parameters — setting the placeholder learning rate equal to ALPHA times the rewards captured. game_lengths = [] TRAINING = True # Boolean specifies training mode ALPHA = 0.06 # step size for game in range ( 10000 ): board_position_log , action_log , hit_log = play_game ( training = TRAINING ) game_lengths . append ( len ( action_log )) rewards_log = rewards_calculator ( hit_log ) for reward , current_board , action in zip ( rewards_log , board_position_log , action_log ): # Take step along gradient if TRAINING : sess . run ([ train_step ], feed_dict = { input_positions : current_board , labels :[ action ], learning_rate : ALPHA * reward }) Running this last cell, we see that the training works! The following is an example trace from the play_game() method, with the variable TRAINING set to False. This illustrates an intelligent move selection process. # Example game trace output ([[[ - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 ]], [[ - 1 , - 1 , 0 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 ]], [[ - 1 , - 1 , 0 , - 1 , - 1 , 0 , - 1 , - 1 , - 1 , - 1 ]], [[ - 1 , - 1 , 0 , - 1 , - 1 , 0 , 1 , - 1 , - 1 , - 1 ]], [[ - 1 , - 1 , 0 , - 1 , - 1 , 0 , 1 , 1 , - 1 , - 1 ]]], [ 2 , 5 , 6 , 7 , 8 ], [ 0 , 0 , 1 , 1 , 1 ]) Here, the first five lines are the board encodings that the network was fed each step — using (\\ref{input}). The second to last row presents the sequential grid selections that were chosen. Finally, the last row is the hit log. Notice that the first two moves nicely sample different regions of the board. After this, a hit was recorded at \\(6\\) . The algorithm then intelligently selects \\(7\\) and \\(8\\) , which it can infer must be the final locations of the ship. The plot below provides further characterization of the learning process. This shows the running average game length (steps required to fully bomb ship) versus training epoch. The program learns the basics quite quickly, then continues to gradually improve over time [2]. Summary In this post, we have covered a variant of RL — namely, the policy-gradient, deep RL scheme. This is a method that typically defaults to the currently best-known strategy, but occasionally samples from other approaches, ultimately resulting in an iterative improvement in policy. The two main ingredients here are the policy network and the rewards function. Although network architecture design is usually the place where most of the thinking is involved in supervised learning, it is the rewards function that typically requires the most thought in the RL context. A good choice should be as local in time as possible, so as to facilitate training (distant forecast dependence will result in a slow learning process). However, the rewards function should also directly attack the ultimate end of the process (\"winning\" the game — encouragement of side quests that aren't necessary can often occur if care is not taken). Balancing these two competing demands can be a challenge, and rewards function design is therefore something of an art form. Our brief introduction here was intended only to illustrate the gist of how RL is carried out in practice. For further details, we can recommend two resources: the text book by Sutton and Barto [3] and a recent talk by John Schulman [4]. Footnotes and references [1] Game rules: Battleship is a two-player game. Both players begin with a finite regular grid of positions — hidden from their opponent — and a set of \"ships\". Each player receives the same quantity of each type of ship. At the start of the game, each player places the ships on their grid in whatever locations they like, subject to some constraints: A ship of length 2, say, must occupy two contiguous indices on the board, and no two ships can occupy the same grid location. Once placed, the ships are fixed in position for the remainder of the game. At this point, game play begins, with the goal being to sink the opponent ships. The locations of the enemy ships are initially unknown because we cannot see the opponent's grid. To find the ships, one \"bombs\" indices on the enemy grid — with bombing occurs in turns. When an opponent index is bombed, the opponent must truthfully state whether or not a ship was located at the index bombed. Whoever succeeds in bombing all their opponent's occupied indices first wins the game. Therefore, the problem reduces to finding the enemy ship indices as quickly as possible. [2] One of my colleagues ( HC ) has suggested that the program likely begins to overfit at some point. However, the 1-d version of the game has so few possible ship locations that characterization of this effect via a training and test set split does not seem appropriate. However, this approach could work were we to move to higher dimensions and introduce multiple ships. [3] Sutton and Barto, (2016). \"Reinforcement Learning: An Introduction\". Text site, here . [4] John Schulman, (2016). \"Bay Area Deep Learning School\". Youtube recording of talk available here . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods, Theory","url":"https://efavdb.com/battleship","loc":"https://efavdb.com/battleship"},{"title":"GPU -accelerated Theano & Keras with Windows 10","text":"There are many tutorials with directions for how to use your Nvidia graphics card for GPU -accelerated Theano and Keras for Linux, but there is only limited information out there for you if you want to set everything up with Windows and the current CUDA toolkit. This is a shame however because there are a large number of computers out there with very nice video cards that are only running windows, and it is not always practical to use a Virtual Machine, or Dual-Boot. So for today's post we will go over how to get everything running in Windows 10 by saving you all the trial and error I went through. (All of these steps should also work in earlier versions of Windows). Dependencies Before getting started, make sure you have the following: NVIDIA card that supports CUDA ( link ) Python 2.7 ( Anaconda preferably) Compilers for C/C++ CUDA 7.5 GCC for code generated by Theano Setup Visual Studio 2013 Community Edition Update 4 First, go and download the installer for Visual Studio 2013 Community Edition Update 4 . You can not use the 2015 version because it is still not supported by CUDA . When installing, there is no need to install any of the optional packages. When you are done add the compiler, C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\ VC \\bin , to your windows path. To add something to your windows path go to System, and then Advanced system settings. System → Advanced system settings → Environment Variables → Path. CUDA Next, go the NVIDIA 's website and download the CUDA 7.5 toolkit. Select the right version for you computer. When you are installing it, make sure to pick custom install if you don't want your video card drivers to be overwritten with the version that comes with the toolkit, which are often out of date. If it turns out that your version of the drivers are older than what comes with the toolkit,then there is no harm in updating your drivers, otherwise only pick the three boxes starting with CUDA . GCC The last thing we need to do GCC compiler, I recommend TDM -gcc . Install the 64 bit version, and then add the compiler to your windows path, the install has an option to do that for you automatically if you wish. To make sure that everything is working at this point, run the the following command on the command line (cmd.exe) . If if finds the path for everything you are good to go. where gcc where cl where nvcc where cudafe where cudafe++ Theano and Keras At this point it is easy to install Theano and Keras, just you pip (or conda and pip)! conda install mingw libpython pip install theano pip install keras After installing the python libraries you need to tell Theano to use the GPU instead of the CPU . A lot of older posts would have you set this in the system environment, but it is possible to make a config file in your home directory named \" .theanorc.txt \" instead. This also makes it easy to switch out config files. Inside the file put the following: [global] device = gpu floatX = float32 [nvcc] compiler_bindir = C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\bin Lastly, set up the Keras config file ~/.keras/keras.json . If you haven't started Keras yet, the folder and file won't be there but you can create it. Inside the config put the following. { \"image_dim_ordering\": \"tf\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"theano\" } Testing Theano with GPU Using the following python code, check if your installation of Theano is using your GPU . from theano import function , config , shared , sandbox import theano.tensor as T import numpy import time vlen = 10 * 30 * 768 # 10 x #cores x # threads per core iters = 1000 rng = numpy . random . RandomState ( 22 ) x = shared ( numpy . asarray ( rng . rand ( vlen ), config . floatX )) f = function ([], T . exp ( x )) print ( f . maker . fgraph . toposort ()) t0 = time . time () for i in range ( iters ): r = f () t1 = time . time () print ( \"Looping %d times took %f seconds\" % ( iters , t1 - t0 )) print ( \"Result is %s \" % ( r ,)) if numpy . any ([ isinstance ( x . op , T . Elemwise ) for x in f . maker . fgraph . toposort ()]): print ( 'Used the cpu' ) else : print ( 'Used the gpu' ) Testing Keras with GPU This code will make sure that everything is working and train a model on some random data. The first time might take a little longer because it the software needs to do some compiling. from keras.models import Sequential from keras.layers import Dense , Activation # for a single-input model with 2 classes (binary): model = Sequential () model . add ( Dense ( 1 , input_dim = 784 , activation = 'sigmoid' )) model . compile ( optimizer = 'rmsprop' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ]) # generate dummy data import numpy as np data = np . random . random (( 1000 , 784 )) labels = np . random . randint ( 2 , size = ( 1000 , 1 )) # train the model, iterating on the data in batches # of 32 samples model . fit ( data , labels , nb_epoch = 10 , batch_size = 32 ) If everything works you will see something like this! Now you can start playing with neural networks using your GPU !","tags":"Methods, Tools","url":"https://efavdb.com/gpu-accelerated-theano-keras-with-windows-10","loc":"https://efavdb.com/gpu-accelerated-theano-keras-with-windows-10"},{"title":"Hyperparameter sample-size dependence","text":"Here, we briefly review a subtlety associated with machine-learning model selection: the fact that the optimal hyperparameters for a model can vary with training set size, \\(N.\\) To illustrate this point, we derive expressions for the optimal strength for both \\(L_1\\) and \\(L_2\\) regularization in single-variable models. We find that the optimal \\(L_2\\) approaches a finite constant as \\(N\\) increases, but that the optimal \\(L_1\\) decays exponentially fast with \\(N.\\) Sensitive dependence on \\(N\\) such as this should be carefully extrapolated out when optimizing mission-critical models. Introduction There are two steps one must carry out to fit a machine-learning model. First, a specific model form and cost function must be selected, and second the model must be fit to the data. The first of these steps is often treated by making use of a training-test data split: One trains a set of candidate models to a fraction of the available data and then validates their performance using a hold-out, test set. The model that performs best on the latter is then selected for production. Our purpose here is to highlight a subtlety to watch out for when carrying out an optimization as above: the fact that the optimal model can depend sensitively on training set size \\(N\\) . This observation suggests that the training-test split paradigm must sometimes be applied with care: Because a subsample is used for training in the first, selection step, the model identified as optimal there may not be best when training on the full data set. To illustrate the above points, our main effort here is to present some toy examples where the optimal hyperparameters can be characterized exactly: We derive the optimal \\(L_1\\) and \\(L_2\\) regularization strength for models having only a single variable. These examples illustrate two opposite limits: The latter approaches a finite constant as \\(N\\) increases, but the former varies exponentially with \\(N\\) . This shows that strong \\(N\\) -dependence can sometimes occur, but is not necessarily always an issue. In practice, a simple way to check for sensitivity is to vary the size of your training set during model selection: If a strong dependence is observed, care should be taken during the final extrapolation. We now walk through our two examples. \\(L_2\\) optimization We start off by positing that we have a method for generating a Bayesian posterior for a parameter \\(\\theta\\) that is a function of a vector of \\(N\\) random samples \\(\\textbf{x}\\) . To simplify our discussion, we assume that — given a flat prior — this is unbiased and normal with variance \\(\\sigma&#94;2\\) . We write \\(\\theta_0 \\equiv \\theta_0(\\textbf{x})\\) for the maximum a posteriori ( MAP ) value under the flat prior. With the introduction of an \\(L_2\\) prior, the posterior for \\(\\theta\\) is then $$\\tag{1} P\\left(\\theta \\vert \\theta_0(\\textbf{x})\\right) \\propto \\exp\\left( - \\frac{(\\theta - \\theta_0)&#94;2}{2 \\sigma&#94;2} - \\Lambda \\theta&#94;2 \\right). $$ Setting the derivative of the above to zero, the point-estimate, MAP is given by $$\\tag{2} \\hat{\\theta} = \\frac{\\theta_0}{1 + 2 \\Lambda \\sigma&#94;2}. $$ The average squared error of this estimate is obtained by averaging over the possible \\(\\theta_0\\) values. Our assumptions above imply that \\(\\theta_0\\) is normal about the true parameter value, \\(\\theta_*\\) , so we have \\begin{eqnarray} \\langle (\\hat{\\theta} - \\theta_*)&#94;2 \\rangle &\\equiv& \\int_{\\infty}&#94;{\\infty} \\frac{1}{\\sqrt{2 \\pi \\sigma&#94;2}} e&#94;{ - \\frac{(\\theta_0 - \\theta_*)&#94;2}{2 \\sigma&#94;2}} \\left ( \\frac{\\theta_0}{1 + 2 \\Lambda \\sigma&#94;2} - \\theta_* \\right)&#94;2 d \\theta_0 \\\\ &=& \\frac{ 4 \\Lambda&#94;2 \\sigma&#94;4 \\theta_*&#94;2 }{(1 + 2 \\Lambda \\sigma&#94;2 )&#94;2} + \\frac{\\sigma&#94;2}{\\left( 1 + 2 \\Lambda \\sigma&#94;2 \\right)&#94;2}. \\tag{3} \\label{error} \\end{eqnarray} The optimal \\(\\Lambda\\) is readily obtained by minimizing this average error. This gives, $$ \\label{result} \\Lambda = \\frac{1}{2 \\theta_*&#94;2}, \\tag{4} $$ a constant, independent of sample size. The mean squared error with this choice is obtained by plugging (\\ref{result}) into (\\ref{error}). This gives $$ \\langle (\\hat{\\theta} - \\theta_*)&#94;2 \\rangle = \\frac{\\sigma&#94;2}{1 + \\sigma&#94;2 / \\theta_*&#94;2}. \\tag{5} $$ Notice that this is strictly less than \\(\\sigma&#94;2\\) — the variance one would get without regularization — and that the benefit is largest when \\(\\sigma&#94;2 \\gg \\theta_*&#94;2\\) . That is, \\(L_2\\) regularization is most effective when \\(\\theta_*\\) is hard to differentiate from zero — an intuitive result! \\(L_1\\) optimization The analysis for \\(L_1\\) optimization is similar to the above, but slightly more involved. We go through it quickly. The posterior with an \\(L_1\\) prior is given by $$ \\tag{6} P\\left(\\theta \\vert \\theta_0(\\textbf{x})\\right) \\propto \\exp\\left( - \\frac{(\\theta - \\theta_0)&#94;2}{2 \\sigma&#94;2} - \\Lambda \\vert \\theta \\vert \\right). $$ Assuming for simplicity that \\(\\hat{\\theta} > 0\\) , the MAP value is now $$ \\tag{7} \\hat{\\theta} = \\begin{cases} \\theta_0 - \\Lambda \\sigma&#94;2 & \\text{if } \\theta_0 - \\Lambda \\sigma&#94;2 > 0 \\\\ 0 & \\text{else}. \\end{cases} $$ The mean squared error of the estimator is $$ \\tag{8} \\langle (\\hat{\\theta} - \\theta_*)&#94;2 \\rangle \\equiv \\int \\frac{1}{\\sqrt{2 \\pi \\sigma&#94;2}} e&#94;{ - \\frac{(\\theta_0 - \\theta_*)&#94;2}{2 \\sigma&#94;2}} \\left ( \\hat{\\theta} - \\theta_* \\right)&#94;2 d \\theta_0. $$ This can be evaluated in terms of error functions. The optimal value of \\(\\Lambda\\) is obtained by differentiating the above. Doing this, one finds that it satisfies the equation $$ \\tag{9} e&#94;{ - \\frac{(\\tilde{\\Lambda}- \\tilde{\\theta_*})&#94;2}{2} } + \\sqrt{\\frac{\\pi}{2}} \\tilde{\\Lambda} \\ \\text{erfc}\\left( \\frac{\\tilde{\\Lambda} - \\tilde{\\theta_*}}{\\sqrt{2}} \\right ) = 0, $$ where \\(\\tilde{\\Lambda} \\equiv \\sigma \\Lambda\\) and \\(\\tilde{\\theta_*} \\equiv \\theta_* / \\sigma\\) . In general, the equation above must be solved numerically. However, in the case where \\(\\theta_* \\gg \\sigma\\) — relevant when \\(N\\) is large — we can obtain a clean asymptotic solution. In this case, we have \\(\\tilde{\\theta_*} \\gg 1\\) and we expect \\(\\Lambda\\) small. This implies that the above equation can be approximated as $$ \\tag{10} e&#94;{ - \\frac{\\tilde{\\theta_*}&#94;2}{2} } - \\sqrt{2 \\pi} \\tilde{\\Lambda} \\sim 0. $$ Solving gives \\begin{eqnarray} \\tag{11} \\Lambda \\sim \\frac{1}{\\sqrt{2 \\pi \\sigma&#94;2}} e&#94;{ - \\frac{\\theta_*&#94;2}{2 \\sigma&#94;2}} \\sim \\frac{\\sqrt{N}}{\\sqrt{2 \\pi \\sigma_1&#94;2}} e&#94;{ - \\frac{N \\theta_*&#94;2}{2 \\sigma_1&#94;2}}. \\end{eqnarray} Here, in the last line we have made the \\(N\\) -dependence explicit, writing \\(\\sigma&#94;2 = \\sigma_1&#94;2 / N\\) — a form that follows when our samples \\(\\textbf{x}\\) are independent. Whereas the optimal \\(L_2\\) regularization strength approaches a constant, our result here shows that the optimal \\(L_1\\) strength decays exponentially to zero as \\(N\\) increases. Summary The subtlety that we have discussed here is likely already familiar to those with significant applied modeling experience: optimal model hyperparameters can vary with training set size. However, the two toy examples we have presented are interesting in that they allow for this \\(N\\) dependence to be derived explicitly. Interestingly, we have found that the MSE -minimizing \\(L_2\\) regularization remains finite, even at large training set size, but the optimal \\(L_1\\) regularization goes to zero in this same limit. For small and medium \\(N\\) , this exponential dependence represents a strong sensitivity to \\(N\\) — one that must be carefully taken into account when extrapolating to the full training set. One can imagine many other situations where hyperparameters vary strongly with \\(N\\) . For example, very complex systems may allow for ever-increasing model complexity as more data becomes available. Again, in practice, the most straightforward method to check for this is to vary the size of the training set during model selection. If strong dependence is observed, this should be extrapolated out to obtain the truly optimal model for production. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"https://efavdb.com/model-selection","loc":"https://efavdb.com/model-selection"},{"title":"Bayesian Statistics: MCMC","text":"We review the Metropolis algorithm — a simple Markov Chain Monte Carlo ( MCMC ) sampling method — and its application to estimating posteriors in Bayesian statistics. A simple python example is provided. Introduction One of the central aims of statistics is to identify good methods for fitting models to data. One way to do this is through the use of Bayes' rule: If \\(\\textbf{x}\\) is a vector of \\(k\\) samples from a distribution and \\(\\textbf{z}\\) is a vector of model parameters, Bayes' rule gives \\begin{align} \\tag{1} \\label{Bayes} p(\\textbf{z} \\vert \\textbf{x}) = \\frac{p(\\textbf{x} \\vert \\textbf{z}) p(\\textbf{z})}{p(\\textbf{x})}. \\end{align} Here, the probability at left, \\(p(\\textbf{z} \\vert \\textbf{x})\\) — the \"posterior\" — is a function that tells us how likely it is that the underlying true parameter values are \\(\\textbf{z}\\) , given the information provided by our observations \\(\\textbf{x}\\) . Notice that if we could solve for this function, we would be able to identify which parameter values are most likely — those that are good candidates for a fit. We could also use the posterior's variance to quantify how uncertain we are about the true, underlying parameter values. Bayes' rule gives us a method for evaluating the posterior — now our goal: We need only evaluate the right side of (\\ref{Bayes}). The quantities shown there are \\(p(\\textbf{x} \\vert \\textbf{z})\\) — This is the probability of seeing \\(\\textbf{x}\\) at fixed parameter values \\(\\textbf{z}\\) . Note that if the model is specified, we can often immediately write this part down. For example, if we have a Normal distribution model, specifying \\(\\textbf{z}\\) means that we have specified the Normal's mean and variance. Given these, we can say how likely it is to observe any \\(\\textbf{x}\\) . \\(p(\\textbf{z})\\) — the \"prior\". This is something we insert by hand before taking any data. We choose its form so that it covers the values we expect are reasonable for the parameters in question. \\(p(\\textbf{x})\\) — the denominator. Notice that this doesn't depend on \\(\\textbf{z}\\) , and so represents a normalization constant for the posterior. It turns out that the last term above can sometimes be difficult to evaluate analytically, and so we must often resort to numerical methods for estimating the posterior. Monte Carlo sampling is one of the most common approaches taken for doing this. The idea behind Monte Carlo is to take many samples \\(\\{\\textbf{z}_i\\}\\) from the posterior (\\ref{Bayes}). Once these are obtained, we can approximate population averages by averages over the samples. For example, the true posterior average \\(\\langle\\textbf{z} \\rangle \\equiv \\int \\textbf{z} p(\\textbf{z} \\vert \\textbf{x}) d \\textbf{z}\\) can be approximated by \\(\\overline{\\textbf{z}} \\equiv \\frac{1}{N}\\sum_i \\textbf{z}_i\\) , the sample average. By the law of large numbers, the sample averages are guaranteed to approach the distribution averages as \\(N \\to \\infty\\) . This means that Monte Carlo can always be used to obtain very accurate parameter estimates, provided we take \\(N\\) sufficiently large — and that we can find a convenient way to sample from the posterior. In this post, we review one simple variant of Monte Carlo that allows for posterior sampling: the Metropolis algorithm. Metropolis Algorithm Iterative Procedure Metropolis is an iterative, try-accept algorithm. We initialize the algorithm by selecting a parameter vector \\(\\textbf{z}\\) at random. Following this, we repeatedly carry out the following two steps to obtain additional posterior samples: Identify a next candidate sample \\(\\textbf{z}_j\\) via some random process. This candidate selection step can be informed by the current sample's position, \\(\\textbf{z}_i\\) . For example, one could require that the next candidate be selected from those parameter vectors a given step-size distance from the current sample, \\(\\textbf{z}_j \\in \\{\\textbf{z}_k: \\vert \\textbf{z}_i - \\textbf{z}_k \\vert = \\delta \\}\\) . However, while the candidate selected can depend on the current sample, it must not depend on any prior history of the sampling process. Whatever the process chosen (there's some flexibility here), we write \\(t_{i,j}\\) for the rate of selecting \\(\\textbf{z}_j\\) as the next candidate given the current sample is \\(\\textbf{z}_i\\) . Once a candidate is identified, we either accept or reject it via a second random process. If it is accepted, we mark it down as the next sample, then go back to step one, using the current sample to inform the next candidate selection. Otherwise, we mark the current sample down again, taking it as a repeat sample, and then use it to return to candidate search step, as above. Here, we write \\(A_{i,j}\\) for the rate of accepting \\(\\textbf{z}_j\\) , given that it was selected as the next candidate, starting from \\(\\textbf{z}_i\\) . Selecting the trial and acceptance rates In order to ensure that our above process selects samples according to the distribution (\\ref{Bayes}), we need to appropriately set the \\(\\{t_{i,j}\\}\\) and \\(\\{A_{i,j}\\}\\) values. To do that, note that at equilibrium one must see the same number of hops from \\(\\textbf{z}_i\\) to \\(\\textbf{z}_j\\) as hops from \\(\\textbf{z}_j\\) from \\(\\textbf{z}_i\\) (if this did not hold, one would see a net shifting of weight from one to the other over time, contradicting the assumption of equilibrium). If \\(\\rho_i\\) is the fraction of samples the process takes from state \\(i\\) , this condition can be written as \\begin{align} \\label{inter} \\rho_i t_{i,j} A_{i,j} = \\rho_j t_{j,i} A_{j,i} \\tag{3} \\end{align} To select a process that returns the desired sampling weight, we solve for \\(\\rho_i\\) over \\(\\rho_j\\) in (\\ref{inter}) and then equate this to the ratio required by (\\ref{Bayes}). This gives \\begin{align} \\tag{4} \\label{cond} \\frac{\\rho_i}{\\rho_j} = \\frac{t_{j,i} A_{j,i}}{t_{i,j} A_{i,j}} \\equiv \\frac{p(\\textbf{x} \\vert \\textbf{z}_i)p(\\textbf{z}_i)}{p(\\textbf{x} \\vert \\textbf{z}_j)p(\\textbf{z}_j)}. \\end{align} Now, the single constraint above is not sufficient to pin down all of our degrees of freedom. In the Metropolis case, we choose the following working balance: The trial rates between states are set equal, \\(t_{i,j} = t_{j,i}\\) (but remain unspecified — left to the discretion of the coder on a case-by-case basis), and we set $$ \\tag{5} A_{i,j} = \\begin{cases} 1, & \\text{if } p(\\textbf{z}_j \\vert \\textbf{x}) > p(\\textbf{z}_i \\vert \\textbf{x}) \\\\ \\frac{p(\\textbf{x} \\vert \\textbf{z}_j)p(\\textbf{z}_j)}{p(\\textbf{x} \\vert \\textbf{z}_i)p(\\textbf{z}_i)} \\equiv \\frac{p(\\textbf{z}_j \\vert \\textbf{x})}{p(\\textbf{z}_i \\vert \\textbf{x})}, & \\text{else}. \\end{cases} $$ This last equation says that we choose to always accept a candidate sample if it is more likely than the current one. However, if the candidate is less likely, we only accept a fraction of the time — with rate equal to the relative probability ratio of the two states. For example, if the candidate is only \\(80%\\) as likely as the current sample, we accept it \\(80%\\) of the time. That's it for Metropolis — a simple MCMC algorithm, guaranteed to satisfy (\\ref{cond}), and to therefore equilibrate to (\\ref{Bayes})! An example follows. Coding example The following python snippet illustrates the Metropolis algorithm in action. Here, we take 15 samples from a Normal distribution of variance one and true mean also equal to one. We pretend not to know the mean (but assume we do know the variance), assume a uniform prior for the mean, and then run the algorithm to obtain two hundred thousand samples from the mean's posterior. The histogram at right summarizes the results, obtained by dropping the first 1% of the samples (to protect against bias towards the initialization value). Averaging over the samples returns a mean estimate of \\(\\mu \\approx 1.4 \\pm 0.5\\) (95% confidence interval), consistent with the true value of \\(1\\) . % matplotlib inline import matplotlib.pyplot as plt import numpy as np # Take some samples true_mean = 1 X = np . random . normal ( loc = true_mean , size = 15 ) total_samples = 200000 # Function used to decide move acceptance def posterior_numerator ( mu ): prod = 1 for x in X : prod *= np . exp ( - ( x - mu ) ** 2 / 2 ) return prod # Initialize MCMC, then iterate z1 = 0 posterior_samples = [ z1 ] while len ( posterior_samples ) < total_samples : z_current = posterior_samples [ - 1 ] z_candidate = z_current + np . random . rand () - 0.5 rel_prob = posterior_numerator ( z_candidate ) / posterior_numerator ( z_current ) if rel_prob > 1 : posterior_samples . append ( z_candidate ) else : trial_toss = np . random . rand () if trial_toss < rel_prob : posterior_samples . append ( z_candidate ) else : posterior_samples . append ( z_current ) # Drop some initial samples and thin thinned_samples = posterior_samples [ 2000 :] plt . hist ( thinned_samples ) plt . title ( \"Histogram of MCMC samples\" ) plt . show () Summary To summarize, we have reviewed the application of MCMC to Bayesian statistics. MCMC is a general tool for obtaining samples from a probability distribution. It can be applied whenever one can conveniently specify the relative probability of two states — and so is particularly apt for situations where only the normalization constant of a distribution is difficult to evaluate, precisely the problem with the posterior (\\ref{Bayes}). The method entails carrying out an iterative try-accept algorithm, where the rates of trial and acceptance can be adjusted, but must be balanced so that the equilibrium distribution that results approaches the desired form. The key equation enabling us to strike this balance is (\\ref{inter}) — the zero flux condition (aka the detailed balance condition to physicists) that holds between states at equilibrium. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods, Theory","url":"https://efavdb.com/metropolis","loc":"https://efavdb.com/metropolis"},{"title":"Interpreting the results of linear regression","text":"Our last post showed how to obtain the least-squares solution for linear regression and discussed the idea of sampling variability in the best estimates for the coefficients. In this post, we continue the discussion about uncertainty in linear regression — both in the estimates of individual linear regression coefficients and the quality of the overall fit. Specifically, we'll discuss how to calculate the 95% confidence intervals and p-values from hypothesis tests that are output by many statistical packages like python's statsmodels or R. An example with code is provided at the end. Review We wish to predict a scalar response variable \\(y_i\\) given a vector of predictors \\(\\vec{x}_i\\) of dimension \\(K\\) . In linear regression, we assume that \\(y_i\\) is a linear function of \\(\\vec{x}_i\\) , parameterized by a set of coefficients \\(\\vec{\\beta}\\) and an error term \\(\\epsilon_i\\) . The linear model (in matrix format and dropping the arrows over the vectors) for predicting \\(N\\) response variables is \\begin{align}\\tag{1} y = X\\beta + \\epsilon. \\end{align} The dimensions of each component are: dim( \\(X\\) ) = ( \\(N\\) , \\(K\\) ), dim( \\(\\beta\\) ) = ( \\(K\\) ,1), dim( \\(y\\) ) = dim( \\(\\epsilon\\) ) = ( \\(N\\) ,1), where \\(N\\) = # of examples, \\(K\\) = # of regressors / predictors, counting an optional intercept/constant term. The ordinary least-squares best estimator of the coefficients, \\(\\hat{\\beta}\\) , was derived last time : \\begin{align}\\tag{2}\\label{optimal} \\hat{\\beta} = (X'X)&#94;{-1}X'y, \\end{align} where the hat \"&#94;\" denotes an estimator, not a true population parameter. (\\ref{optimal}) is a point estimate, but fitting different samples of data from the population will cause the best estimators to shift around. The amount of shifting can be explained by the variance-covariance matrix of \\(\\hat{\\beta}\\) , also derived last time (independent of assumptions of normality): \\begin{align}\\tag{3}\\label{cov} cov(\\hat{\\beta}, \\hat{\\beta}) = \\sigma&#94;2 (X'X)&#94;{-1}. \\end{align} Goodness of fit - \\(R&#94;2\\) To get a better feel for (\\ref{cov}), it's helpful to rewrite it in terms of the coefficient of determination \\(R&#94;2\\) . \\(R&#94;2\\) measures how much of the variation in the response variable \\(y\\) is explained by variation in the regressors \\(X\\) (as opposed to the unexplained variation from \\(\\epsilon\\) ). The variation in \\(y\\) , i.e. the \"total sum of squares\" \\(SST\\) , can be partitioned into the sum of two terms, \"regression sum of squares\" and \"error sum of squares\": \\(SST = SSR + SSE\\) . For convenience, let's center \\(y\\) and \\(X\\) around their means, e.g. \\(y \\rightarrow y - \\bar{y}\\) so that the mean \\(\\bar{y}=0\\) for the centered variables. Then, \\begin{align}\\tag{4}\\label{SS} SST &= \\sum_i&#94;N (y - \\bar{y})&#94;2 = y'y \\\\ SSR &= \\sum_i&#94;N (X\\hat{\\beta} - \\bar{y})&#94;2 = \\hat{y}'\\hat{y} \\\\ SSE &= \\sum_i&#94;N (y - \\hat{y})&#94;2 = e'e, \\end{align} where \\(\\hat{y} \\equiv X\\hat{\\beta}\\) . Then \\(R&#94;2\\) is defined as the ratio of the regression sum of squares to the total sum of squares: \\begin{align}\\tag{5}\\label{R2} R&#94;2 \\equiv \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} \\end{align} \\(R&#94;2\\) ranges between 0 and 1, with 1 being a perfect fit. According to (\\ref{cov}), the variance of a single coefficient \\(\\hat{\\beta}_k\\) is proportional to the quantity \\((X'X)_{kk}&#94;{-1}\\) , where \\(k\\) denotes the kth diagonal element of \\((X'X)&#94;{-1}\\) , and can be rewritten as \\begin{align}\\tag{6}\\label{cov2} var(\\hat{\\beta}_k) &= \\sigma&#94;2 (X'X)_{kk}&#94;{-1} \\\\ &= \\frac{\\sigma&#94;2}{(1 - R_k&#94;2)\\sum_i&#94;N (x_{ik} - \\bar{x}_k)&#94;2}, \\end{align} where \\(R_k&#94;2\\) is the \\(R&#94;2\\) in the regression of the kth variable, \\(x_k\\) , against the other predictors [A1] . The key observation from (\\ref{cov2}) is that the precision in the estimator decreases if the fit is made over highly correlated regressors, for which \\(R_k&#94;2\\) approaches 1. This problem of multicollinearity in linear regression will be manifested in our simulated example. (\\ref{cov2}) is also consistent with the observation from our previous post that, all things being equal, the precision in the estimator increases if the fit is made over a direction of greater variance in the data. In the next section, \\(R&#94;2\\) will again be useful for interpreting the behavior of one of our test statistics. Calculating test statistics If we assume that the vector of residuals has a multivariate normal distribution, \\(\\epsilon \\sim N(0, \\sigma&#94;2I)\\) , then we can construct test statistics to characterize the uncertainty in the regression. In this section, we'll calculate ​(a) confidence intervals - random intervals around individual estimators \\(\\hat{\\beta}_k\\) that, if constructed for regressions over multiple samples, would contain the true population parameter, \\(\\beta_k\\) , a certain fraction, e.g. 95%, of the time. (b) p-value - the probability of events as extreme or more extreme than an observed value (a test statistic) occurring under the null hypothesis. If the p-value is less than a given significance level \\(\\alpha\\) (a common choice is \\(\\alpha = 0.05\\) ), then the null hypothesis is rejected, e.g. a regression coefficient is said to be significant. From the assumption of the distribution of \\(\\epsilon\\) , it follows that \\(\\hat{\\beta}\\) has a multivariate normal distribution [A2] : \\begin{align}\\tag{7} \\hat{\\beta} \\sim N(\\beta, \\sigma&#94;2 (X'X)&#94;{-1}). \\end{align} To be explicit, a single coefficient, \\(\\hat{\\beta}_k\\) , is distributed as \\begin{align}\\tag{8} \\hat{\\beta}_k \\sim N(\\beta_k, \\sigma&#94;2 (X'X)_{kk}&#94;{-1}). \\end{align} This variable can be standardized as a z-score: \\begin{align}\\tag{9} z_k = \\frac{\\hat{\\beta}_k - \\beta_k}{\\sigma&#94;2 (X'X)_{kk}&#94;{-1}} \\sim N(0,1) \\end{align} In practice, we don't know the population parameter, \\(\\sigma&#94;2\\) , so we can't use the z-score. Instead, we can construct a pivotal quantity, a t-statistic. The t-statistic for \\(\\hat{\\beta}_k\\) follows a t-distribution with n-K degrees of freedom [1] , \\begin{align}\\tag{10}\\label{tstat} t_{\\hat{\\beta}_k} = \\frac{\\hat{\\beta}_k - \\beta_k}{s(\\hat{\\beta}_k)} \\sim t_{n-K}, \\end{align} where \\(s(\\hat{\\beta}_k)\\) is the standard error of \\(\\hat{\\beta}_k\\) \\begin{align}\\tag{11} s(\\hat{\\beta}_k)&#94;2 = \\hat{\\sigma}&#94;2 (X'X)_{kk}&#94;{-1}, \\end{align} and \\(\\hat{\\sigma}&#94;2\\) is the unbiased estimator of \\(\\sigma&#94;2\\) \\begin{align}\\tag{12} \\hat{\\sigma}&#94;2 = \\frac{\\epsilon'\\epsilon}{n - K}. \\end{align} Confidence intervals around regression coefficients The \\((1-\\alpha)\\) confidence interval around an estimator, \\(\\hat{\\beta}_k \\pm \\Delta\\) , is defined such that the probability of a random interval containing the true population parameter is \\((1-\\alpha)\\) : \\begin{align}\\tag{13} P[\\hat{\\beta}_k - \\Delta < \\beta_k < \\hat{\\beta}_k + \\Delta ] = 1 - \\alpha, \\end{align} where \\(\\Delta = t_{1-\\alpha/2, n-K} s(\\hat{\\beta}_k)\\) , and \\(t_{1-\\alpha/2, n-K}\\) is the \\(\\alpha/2\\) -level critical value for the t-distribution with \\(n-K\\) degrees of freedom. t-test for the significance of a predictor Directly related to the calculation of confidence intervals is testing whether a regressor, \\(\\hat{\\beta}_k\\) , is statistically significant. The t-statistic for the kth regression coefficient under the null hypothesis that \\(x_k\\) and \\(y\\) are independent follows a t-distribution with n-K degrees of freedom, c.f. (\\ref{tstat}) with \\(\\beta_k = 0\\) : \\begin{align}\\tag{14} t = \\frac{\\hat{\\beta}_k - 0}{s(\\hat{\\beta}_k)} \\sim t_{n-K}. \\end{align} We reject the null-hypothesis if \\(P[t] < \\alpha\\) . According to (\\ref{cov2}), \\(s(\\hat{\\beta}_k)\\) increases with multicollinearity. Hence, the estimator must be more \"extreme\" in order to be statistically significant in the presence of multicollinearity. F-test for the significance of the regression Whereas the t-test considers the significance of a single regressor, the F-test evaluates the significance of the entire regression, where the null hypothesis is that all the regressors except the constant are equal to zero: \\(\\hat{\\beta}_1 = \\hat{\\beta}_2 = ... = \\hat{\\beta}_{K-1} = 0\\) . The F-statistic under the null hypothesis follows an F-distribution with {K-1, N-K} degrees of freedom [1] : \\begin{align}\\tag{15}\\label{F} F = \\frac{SSR/(K-1)}{SSE/(N-K)} \\sim F_{K-1, N-K}. \\end{align} It is useful to rewrite the F-statistic in terms of \\(R&#94;2\\) by substituting the expressions from (\\ref{ SS }) and (\\ref{R2}): \\begin{align}\\tag{16}\\label{F2} F = \\frac{(N-K) R&#94;2}{(K-1) (1-R&#94;2)} \\end{align} Notice how, for fixed \\(R&#94;2\\) , the F-statistic decreases with an increasing number of predictors \\(K\\) . Adding uninformative predictors to the model will decrease the significance of the regression, which motivates parsimony in constructing linear models. Example With these formulas in hand, let's consider the problem of predicting the weight of adult women using some simulated data (loosely based on reality). We'll look at two models: (1) weight ~ height . As expected, height will be a strong predictor of weight, corroborated by a significant p-value for the coefficient of height in the model. (2) weight ~ height + shoe size . Height and shoe size are strongly correlated in the simulated data, while height is still a strong predictor of weight. We'll find that neither of the predictors has a significant individual p-value, a consequence of collinearity. First, import some libraries. We use statsmodels.api.OLS for the linear regression since it contains a much more detailed report on the results of the fit than sklearn.linear_model.LinearRegression . import numpy as np import statsmodels.api as sm from scipy.stats import t import random Next, set the population parameters for the simulated data. # height ( inches ) mean_height = 65 std_height = 2 . 25 # shoe size ( inches ) mean_shoe_size = 7 . 5 std_shoe_size = 1 . 25 # correlation between height and shoe size r_height_shoe = 0 . 98 # height and shoe size are highly correlated # covariance b / w height and shoe size var_height_shoe = r_height_shoe * std_height * std_shoe_size # matrix of means , mu , and covariance , cov mu = ( mean_height , mean_shoe_size ) cov = [[ np . square ( std_height ), var_height_shoe ], [ var_height_shoe , np . square ( std_shoe_size )]] Generate the simulated data: # number of data points n = 20 random . seed ( 85 ) # height and shoe size X1 = np . random . multivariate_normal ( mu , cov , n ) # height , alone X0 = X1 [:, 0 ] weight = - 220 + np . random . normal ( X0 * 5 . 5 , 10 , n ) Below is the simulated data plotted against each other. Fit the linear models: # add column of ones for intercept X0 = sm . add_constant ( X0 ) X1 = sm . add_constant ( X1 ) # \"OLS\" stands for Ordinary Least Squares sm0 = sm . OLS ( weight , X0 ). fit () sm1 = sm . OLS ( weight , X1 ). fit () Look at the summary report, sm0.summary() , for the weight ~ height model. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.788 Model: OLS Adj. R-squared: 0.776 Method: Least Squares F-statistic: 66.87 Date: Wed, 29 Jun 2016 Prob (F-statistic): 1.79e-07 Time: 14:28:08 Log-Likelihood: -70.020 No. Observations: 20 AIC: 144.0 Df Residuals: 18 BIC: 146.0 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [95.0% Conf. Int.] ------------------------------------------------------------------------------ const -265.2764 49.801 -5.327 0.000 -369.905 -160.648 x1 6.1857 0.756 8.178 0.000 4.596 7.775 ============================================================================== Omnibus: 0.006 Durbin-Watson: 2.351 Prob(Omnibus): 0.997 Jarque-Bera (JB): 0.126 Skew: 0.002 Prob(JB): 0.939 Kurtosis: 2.610 Cond. No. 1.73e+03 ============================================================================== The height variable, x1 , is significant according to the t-test, as is the intercept, denoted const in the report. Also, notice the coefficient used to simulate the dependence of weight on height ( \\(\\beta_1\\) = 5.5), is contained in the 95% confidence interval of x1 . Next, let's look at the summary report, sm1.summary() , for the weight ~ height + shoe_size model. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.789 Model: OLS Adj. R-squared: 0.765 Method: Least Squares F-statistic: 31.86 Date: Wed, 29 Jun 2016 Prob (F-statistic): 1.78e-06 Time: 14:28:08 Log-Likelihood: -69.951 No. Observations: 20 AIC: 145.9 Df Residuals: 17 BIC: 148.9 Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [95.0% Conf. Int.] ------------------------------------------------------------------------------ const -333.1599 204.601 -1.628 0.122 -764.829 98.510 x1 7.4944 3.898 1.923 0.071 -0.729 15.718 x2 -2.3090 6.739 -0.343 0.736 -16.527 11.909 ============================================================================== Omnibus: 0.015 Durbin-Watson: 2.342 Prob(Omnibus): 0.993 Jarque-Bera (JB): 0.147 Skew: 0.049 Prob(JB): 0.929 Kurtosis: 2.592 Cond. No. 7.00e+03 ============================================================================== Neither of the regressors x1 and x2 is significant at a significance level of \\(\\alpha=0.05\\) . In the simulated data, adult female weight has a positive linear correlation with height and shoe size, but the strong collinearity of the predictors (simulated with a correlation coefficient of 0.98) causes each variable to fail a t-test in the model — and even results in the wrong sign for the dependence on shoe size. Although the predictors fail individual t-tests, the overall regression is significant, i.e. the predictors are jointly informative, according to the F-test. Notice, however, that the p-value of the F-test has decreased compared to the simple linear model, as expected from (\\ref{F2}), since including the extra variable, shoe size, did not improve \\(R&#94;2\\) but did increase \\(K\\) . Let's manually calculate the standard error, t-statistics, F-statistic, corresponding p-values, and confidence intervals using the equations from above. # OLS solution , eqn of form ax = b => ( X 'X)*beta_hat = X' * y beta_hat = np . linalg . solve ( np . dot ( X1 . T , X1 ), np . dot ( X1 . T , weight )) # residuals epsilon = weight - np . dot ( X1 , beta_hat ) # degrees of freedom of residuals dof = X1 . shape [ 0 ] - X1 . shape [ 1 ] # best estimator of sigma sigma_hat = np . sqrt ( np . dot ( epsilon , epsilon ) / dof ) # standard error of beta_hat s = sigma_hat * np . sqrt ( np . diag ( np . linalg . inv ( np . dot ( X1 . T , X1 )), 0 )) # 95 % confidence intervals # +/- t_ { 1 - alpha / 2 , n - K } = t . interval ( 1 - alpha , dof ) conf_intervals = beta_hat . reshape ( 3 , 1 ) + s . reshape ( 3 , 1 ) * np . array ( t . interval ( 0 . 95 , dof )) # t - statistics under null hypothesis t_stat = beta_hat / s # p - values # survival function sf = 1 - CDF p_values = t . sf ( abs ( t_stat ), dof ) * 2 # SSR ( regression sum of squares ) y_hat = np . dot ( X1 , beta_hat ) y_mu = np . mean ( weight ) mean_SSR = np . dot (( y_hat - y_mu ). T , ( y_hat - y_mu )) / ( len ( beta_hat ) - 1 ) # f - statistic f_stat = mean_SSR / np . square ( sigma_hat ) print ( 'f-statistic:' , f_stat , '\\n' ) # p - value of f - statistic p_values_f_stat = f . sf ( abs ( f_stat ), dfn = ( len ( beta_hat ) - 1 ), dfd = dof ) print ( 'p-value of f-statistic:' , p_values_f_stat , '\\n' ) The output values, below, from printing the manual calculations are consistent with the summary report: beta_hat : [ - 333 . 15990097 7 . 49444671 - 2 . 30898743 ] degrees of freedom of residuals : 17 sigma_hat : 8 . 66991550428 standard error of beta_hat : [ 204 . 60056111 3 . 89776076 6 . 73900599 ] confidence intervals : [[ - 7 . 64829352 e + 02 9 . 85095501 e + 01 ] [ - 7 . 29109662 e - 01 1 . 57180031 e + 01 ] [ - 1 . 65270473 e + 01 1 . 19090724 e + 01 ]] t - statistics : [ - 1 . 62834305 1 . 92275698 - 0 . 34263027 ] p - values of t - statistics : [ 0 . 1218417 0 . 07142839 0 . 73607656 ] f - statistic : 31 . 8556171105 p - value of f - statistic : 1 . 77777555162 e - 06 The full code is available as an IPython notebook on github . Summary Assuming a multivariate normal distribution for the residuals in linear regression allows us to construct test statistics and therefore specify uncertainty in our fits. A t-test judges the explanatory power of a predictor in isolation, although the standard error that appears in the calculation of the t-statistic is a function of the other predictors in the model. On the other hand, an F-test is a global test that judges the explanatory power of all the predictors together, and we've seen that parsimony in choosing predictors can improve the quality of the overall regression. We've also seen that multicollinearity can throw off the results of individual t-tests as well as obscure the interpretation of the signs of the fitted coefficients. A symptom of multicollinearity is when none of the individual coefficients are significant but the overall F-test is significant. Reference [1] Greene, W., Econometric Analysis, Seventh edition, Prentice Hall, 2011 - chapters available online Appendix [A1] We specifically want the kth diagonal element from the inverse moment matrix, \\((X'X)&#94;{-1}\\) . The matrix \\(X\\) can be partitioned as $$[X_{(k)} \\vec{x}_k],$$ where \\(\\vec{x}_k\\) is an N x 1 column vector containing the kth variable of each of the N samples, and \\(X_{(k)}\\) is the N x (K-1) matrix containing the rest of the variables and constant intercept. For convenience, let \\(X_{(k)}\\) and \\(\\vec{x}_k\\) be centered about their (column-wise) means. Matrix multiplication of the block-partitioned form of \\(X\\) with its transpose results in the following block matrix: \\begin{align} (X'X) = \\begin{bmatrix} X_{(k)}'X_{(k)} & X_{(k)}'\\vec{x}_k \\ \\vec{x}_k'X_{(k)} & \\vec{x}_k'\\vec{x}_k \\end{bmatrix} \\end{align} The above matrix has four blocks, and can be inverted blockwise to obtain another matrix with four blocks. The lower right block corresponding to the kth diagonal element of the inverted matrix is a scalar: \\begin{align} (X'X)&#94;{-1}_{kk} &= [\\vec{x}_k'\\vec{x}_k - \\vec{x}_k'X_{(k)}(X_{(k)}'X_{(k)})&#94;{-1}X_{(k)}'\\vec{x}_k]&#94;{-1} \\\\ &= \\left[\\vec{x}_k'\\vec{x}_k \\left( 1 - \\frac{\\vec{x}_k'X_{(k)}(X_{(k)}'X_{(k)})&#94;{-1}X_{(k)}'\\vec{x}_k}{\\vec{x}_k'\\vec{x}_k} \\right)\\right]&#94;{-1} \\end{align} Then the numerator of the fraction in the parentheses above can be simplified: \\begin{align} \\vec{x}_k'X_{(k)} ((X_{(k)}'X_{(k)})&#94;{-1}X_{(k)}'\\vec{x}_k) &= \\vec{x}_k' X_{(k)} \\hat{\\beta}_{(k)} \\\\ &= (X_{(k)}\\hat{\\beta}_{(k)} + \\epsilon_k)'X_{(k)}\\hat{\\beta}_{(k)} \\\\ &= \\hat{x}_k'\\hat{x}_k, \\end{align} where \\(\\hat{\\beta}_{(k)}\\) is the OLS solution for the coefficients in the regression on the \\(\\vec{x}_k\\) by the remaining variables \\(X_{(k)}\\) : \\(\\vec{x}_k = X_{(k)} \\beta_{(k)} + \\epsilon_k\\) . In the last line, we used one of the constraints on the residuals — that the residuals and predictors are uncorrelated, \\(\\epsilon_k'X_{(k)} = 0\\) . Plugging in this simplification for the numerator and using the definition of \\(R&#94;2\\) from (\\ref{R2}), we obtain our final result: \\begin{align} (X'X)&#94;{-1}_{kk} &= \\left[\\vec{x}_k'\\vec{x}_k \\left( 1 - \\frac{\\hat{x}_k'\\hat{x}_k}{\\vec{x}_k'\\vec{x}_k} \\right)\\right]&#94;{-1} \\\\ &= \\left[\\vec{x}_k'\\vec{x}_k ( 1 - R_k&#94;2 )\\right]&#94;{-1} \\end{align} [A2] \\begin{align} \\hat{\\beta} &= (X'X)&#94;{-1}X'y \\\\ &= (X'X)&#94;{-1}X'(X\\beta + \\epsilon) \\\\ &= \\beta + (X'X)&#94;{-1}X'N(0, \\sigma&#94;2I) \\\\ & \\sim N(\\beta, \\sigma&#94;2 (X'X)&#94;{-1}) \\end{align} The last line is by properties of affine transformations on multivariate normal distributions . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods, Theory","url":"https://efavdb.com/interpret-linear-regression","loc":"https://efavdb.com/interpret-linear-regression"},{"title":"Linear Regression","text":"We review classical linear regression using vector-matrix notation. In particular, we derive a) the least-squares solution, b) the fit's coefficient covariance matrix — showing that the coefficient estimates are most precise along directions that have been sampled over a large range of values (the high variance directions, a la PCA ), and c) an unbiased estimate for the underlying sample variance (assuming normal sample variance in this last case). We then review how these last two results can be used to provide confidence intervals / hypothesis tests for the coefficient estimates. Finally, we show that similar results follow from a Bayesian approach. Last edited July 23, 2016. Introduction Here, we consider the problem of fitting a linear curve to \\(N\\) data points of the form \\((\\vec{x}_i, y_i),\\) where the \\(\\{\\vec{x}_i\\}\\) are column vectors of predictors that sit in an \\(L\\) -dimensional space and the \\(\\{y_i\\}\\) are the response values we wish to predict given the \\(\\{x_i\\}\\) . The linear approximation will be defined by a set of coefficients, \\(\\{\\beta_j\\}\\) so that \\begin{align} \\hat{y}_i \\equiv \\sum_j x_{i,j} \\beta_j = \\vec{x}_i&#94;T \\cdot \\vec{\\beta} . \\tag{1} \\label{1} \\end{align} We seek the \\(\\vec{\\beta}\\) that minimizes the average squared \\(y\\) error, \\begin{align} \\tag{2} \\label{2} J = \\sum_i \\left ( y_i - \\hat{y}_i \\right)&#94;2 = \\sum_i \\left (y_i - \\vec{x}_i&#94;T \\cdot \\vec{\\beta} \\right)&#94;2. \\end{align} It turns out that this is a problem where one can easily derive an analytic expression for the optimal solution. It's also possible to derive an expression for the variance in the optimal solution — that is, how much we might expect the optimal parameter estimates to change were we to start with some other \\(N\\) data points instead. These estimates can then be used to generate confidence intervals for the coefficient estimates. Here, we review these results, give a simple interpretation to the theoretical variance, and finally show that the same results follow from a Bayesian approach. Optimal solution We seek the coefficient vector that minimizes (\\ref{2}). We can find this by differentiating this cost function with respect to \\(\\vec{\\beta}\\) , setting the result to zero. This gives, \\begin{align} \\tag{3} \\partial_{\\beta_j} J = 2 \\sum_i \\left (y_i - \\sum_k x_{i,k} \\beta_k \\right) x_{i,j} = 0. \\end{align} We next define the matrix \\(X\\) so that \\(X_{i,j} = \\vec{x}_{i,j}\\) . Plugging this into the above, we obtain \\begin{align} \\partial_{\\beta_j} J &= 2 \\sum_i X_{j,i}&#94;T \\left (y_i - \\sum_k X_{i,k} \\beta_k \\right) = 0 \\\\ &= X&#94;T \\cdot \\left ( \\vec{y} - X \\cdot \\vec{\\beta}\\right ) = 0.\\tag{4} \\end{align} Rearranging gives \\begin{align} X&#94;T X \\cdot \\vec{\\beta} = X&#94;T \\cdot \\vec{y} \\to \\vec{\\beta} = (X&#94;T X)&#94;{-1} \\cdot X&#94;T \\cdot \\vec{y} \\tag{5} \\label{optimal} \\end{align} This is the squared-error-minimizing solution. Parameter covariance matrix Now, when one carries out a linear fit to some data, the best line often does not go straight through all of the data. Here, we consider the case where the reason for the discrepancy is not that the posited linear form is incorrect, but that there are some hidden variables not measured that the \\(y\\) -values also depend on. Assuming our data points represent random samples over these hidden variables, we can model their effect as adding a random noise term to the form (\\ref{1}), so that \\begin{align}\\tag{6} \\label{noise} y_i = \\vec{x}_i&#94;T \\cdot \\vec{\\beta}_{true} + \\epsilon_i, \\end{align} with \\(\\langle \\epsilon_i \\rangle =0\\) , \\(\\langle \\epsilon_i&#94;2 \\rangle = \\sigma&#94;2\\) , and \\(\\vec{\\beta}_{true}\\) the exact (but unknown) coefficient vector. Plugging (\\ref{noise}) into (\\ref{optimal}), we see that \\(\\langle \\vec{\\beta} \\rangle = \\vec{\\beta}_{true}\\) . However, the variance of the \\(\\epsilon_i\\) injects some uncertainty into our fit: Each realization of the noise will generate slightly different \\(y\\) values, causing the \\(\\vec{\\beta}\\) fit coefficients to vary. To estimate the magnitude of this effect, we can calculate the covariance matrix of \\(\\vec{\\beta}\\) . At fixed (constant) \\(X\\) , plugging in (\\ref{optimal}) for \\(\\vec{\\beta}\\) gives \\begin{align} cov(\\vec{\\beta}, \\vec{\\beta}) &= cov \\left( (X&#94;T X)&#94;{-1} \\cdot X&#94;T \\cdot \\vec{y} , \\vec{y}&#94;T \\cdot X \\cdot (X&#94;T X)&#94;{-1, T} \\right) \\\\ &= (X&#94;T X)&#94;{-1} \\cdot X&#94;T \\cdot cov(\\vec{y}&#94;T, \\vec{y} ) \\cdot X \\cdot (X&#94;T X)&#94;{-1, T} \\\\ &= \\sigma&#94;2 \\left( X&#94;T X \\right)&#94;{-1} \\cdot X&#94;T X \\cdot \\left( X&#94;T X \\right)&#94;{-1, T} \\\\ &= \\sigma&#94;2 \\left( X&#94;T X \\right)&#94;{-1}. \\tag{7} \\label{cov} \\end{align} In the third line here, note that we have assumed that the \\(\\epsilon_i\\) are independent, so that \\(cov(\\vec{y},\\vec{y}) = \\sigma&#94;2 I.\\) We've also used the fact that \\(X&#94;T X\\) is symmetric. To get a feel for the significance of (\\ref{cov}), it is helpful to consider the case where the average \\(x\\) values are zero. In this case, \\begin{align} \\left( X&#94;T X \\right)_{i,j} &\\equiv& \\sum_k \\delta X_{k,i} \\delta X_{k,j} \\equiv N \\times \\langle x_i, x_j\\rangle. \\tag{8} \\label{corr_mat} \\end{align} That is, \\(X&#94;T X\\) is proportional to the correlation matrix of our \\(x\\) values. This correlation matrix is real and symmetric, and thus has an orthonormal set of eigenvectors. The eigenvalue corresponding to the \\(k\\) -th eigenvector gives the variance of our data set's \\(k\\) -th component values in this basis — details can be found in our article on PCA . This implies a simple interpretation of (\\ref{cov}): The variance in the \\(\\vec{\\beta}\\) coefficients will be lowest for predictors parallel to the highest variance PCA components (eg \\(x_1\\) in the figure shown) and highest for predictors parallel to the lowest variance PCA components ( \\(x_2\\) in the figure). This observation can often be exploited during an experiment's design: If a particular coefficient is desired to high accuracy, one should make sure to sample the corresponding predictor over a wide range. [Note: Cathy gives an interesting, alternative interpretation for the parameter estimate variances in a follow-up post, here .] Unbiased estimator for \\(\\sigma&#94;2\\) The result (\\ref{cov}) gives an expression for the variance of the parameter coefficients in terms of the underlying sample variance \\(\\sigma&#94;2\\) . In practice, \\(\\sigma&#94;2\\) is often not provided and must be estimated from the observations at hand. Assuming that the \\(\\{\\epsilon_i\\}\\) in (\\ref{noise}) are independent \\(\\mathcal{N}(0, \\sigma&#94;2)\\) random variables, we now show that the following provides an unbiased estimate for this variance: $$ S&#94;2 \\equiv \\frac{1}{N-L} \\sum_i \\left ( y_i - \\vec{x}_i&#94;T \\cdot \\vec{\\beta} \\right) &#94;2. \\tag{9} \\label{S} $$ Note that this is a normalized sum of squared residuals from our fit, with \\((N-L)\\) as the normalization constant — the number of samples minus the number of fit parameters. To prove that \\(\\langle S&#94;2 \\rangle = \\sigma&#94;2\\) , we plug in (\\ref{optimal}) for \\(\\vec{\\beta}\\) , combining with (\\ref{noise}) for \\(\\vec{y}\\) . This gives \\begin{align} \\nonumber S&#94;2 &= \\frac{1}{N-L} \\sum_i \\left ( y_i - \\vec{x}_i&#94;T \\cdot (X&#94;T X)&#94;{-1} \\cdot X&#94;T \\cdot \\{ X \\cdot \\vec{\\beta}_{true} + \\vec{\\epsilon} \\} \\right) &#94;2 \\\\ \\nonumber &= \\frac{1}{N-L} \\sum_i \\left ( \\{y_i - \\vec{x}_i&#94;T \\cdot\\vec{\\beta}_{true} \\} - \\vec{x}_i&#94;T \\cdot (X&#94;T X)&#94;{-1} \\cdot X&#94;T \\cdot \\vec{\\epsilon} \\right) &#94;2 \\\\ &= \\frac{1}{N-L} \\sum_i \\left ( \\epsilon_i - \\vec{x}_i&#94;T \\cdot (X&#94;T X)&#94;{-1} \\cdot X&#94;T \\cdot \\vec{\\epsilon} \\right) &#94;2 \\tag{10}. \\label{S2} \\end{align} The second term in the last line is the \\(i\\) -th component of the vector $$ X \\cdot (X&#94;T X)&#94;{-1} \\cdot X&#94;T \\cdot \\vec{\\epsilon} \\equiv \\mathbb{P} \\cdot \\vec{\\epsilon}. \\tag{11} \\label{projection} $$ Here, \\(\\mathbb{P}\\) is a projection operator — this follows from the fact that \\(\\mathbb{P}&#94;2 = \\mathbb{P}\\) . When it appears in (\\ref{projection}), \\(\\mathbb{P}\\) maps \\(\\vec{\\epsilon}\\) into the \\(L\\) -dimensional coordinate space spanned by the \\(\\{\\vec{x_i}\\}\\) , scales the result using (\\ref{corr_mat}), then maps it back into its original \\(N\\) -dimensional space. The net effect is to project \\(\\vec{\\epsilon}\\) into an \\(L\\) -dimensional subspace of the full \\(N\\) -dimensional space (more on the \\(L\\) -dimensional subspace just below). Plugging (\\ref{projection}) into (\\ref{S2}), we obtain $$ S&#94;2 = \\frac{1}{N-L} \\sum_i \\left ( \\epsilon_i - (\\mathbb{P} \\cdot \\vec{\\epsilon})_i \\right)&#94;2 \\equiv \\frac{1}{N-L} \\left \\vert \\vec{\\epsilon} - \\mathbb{P} \\cdot \\vec{\\epsilon} \\right \\vert&#94;2. \\label{S3} \\tag{12} $$ This final form gives the result: \\(\\vec{\\epsilon}\\) is an \\(N\\) -dimensional vector of independent, \\(\\mathcal{N}(0, \\sigma&#94;2)\\) variables, and (\\ref{S3}) shows that \\(S&#94;2\\) is equal to \\(1/(N-L)\\) times the squared length of an \\((N-L)\\) -dimensional projection of it (the part along \\(\\mathbb{I} - \\mathbb{P}\\) ). The length of this projection will on average be \\((N-L) \\sigma&#94;2\\) , so that \\(\\langle S&#94;2 \\rangle = \\sigma&#94;2\\) . We need to make two final points before moving on. First, because \\(S&#94;2\\) is a sum of \\((N-L)\\) independent \\(\\mathcal{N}(0, \\sigma&#94;2)\\) random variables, it follows that $$ \\frac{(N-L) S&#94;2}{\\sigma&#94;2} \\sim \\chi_{N-L}&#94;2. \\tag{13} \\label{chi2} $$ Second, \\(S&#94;2\\) is independent of \\(\\vec{\\beta}\\) : We can see this by rearranging (\\ref{optimal}) as $$ \\vec{\\beta} = \\vec{\\beta}_{true} + (X&#94;T X)&#94;{-1} \\cdot X&#94;T \\cdot \\vec{\\epsilon}. \\tag{14} \\label{beta3} $$ We can left multiply this by \\(X\\) without loss to obtain $$ X \\cdot \\vec{\\beta} = X \\cdot \\vec{\\beta}_{true} + \\mathbb{P} \\cdot \\vec{\\epsilon}, \\tag{15} \\label{beta2} $$ where we have used (\\ref{projection}). Comparing (\\ref{beta2}) and (\\ref{S3}), we see that the components of \\(\\vec{\\epsilon}\\) that inform \\(\\vec{\\beta}\\) are in the subspace fixed by \\(\\mathbb{P}\\) . This is the space complementary to that informing \\(S&#94;2\\) , implying that \\(S&#94;2\\) is independent of \\(\\vec{\\beta}\\) . Confidence intervals and hypothesis tests The results above immediately provide us with a method for generating confidence intervals for the individual coefficient estimates (continuing with our Normal error assumption): From (\\ref{beta3}), it follows that the coefficients are themselves Normal random variables, with variance given by (\\ref{cov}). Further, \\(S&#94;2\\) provides an unbiased estimate for \\(\\sigma&#94;2\\) , proportional to a \\(\\chi&#94;2_{N-L}\\) random variable. Combining these results gives $$ \\frac{\\beta_{i,true} - \\beta_{i}}{\\sqrt{\\left(X&#94;T X\\right)&#94;{-1}_{ii} S&#94;2}} \\sim t_{(N-L)}. \\tag{16} $$ That is, the pivot at left follows a Student's \\(t\\) -distribution with \\((N-L)\\) degrees of freedom (i.e., it's proportional to the ratio of a standard Normal and the square root of a chi-squared variable with that many degrees of freedom). A rearrangement of the above gives the following level \\(\\alpha\\) confidence interval for the true value: $$ \\beta_i - t_{(N-L), \\alpha /2} \\sqrt{\\left(X&#94;T X \\right)&#94;{-1}_{ii} S&#94;2}\\leq \\beta_{i, true} \\leq \\beta_i + t_{(N-L), \\alpha /2} \\sqrt{\\left(X&#94;T X \\right)&#94;{-1}_{ii} S&#94;2} \\tag{17} \\label{interval}, $$ where \\(\\beta_i\\) is obtained from the solution (\\ref{optimal}). The interval above can be inverted to generate level \\(\\alpha\\) hypothesis tests. In particular, we note that a test of the null — that a particular coefficient is actually zero — would not be rejected if (\\ref{interval}) contains the origin. This approach is often used to test whether some data is consistent with the assertion that a predictor is linearly related to the response. [Again, see Cathy's follow-up post here for an alternate take on these results.] Bayesian analysis The final thing we wish to do here is consider the problem from a Bayesian perspective, using a flat prior on the \\(\\vec{\\beta}\\) . In this case, assuming a Gaussian form for the \\(\\epsilon_i\\) in (\\ref{noise}) gives \\begin{align}\\tag{18} \\label{18} p(\\vec{\\beta} \\vert \\{y_i\\}) \\propto p(\\{y_i\\} \\vert \\vec{\\beta}) p(\\vec{\\beta}) = \\mathcal{N} \\exp \\left [ -\\frac{1}{2 \\sigma&#94;2}\\sum_i \\left (y_i - \\vec{\\beta} \\cdot \\vec{x}_i \\right)&#94;2\\right]. \\end{align} Notice that this posterior form for \\(\\vec{\\beta}\\) is also Gaussian, and is centered about the solution (\\ref{optimal}). Formally, we can write the exponent here in the form \\begin{align} -\\frac{1}{2 \\sigma&#94;2}\\sum_i \\left (y_i - \\vec{\\beta} \\cdot \\vec{x}_i \\right)&#94;2 \\equiv -\\frac{1}{2} \\vec{\\beta}&#94;T \\cdot \\frac{1}{\\Sigma&#94;2} \\cdot \\vec{\\beta}, \\tag{19} \\end{align} where \\(\\Sigma&#94;2\\) is the covariance matrix for the components of \\(\\vec{\\beta}\\) , as implied by the posterior form (\\ref{18}). We can get the components of its inverse by differentiating (\\ref{18}) twice. This gives, \\begin{align} \\left ( \\frac{1}{\\Sigma&#94;2}\\right)_{jk} &= \\frac{1}{2 \\sigma&#94;2} \\partial_{\\beta_j} \\partial_{\\beta_k} \\sum_i \\left (y_i - \\vec{\\beta} \\cdot \\vec{x}_i \\right)&#94;2 \\\\ &= -\\frac{1}{\\sigma&#94;2}\\partial_{\\beta_j} \\sum_i \\left (y_i - \\vec{\\beta} \\cdot \\vec{x}_i \\right) x_{i,k} \\\\ &= \\frac{1}{\\sigma&#94;2} \\sum_i x_{i,j} x_{i,k} = \\frac{1}{\\sigma&#94;2} (X&#94;T X)_{jk}. \\tag{20} \\end{align} In other words, \\(\\Sigma&#94;2 = \\sigma&#94;2 (X&#94;T X)&#94;{-1}\\) , in agreement with the classical expression (\\ref{cov}). Summary In summary, we've gone through one quick derivation of linear fit solution that minimizes the sum of squared \\(y\\) errors for a given set of data. We've also considered the variance of this solution, showing that the resulting form is closely related to the principal components of the predictor variables sampled. The covariance solution (\\ref{cov}) tells us that all parameters have standard deviations that decrease like \\(1/\\sqrt{N}\\) , with \\(N\\) the number of samples. However, the predictors that are sampled over wider ranges always have coefficient estimates that more precise. This is due to the fact that sampling over many different values allows one to get a better read on how the underlying function being fit varies with a predictor. Following this, assuming normal errors, we showed that \\(S&#94;2\\) provides an unbiased estimate, chi-squared estimator for the sample variance — one that is independent of parameter estimates. This allowed us to then write down a confidence interval for the \\(i\\) -th coefficient. The final thing we have shown is that the Bayesian, Gaussian approximation gives similar results: In this approach, the posterior that results is centered about the classical solution, and has a covariance matrix equal to that obtained by classical approach. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods, Theory","url":"https://efavdb.com/linear-regression","loc":"https://efavdb.com/linear-regression"},{"title":"Average queue wait times with random arrivals","text":"Queries ping a certain computer server at random times, on average \\(\\lambda\\) arriving per second. The server can respond to one per second and those that can't be serviced immediately are queued up. What is the average wait time per query? Clearly if \\(\\lambda \\ll 1\\) , the average wait time is zero. But if \\(\\lambda > 1\\) , the queue grows indefinitely and the answer is infinity! Here, we give a simple derivation of the general result — (9) below. Introduction The mathematics of queue waiting times — first worked out by Agner Krarup Erlang — is interesting for two reasons. First, as noted above, queues can exhibit phase-transition like behaviors: If the average arrival time is shorter than the average time it takes to serve a customer, the line will grow indefinitely, causing the average wait time to diverge. Second, when the average arrival time is less than the service time, waiting times are governed entirely by fluctuations — and so can't be estimated well using mean-field arguments. For example, in the very low arrival rate limit, the only situation where anyone would ever have to wait at all is that where someone else happens to arrive just before them — an unlucky, rare event. Besides being interesting from a theoretical perspective, an understanding of queue formation phenomena is also critical for many practical applications — both in computer science and in wider industry settings. Optimal staffing of a queue requires a careful estimate of the expected customer arrival rate. If too many workers are staffed, the average wait time will be nicely low, but workers will largely be idle. Staff too few, and the business could enter into the divergent queue length regime — certainly resulting in unhappy customers and lost business (or dropped queries). Staffing just the right amount requires a sensitive touch — and in complex cases, a good understanding of the theory. In order to derive the average wait time for queues of different sorts, one often works within the framework of Markov processes. This approach is very general and elementary, but requires a bit of effort to develop the machinery needed get to the end results. Here, we demonstrate an alternative, sometimes faster approach that is based on writing down an integral equation for the wait time distribution. We consider only a simple case — that where the queue is serviced by only one staff member, the customers arrive at random times via a Poisson process, and each customer requires the same time to service, one second. Integral equation formulation Suppose the \\(N\\) -th customer arrives at time \\(0\\) , and let \\(P(t)\\) be the probability that this customer has to wait a time \\(t\\geq 0\\) before being served. This wait time can be written in terms of the arrival and wait times of the previous customer: If this previous customer arrived at time \\(t&#94;{\\prime}\\) and has to wait a time \\(w\\) before being served, his service will conclude at time \\(t = t&#94;{\\prime} + w + 1\\) . If this is greater than \\(0\\) , the \\(N\\) -th customer will have to wait before being served. In particular, he will wait \\(t\\) if the previous customer waited \\(w = t - t&#94;{\\prime} - 1\\) . The above considerations allow us to write down an equation satisfied by the wait time distribution. If we let the probability that the previous customer arrived at \\(t&#94;{\\prime}\\) be \\(A(t&#94;{\\prime})\\) , we have (for \\(t > 0\\) ) \\begin{eqnarray} \\tag{1} \\label{int_eqn} P(t) &=& \\int_{-\\infty}&#94;{0&#94;-} A(t&#94;{\\prime}) P(t - t&#94;{\\prime} - 1) d t&#94;{\\prime} \\\\ &=& \\int_{-\\infty}&#94;{0&#94;-} \\lambda e&#94;{\\lambda t&#94;{\\prime}} P(t - t&#94;{\\prime} - 1) d t&#94;{\\prime} \\end{eqnarray} Here, in the first equality we're simply averaging over the possible arrival times of the previous customer (which had to occur before the \\(N\\) -th, at \\(0\\) ), multiplying by the probability \\(P(t - t&#94;{\\prime} - 1)\\) that this customer had to wait the amount of time \\(w\\) needed so that the \\(N\\) -th customer will wait \\(t\\) . We also use the symmetry that each customer has the same wait time distribution at steady state. In the second equality, we have plugged in the arrival time distribution appropriate for our Poisson model. To proceed, we differentiate both sides of (\\ref{int_eqn}) with respect to \\(t\\) , \\begin{eqnarray}\\tag{2} \\label{int2} P&#94;{\\prime}(t) &=& \\int_{-\\infty}&#94;{0&#94;-} \\lambda e&#94;{\\lambda t&#94;{\\prime}} \\frac{d}{dt}P(t - t&#94;{\\prime} - 1) d t&#94;{\\prime} \\\\ &=& - \\int_{-\\infty}&#94;{0&#94;-} \\lambda e&#94;{\\lambda t&#94;{\\prime}} \\frac{d}{dt&#94;{\\prime}}P(t - t&#94;{\\prime} - 1) d t&#94;{\\prime}. \\end{eqnarray} The second equality follows after noticing that we can switch the parameter being differentiated in the first. Integrating by parts, we obtain \\begin{eqnarray} P&#94;{\\prime}(t) = \\lambda \\left [P(t) - P(t-1) \\right], \\tag{3} \\label{sol} \\end{eqnarray} a delay differential equation for the wait time distribution. This could be integrated numerically to get the full solution. However, our interest here is primarily the mean waiting time — as we show next, it's easy to extract this part of the solution analytically. Probability of no wait and the mean wait time We can obtain a series of useful relations by multiplying (\\ref{sol}) by powers of \\(t\\) and integrating. The first such expression is obtained by multiplying by \\(t&#94;1\\) . Doing this and integrating its left side, we obtain \\begin{eqnarray} \\tag{4} \\label{int3} \\int_{0&#94;{+}}&#94;{\\infty} P&#94;{\\prime}(t) t dt = \\left . P(t) t \\right |_{0&#94;{+}}&#94;{\\infty} - \\int_{0&#94;+}&#94;{\\infty} P(t) dt = 1 - P(0). \\end{eqnarray} Similarly integrating its right side, we obtain \\begin{eqnarray}\\tag{5} \\label{int4} \\lambda \\int_{0&#94;{+}}&#94;{\\infty} t \\left [P(t) - P(t-1) \\right] = \\lambda [ \\overline{t} - \\overline{(t + 1)} ] = - \\lambda. \\end{eqnarray} Equating the last two lines, we obtain the probability of no wait, \\begin{eqnarray} \\tag{6} \\label{int5} P(0) = 1 - \\lambda. \\end{eqnarray} This shows that when the arrival rate is low, the probability of no wait goes to one — an intuitively reasonable result. On the other hand, as \\(\\lambda \\to 1\\) , the probability of no wait approaches zero. In between, the idle time fraction of our staffer (which is equal to the probability of no wait, given a random arrival time) grows linearly, connecting these two limits. To obtain an expression for the average wait time, we carry out a similar analysis to that above, but multiply (\\ref{sol}) by \\(t&#94;2\\) instead. The integral on left is then \\begin{eqnarray} \\tag{7} \\label{int6} \\int_{0&#94;{+}}&#94;{\\infty} P&#94;{\\prime}(t) t&#94;2 dt = \\left . P(t) t&#94;2 \\right |_{0&#94;{+}}&#94;{\\infty} - 2\\int_{0&#94;+}&#94;{\\infty} P(t) t dt = - 2 \\overline{t}. \\end{eqnarray} Similarly, the integral at right is \\begin{eqnarray} \\tag{8} \\label{fin_int} \\lambda \\int_{0&#94;{+}}&#94;{\\infty} t&#94;2 \\left [P(t) - P(t-1) \\right] &=& \\lambda \\overline{ t&#94;2} - \\overline{ (t + 1)&#94;2} \\\\ &=& - \\lambda (2 \\overline{t} +1). \\end{eqnarray} Equating the last two lines and rearranging gives our solution for the average wait, \\begin{eqnarray} \\tag{9} \\label{fin} \\overline{t} = \\frac{\\lambda}{2 (1 - \\lambda)}. \\end{eqnarray} As advertised, this diverges as \\(\\lambda \\to 1\\) , see illustration in the plot below. It's very interesting that even as \\(\\lambda\\) approaches this extreme limit, the line is still empty a finite fraction of the time — see (\\ref{int5}). Evidently a finite idle time fraction can't be avoided, even as one approaches the divergent \\(\\lambda = 1\\) limit. Conclusions and extensions To carry this approach further, one could consider the case where the queue feeds \\(k\\) staff, rather than just one. I've made progress on this effort in certain cases, but have been stumped on the general problem. One interesting thing you can intuit about this \\(k\\) -staff version is that one approaches the mean-field analysis as \\(k\\to \\infty\\) (adding more staff tends to smooth things over, resulting in a diminishing of the importance of the randomness of the arrival times). This means that as \\(k\\) grows, we'll have very little average wait time for any \\(\\lambda<1\\) , but again divergent wait times for any \\(\\lambda \\geq 1\\) — like an infinite step function. Another direction one could pursue is to allow the service times to follow a distribution. Both cases can also be worked out using the Markov approach — references to such work can be found in the link provided in the introduction. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"https://efavdb.com/average-queue-wait-times-with-random-arrivals","loc":"https://efavdb.com/average-queue-wait-times-with-random-arrivals"},{"title":"Improved Bonferroni correction factors for multiple pairwise comparisons","text":"A common task in applied statistics is the pairwise comparison of the responses of \\(N\\) treatment groups in some statistical test — the goal being to decide which pairs exhibit differences that are statistically significant. Now, because there is one comparison being made for each pairing, a naive application of the Bonferroni correction analysis suggests that one should set the individual pairwise test sizes to \\(\\alpha_i \\to \\alpha_f/{N \\choose 2}\\) in order to obtain a desired family-wise type 1 error rate of \\(\\alpha_f\\) . Indeed, this solution is suggested by many texts. However, implicit in the Bonferroni analysis is the assumption that the comparisons being made are each mutually independent. This is not the case here, and we show that as a consequence the naive approach often returns type 1 error rates far from those desired. We provide adjusted formulas that allow for error-free Bonferroni-like corrections to be made. (edit (7/4/2016): After posting this article, I've since found that the method we suggest here is related to / is a generalization of Tukey's range test — see here .) (edit (6/11/2018): I've added the notebook used below to our Github, here ) Introduction In this post, we consider a particular kind of statistical test where one examines \\(N\\) different treatment groups, measures some particular response within each, and then decides which of the \\({N \\choose 2}\\) pairs appear to exhibit responses that differ significantly. This is called the pairwise comparison problem (or sometimes \"posthoc analysis\"). It comes up in many contexts, and in general it will be of interest whenever one is carrying out a multiple-treatment test. Our specific interest here is in identifying the appropriate individual measurement error bars needed to guarantee a given family-wise type 1 error rate, \\(\\alpha_f\\) . Briefly, \\(\\alpha_f\\) is the probability that we incorrectly make any assertion that two measurements differ significantly when the true effect sizes we're trying to measure are actually all the same. This can happen due to the nature of statistical fluctuations. For example, when measuring the heights of \\(N\\) identical objects, measurement error can cause us to incorrectly think that some pair have slightly different heights, even though that's not the case. A classical approach to addressing this problem is given by the Bonferroni approximation: If we consider \\(\\mathcal{N}\\) independent comparisons, and each has an individual type 1 error rate of \\(\\alpha_i,\\) then the family-wise probability of not making any type 1 errors is simply the product of the probabilities that we don't make any individual type 1 errors, $$ \\tag{1} \\label{bon1} p_f = (1 - \\alpha_f) = p_i&#94;{\\mathcal{N}} \\equiv \\left ( 1 - \\alpha_i \\right)&#94;{\\mathcal{N}} \\approx 1 - \\mathcal{N} \\alpha_i. $$ The last equality here is an expansion that holds when \\(p_f\\) is close to \\(1\\) , the limit we usually work in. Rearranging (\\ref{bon1}) gives a simple expression, $$ \\tag{2} \\label{bon2} \\alpha_i = \\frac{\\alpha_f}{\\mathcal{N}}. $$ This is the (naive) Bonferroni approximation — it states that one should use individual tests of size \\(\\alpha_f / \\mathcal{N}\\) in order to obtain a family-wise error rate of \\(\\alpha_f\\) . The reason why we refer to (\\ref{bon2}) as the naive Bonferroni approximation is that it doesn't actually apply to the problem we consider here. The reason why is that \\(p_f \\not = p_i&#94;{\\mathcal{N}}\\) in (\\ref{bon1}) if the \\(\\mathcal{N}\\) comparisons considered are not independent: This is generally the case for our system of \\(\\mathcal{N} = {N \\choose 2}\\) comparisons, since they are based on an underlying set of measurements having only \\(N\\) degrees of freedom (the object heights, in our example). Despite this obvious issue, the naive approximation is often applied in this context. Here, we explore the nature of the error incurred in such applications, and we find that it is sometimes very significant. We also show that it's actually quite simple to apply the principle behind the Bonferroni approximation without error: One need only find a way to evaluate the true \\(p_f\\) for any particular choice of error bars. Inverting this then allows one to identify the error bars needed to obtain the desired \\(p_f\\) . General treatment In this section, we derive a formal expression for the type 1 error rate in the pairwise comparison problem. For simplicity, we will assume 1) that the uncertainty in each of our \\(N\\) individual measurements is the same (e.g., the variance in the case of Normal variables), and 2) that our pairwise tests assert that two measurements differ statistically if and only if they are more than \\(k\\) units apart. To proceed, we consider the probability that a type 1 error does not occur, \\(p_f\\) . This requires that all \\(N\\) measurements sit within \\(k\\) units of each other. For any set of values satisfying this condition, let the smallest of the set be \\(x\\) . We have \\(N\\) choices for which of the treatments sit as this position. The remaining \\((N-1)\\) values must all be within the region \\((x, x+k)\\) . Because we're considering the type 1 error rate, we can assume that each of the independent measurements takes on the same distribution \\(P(x)\\) . These considerations imply $$ \\tag{3} \\label{gen} p_{f} \\equiv 1 - \\alpha_{f} = N \\int_{-\\infty}&#94;{\\infty} P(x) \\left \\{\\int_x&#94;{x+k} P(y) dy \\right \\}&#94;{N-1} dx. $$ Equation (\\ref{gen}) is our main result. It is nice for a couple of reasons. First, its form implies that when \\(N\\) is large it will scale like \\(a \\times p_{1,eff}&#94;N\\) , for some \\(k\\) -dependent numbers \\(a\\) and \\(p_{1,eff}\\) . This is reminiscent of the expression (\\ref{bon1}), where \\(p_f\\) took the form \\(p_i&#94;{\\mathcal{N}}\\) . Here, we see that the correct value actually scales like some number to the \\(N\\) -th power, not the \\(\\mathcal{N}\\) -th. This reflects the fact that we actually only have \\(N\\) independent degrees of freedom here, not \\({N \\choose 2}\\) . Second, when the inner integral above can be carried out formally, (\\ref{gen}) can be expressed as a single one-dimensional integral. In such cases, the integral can be evaluated numerically for any \\(k\\) , allowing one to conveniently identify the \\(k\\) that returns any specific, desired \\(p_f\\) . We illustrate both points in the next two sections, where we consider Normal and Cauchy variables, respectively. Normally-distributed responses We now consider the case where the individual statistics are each Normally-distributed about zero, and we reject any pair if they are more than \\(k \\times \\sqrt{2} \\sigma\\) apart, with \\(\\sigma&#94;2\\) the variance of the individual statistics. In this case, the inner integral of (\\ref{gen}) goes to $$\\tag{4} \\label{inner_g} \\frac{1}{\\sqrt{2 \\pi \\sigma&#94;2}} \\int_x&#94;{x+k \\sqrt{2} \\sigma} \\exp\\left [ -\\frac{y&#94;2}{2 \\sigma&#94;2} \\right] dy = \\frac{1}{2} \\left [\\text{erf}(k + \\frac{x}{\\sqrt{2} \\sigma}) - \\text{erf}(\\frac{x}{\\sqrt{2} \\sigma})\\right]. $$ Plugging this into (\\ref{gen}), we obtain $$\\tag{5} \\label{exact_g} p_f = \\int \\frac{N e&#94;{-x&#94;2 / 2 \\sigma&#94;2}}{\\sqrt{2 \\pi \\sigma&#94;2}} \\exp \\left ((N-1) \\log \\frac{1}{2} \\left [\\text{erf}(k + \\frac{x}{\\sqrt{2} \\sigma}) - \\text{erf}(\\frac{x}{\\sqrt{2} \\sigma})\\right]\\right)dx. $$ This exact expression (\\ref{exact_g}) can be used to obtain the \\(k\\) value needed to achieve any desired family-wise type 1error rate. Example solutions obtained in this way are compared to the \\(k\\) -values returned by the naive Bonferroni approach in the table below. The last column \\(p_{f,Bon}\\) shown is the family-wise success rate that you get when you plug in \\(k_{Bon},\\) the naive Bonferroni \\(k\\) value targeting \\(p_{f,exact}\\) . \\(N\\) \\(p_{f,exact}\\) \\(k_{exact}\\) \\(k_{Bon}\\) \\(p_{f, Bon}\\) 4 0.9 2.29 2.39 0.921 8 0.9 2.78 2.91 0.929 4 0.95 2.57 2.64 0.959 8 0.95 3.03 3.1 0.959 Examining the table shown, you can see that the naive approach is consistently overestimating the \\(k\\) values (error bars) needed to obtain the desired family-wise rates — but not dramatically so. The reason for the near-accuracy is that two solutions basically scale the same way with \\(N\\) . To see this, one can carry out an asymptotic analysis of (\\ref{exact_g}). We skip the details and note only that at large \\(N\\) we have $$\\tag{6} \\label{asy_g} p_f \\sim \\text{erf} \\left ( \\frac{k}{2}\\right)&#94;N \\sim \\left (1 - \\frac{e&#94;{-k&#94;2 / 4}}{k \\sqrt{\\pi}/2} \\right)&#94;N. $$ This is interesting because the individual pairwise tests have p-values given by $$ \\tag{7} \\label{asy_i} p_i = \\int_{-k\\sqrt{2}\\sigma}&#94;{k\\sqrt{2}\\sigma} \\frac{e&#94;{-x&#94;2 / (4 \\sigma&#94;2)}}{\\sqrt{4 \\pi \\sigma&#94;2 }} = \\text{erf}(k /\\sqrt{2}) \\sim 1 - \\frac{e&#94;{-k&#94;2/2}}{k \\sqrt{\\pi/2}}. $$ At large \\(k\\) , this is dominated by the exponential. Comparing with (\\ref{asy_g}), this implies $$ \\tag{8} \\label{fin_g} p_f \\sim \\left (1 - \\alpha_i&#94;{1/2} \\right)&#94;N \\sim 1 - N \\alpha_i&#94;{1/2} \\equiv 1 - \\alpha_f. $$ Fixing \\(\\alpha_f\\) , this requires that \\(\\alpha_i\\) scale like \\(N&#94;{-2}\\) , the same scaling with \\(N\\) as the naive Bonferroni solution. Thus, in the case of Normal variables, the Bonferroni approximation provides an inexact, but reasonable approximation (nevertheless, we suggest going with the exact approach using (\\ref{exact_g}), since it's just as easy!). We show in the next section that this is not the case for Cauchy variables. Cauchy-distributed variables We'll now consider the case of \\(N\\) independent, identically-distributed Cauchy variables having half widths \\(a\\) , $$ \\tag{9} \\label{c_dist} P(x) = \\frac{a}{\\pi} \\frac{1}{a&#94;2 + x&#94;2}. $$ When we compare any two, we will reject the null if they are more than \\(ka\\) apart. With this choice, the inner integral of (\\ref{gen}) is now $$ \\tag{10} \\label{inner_c} \\frac{a}{\\pi} \\int_x&#94;{x+ k a} \\frac{1}{a&#94;2 + y&#94;2} dy =\\\\ \\frac{1}{\\pi} \\left [\\tan&#94;{-1}(k + x/a) - \\tan&#94;{-1}(x/a) \\right]. $$ Plugging into into (\\ref{gen}) now gives $$\\tag{11} \\label{exact_c} p_f = \\int \\frac{N a/\\pi}{a&#94;2 + x&#94;2} e&#94;{(N-1) \\log \\frac{1}{\\pi} \\left [\\tan&#94;{-1}(k + x/a) - \\tan&#94;{-1}(x/a) \\right] }. $$ This is the analog of (\\ref{exact_g}) for Cauchy variables — it can be used to find the exact \\(k\\) value needed to obtain a given family-wise type 1 error rate. The table below compares the exact values to those returned by the naive Bonferroni analysis [obtained using the fact that the difference between two independent Cauchy variables of width \\(a\\) is itself a Cauchy distributed variable, but with width \\(2a\\) ]. \\(N\\) \\(p_{f,exact}\\) \\(k_{exact}\\) \\(k_{Bon}\\) \\(p_{f, Bon}\\) 4 0.9 27 76 0.965 8 0.9 55 350 0.985 4 0.95 53 153 0.983 8 0.95 107 700 0.993 In this case, you can see that the naive Bonferroni approximation performs badly. For example, in the last line, it suggests using error bars that are seven times too large for each point estimate. The error gets even worse as \\(N\\) grows: Again, skipping the details, we note that in this limit, (\\ref{exact_c}) scales like $$\\tag{12} \\label{asym_c} p_f \\sim \\left [\\frac{2}{\\pi} \\tan&#94;{-1}(k/2) \\right]&#94;N. $$ This can be related to the individual \\(p_i\\) values, which are given by $$ \\tag{13} \\label{asym2_c} p_i = \\int_{-ka}&#94;{ka} \\frac{2 a / \\pi}{4 a&#94;2 + x&#94;2}dx = \\frac{2}{\\pi}\\tan&#94;{-1}(k/2). $$ Comparing the last two lines, we obtain $$ \\tag{14} \\label{asym3_c} p_f \\equiv 1 - \\alpha_f \\sim p_i&#94;N \\sim 1 - N \\alpha_i. $$ Although we've been a bit sloppy with coefficients here, (\\ref{asym3_c}) gives the correct leading \\(N\\) -dependence: \\(k_{exact} \\sim 1/\\alpha_i \\propto N\\) . We can see this linear scaling in the table above. This explains why \\(k_{exact}\\) and \\(k_{Bon}\\) — which scales like \\({N \\choose 2} \\sim N&#94;2\\) — differ more and more as \\(N\\) grows. In this case, you should definitely never use the naive approximation, but instead stick to the exact analysis based on (\\ref{exact_c}). Conclusion Some people criticize the Bonferroni correction factor as being too conservative. However, our analysis here suggests that this feeling may be due in part to its occasional improper application. The naive approximation simply does not apply in the case of pairwise comparisons because the \\({N \\choose 2}\\) pairs considered are not independent — there are only \\(N\\) independent degrees of freedom in this problem. Although the naive correction does not apply to the problem of pairwise comparisons, we've shown here that it remains a simple matter to correctly apply the principle behind it: One can easily select any desired family-wise type 1 error rate through an appropriate selection of the individual test sizes — just use (\\ref{gen})! We hope you enjoyed this post — we anticipate writing a bit more on hypothesis testing in the near future. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"https://efavdb.com/bonferroni-correction-for-multiple-pairwise-comparison-tests","loc":"https://efavdb.com/bonferroni-correction-for-multiple-pairwise-comparison-tests"},{"title":"Try Caffe pre-installed on a VirtualBox image","text":"A previous post showed beginners how to try out deep learning libraries by using an Amazon Machine Image ( AMI ) pre-installed with deep learning libraries setting up a Jupyter notebook server to play with said libraries If you have VirtualBox and Vagrant , you can follow a similar procedure on your own computer. The advantage is that you can develop locally, then deploy on an expensive AWS EC2 gpu instance when your scripts are ready. For example, Caffe , the machine vision framework, allows you to seamlessly transition between cpu- and gpu-mode, and is available as a vagrant box running Ubuntu 14.04 ( ** 64-bit), with Caffe pre-installed. To add the box, type on the command line: vagrant box add malthejorgensen/caffe-deeplearning If you don't already have VirtualBox and Vagrant installed, you can find instructions online, or look at my dotfiles to get an idea. Gotchas SSH authentication failure For me, the box had the wrong public key in /home/vagrant/.ssh/authorized_keys file , which gave me \"authentication failure\" upon starting up the box with vagrant up . This was fixed by: Manually ssh into the box: vagrant ssh . Then type (key taken from here ): echo \"ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key\" > ~/.ssh/authorized_keys Log out of the box, reload the box with vagrant reload , and hopefully the ssh authentication error is fixed. Jupyter notebook server By default, the box has a notebook server on port 8003 that starts up from the /home/vagrant/caffe/examples directory, to be used in conjunction with port forwarding set in the Vagrant file: config.vm.network \"forwarded_port\", guest: 8003, host: 8003 With the default setup, go to http://localhost:8003 in your browser to access /home/vagrant/caffe/examples. The default server setup limits access to only /home/vagrant/caffe/examples, so I prefer to set up my own configuration of the jupyter notebook server on port 8888 (allowing port forwarding of port 8888 in the Vagrantfile as well) and then start up the server from /home/vagrant, or wherever I'm working. To do this, Log in to the box: vagrant ssh Then create the notebook config file ~/.jupyter/jupyter_notebook_config.py containing the following lines: c.NotebookApp.ip = '*' c.NotebookApp.open_browser = False c.NotebookApp.port = 8888 Vagrantfile Here's the vagrant file that worked for me: Scripts to install Virtualbox (line 31 and onwards) and install Vagrant . ** This is a 64-bit box, so you need to have Intel VT -x enabled in your BIOS .","tags":"Tools","url":"https://efavdb.com/caffe-virtualbox","loc":"https://efavdb.com/caffe-virtualbox"},{"title":"Start deep learning with Jupyter notebooks in the cloud","text":"Want a quick and easy way to play around with deep learning libraries? Puny GPU got you down? Thanks to Amazon Web Services ( AWS ) — specifically, AWS Elastic Compute Cloud ( EC2 ) — no data scientist need be left behind. Jupyter/IPython notebooks are indispensable tools for learning and tinkering. This post shows how to set up a public Jupyter notebook server in EC2 and then access it remotely through your web browser, just as you would if you were using a notebook launched from your own laptop. For a beginner, having to both set up deep learning libraries and navigate the AWS menagerie feels like getting thrown into the deep end when you just want to stick a toe in. You can skip the hassle of setting up deep learning frameworks from scratch by choosing an Amazon Machine Image ( AMI ) that comes pre-installed with the libraries and their dependencies. (Concerned about costs? — see the note * at the bottom of this post.) For example, the Stanford class, CS231n: Convolutional Neural Networksfor Visual Recognition , has provided a public AMI with these specs: cs231n_caffe_torch7_keras_lasagne_v2 AMI ID : ami-125b2c72 in the us-west-1 region Use a g2.2xlarge instance. Caffe, Torch7, Theano, Keras and Lasagne are pre-installed. Python bindings of caffe are available. It has CUDA 7.5 and CuDNN v3. If you're new to AWS , CS231n provides a nice step-by-step AWS tutorial with lots of screenshots. We're just going to tweak their procedure to enable access to Jupyter/IPython notebooks. After you're done, you'll be able to work through tutorials in notebook format like those provided by caffe in their examples folder, e.g. 00-classification.ipynb . We've written a little bash script jupyter_userdata.sh to execute Jupyter's instructions for setting up a public notebook server, so you don't have to manually configure the notebook server every time you want to spin up a new AMI instance. For the script to work, Jupyter itself should already be installed — which it is in the CS231n AMI . You just have to edit the password in the script. To generate a hashed password, use IPython: In [ 1 ]: from notebook.auth import passwd In [ 2 ]: passwd () Enter password : Verify password : Out [ 2 ]: 'sha1:bcd259ccf...<your hashed password here>' Replace the right hand side of line 24 in the script with the hashed password you just generated. Then, follow these steps to launch an EC2 instance. 1. First, follow the CS231n AWS tutorial up until the step \"Choose the instance type g2.2xlarge , and click on \"Review and Launch\" . Don't click on \"Review and Launch\" yet! 2. Here's where we add a couple extra steps to the tutorial. ** We're going to supply the shell script as user-data , a way to pass in scripts to automate configurations to your AMI . Instead of clicking on \"Review and Launch\", click on the gray button in the lower right \"Next: Configure Instance Details\". In the next page, click on the arrowhead next to \"Advanced Details\" to expand its options. Click on the radio button next to \"As text\", then copy and paste the text from jupyter_userdata.sh (modified with your password) into the field. Warning: if you click on \"As file\" instead and browse to wherever you saved jupyter_userdata.sh , the file must first be base64-encoded . 3. Next, (skipping steps 4. and 5.) click on the link to \"6. Configure Security Group\" near the top of the page. By default, SSH is enabled, but we need to enable access to the notebook server, whose port we've set as 8888 in the bash script. Click on the grey button \"Add Rule\", then for the new rule, choose Type: Custom TCP Rule; Protocol: TCP ; Port Range: 8888; Source: Anywhere. 4. Now, pick up where you left off in the CS231n tutorial (\" … click on \"Review and Launch \".), which takes you to \"Step 7. Review Instance Launch\". Complete the tutorial. Check that the Jupyter notebook server was set up correctly: ssh into your instance (see CS231n instructions). Navigate to ~/caffe/examples . Start the notebook server using the jupyter notebook command. In your web browser, access the notebook server with https://PUBLIC_IP:8888, where PUBLIC_IP is the public IP of your instance, displayed from the instance description on your AWS dashboard. Your browser will warn that your self-signed certificate is insecure or unrecognized. That's ok — click past the warnings, and you should get a sign-in page. Type in your password. Next, you should see the files and directories in /home/ubuntu/caffe/examples Open one of the example notebooks, e.g. 00-classification.ipynb , and try running some cells to make sure everything is working. Voila! We hope this guide removes some obstacles to getting started. Happy learning! The cost of running a GPU instance is high compared to many other instance types, but still very reasonable if you're just tinkering for a few hours on a pre-trained model, not training a whole neural network from scratch. Check out the pricing for an EC2 instance in the section \"On-Demand Instance Prices\" and selecting the region of your AMI . At the time of writing, the cost of an on-demand g2.2xlarge instance in the US West (Northern California) region was $0.7/hour, whereas the price of a spot instance (a cheaper alternative which will automatically terminate when the spot pricing exceeds your bid) was $0.3/hour. If you followed the CS231n tutorial exactly and forgot to supply user data, you can still use this script. First modify the security configuration of your instance according to step 3**. Then use the scp command to copy the script from your local computer to your instance, ssh into your instance, then execute the script: source jupyter_userdata.sh . If you need help with using scp , see \"To use SCP to transfer a file\" in this guide .","tags":"Tools","url":"https://efavdb.com/deep-learning-with-jupyter-on-aws","loc":"https://efavdb.com/deep-learning-with-jupyter-on-aws"},{"title":"Dotfiles for peace of mind","text":"Reinstalling software and configuring settings on a new computer is a pain. After my latest hard drive failure set the stage for yet another round of download-extract-install and configuration file twiddling, it was time to overhaul my approach. \"Enough is enough!\" This post walks through how to back up and automate the installation and configuration process how to set up a minimal framework for data science We'll use a dotfiles repository on Github to illustrate both points in parallel. Dotfiles are named after the configuration files that start with a dot in Unix-based systems. These files are hidden from view in your home directory, but visible with a $ ls -a command. Some examples are .bashrc (for configuring the bash shell), .gitconfig (for configuring git), and .emacs (for configuring the Emacs text editor). Let's provide a concrete example of a customization: suppose you have a hard time remembering the syntax to extract a file (\"Is it tar -xvf, -jxvf, or -zxvf?\"). If you're using a bash shell, you can define a function, extract() in your .bashrc file that makes life a little easier: extract() { if [ -f \"$1\" ]; then case \"$1\" in *.tar.bz2) tar -jxvf \"$1\" ;; *.tar.gz) tar -zxvf \"$1\" ;; *.bz2) bunzip2 \"$1\" ;; *.dmg) hdiutil mount \"$1\" ;; *.gz) gunzip \"$1\" ;; *.tar) tar -xvf \"$1\" ;; *.tbz2) tar -jxvf \"$1\" ;; *.tgz) tar -zxvf \"$1\" ;; *.zip) unzip \"$1\" ;; *.ZIP) unzip \"$1\" ;; *.pax) cat \"$1\" | pax -r ;; *.pax.Z) uncompress \"$1\" --stdout | pax -r ;; *.Z) uncompress \"$1\" ;; *) echo \"'$1' cannot be extracted/mounted via extract()\" ;; esac else echo \"'$1' is not a valid file to extract\" fi } So the next time you have to extract a file some_file.tar.bz2 , just type extract some_file.tar.bz2 in bash. (This example was found in this dotfiles repo .) The structure of my dotfiles takes after the repo described by Lars Kappert in the article \"Getting Started With Dotfiles\" . However, my repo is pared down significantly, with minor modifications for my Linux Mint system (his is OS X) and a focus on packages for data science. A framework for data science This starter environment only has a few parts. We need a text editor — preferably one that can support multiple languages encountered in data science — and a way to manage scientific/statistical software packages. Components The setup consists of: Emacs — a powerful text editor that can be customized to provide an IDE -like experience for both python and R, while providing syntax highlighting for other languages, e.g. markdown, LaTeX, shell, lisp, and so on. (More on customizing Emacs in a future post.) Conda — both a package manager and environment manager. Advantages: Packages are easy to install compared to pip, e.g. see a post by the author of numpy . Conda is language agnostic in terms of both managing packages and environments for different languages (as opposed to pip/virtualenv/venv). This feature is great if you use both python and R. Standard python scientific computing libraries like numpy, scipy, matplotlib, etc. are available in the conda repository. I use the system package manager (i.e. apt-get install ... ) to install a few packages like git, but otherwise rely on Conda to install Python (multiple versions are okay!), R, and their libraries. I like how clean the conda installation feels. Any packages installed by Conda, as well as different versions of Python itself, are neatly organized under the miniconda3 directory in my home directory. In contrast, my previous Linux setups were littered with software installations from various package managers, along with sometimes unsuccessful attempts to compile software from source. Workflow My workflow with Conda follows this helpful post by Tim Hopper. Each project gets its own directory and is associated with an environment whose dependencies are specified by an environment.yml file. For example, create a folder for a project, my_proj. Within the project folder, create a bare-bones environment.yml file to specify a dependency on python 3 and matplotlib: name : my_proj dependencies : - python = 3 - matplotlib Then, to create the conda environment named after that directory, run $ conda env create inside the my_proj directory. To activate the virtual environment, run $ source activate my_proj . Activating a conda environment can be further automated with autoenv . Autoenv automatically activates the environment for you when you $ cd into a project directory. You just need to create a .env file that contains the command to activate your environment, e.g. source activate my_proj , under the project directory. Tim has written a convenient bash function, conda-env-file (see below ), for generating a basic environment.yml file and .env file, which I've incorporated into my own dotfiles, along with autoenv. The order of commands that I type in bash then follows: mkdir my_proj # create project folder cd my_proj # enter project directory conda-env-file # execute homemade function to create environment.yml and .env conda env create # conda creates an environment \"my_proj\" that is named after the project directory (using environment.yml) cd .. cd my_proj # autoenv automatically activates environment (using the file .env) when you re-enter the directory The dotfiles layout Below is the layout of the directories and files (generated by the tree command) in the dotfiles repo . . ├── install │ ├── apt-get.sh │ ├── conda.sh │ ├── git.sh │ ├── install-emacs.sh │ └── install-miniconda.sh ├── install.sh ├── runcom │ ├── .bash_profile │ ├── .bashrc │ └── .profile └── system ├── env ├── functions └── path Configuration There any number of dotfiles that can be configured (for example, see the collection here ), but this repo only provides customizations for the dotfiles .profile , .bash_profile , and .bashrc — located in the directory, runcom (which stands for \"run commands\") — that contain commands that are executed at login or during interactive non-login shell sessions. For details about the role of shell initialization dotfiles, see the end of this post. Instead of putting all our customizations in one long, unwieldy dotfile, it's helpful to divide them into chunks, which we keep in the subfolder, system . The files env , functions , path are sourced in a loop by the dotfiles in runcom . For example, .bashrc sources functions and env : for DOTFILE in \"$DOTFILES_DIR\"/system/{functions,env}; do [ -f \"$DOTFILE\" ] && . \"$DOTFILE\" done Let's take a look at the configurations in each of these files: env - enables autoenv for activating virtual environments [ -f /opt/autoenv/activate.sh ] && . /opt/autoenv/activate.sh functions - defines a custom function, conda-env-file , that generates an environment.yml that lists the dependencies for a conda virtual environment, and a one-line file .env (not to be confused with env in the previous bullet point) used by autoenv. (In addition to pip and python, I include the dependencies ipython, jedi, and flake8 needed by my Emacs python IDE setup.) function conda - env - file { # Create conda environment . yml file and autoenv activation file # based on directory name . autoenvfilename = '.env' condaenvfilename = 'environment.yml' foldername = $ ( basename $ PWD ) if [ ! - f $ condaenvfilename ]; then printf \"name: $foldername\\ndependencies:\\n- pip\\n- python\\n- ipython\\n- jedi\\n- flake8\" > $ condaenvfilename echo \"$condaenvfilename created.\" else echo \"$condaenvfilename already exists.\" fi if [ ! - f $ autoenvfilename ]; then printf \"source activate $foldername\\n\" > $ autoenvfilename echo \"$autoenvfilename created.\" else echo \"$autoenvfilename already exists.\" fi } path - prepends the miniconda3 path to the PATH environment variable. For example, calls to python will default to the Miniconda3 version (3.5.1 in my case) rather than my system version (2.7). export PATH=\"/home/$USER/miniconda3/bin:$PATH\" Now, we're done with configuring the dotfiles in this repo (apart from Emacs, which is treated separately). We just have to create symlinks in our home directory to the dotfiles in runcom , which is performed by the shell script, install.sh : ## ... ln - sfv \"$DOTFILES_DIR/runcom/.bash_profile\" ~ ln - sfv \"$DOTFILES_DIR/runcom/.profile\" ~ ln - sfv \"$DOTFILES_DIR/runcom/.bashrc\" ~ ## ... Installation In addition to setting up dotfiles symlinks, install.sh automates the installation of all our data science tools via calls to each of the scripts in the install subfolder. Each script is named after the mechanism of installation (i.e. apt-get , conda , git ) or purpose (to install Miniconda and Emacs). apt-get.sh - installs a handful of programs using the system package manager, including build-essentials , which is needed to compile programs from source. Also enables source-code repositories (not enabled by default in Linux Mint 17), to be used for compiling emacs from source. install-emacs.sh - build Emacs 24.4 from source, which is needed for compatibility with the Magit plug-in (git for Emacs). At the time of writing, only Emacs 24.3 was available on the system repo. install-miniconda.sh - miniconda includes just conda, conda-build, and python. I prefer this lightweight version to the Anaconda version, which comes with more than 150 scientific packages by default. A note from the Miniconda downloads page: \"There are two variants of the installer: Miniconda is Python 2 based and Miniconda3 is Python 3 based… the choice of which Miniconda is installed only affects the root environment. Regardless of which version of Miniconda you install, you can still install both Python 2.x and Python 3.x environments. The other difference is that the Python 3 version of Miniconda will default to Python 3 when creating new environments and building packages.\" (I chose Miniconda3.) conda.sh - Use conda to install popular scientific packages for python, R, some popular R packages, and packages for IDE support in Emacs. git.sh - Install autoenv for working with virtual environment directories. Also clone the configurations from my Emacs repo . Conclusion The dotfiles repo discussed in this post will remain in this minimal state on GitHub so that it can be easily parsed and built upon. It's the most straightforward to adopt if you are on a similar system (Linux Mint or Ubuntu 14.04), as I haven't put in checks for OSX . If you don't like Emacs, feel free to comment out the relevant lines in install.sh and install/git.sh , and replace with your editor of choice. Also take a look at other collections of awesome dotfiles for nuggets (like the extract() function) to co-opt. And enjoy the peace of mind that comes with having dotfiles insurance! Notes on shell initialization dotfiles The handling of the dotfiles .profile, .bash_profile, and .bashrc is frequently a source of confusion that we'll try to clear up here. For example, .profile and .bash_profile are both recommended for setting environment variables, so what's the point of having both? .profile .profile is loaded upon login to a Unix system (for most distributions) and is where you should put customizations that apply to your whole session, e.g. environment variable assignments like PATH that are not specifically related to bash. .profile holds anything that should be (1) available to graphical applications — like launching a program from a GUI by clicking on an icon or menu — or (2) to sh , which is run by graphical display managers like GDM /LightDM/ MDM when your computer boots up in graphics mode (the most common scenario these days). Note, even though the default login shell is bash in Ubuntu, the default system shell that is used during the bootup process in Ubuntu is dash, not bash , ( readlink -f /bin/sh outputs /bin/dash ). Let's give a concrete example of case (1): the miniconda installer provides a default option to add the miniconda binaries to the search path in .bashrc: export PATH=\"/home/$USER/miniconda3/bin:$PATH\" . Assuming you've used conda (not apt-get ) to install python scientific computing libraries and have set the path in .bashrc, if Emacs is launched from an icon on the desktop, then Emacs plugins that depend on those libraries (e.g. ein , a plugin that integrates IPython with Emacs) will throw an error; since the graphical invocation only loads .profile, the miniconda binaries would not be in the search path. (On the other hand, there would be no problem launching Emacs from the terminal via $ emacs .) For this reason, it's preferable to add the miniconda path in .profile instead of .bashrc. For changes to .profile to take effect, you have to log out entirely and then log back in. .bash_profile Like .profile, .bash_profile should contain environment variable definitions. I haven't yet encountered a situation where a configuration can be set in .bash_profile that can't be set in .profile or .bashrc. Therefore, my .bash_profile just loads .profile and .bashrc. Some choose to bypass .bash_profile entirely and only have .profile (which bash reads if .bash_profile or .bash_login don't exist) and .bashrc. .bashrc Definitions of alias, functions, and other settings you'd want in an interactive command line should be put in .bashrc. .bashrc is sourced by interactive, non-login shells. login, non-login, interactive, and non-interactive shells To check if you're in a login shell, type on the command line echo $0 . If the output is -bash , then you're in a login shell. If the output is bash , then it's not a login shell (see man bash ). Usually, a shell started from a new terminal in a GUI will be an interactive, non-login shell. The notable exception is OSX , whose terminal defaults to starting login shells. Thus, an OSX user may blithely sweep customizations that would ordinarily be placed in .bashrc — like aliases and functions — into .bash_profile and not bother with creating a .bashrc at all. However, those settings would not be properly initialized if the terminal default is changed to non-login shells. If you ssh in or login on a text console, then you get an interactive, login shell. More examples in this StackExchange thread . This discussion might seem pedantic since you can often get away with a less careful setup. In my experience, though, what can go wrong will probably go wrong, so best to be proactive.","tags":"Programming","url":"https://efavdb.com/dotfiles","loc":"https://efavdb.com/dotfiles"},{"title":"Independent component analysis","text":"Two microphones are placed in a room where two conversations are taking place simultaneously. Given these two recordings, can one \"remix\" them in some prescribed way to isolate the individual conversations? Yes! In this post, we review one simple approach to solving this type of problem, Independent Component Analysis ( ICA ). We share an ipython document implementing ICA and link to a youtube video illustrating its application to audio de-mixing. Introduction To formalize the problem posed in the abstract, let two desired conversation signals be represented by \\(c_1(t)\\) and \\(c_2(t)\\) , and two mixed microphone recordings of these by \\(m_1(t)\\) and \\(m_2(t)\\) . We'll assume that the latter are both linear combinations of the former, with \\begin{align} \\label{mean} m_1(t) &= a_1 c_1(t) + a_2 c_2(t) \\\\ m_2(t) &= a_3 c_1(t) + a_4 c_2(t). \\label{1} \\tag{1} \\end{align} Here, we stress that the \\(a_i\\) coefficients in (\\ref{1}) are hidden from us: We only have access to the \\(m_i\\) . Hypothetical illustrations are given in the figure below. Given only these mixed signals, we'd like to recover the underlying \\(c_i\\) used to construct them (spoiler: a sine wave and a saw-tooth function were used for this figure). Amazingly, it turns out that with the introduction of a modest assumption, a simple solution to our problem can be obtained: We need only assume that the desired \\(c_i\\) are mutually independent \\(&#94;1\\) . This assumption is helpful because it turns out that when two independent signals are added together, the resulting mixture is always \"more Gaussian\" than either of the individual, independent signals (a la the central limit theorem). Seeking linear combinations of the available \\(m_i\\) that locally extremize their non-Gaussian character therefore provides a way to identify the pure, unmixed signals. This approach to solving the problem is called \"Independent Component Analysis\", or ICA . Here, we demonstrate the principle of ICA through consideration of the audio de-mixing problem. This is a really impressive application. However, one should strive to remember that the algorithm is not a one-trick-pony. ICA is an unsupervised machine learning algorithm of general applicability — similar in nature, and complementary to, the more familiar PCA algorithm. Whereas in PCA we seek the feature-space directions that maximize captured variance, in ICA we seek those directions that maximize the \"interestingness\" of the distribution — i.e., the non-Gaussian character of the resulting projections. It can be fruitfully applied in many contexts \\(&#94;2\\) . We turn now to the problem of audio de-mixing via ICA . Audio de-mixing In this post, we use the kurtosis of a signal to quantify its degree of \"non-Gaussianess\". For a given signal \\(x(t)\\) , this is defined as $$ \\kappa(x) \\equiv \\left \\langle \\left (x- \\langle x \\rangle \\right)&#94;4 \\right \\rangle - 3 \\left \\langle \\left (x- \\langle x \\rangle \\right)&#94;2 \\right \\rangle&#94;2, \\label{2} \\tag{2} $$ where brackets represent an average over time (or index). It turns out that the kurtosis is always zero for a Gaussian-distributed signal, so (\\ref{2}) is a natural choice of score function for measuring deviation away from Gaussian behavior \\(&#94;3\\) . Essentially, it's a measure of how flat a distribution is — with numbers greater (smaller) than 0 corresponding to distributions that are more (less) flat than a Gaussian. With (\\ref{2}) chosen as our score function, we can now jump right into applying ICA . The code snippet below considers all possible mixtures of two mixed signals \\(m_1\\) and \\(m_2\\) , obtains the resulting signal kurtosis values, and plots the result. def kurtosis_of_mixture ( c1 ): c2 = np . sqrt ( 1 - c1 ** 2 ) s = c1 * m1 + c2 * m2 s = s / np . std ( s ) k = mean ([ item ** 4 for item in s ]) - 3 return k c_array = np . arange ( - 1 , 1 , 0.001 ) k_array = [ kurtosis_of_mixture ( item ) for item in c_array ] plt . plot ( c_array , k_array ) In line \\((3)\\) of the code here, we define the \"remixed\" signal \\(s\\) , which is a linear combination of the two mixed signals \\(m_1\\) and \\(m_2\\) . Note that in line \\((4)\\) , we normalize the signal so that it always has variance \\(1\\) — this simply eliminates an arbitrary scale factor from the analysis. Similarly in line \\((2)\\) , we specify \\(c_2\\) as a function of \\(c_1\\) , requiring the sum of their squared values to equal one — this fixes another arbitrary scale factor. When we applied the code above to the two signals shown in the introduction, we obtained the top plot at right. This shows the kurtosis of \\(s\\) as a function of \\(c_1\\) , the weight applied to signal \\(m_1\\) . Notice that there are two internal extrema in this plot: a peak near \\(-0.9\\) and a local minimum near \\(-0.7\\) . These are the two \\(c_1\\) weight choices that ICA suggests may relate to the pure, underlying signals we seek. To plot each of these signals, we used code similar to the following (the code shown is just for the maximum) index1 = k_array . index ( max ( k_array )) c1 = c_array [ index1 ] c2 = np . sqrt ( 1 - c1 ** 2 ) s = np . array ([ int16 ( item ) for item in c1 * x1 + c2 * x2 ]) plot ( s ) This code finds the index where the kurtosis was maximized, generates the corresponding remix, and plots the result. Applying this, the bottom figure at right popped out. It worked! — and with just a few lines of code, which makes it seem all the more amazing. In summary, we looked for linear combinations of the \\(m_i\\) shown in the introduction that resulted in a stationary kurtosis — plotting these combinations, we found that these were precisely the pure signals we sought \\(&#94;4\\) . A second application to actual audio clips is demoed in our youtube video linked below. The full ipython file utilized in the video can be downloaded on our github page, here \\(&#94;5\\) . Conclusion We hope this little post has you convinced that ICA is a powerful, yet straightforward algorithm \\(&#94;6\\) . Although we've only discussed one application here, many others can be found online: Analysis of financial data, an idea to use ICA to isolate a desired wifi signal from a crowded frequency band, and the analysis of brain waves — see discussion in the article mentioned in reference 2 — etc. In general, the potential application set of ICA may be as large as that for PCA . Next time you need to do some unsupervised learning or data compression, definitely keep it in mind. Footnotes and references [1] Formally, saying that two signals are independent means that the evolution of one conveys no information about that of the other. [2] For those interested in further reading on the theory and applications of ICA , we can recommend the review article by Hyvärinen and Oja — \"Independent Component Analysis: Algorithms and Applications\" — available for free online. [3] Other metrics can also be used in the application of ICA . The kurtosis is easy to evaluate and is also well-motivated because of the fact that it is zero for any Gaussian. However, there are non-Gaussian distributions that also have zero kurtosis. Further, as seen in our linked youtube video, peaks in the kurtosis plot need not always correspond to the pure signals. A much more rigorous approach is to use the mutual information of the signals as your score. This function is zero if and only if you've found a projection that results in a fully independent set of signals. Thus, it will always work. The problem with this choice is that it is much harder to evaluate — thus, simpler scores are often used in practice, even though they aren't necessarily rigorously correct. The article mentioned in footnote 2 gives a good review of some other popular score function choices. [4] In general, symmetry arguments imply that the pure signals will correspond to local extrema in the kurtosis landscape. This works because the kurtosis of \\(x_1 + a x_2\\) is the same as that of \\(x_1 - a x_2\\) , when \\(x_1\\) and \\(x_2\\) are independent. To complete the argument, you need to consider coefficient expansions in the mixed space. The fact that the pure signals can sometimes sit at kurtosis local minima doesn't really jive with the intuitive argument about mixtures being more Gaussian — but that was a vague statement anyways. A rigorous, alternative introduction could be made via mutual information, as mentioned in the previous footnote. [5] To run the script, you'll need ipython installed, as well as the python packages: scipy, numpy, matplotlib, and pyaudio — see instructions for the latter here . The pip install command for pyaudio didn't work for me on my mac, but the following line did: pip install --global-option='build_ext' --global-option='-I/usr/local/include' --global-option='-L/usr/local/lib' pyaudio [6] Of course, things get a bit more complicated when you have a large number of signals. However, fast, simple algorithms have been found to carry this out even in high dimensions. See the reference in footnote 2 for discussion. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods, Theory","url":"https://efavdb.com/independent-component-analysis","loc":"https://efavdb.com/independent-component-analysis"},{"title":"Maximum-likelihood asymptotics","text":"In this post, we review two facts about maximum-likelihood estimators: 1) They are consistent, meaning that they converge to the correct values given a large number of samples, \\(N\\) , and 2) They satisfy the Cramer-Rao lower bound for unbiased parameter estimates in this same limit — that is, they have the lowest possible variance of any unbiased estimator, in the \\(N\\gg 1\\) limit. Introduction We begin with a simple example maximum-likelihood inference problem: Suppose one has obtained \\(N\\) independent samples \\(\\{x_1, x_2, \\ldots, x_N\\}\\) from a Gaussian distribution of unknown mean \\(\\mu\\) and variance \\(\\sigma&#94;2\\) . In order to obtain a maximum-likelihood estimate for these parameters, one asks which \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}&#94;2\\) would be most likely to generate the samples observed. To find these, we first write down the probability of observing the samples, given our model. This is simply $$ P(\\{x_1, x_2, \\ldots, x_N\\} \\vert \\mu, \\sigma&#94;2) =\\\\ \\exp\\left [ \\sum_{i=1}&#94;N \\left (-\\frac{1}{2} \\log (2 \\pi \\sigma&#94;2) -\\frac{1}{2 \\sigma&#94;2} (x_i - \\mu)&#94;2\\right ) \\right ]. \\tag{1} \\label{1} $$ To obtain the maximum-likelihood estimates, we maximize (\\ref{1}): Setting its derivatives with respect to \\(\\mu\\) and \\(\\sigma&#94;2\\) to zero and solving gives \\begin{align}\\label{mean} \\hat{\\mu} &= \\frac{1}{N} \\sum_i x_i \\tag{2} \\\\ \\hat{\\sigma}&#94;2 &= \\frac{1}{N} \\sum_i (x_i - \\hat{\\mu})&#94;2. \\tag{3} \\label{varhat} \\end{align} These are mean and variance values that would be most likely to generate our observation set \\(\\{x_i\\}\\) . Our solutions show that they are both functions of the random observation set. Because of this, \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}&#94;2\\) are themselves random variables, changing with each sample set that happens to be observed. Their distributions can be characterized by their mean values, variances, etc. The average squared error of a parameter estimator is determined entirely by its bias and variance — see eq (2) of prior post . Now, one can show that the \\(\\hat{\\mu}\\) estimate of (\\ref{mean}) is unbiased, but this is not the case for the variance estimator (\\ref{varhat}) — one should (famously) divide by \\(N-1\\) instead of \\(N\\) here to obtain an unbiased estimator \\(&#94;1\\) . This shows that maximum-likelihood estimators need not be unbiased. Why then are they so popular? One reason is that these estimators are guaranteed to be unbiased when \\(N\\) , the sample size, is large. Further, in this same limit, these estimators achieve the minimum possible variance for any unbiased parameter estimate — as set by the fundamental Cramer-Rao bound. The purpose of this post is to review simple proofs of these latter two facts about maximum-likelihood estimators \\(&#94;2\\) . Consistency Let \\(P(x \\vert \\theta&#94;*)\\) be some distribution characterized by a parameter \\(\\theta&#94;*\\) that is unknown. We will show that the maximum-likelihood estimator converges to \\(\\theta&#94;*\\) when \\(N\\) is large: As in (\\ref{1}), the maximum-likelihood solution is that \\(\\theta\\) that maximizes $$\\tag{4} \\label{4} J \\equiv \\frac{1}{N}\\sum_{i=1}&#94;N \\log P(x_i \\vert \\theta), $$ where the \\(\\{x_i\\}\\) are the independent samples taken from \\(P(x \\vert \\theta&#94;*)\\) . By the law of large numbers, when \\(N\\) is large, this average over the samples converges to its population mean. In other words, $$\\tag{5} \\lim_{N \\to \\infty}J \\rightarrow \\int_x P(x \\vert \\theta&#94;*) \\log P(x \\vert \\theta) dx. $$ We will show that \\(\\theta&#94;*\\) is the \\(\\theta\\) value that maximizes the above. We can do this directly, writing $$ \\begin{align} J(\\theta) - J(\\theta&#94;*) & = \\int_x P(x \\vert \\theta&#94;*) \\log \\left ( \\frac{P(x \\vert \\theta) }{P(x \\vert \\theta&#94;*)}\\right) \\\\ & \\leq \\int_x P(x \\vert \\theta&#94;*) \\left ( \\frac{P(x \\vert \\theta) }{P(x \\vert \\theta&#94;*)} - 1 \\right) \\\\ & = \\int_x P(x \\vert \\theta) - P(x \\vert \\theta&#94;*) = 1 - 1 = 0. \\tag{6} \\label{6} \\end{align} $$ Here, we have used \\(\\log t \\leq t-1\\) in the second line. Rearranging the above shows that \\(J(\\theta&#94;*) \\geq J(\\theta)\\) for all \\(\\theta\\) — when \\(N \\gg 1\\) , meaning that \\(J\\) is maximized at \\(\\theta&#94;*\\) . That is, the maximum-likelihood estimator \\(\\hat{\\theta} \\to \\theta&#94;*\\) in this limit \\(&#94;3\\) . Optimal variance To derive the variance of a general maximum-likelihood estimator, we will see how its average value changes upon introduction of a small Bayesian prior, \\(P(\\theta) \\sim \\exp(\\Lambda \\theta)\\) . The trick will be to evaluate the change in two separate ways — this takes a few lines, but is quite straightforward. In the first approach, we do a direct maximization: The quantity to be maximized is now $$ \\label{7} J = \\sum_{i=1}&#94;N \\log P(x_i \\vert \\theta) + \\Lambda \\theta. \\tag{7} $$ Because we take \\(\\Lambda\\) small, we can use a Taylor expansion to find the new solution, writing $$ \\label{8} \\hat{\\theta} = \\theta&#94;* + \\theta_1 \\Lambda + O(\\Lambda&#94;2). \\tag{8} $$ Setting the derivative of (\\ref{7}) to zero, with \\(\\theta\\) given by its value in (\\ref{8}), we obtain $$ \\sum_{i=1}&#94;N \\partial_{\\theta} \\left . \\log P(x_i \\vert \\theta) \\right \\vert_{\\theta&#94;*} + \\\\ \\sum_{i=1}&#94;N \\partial_{\\theta}&#94;2 \\left . \\log P(x_i \\vert \\theta) \\right \\vert_{\\theta&#94;*} \\times \\theta_1 \\Lambda + \\Lambda + O(\\Lambda&#94;2) = 0. \\tag{9} \\label{9} $$ The first term here goes to zero at large \\(N\\) , as above. Setting the terms at \\(O(\\Lambda&#94;1)\\) to zero gives $$ \\theta_1 = - \\frac{1}{ \\sum_{i=1}&#94;N \\partial_{\\theta}&#94;2 \\left . \\log P(x_i \\vert \\theta) \\right \\vert_{\\theta&#94;*} }. \\tag{10} \\label{10} $$ Plugging this back into (\\ref{8}) gives the first order correction to \\(\\hat{\\theta}\\) due to the perturbation. Next, as an alternative approach, we evaluate the change in \\(\\theta\\) by maximizing the \\(P(\\theta)\\) distribution, expanding about its unperturbed global maximum, \\(\\theta&#94;*\\) : We write, formally, $$\\tag{11} \\label{11} P(\\theta) = e&#94;{ - a_0 - a_2 (\\theta - \\theta&#94;*)&#94;2 - a_3 (\\theta - \\theta&#94;*)&#94;3 + \\ldots + \\Lambda \\theta}. $$ Differentiating to maximize (\\ref{11}), and again assuming a solution of form (\\ref{8}), we obtain $$\\label{12} \\tag{12} -2 a_2 \\times \\theta_1 \\Lambda + \\Lambda + O(\\Lambda&#94;2) = 0 \\ \\ \\to \\ \\ \\theta_1 = \\frac{1}{2 a_2}. $$ We now require consistency between our two approaches, equating (\\ref{10}) and (\\ref{12}). This gives an expression for \\(a_2\\) . Plugging this back into (\\ref{11}) then gives (for the unperturbed distribution) $$\\tag{13} \\label{13} P(\\theta) = \\mathcal{N} \\exp \\left [ N \\frac{ \\langle \\partial_{\\theta}&#94;2 \\left . \\log P(x, \\theta) \\right \\vert_{\\theta&#94;*} \\rangle }{2} (\\theta - \\theta&#94;*)&#94;2 + \\ldots \\right]. $$ Using this Gaussian approximation \\(&#94;4\\) , we can now read off the large \\(N\\) variance of \\(\\hat{\\theta}\\) as $$\\tag{14} \\label{14} var(\\hat{\\theta}) = - \\frac{1}{N} \\times \\frac{1}{\\langle \\partial_{\\theta}&#94;2 \\left . \\log P(x, \\theta) \\right \\vert_{\\theta&#94;*} \\rangle }. $$ This is the lowest possible value for any unbiased estimator, as set by the Cramer-Rao bound. The proof shows that maximum-likelihood estimators always saturate this bound, in the large \\(N\\) limit — a remarkable result. We discuss the intuitive meaning of the Cramer-Rao bound in a prior post . Footnotes [1] To see that (\\ref{varhat}) is biased, we just need to evaluate the average of \\(\\sum_i (x_i - \\hat{\\mu})&#94;2\\) . This is $$ \\overline{\\sum_i x_i&#94;2 - 2 \\sum_{i,j} \\frac{x_i x_j}{N} + \\sum_{i,j,k} \\frac{x_j x_k}{N&#94;2}} = N \\overline{x&#94;2} - (N-1) \\overline{x}&#94;2 - \\overline{x&#94;2} \\\\ = (N-1) \\left ( \\overline{x&#94;2} - \\overline{x}&#94;2 \\right) \\equiv (N-1) \\sigma&#94;2. $$ Dividing through by \\(N\\) , we see that \\(\\overline{\\hat{\\sigma}&#94;2} = \\left(\\frac{N-1}{N}\\right)\\sigma&#94;2\\) . The deviation from the true variance \\(\\sigma&#94;2\\) goes to zero at large \\(N\\) , but is non-zero for any finite \\(N\\) : The estimator is biased, but the bias goes to zero at large \\(N\\) . [2] The consistency proof is taken from lecture notes by D. Panchenko, see here . Professor Panchenko is quite famous for having proven the correctness of the Parisi ansatz in replica theory. Our variance proof is original — please let us know if you have seen it elsewhere. Note that it can also be easily extended to derive the covariance matrix of a set of maximum-likelihood estimators that are jointly distributed — we cover only the scalar case here, for simplicity. [3] The proof here actually only shows that there is no \\(\\theta\\) that gives larger likelihood than \\(\\theta&#94;*\\) in the large \\(N\\) limit. However, for some problems, it is possible that more than one \\(\\theta\\) maximizes the likelihood. A trivial example is given by the case where the distribution is actually only a function of \\((\\theta - \\theta_0)&#94;2\\) . In this case, both values \\(\\theta_0 \\pm (\\theta&#94;* - \\theta_0)\\) will necessarily maximize the likelihood. [4] It's a simple matter to carry this analysis further, including the cubic and higher order terms in the expansion (\\ref{11}). These lead to correction terms for (\\ref{14}), smaller in magnitude than that given there. These terms become important when \\(N\\) decreases in magnitude. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"https://efavdb.com/maximum-likelihood-asymptotics","loc":"https://efavdb.com/maximum-likelihood-asymptotics"},{"title":"Principal component analysis","text":"We review the two essentials of principal component analysis (\" PCA \"): 1) The principal components of a set of data points are the eigenvectors of the correlation matrix of these points in feature space. 2) Projecting the data onto the subspace spanned by the first \\(k\\) of these — listed in descending eigenvalue order — provides the best possible \\(k\\) -dimensional approximation to the data, in the sense of captured variance. Introduction One way to introduce principal component analysis is to consider the problem of least-squares fits: Consider, for example, the figure shown below. To fit a line to this data, one might attempt to minimize the squared \\(y\\) residuals (actual minus fit \\(y\\) values). However, if the \\(x\\) and \\(y\\) values are considered to be on an equal footing, this \\(y\\) -centric approach is not quite appropriate. A natural alternative is to attempt instead to find the line that minimizes the total squared projection error : If \\((x_i, y_i)\\) is a data point, and \\((\\hat{x}_i, \\hat{y}_i)\\) is the point closest to it on the regression line (aka, its \"projection\" onto the line), we attempt to minimize $$\\tag{1} \\label{score} J = \\sum_i (x_i - \\hat{x}_i)&#94;2 + (y_i - \\hat{y}_i)&#94;2. $$ The summands here are illustrated in the figure: The dotted lines shown are the projection errors for each data point relative to the red line. The minimizer of (\\ref{score}) is the line that minimizes the sum of the squares of these values. Generalizing the above problem, one could ask which \\(k\\) -dimensional hyperplane passes closest to a set of data points in \\(N\\) -dimensions. Being able to identify the solution to this problem can be very helpful when \\(N \\gg 1\\) . The reason is that in high-dimensional, applied problems, many features are often highly-correlated. When this occurs, projection of the data onto a \\(k\\) -dimensional subspace can often result in a great reduction in memory usage (one moves from needing to store \\(N\\) values for each data point to \\(k\\) ) with minimal loss of information (if the points are all near the plane, replacing them by their projections causes little distortion). Projection onto subspaces can also be very helpful for visualization: For example, plots of \\(N\\) -dimensional data projected onto a best two-dimensional subspace can allow one to get a feel for a dataset's shape. At first glance, the task of actually minimizing (\\ref{score}) may appear daunting. However, it turns out this can be done easily using linear algebra. One need only carry out the following three steps: Preprocessing: If appropriate, shift features and normalize so that they all have mean \\(\\mu = 0\\) and variance \\(\\sigma&#94;2 = 1\\) . The latter, scaling step is needed to account for differences in units, which may cause variations along one component to look artificially large or small relative to those along other components (eg, one raw component might be a measure in centimeters, and another in kilometers). Compute the covariance matrix. Assuming there are \\(m\\) data points, the \\(i\\) , \\(j\\) component of this matrix is given by: $$\\tag{2} \\label{2} \\Sigma_{ij}&#94;2 = \\frac{1}{m}\\sum_{l=1}&#94;m \\langle (f_{l,i} - \\mu_i) (f_{l,j} - \\mu_j) \\rangle\\\\ = \\langle x_i \\vert \\left (\\frac{1}{m} \\sum_{l=1}&#94;m \\vert \\delta f_l \\rangle \\langle \\delta f_l \\vert \\right) \\vert x_j \\rangle.$$ Note that, at right, we are using bracket notation for vectors. We make further use of this below — see footnote [1] at bottom for review. We've also written \\(\\vert \\delta f_l \\rangle\\) for the vector \\(\\vert f_l \\rangle - \\sum_{i = 1}&#94;n \\mu_i \\vert x_i \\rangle\\) — the vector \\(\\vert f_l \\rangle\\) with the dataset's centroid subtracted out. Project all feature vectors onto the \\(k\\) eigenvectors \\(\\{\\vert v_j \\rangle\\) , \\(j = 1 ,2 \\ldots, k\\}\\) of \\(\\Sigma&#94;2\\) that have the largest eigenvalues \\(\\lambda_j\\) , writing $$\\tag{3} \\label{3} \\vert \\delta f_i \\rangle \\approx \\sum_{j = 1}&#94;k \\langle v_j \\vert \\delta f_i \\rangle \\times \\vert v_j\\rangle. $$ The term \\(\\langle v_j \\vert \\delta f_i \\rangle\\) above is the coefficient of the vector \\(\\vert \\delta f_i \\rangle\\) along the \\(j\\) -th principal component. If we set \\(k = N\\) above, (\\ref{3}) becomes an identity. However, when \\(k < N\\) , the expression represents an approximation only, with the vector \\(\\vert \\delta f_i \\rangle\\) approximated by its projection into the subspace spanned by the largest \\(k\\) principal components. The steps above are all that are needed to carry out a PCA analysis/compression of any dataset. We show in the next section why this solution will indeed provide the \\(k\\) -dimensional hyperplane resulting in minimal dataset projection error. Mathematics of PCA To understand PCA , we proceed in three steps. Significance of a partial trace: Let \\(\\{\\textbf{u}_j \\}\\) be some arbitrary orthonormal basis set that spans our full \\(N\\) -dimensional space, and consider the sum \\begin{align}\\tag{4} \\label{4} \\sum_{j = 1}&#94;k \\Sigma&#94;2_{jj} = \\frac{1}{m} \\sum_{i,j} \\langle u_j \\vert \\delta f_i \\rangle \\langle \\delta f_i \\vert u_j \\rangle\\\\ = \\frac{1}{m} \\sum_{i,j} \\langle \\delta f_i \\vert u_j \\rangle \\langle u_j \\vert \\delta f_i \\rangle\\\\ \\equiv \\frac{1}{m} \\sum_{i} \\langle \\delta f_i \\vert P \\vert \\delta f_i \\rangle. \\end{align} To obtain the first equality here, we have used \\(\\Sigma&#94;2 = \\frac{1}{m} \\sum_{i} \\vert \\delta f_i \\rangle \\langle \\delta f_i \\vert\\) , which follows from (\\ref{2}). To obtain the last, we have written \\(P\\) for the projection operator onto the space spanned by the first \\(k\\) \\(\\{\\textbf{u}_j \\}\\) . Note that this last equality implies that the partial trace is equal to the average squared length of the projected feature vectors — that is, the variance of the projected data set. Notice that the projection error is simply given by the total trace of \\(\\Sigma&#94;2\\) , minus the partial trace above. Thus, minimization of the projection error is equivalent to maximization of the projected variance, (\\ref{4}). We now consider which basis maximizes (\\ref{4}). To do that, we decompose the \\(\\{\\textbf{u}_i \\}\\) in terms of the eigenvectors \\(\\{\\textbf{v}_j\\}\\) of \\(\\Sigma&#94;2\\) , writing \\begin{align} \\tag{5} \\label{5} \\vert u_i \\rangle = \\sum_j \\vert v_j \\rangle \\langle v_j \\vert u_i \\rangle \\equiv \\sum_j u_{ij} \\vert v_j \\rangle. \\end{align} Here, we've inserted the identity in the \\(\\{v_j\\}\\) basis, and written \\( \\langle v_j \\vert u_i \\rangle \\equiv u_{ij}\\) . With these definitions, the partial trace becomes \\begin{align}\\tag{6} \\label{6} \\sum_{i=1}&#94;k \\langle u_i \\vert \\Sigma&#94;2 \\vert u_i \\rangle = \\sum_{i,j,l} u_{ij}u_{il} \\langle v_j \\vert \\Sigma&#94;2 \\vert v_l \\rangle \\\\= \\sum_{i=1}&#94;k\\sum_{j} u_{ij}&#94;2 \\lambda_j. \\end{align} The last equality here follows from the fact that the \\(\\{\\textbf{v}_i\\}\\) are the eigenvectors of \\(\\Sigma&#94;2\\) — we have also used the fact that they are orthonormal, which follows from the fact that \\(\\Sigma&#94;2\\) is a real, symmetric matrix. The sum (\\ref{6}) is proportional to a weighted average of the eigenvalues of \\(\\Sigma&#94;2\\) . We have a total mass of \\(k\\) to spread out amongst the \\(N\\) eigenvalues. The maximum mass that can sit on any one eigenvalue is one. This follows since \\(\\sum_{i = 1}&#94;k u_{ij}&#94;2 \\leq \\sum_{i = 1}&#94;N u_{ij}&#94;2 =1\\) , the latter equality following from the fact that \\( \\sum_{i = 1}&#94;N u_{ij}&#94;2\\) is an expression for the squared length of \\(\\vert v_j\\rangle\\) in the \\(\\{u_i\\}\\) basis. Under these constraints, the maximum possible average one can get in (\\ref{6}) occurs when all the mass sits on the largest \\(k\\) eigenvalues, with each of these eigenvalues weighted with mass one. This condition occurs if and only if the first \\(k\\) \\(\\{\\textbf{u}_i\\}\\) span the same space as that spanned by the first \\(k\\) \\(\\{\\textbf{v}_j\\}\\) — those with the \\(k\\) largest eigenvalues. That's it for the mathematics of PCA . Footnotes [1] Review of bracket notation : \\(\\vert x \\rangle\\) represents a regular vector, \\(\\langle x \\vert\\) is its transpose, and \\(\\langle y \\vert x \\rangle\\) represents the dot product of \\(x\\) and \\(y\\) . So, for example, when the term in parentheses at the right side of (\\ref{2}) acts on the vector \\(\\vert x_j \\rangle\\) to its right, you get \\( \\frac{1}{m} \\sum_{k=1}&#94;m \\vert \\delta f_k \\rangle \\left (\\langle \\delta f_k \\vert x_j \\rangle\\right).\\) Here, \\( \\left (\\langle \\delta f_k \\vert x_j \\rangle\\right)\\) is a dot product, a scalar, and \\(\\vert \\delta f_k \\rangle\\) is a vector. The result is thus a weighted sum of vectors. In other words, the bracketed term (\\ref{2}) acts on a vector and returns a linear combination of other vectors. That means it is a matrix, as is any other object of form \\(\\sum_i \\vert a_i \\rangle \\langle b_i \\vert\\) . A special, important example is the identity matrix: Given any complete, orthonormal set of vectors \\(\\{x_j\\}\\) , the identity matrix \\(I\\) can be written as \\(I = \\sum_i \\vert x_i \\rangle \\langle x_i \\vert\\) . This identity is often used to make a change of basis. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods, Theory","url":"https://efavdb.com/principal-component-analysis","loc":"https://efavdb.com/principal-component-analysis"},{"title":"NBA 2015-16!!!","text":"NBA is back this Tuesday! The dashboard and weekly predictions are now live*, once again. These will each be updated daily, with game winner predictions, hypothetical who-would-beat-whom daily matchup predictions, and more. For a discussion on how we make our predictions, see our first post on this topic. Note that our approach does not make use of any bookie predictions (unlike many other sites), and so provide an independent look on the game. This season, we hope to crack 70% accuracy! Note that we have left up last season's completed games results, for review purposes. Once every team has played one game, we'll switch it over to the current season's results.","tags":"NBA prediction project","url":"https://efavdb.com/nba-2015-16","loc":"https://efavdb.com/nba-2015-16"},{"title":"Support Vector Machines for classification","text":"To whet your appetite for support vector machines, here's a quote from machine learning researcher Andrew Ng: \"SVMs are among the best (and many believe are indeed the best) ‘off-the-shelf' supervised learning algorithms.\" Professor Ng covers SVMs in his excellent Machine Learning MOOC , a gateway for many into the realm of data science, but leaves out some details, motivating us to put together some notes here to answer the question: \" What are the support vectors in support vector machines?\" We also provide python (https://github.com/EFavDB/svm-classification/blob/master/svm.ipynb) using scikit-learn's svm module to fit a binary classification problem using a custom kernel, along with code to generate the (awesome!) interactive plots in Part 3. This post consists of three sections: Part 1 sets up the problem from a geometric point of view and then shows how it can be framed as an optimization problem. Part 2 transforms the optimization problem and uncovers the support vectors in the process. Part 3 discusses how kernels can be used to separate non-linearly separable data. Part 1: Defining the margin Maximizing the margin The figure below is a binary classification problem (points labeled \\(y_i = \\pm 1\\) ) that is linearly separable. There are many possible decision boundaries that would perfectly separate the two classes, but an SVM will choose the line in 2-d (or \"hyperplane\", more generally) that maximizes the margin around the boundary. Intuitively, we can be very confident about the labels of points that fall far from the boundary, but we're less confident about points near the boundary. Formulating the margin with geometry Any point \\(\\boldsymbol{x}\\) lying on the separating hyperplane satisfies: \\(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b = 0\\) \\(\\boldsymbol{w}\\) is the vector normal to the plane, and \\(b\\) is a constant that describes how much the plane is shifted relative to the origin. The distance of the plane from the origin is \\(b / \\| \\boldsymbol{w} \\|\\) . Now draw parallel planes on either side of the decision boundary, so we have what looks like a road, with the decision boundary as the median, and the additional planes as gutters. The margin, i.e. the width of the road, is ( \\(d_+ + d_-\\) ) and is restricted by the data points closest to the boundary, which lie on the gutters. The half-spaces bounded by the planes on the gutters are: \\(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b \\geq +a\\) , for \\(y_i = +1\\) \\(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b \\leq -a\\) , for \\(y_i = -1\\) These two conditions can be put more succinctly: \\(y_i (\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) \\geq a, \\forall \\; i\\) Some arithmetic leads to the equation for the margin: \\(d_+ + d_- = 2a / \\| \\boldsymbol{w} \\|\\) Without loss of generality, we can set \\(a=1\\) , since it only sets the scale (units) of \\(b\\) and \\(\\boldsymbol{w}\\) . So to maximize the margin, we have to maximize \\(1 / \\| \\boldsymbol{w} \\|\\) . However, this is an unpleasant (non-convex) objective function. Instead we minimize \\(\\| \\boldsymbol{w}\\|&#94;2\\) , which is convex. The optimization problem Maximizing the margin boils down to a constrained optimization problem: minimize some quantity \\(f(w)\\) , subject to constraints \\(g(w,b)\\) . This optimization problem is particularly nice because it is convex; the objective \\(\\| \\boldsymbol{w}\\|&#94;2\\) is convex, as are the constraints, which are linear. In other words, we are faced with a quadratic programming problem. The standard format of the optimization problem for the separable case is $$\\tag{1} \\label{problem} \\begin{align} \\text{minimize} \\quad & f(w) \\equiv (1/2) \\| \\boldsymbol{w}\\|&#94;2 \\ \\text{subject to} \\quad & g(w,b) \\equiv -y_i (\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) + 1 \\leq 0, \\; i = 1 \\ldots m \\end{align} $$ Before we address how to solve this optimization problem in Part 2, let's first consider the case when data is non-separable. Soft margin SVM : the non-separable problem and regularization For non-separable data, we relax the constraints in (\\ref{problem}) while penalizing misclassified points via a cost parameter \\(C\\) and slack variables \\(\\xi_i\\) that define the amount by which data points are on the wrong side of the margin. $$\\tag{2} \\label{regularization} \\begin{align} \\text{minimize} \\quad & (1/2) \\| \\boldsymbol{w}\\|&#94;2 + C \\sum_i&#94;m \\xi_i \\\\ \\text{subject to} \\quad & y_i (\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) \\geq 1 - \\xi_i, \\; i = 1 \\ldots m \\\\ & \\xi_i \\geq 0, \\quad i = 1 \\ldots m \\end{align} $$ A large penalty — large \\(C\\) — for misclassifications will lead to learning a lower bias, higher variance SVM , and vice versa for small \\(C\\) . The soft margin is used in practice; even in the separable case, it can be desirable to allow tradeoffs between the size of the margin and number of misclassifications. Outliers can skew the decision boundary learned by (\\ref{problem}) towards a model with small margins + perfect classification, in contrast to a possibly more robust model learned by (\\ref{regularization}) with large margins + some misclassified points. Part 2: Solving the optimization problem In Part 1, we showed how to set up SVMs as an optimization problem. In this section, we'll see how the eponymous support vectors emerge when we rephrase the minimization problem as an equivalent maximization problem. To recap: Given \\(m\\) training points that are labeled \\(y_i = \\pm 1\\) , our goal is to maximize the margin of the hyperplane defined by \\(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b = 0\\) . We'll use the separable case (\\ref{problem}) as our starting point, but the steps in the procedure and final result are similar for the non-separable case (also worked out in ref [ 3 ]). The Lagrangian formulation How do we solve this optimization problem? Minimizing a function without constraints is probably familiar: set the derivative of the function (the objective) to zero and solve. With constraints, the procedure is similar to setting the derivative of the objective equal to zero. Instead of taking the derivative of the objective itself, however, we'll operate on the Lagrangian \\(\\mathcal{L}\\) , which combines the objective and inequality constraints into one function: $$\\tag{3} \\label{Lagrangian} \\mathcal{L}(w,b,\\alpha) = f(w) + \\sum_i&#94;m \\alpha_i g_i(w,b) $$ We've just introduced additional variables \\(\\alpha_i\\) , Lagrange multipliers, that make it easier to work with the constraints (see Wikipedia about the method of Lagrange multipliers ). Note, a more general form for the Lagrangian would include another summation term in (\\ref{Lagrangian}) to uphold equality constraints. Since there are only inequality constraints here, we'll omit the extra term. Constructing the dual problem Much of the following discussion is based off ref [2] , which has a nice introduction to duality in the context of SVMs. First, let's make the following observation: $$\\tag{Obs. 1} \\max_{\\alpha} \\mathcal{L}(w,b,\\alpha) = \\begin{cases} f(w), & \\text{if } g_i(w) \\leq 0, \\; \\text{(constraints satisfied)} \\\\ \\infty, & \\text{if } g_i(w) \\gt 0, \\; \\text{(constraints violated)} \\end{cases} $$ Basically, if any constraint \\(j\\) is violated, i.e. \\(g_j(w) > 0\\) , then the Lagrange multiplier \\(\\alpha_j\\) that is multiplying \\(g_j(w)\\) can be made arbitrarily large ( \\(\\rightarrow \\infty\\) ) in order to maximize \\(\\mathcal{L}\\) . On the other hand, if all the constraints are satisfied, \\(g_i(w) \\leq 0\\) \\(\\forall \\; i\\) , then \\(\\mathcal{L}\\) is maximized by setting the \\(\\alpha_i\\) s that are multiplying negative quantities equal to zero. However, Lagrangian multipliers multiplying \\(g_i(w)\\) that satisfy the constraints with equality, \\(g_i(w) = 0\\) , can be non-zero without diminishing \\(\\mathcal{L}\\) . The last statement amounts to the property of \"complementary slackness\" in the Karush-Kuhn-Tucker conditions for the solution: $$\\tag{4} \\label{complementarity} \\alpha_i g_i(w) = 0 $$ Recall from the original geometric picture: only a few points lie exactly on the margins, and those points are described by \\(g_i(w) = 0\\) (and thus have non-zero Lagrange multipliers). The points on the margin are the support vectors. Next, we make use of the Max-Min inequality: $$ \\max_{\\alpha} \\min_{w,b} \\mathcal{L}(w,b,\\alpha) \\leq \\min_{w,b} \\max_{\\alpha} \\mathcal{L}(w,b,\\alpha) $$ This inequality is an equality under certain conditions, which our problem satisfies (convex \\(f\\) and \\(g\\) ). The left side of the inequality is called the dual problem, and the right side is the primal problem. Now we can put it all together: Observation 1 tells us that solving the right side (primal problem) of the Max-Min inequality is the same as solving the original problem. Because our problem is convex, solving the left side (dual) is equivalent to solving the primal problem by the Max-Min inequality. Thus we're set to approach the solution via the dual problem, which is useful for dealing with nonlinear decision boundaries. Solving the dual problem The dual problem to solve is \\(\\max_{\\alpha} \\min_{w,b} \\mathcal{L}(w,b,\\alpha)\\) , subject to constraints * on the Lagrange multipliers: \\(\\alpha_i \\geq 0\\) . Let's work out the inner part of the expression explicitly. We obtain \\(\\min_{w,b} \\mathcal{L}(w,b,\\alpha)\\) by setting: $$ \\nabla_\\boldsymbol{w} \\mathcal{L} = 0; \\quad \\partial_b \\mathcal{L} = 0 $$ These equations for the partial derivatives give us, respectively: $$ \\boldsymbol{w} = \\sum_i \\alpha_i y_i \\boldsymbol{x}_i; \\quad \\sum_i \\alpha_i y_i = 0 $$ \\(\\boldsymbol{w}\\) is a linear combination of the coordinates of the training data. Only the support vectors, which have non-zero \\(\\alpha_i\\) , contribute to the sum. To predict the label for a new test point \\(\\boldsymbol{x_t}\\) , simply evaluate the sign of $$\\tag{5} \\label{testing} \\boldsymbol{w} \\cdot \\boldsymbol{x} + b = \\sum_i \\alpha_i y_i \\boldsymbol{x}_i \\cdot \\boldsymbol{x_t} + b $$ where b can be computed from the KKT complementarity condition (\\ref{complementarity}) by plugging in the values for any support vector. The equation for the separating hyperplane is entirely determined by the support vectors. Plugging the last two equations into \\(\\mathcal{L}\\) leads to the dual formulation of the problem \\( \\max_{\\alpha} \\mathcal{L}_D\\) : $$\\tag{6} \\label{dual} \\begin{align} \\max_{\\alpha} \\quad & \\sum_i \\alpha_i - (1/2) \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j \\boldsymbol{x_i} \\cdot \\boldsymbol{x_j} \\\\ \\text{subject to} \\quad & \\alpha_i \\geq 0, \\; i = 1 \\ldots m \\\\ & \\sum_i \\alpha_i y_i = 0 \\end{align} $$ The dual for the non-separable primal Lagrangian (\\ref{regularization}) — derived using the same procedure we just followed — looks just like the dual for the separable case (\\ref{dual}), except that the Lagrange multipliers are bounded from above by the regularization constant: \\(0 \\leq \\alpha_i \\leq C\\) . Notably, the slack variables \\(\\xi_i\\) do not appear in the dual of the soft margin SVM . The dual (called the Wolfe dual) is easier to solve because of the simpler form of its inequality constraints and is the form used in algorithms such as the Sequential Minimal Optimization algorithm, which is implemented in the popular SVM solver, LIBSVM . The key feature of the dual is that training vectors only appear as dot products \\(\\boldsymbol{x_i} \\cdot \\boldsymbol{x_j}\\) . This property allows us to generalize to the nonlinear case via the \"kernel trick\" discussed in Part 3 of this post. Some of you may be familiar with using Lagrangian multipliers to optimize some function \\(f(\\boldsymbol{x})\\) subject to equality constraints \\(g(\\boldsymbol{x}) = 0\\) , in which case the Lagrangian multipliers are unconstrained. The Karush-Kuhn-Tucker conditions generalize the method to include inequality constraints \\(g(\\boldsymbol{x}) \\leq 0\\) , which results in additional constraints on the associated Lagrangian multipliers (as we have here). Part 3: Kernels Data that is not linearly separable in the original input space may be separable if mapped to a different space. Consider the following example of nonlinearly separable, two-dimensional data: However, if we map the 2-d input data \\(\\boldsymbol{x} = (x, y)\\) to 3-d feature space by a function \\(\\Phi(\\boldsymbol{x}) = (x,\\; y,\\; x&#94;2 + y&#94;2)\\) , the blue and red points can be separated with a plane in the new (3-d) space. See the plot below of the decision boundary, the mapped points, as well as the the original data points in the x-y plane. Drag the figure to rotate it, or zoom in and out with your mouse wheel! Code to generate and fit the data in this example with scikit-learn's SVM module, as well as code to create the plot.ly interactive plots above, is available in IPython notebooks on github . From maps to kernels So how do we incorporate mapping the data into the formulation of the problem? Recall that the data appears as a dot product in the dual Lagrangian (\\ref{dual}). If we decide to train an SVM on the mapped data, then the dot product of the input data in (\\ref{dual}) is replaced by the dot product of the mapped data: \\(\\boldsymbol{x_i} \\cdot \\boldsymbol{x_j} \\rightarrow \\Phi(\\boldsymbol{x_i}) \\cdot \\Phi(\\boldsymbol{x_j})\\) The kernel is simply the dot product of the mapping functions. In the example above, the inner product of the mapping function is an instance of a polynomial kernel: $$ K(x_i, x_j) = \\Phi(\\boldsymbol{x_i}) \\cdot \\Phi(\\boldsymbol{x_j}) = x_i x_j + y_i y_j + (x_i&#94;2 + y_i&#94;2)(x_j&#94;2 + y_j&#94;2) $$ In practice, we work directly with the kernel \\(K(x_i, x_j)\\) rather than explicitly computing the map of the data points ** . Computing the kernel directly allows us to sidestep the computationally expensive operation of mapping data to a high dimensional space and then taking a dot product (see ref [ 2 ] for examples comparing computational times of the two methods). Using a kernel, the second term in the objective of the dual problem (\\ref{dual}) becomes $$ \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) $$ The kernel also appears in the evaluation of (\\ref{testing}) to predict the classification of a test point \\(\\boldsymbol{x_t}\\) : $$\\tag{8} \\label{testingKernel} sgn \\left(\\sum_i \\alpha_i y_i K(x_i, x_t) + b \\right) $$ Which functions are valid kernels to use in the kernel trick? i.e. given \\(K(x_i, x_j)\\) , does some feature map \\(\\Phi\\) exist such that \\(K(x_i, x_j)=\\Phi(\\boldsymbol{x_i}) \\cdot \\Phi(\\boldsymbol{x_j})\\) for any \\(i,\\ j\\) ? Mercer's condition states that a necessary and sufficient condition for \\(K\\) to be a valid kernel is that it is symmetric and positive semi-definite \\(&#94;\\dagger\\) . Some popular kernels are: $$ \\begin{align} \\text{polynomial:} & \\quad (\\boldsymbol{x_i} \\cdot \\boldsymbol{x_j} + c)&#94;p \\\\ \\text{Gaussian radial basis function:} & \\quad \\exp(-\\|\\boldsymbol{x_i} - \\boldsymbol{x_j} \\|&#94;2/2\\sigma&#94;2) \\end{align} $$ The optimal parameters for the degree of the polynomial \\(p\\) and spread of the Gaussian \\(\\sigma\\) (as well as the regularization parameter) are determined by cross-validation. Computing the above kernels takes \\(\\mathcal{O}(d)\\) time, where \\(d\\) is the dimension of the input space, since we have to evaluate \\(\\boldsymbol{x_i} \\cdot \\boldsymbol{x_j}\\) in the polynomial kernel and \\(\\boldsymbol{x_i} - \\boldsymbol{x_j}\\) in the Gaussian kernel. Comparing runtimes of linear and nonlinear kernels The computational complexity for classification/prediction , i.e. at test time, can be obtained by eyeballing (\\ref{testing}) and (\\ref{testingKernel}). Let \\(d\\) be the dimension of the input space and \\(n\\) be the size of the training set, and assume the number of support vectors \\(n_S\\) is some fraction of \\(n\\) , \\(n_S \\sim \\mathcal{O}(n)\\) . In the case of working with the linear kernel/original input space, \\(\\boldsymbol{w}\\) can be explicitly evaluated to obtain the separating hyperplane parameters, so that classification in (\\ref{testing}) takes \\(\\mathcal{O}(d)\\) time. On the other hand, with the kernel trick, the hyperplane parameters are not explicitly evaluated. Assume calculating a kernel takes \\(\\mathcal{O}(d)\\) time, cf. the polynomial and Gaussian kernels; then test time for a nonlinear \\(K\\) in (\\ref{testingKernel}) takes \\(\\mathcal{O}(nd)\\) time. Estimating the computational complexity for training is complicated, so we defer the discussion to refs [ 4a , 4b ] and simply state the result: training for linear kernels is \\(\\mathcal{O}(nd)\\) while training for nonlinear kernels using the Sequential Minimal Optimization algorithm is \\(\\mathcal{O}(n&#94;2)\\) to \\(\\mathcal{O}(n&#94;3)\\) (dependent on the regularization parameter \\(C\\) ), making nonlinear kernel SVMs impractical for larger datasets (a couple of 10,000 samples according to scikit-learn ). ** More than one mapping and feature space (dimension) may exist for a particular kernel. See section 4 of ref [ 1 ] for examples. \\(&#94;\\dagger\\) See ref [ 2 ] for a simple proof in terms of the Kernel (Gram) matrix, i.e. the kernel function evaluated on a finite set of points. Discussion We've glimpsed the elegant theory behind the construction of SVMs and seen how support vectors pop out of the mathematical machinery. Geometrically, the support vectors are the points lying on the margins of the decision boundary. How about using SVMs in practice? In his Coursera course, Professor Ng recommends linear and Gaussian kernels for most use cases. He also provides some rules of thumb (based on the current state of SVM algorithms) for different sample sizes \\(n\\) and input data dimension/number of features \\(d\\) , restated here: case method \\(n\\) \\(d\\) \\(n \\ll d\\) , e.g. genomics, bioinformatics data linear kernel SVM or logistic regression 10 - 1000 10,000 \\(n\\) intermediate, \\(d\\) small Gaussian kernel SVM 10 - 10,000 1 - 1000 \\(n \\gg d\\) create features, then linear kernel SVM or logistic regression 50,000+ 1 - 1000 The creators of the LIBSVM and LIBLINEAR packages also provide a user's guide for novices, which includes a study of when to use linear instead of radial basis function kernels. They recommend linear SVMs when \\(d\\) and \\(n\\) are both large, often encountered in document classification problems where bag-of-words approaches can generate huge numbers of features (in their example \\(n =\\) 20,000, \\(d =\\) 47,000). The idea is that if the input data is already high-dimensional, then it shouldn't be necessary to apply nonlinear transformations to it in order to obtain a separating hyperplane. Tip: LIBLINEAR is specifically optimized for linear kernels and should be used instead of LIBSVM in the linear case. Further reading In addition to the many excellent written tutorials on SVMs online, we highly recommend viewing lectures 14 and 15 of Yaser Abu-Mostafa's MOOC , Learning from Data , which cover SVMs at about the same level as this post, with the considerable added benefit of Professor Abu-Mostafa's explanations. He also discusses the generalization performance of SVMs as a function of the number of support vectors using VC theory (also see [ 1 ]). There is a lot more theory on SVMs that we haven't touched upon. For example, SVMs can be framed as a penalization method [ 3 ] or \"regularization network\" , c.f. ridge regression, but with a hinge loss rather than squared error. Insights about the choice of a kernel have also been developed in that framework. [ 1 ] Burges, C. J.C. (1998). A Tutorial on Support Vector Machines for Pattern Recognition. Knowledge Discovery and Data Mining 2 (2) 121-167. [ 2 ] Ng, A. Support Vector Machines [ PDF document]. Retrieved from lecture notes online: http://cs229.stanford.edu/notes/cs229-notes3.pdf Lecture notes by Andrew Ng for a more advanced class (but still in his signature intuitive style). [ 3 ] Hastie, T., Tibshirani, R., and Friedman, J. (2009). The Elements of Statistical Learning. See section 12.2.1, page 420, for derivation of the dual Lagrangian for the nonseparable case. [ 4a ] Bottou, L. and Lin C-J., (2006). Support Vector Machine Solvers. [ 4b ] Chang, C-C. and Lin C-J., (2013). LIBSVM : A Library for Support Vector Machines. [ 5 ] Ben-Hur, A. and Weston, J. (2009). A User's Guide to Support Vector Machines. In Carugo, O. and Eisenhaber, F. (Eds.), Methods in Molecular Biology 609, 223-229. Andrew Ng photo credit: InverseHypercube , creative commons license . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Theory","url":"https://efavdb.com/svm-classification","loc":"https://efavdb.com/svm-classification"},{"title":"A review of parameter regularization and Bayesian regression","text":"Here, we review parameter regularization, which is a method for improving regression models through the penalization of non-zero parameter estimates. Why is this effective? Biasing parameters towards zero will (of course!) unfavorably bias a model, but it will also reduce its variance. At times the latter effect can win out, resulting in a net reduction in generalization error. We also review Bayesian regressions — in effect, these generalize the regularization approach, biasing model parameters to any specified prior estimates, not necessarily zero. This is the second of a series of posts expounding on topics discussed in the text, \"An Introduction to Statistical Learning\" . Here, we cover material from its Chapters 2 and 6. See prior post here . Introduction and overview In this post, we will be concerned with the problem of fitting a function of the form $$\\label{function} y(\\vec{x}_i) = f(\\vec{x}_i) + \\epsilon_i \\tag{1}, $$ where \\(f\\) is the function's systematic part and \\(\\epsilon_i\\) is a random error. These errors have mean zero and are iid — their presence is meant to take into account dependences in \\(y\\) on features that we don't have access to. To \"fit\" such a function, we will suppose that one has chosen some appropriate regression algorithm (perhaps a linear model, a random forest, etc.) that can be used to generate an approximation \\(\\hat{f}\\) to \\(y\\) , given a training set of example \\((\\vec{x}_i, y_i)\\) pairs. The primary concern when carrying out a regression is often to find a fit that will be accurate when applied to points not included in the training set. There are two sources of error that one has to grapple with: Bias in the algorithm — sometimes the result of using an algorithm that has insufficient flexibility to capture the nature of the function being fit, and variance — this relates to how sensitive the resulting fit is to the samples chosen for the training set. The latter issue is closely related to the concept of overfitting . To mitigate overfitting, parameter regularization is often applied. As we detail below, this entails penalizing non-zero parameter estimates. Although this can favorably reduce the variance of the resulting model, it will also introduce bias. The optimal amount of regularization is therefore determined by appropriately balancing these two effects. In the following, we carefully review the mathematical definitions of model bias and variance, as well as how these effects contribute to the error of an algorithm. We then show that regularization is equivalent to assuming a particular form of Bayesian prior that causes the parameters to be somewhat \"sticky\" around zero — this stickiness is what results in model variance reduction. Because standard regularization techniques bias towards zero, they work best when the underlying true feature dependences are sparse. When this is not true, one should attempt an analogous variance reduction through application of the more general Bayesian regression framework. Squared error decomposition The first step to understanding regression error is the following identity: Given any fixed \\(\\vec{x}\\) , we have $$ \\begin{align} \\overline{\\left (\\hat{f}(\\vec{x}) - y(\\vec{x}) \\right)&#94;2} &= \\overline{\\left (\\hat{f}(\\vec{x}) - \\overline{\\hat{f}(\\vec{x})} \\right)&#94;2} + \\left (\\overline{\\hat{f}(\\vec{x})} - f(\\vec{x}) \\right)&#94;2 + \\overline{ \\epsilon&#94;2} \\\\ & \\equiv var\\left(\\hat{f}(\\vec{x})\\right) + bias\\left(\\hat{f}(\\vec{x})\\right)&#94;2 + \\overline{\\epsilon&#94;2}. \\tag{2}\\label{error_decomp} \\end{align} $$ Here, overlines represent averages over two things: The first is the random error \\(\\epsilon\\) values, and the second is the training set used to construct \\(\\hat{f}\\) . The left side of (\\ref{error_decomp}) gives the average squared error of our algorithm, at point \\(\\vec{x}\\) — i.e., the average squared error we can expect to get, given a typical training set and \\(\\epsilon\\) value. The right side of the equation decomposes this error into separate, independent components. The first term at right — the variance of \\(\\hat{f}(\\vec{x})\\) — relates to how widely the estimate at \\(\\vec{x}\\) changes as one randomly samples from the space of possible training sets. Similarly, the second term — the algorithm's squared bias — relates to the systematic error of the algorithm at \\(\\vec{x}\\) . The third and final term above gives the average squared random error — this provides a fundamental lower bound on the accuracy of any estimator of \\(y\\) . We turn now to the proof of (\\ref{error_decomp}). We write the left side of this equation as $$\\label{detail} \\begin{align} \\tag{3} \\overline{\\left (\\hat{f}(\\vec{x}) - y(\\vec{x}) \\right)&#94;2} &= \\overline{\\left ( \\left \\{\\hat{f}(\\vec{x}) - f(\\vec{x}) \\right \\} - \\left \\{ y(\\vec{x}) - f(\\vec{x}) \\right \\} \\right)&#94;2}\\\\ &= \\overline{\\left ( \\hat{f}(\\vec{x}) - f(\\vec{x}) \\right)&#94;2} - 2 \\overline{ \\left (\\hat{f}(\\vec{x}) - f(\\vec{x}) \\right ) \\left (y(\\vec{x}) - f(\\vec{x}) \\right ) } + \\overline{ \\left (y(\\vec{x}) - f(\\vec{x}) \\right)&#94;2}. \\end{align} $$ The middle term here is zero. To see this, note that it is the average of the product of two independent quantities: The first factor, \\(\\hat{f}(\\vec{x}) - f(\\vec{x})\\) , varies only with the training set, while the second factor, \\(y(\\vec{x}) - f(\\vec{x})\\) , varies only with \\(\\epsilon\\) . Because these two factors are independent, their average product is the product of their individual averages, the second of which is zero, by definition. Now, the third term in (\\ref{detail}) is simply \\(\\overline{\\epsilon&#94;2}\\) . To complete the proof, we need only evaluate the first term above. To do that, we write $$\\begin{align} \\tag{4} \\label{detail2} \\overline{\\left ( \\hat{f}(\\vec{x}) - f(\\vec{x}) \\right)&#94;2} &= \\overline{\\left ( \\left \\{ \\hat{f}(\\vec{x}) - \\overline{\\hat{f}(\\vec{x})} \\right \\}- \\left \\{f(\\vec{x}) -\\overline{\\hat{f}(\\vec{x})} \\right \\}\\right)&#94;2} \\\\ &= \\overline{\\left ( \\hat{f}(\\vec{x}) - \\overline{\\hat{f}(\\vec{x})} \\right)&#94;2} -2 \\overline{ \\left \\{ \\hat{f}(\\vec{x}) - \\overline{\\hat{f}(\\vec{x})} \\right \\} \\left \\{f(\\vec{x}) -\\overline{\\hat{f}(\\vec{x})} \\right \\} } + \\left ( f(\\vec{x}) -\\overline{\\hat{f}(\\vec{x})} \\right)&#94;2. \\end{align} $$ The middle term here is again zero. This is because its second factor is a constant, while the first averages to zero, by definition. The first and third terms above are the algorithm's variance and squared bias, respectively. Combining these observations with (\\ref{detail}), we obtain (\\ref{error_decomp}). Bayesian regression In order to introduce Bayesian regression, we focus on the special case of least-squares regressions. In this context, one posits that the samples generated take the form (\\ref{function}), with the error \\(\\epsilon_i\\) terms now iid, Gaussian distributed with mean zero and standard deviation \\(\\sigma\\) . Under this assumption, the probability of observing values \\((y_1, y_2,\\ldots, y_N)\\) at \\((\\vec{x}_1, \\vec{x}_2,\\ldots,\\vec{x}_N)\\) is given by $$ \\begin{align} \\tag{5} \\label{5} P(\\vec{y} \\vert f) &= \\prod_{i=1}&#94;N \\frac{1}{(2 \\pi \\sigma)&#94;{1/2}} \\exp \\left [-\\frac{1}{2 \\sigma&#94;2} (y_i - f(\\vec{x}_i))&#94;2 \\right]\\\\ &= \\frac{1}{(2 \\pi \\sigma)&#94;{N/2}} \\exp \\left [-\\frac{1}{2 \\sigma&#94;2} (\\vec{y} - \\vec{f})&#94;2 \\right], \\end{align} $$ where \\(\\vec{y} \\equiv (y_1, y_2,\\ldots, y_N)\\) and \\(\\vec{f} \\equiv (f_1, f_2,\\ldots, f_N)\\) . In order to carry out a maximum-likelihood analysis, one posits a parameterization for \\(f(\\vec{x})\\) . For example, one could posit the linear form, $$\\tag{6} f(\\vec{x}) = \\vec{\\theta} \\cdot \\vec{x}. $$ Once a parameterization is selected, its optimal \\(\\vec{\\theta}\\) values are selected by maximizing (\\ref{5}), which gives the least-squares fit. One sometimes would like to nudge (or bias) the parameters away from those that maximize (\\ref{5}), towards some values considered reasonable ahead of time. A simple way to do this is to introduce a Bayesian prior for the parameters \\(\\vec{\\theta}\\) . For example, one might posit a prior of the form $$ \\tag{7} \\label{7} P(f) \\equiv P(\\vec{\\theta}) \\propto \\exp \\left [- \\frac{1}{2\\sigma&#94;2} (\\vec{\\theta} - \\vec{\\theta}_0) \\Lambda (\\vec{\\theta} - \\vec{\\theta}_0)\\right]. $$ Here, \\(\\vec{\\theta}_0\\) represents a best guess for what \\(\\theta\\) should be before any data is taken, and the matrix \\(\\Lambda\\) determines how strongly we wish to bias \\(\\theta\\) to this value: If the components of \\(\\Lambda\\) are large (small), then we strongly (weakly) constrain \\(\\vec{\\theta}\\) to sit near \\(\\vec{\\theta}_0\\) . To carry out the regression, we combine (\\ref{5}-\\ref{7}) with Bayes' rule, giving $$ \\tag{8} P(\\vec{\\theta} \\vert \\vec{y}) = \\frac{P(\\vec{y}\\vert \\vec{\\theta}) P(\\vec{\\theta})}{P(\\vec{y})} \\propto \\exp \\left [-\\frac{1}{2 \\sigma&#94;2} (\\vec{y} - \\vec{\\theta} \\cdot \\vec{x})&#94;2 - \\frac{1}{2\\sigma&#94;2} (\\vec{\\theta} - \\vec{\\theta}_0) \\Lambda (\\vec{\\theta} - \\vec{\\theta}_0)\\right]. $$ The most likely \\(\\vec{\\theta}\\) now minimizes the quadratic \"cost function\", $$\\tag{9} \\label{9} F(\\theta) \\equiv (\\vec{y} - \\vec{\\theta} \\cdot \\vec{x})&#94;2 +(\\vec{\\theta} - \\vec{\\theta}_0) \\Lambda (\\vec{\\theta} - \\vec{\\theta}_0), $$ a Bayesian generalization of the usual squared error. With this, our heavy-lifting is at an end. We now move to a quick review of regularization, which will appear as a simple application of the Bayesian method. Parameter regularization as special cases The most common forms of regularization are the so-called \"ridge\" and \"lasso\". In the context of least-squares fits, the former involves minimization of the quadratic form $$ \\tag{10} \\label{ridge} F_{ridge}(\\theta) \\equiv (\\vec{y} - \\hat{f}(\\vec{x}; \\vec{\\theta}))&#94;2 + \\Lambda \\sum_i \\theta_i&#94;2, $$ while in the latter, one minimizes $$ \\tag{11} \\label{lasso} F_{lasso}(\\theta) \\equiv (\\vec{y} - \\hat{f}(\\vec{x}; \\vec{\\theta}))&#94;2 + \\Lambda \\sum_i \\vert\\theta_i \\vert. $$ The terms proportional to \\(\\Lambda\\) above are the so-called regularization terms. In elementary courses, these are generally introduced to least-squares fits in an ad-hoc manner: Conceptually, it is suggested that these terms serve to penalize the inclusion of too many parameters in the model, with individual parameters now taking on large values only if they are really essential to the fit. While the conceptual argument above may be correct, the framework we've reviewed here allows for a more sophisticated understanding of regularization: (\\ref{ridge}) is a special case of (\\ref{9}), with \\(\\vec{\\theta}_0\\) set to \\((0,0,\\ldots, 0)\\) . Further, the lasso form (\\ref{lasso}) is also a special-case form of Bayesian regression, with the prior set to \\(P(\\vec{\\theta}) \\propto \\exp \\left (- \\frac{\\Lambda}{2 \\sigma&#94;2} \\sum_i \\vert \\theta_i \\vert \\right)\\) . As advertised, regularization is a form of Bayesian regression. Why then does regularization \"work\"? For the same reason any other Bayesian approach does: Introduction of a prior will bias a model (if chosen well, hopefully not by much), but will also effect a reduction in its variance. The appropriate amount of regularization balances these two effects. Sometimes — but not always — a non-zero amount of bias is required. Discussion In summary, our main points here were three-fold: (i) We carefully reviewed the mathematical definitions of model bias and variance, deriving (\\ref{error_decomp}). (ii) We reviewed how one can inject Bayesian priors to regressions: The key is to use the random error terms to write down the probability of seeing a particular observational data point. (iii) We reviewed the fact that the ridge and lasso — (\\ref{ridge}) and (\\ref{lasso}) — can be considered Bayesian priors. Intuitively, one might think introduction of a prior serves to reduce the bias in a model: Outside information is injected into a model, nudging its parameters towards values considered reasonable ahead of time. In fact, this nudging introduces bias! Bayesian methods work through reduction in variance, not bias — A good prior is one that does not introduce too much bias. When, then, should one use regularization? Only when one expects the optimal model to be largely sparse. This is often the case when working on machine learning algorithms, as one has the freedom there to throw a great many feature variables into a model, expecting only a small (a prior, unknown) minority of them to really prove informative. However, when not working in high-dimensional feature spaces, sparseness should not be expected. In this scenario, one should reason some other form of prior, and attempt a variance reduction through the more general Bayesian framework. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"https://efavdb.com/bayesian-linear-regression","loc":"https://efavdb.com/bayesian-linear-regression"},{"title":"Getting started with Pandas","text":"We have made use of Python's Pandas package in a variety of posts on the site. These have showcased some of Pandas' abilities including the following: DataFrames for data manipulation with built in indexing Handling of missing data Data alignment Melting/stacking and Pivoting/unstacking data sets Groupby feature allowing split -> apply -> combine operations on data sets Data merging and joining Pandas is also a high performance library, with much of its code written in Cython or C. Unfortunately, Pandas can have a bit of a steep learning curve — In this post, I'll cover some introductory tips and tricks to help one get started with this excellent package. Notes: This post was partially inspired by Tom Augspurger's Pandas tutorial , which has a youtube video that can be viewed along side it. We also suggest some other excellent resource materials — where relevant — below. The notebook we use below can be downloaded from our github page . Feel free to grab it and follow along. Jupyter (Formally IPython) notebook tips All the exercises I will be referring to here were carried out using IPython notebooks. To start off, here's a few quick tips on notebook navigation that can help make life easier: First, the notebook has two modes, command and edit. If you are in edit mode you have the cursor in one of the cells (boxes) where you can enter code or text. To enter command mode press the Esc key and the cursor will disappear, but the cell will still be highlighted. In command mode you have a variety of keyboard shortcuts, and you can see all of them if you press h . a and b will make new empty cells below and above the current cell. j / k will navigate through the notebook. Shift + Enter will execute the current cell and move to the next one, while Ctrl + Enter will execute the cell but does not move to the next cell. This is nice if you are still tweaking the code in that cell. Pandas is built on top of NumPy To understand Pandas, you gotta understand NumPy, as Pandas is built on top of it. Here, we cover some of its NumPy's basic properties. Ndarray : ndarrays are central to NumPy, and are homogeneous N-dimensional arrays of fixed-size. NumPy also provides fast methods for the ndarrary that are written in C, often making use of vectorized operations such as element wise addition and multiplication. These methods provide a major resource for code speedup, and Pandas takes full advantage of them where possible. In addition to the fast methods, ndarray also requires less memory than a python list because python lists are an array of pointers to Python objects — this is what allows lists to hold mixed data types. This overhead combined with the overhead of the Python objects vs a numpy object can add up quickly. The variabilities in data type also makes it difficult to implement efficient C-loops because every iteration would need to make a call to Python to check the data type. This leads us to the next point. Data types : As mentioned earlier, unlike Python's list object, NumPy arrays can only contain one data type at a time. Giving up mixed data types allows us to achieve much better performance through efficient C-loops. One important thing to note is that missing values (NaN) will cast integer or boolean arrays to floats. [caption id=\"\" align=\"aligncenter\" width=\"526\"] Graph showing data types in NumPy[/caption] Broadcasting: Broadcasting describes how NumPy treats arrays with different shapes. Essentially the smaller array is \"broadcast\" across the larger array so that they have compatible shapes. This makes it possible to have vectorized array operations, enabling the use of C instead of Python for the looping. Broadcasting also prevents needless copies of data from being created by making a new array with repeating copies. The simplest broadcasting example occurs when combining an array with a scaler. This is something Pandas uses for efficiency and ease of use. a = np.array([1.0, 2.0, 3.0]) b = 2.0 a * b >>> array([ 2., 4., 6.]) When operating on two arrays, their shapes are compared starting with the trailing dimensions. Two dimensions are compatible when: they are equal, or one of them is 1 a (3d array): 15 x 3 x 5 b (2d array): 3 x 1 a*b (3d array): 15 x 3 x 5 For those interested, I recommend Jake Vanderplas's talk for learning more about how one can reduce loop usage in your own code using NumPy. Pandas Data Structures Series We now move to the Pandas-specific data types. First up are Series, which are one-dimensional labeled arrays. Unlike ndarrays, these are capable of holding mixed data types. The cells of the series are labeled via the series Index . We will discuss indices more in a bit. The general method of creating a series is as follows. s = Series(data, index=index) pd.Series([1,2,3,4], index=['a', 'b', 'c', 'd']) >>> a 1 b 2 c 3 d 4 dtype: int64 DataFrame As we have seen in other posts on the site, the DataFrame is the main attraction for Pandas. It is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. Like Series, DataFrame accepts many different kinds of input. One way of making a DataFrame is using a dictionary as input. d = { 'one' : [ 1 ., 2 ., 3 ., 4 .], 'two' : [ 4 ., 3 ., 2 ., 1 .], 'good' :[ True , False , False , True ] } df = pd . DataFrame ( d , index = [ 'a' , 'b' , 'c' , 'd' ]) Running the code in a notebook will output a nicely formatted table. good one two a TRUE 1 4 b FALSE 2 3 c FALSE 3 2 d TRUE 4 1 Index The index in Pandas provides axis labeling information for pandas objects — it can serve many purposes. First, an index can provide metadata, potentially important for analysis, visualization, and so on. Second, it can enable automatic data alignment when preforming operations on multiple DataFrames or Series. Third, it can allow easy access to subsets of the data. Selection A huge improvement over numpy arrays is labeled indexing. We can select subsets by column, row, or both. To select columns use []. df['good'] >>> a True b False c False d True Name: good, dtype: bool As we can see here, Pandas will reduce dimensions when possible which is why the output above is a Series instead of a DataFrame — if you wish to force the returned result to be a DataFrame, you must supply a list of arguments, eg df[['good']] . You can also select individual columns with the dot (.) operator, for example df.good will give the same result. However, when using this approach, the column name selected must not have any spaces or special characters, nor can it conflict with any DataFrame methods. In order to add a column to a DataFrame, we write, df['A'] = [1, 2, 3] You can also select multiple columns using a column name list. df[['good', 'two']] good two a TRUE 4 b FALSE 3 c FALSE 2 For row selection, use .loc[row_lables, column_labels] for label-based indexing and use .iloc[row_positions, column_positions] for ordinal/positional selection. df.loc[['a', 'd']] good one two a TRUE 1 d TRUE 4 df.loc['a':'b'] good one two a TRUE 1 b FALSE 2 Notice that the slice is inclusive : It includes both the start and end index — unlike normal python indexing. df.iloc[0:2] good one two a TRUE 1 b FALSE 2 df.loc['b', 'good'] >>> False Reading files into Pandas At this point, we require a larger data set in order to demonstrate other Pandas capabilities. For that purpose, I decided to use unemployment data from ( http://data.bls.gov/ ). The data file is available in our Github repository along with the notebook that generated the examples above. I saved the data as a csv file. To read into the notebook, we'll make use of Pandas' read_csv method — a fast, simple method for directly reading csv files into a DataFrame. After reading in the data, we see that each row corresponds to one unemployment measurement with a different column for each time point. The first column has a numeric id, so the first thing to do is to replace those with human readable strings. Next, we see that we have incomplete data for Jan 2000, so we drop that column. Setting the inplace flag to true here causes the modified DataFrame to replace the original — in this way, we avoid having to copy the original. Next, we assign our IDs to the DataFrame index using set_index , and then drop the column since it is no longer needed. Lastly we transpose the table so that each row corresponds to a different time point and the columns to the separate measures. df = pd . read_csv ( 'data.csv' ) df [ 'Series ID' ] = [ 'Labor force' , 'Participation rate' , 'Rate' , 'Rate - 16-19 yrs' , 'Rate - 20+ yrs (Men)' , 'Rate - 20+ yrs (Women)' , 'Rate - White' , 'Rate - Black or African American' , 'Rate - Asian' , 'Rate - Hispanic or Latino' , 'No High School Diploma' , 'High School Graduates' , 'Some College or Associate Degree' , 'Bachelor degree and higher' , 'Under 5 Weeks' , '5-14 Weeks' , '15 Weeks & over' , '27 Weeks & over' ] df . drop ( 'Jan 2000' , axis = 1 , inplace = True ) df . set_index ( df [ 'Series ID' ], inplace = True ) df . drop ( 'Series ID' , axis = 1 , inplace = True ) df = df . transpose (). convert_objects ( convert_numeric = True ) With these steps, we take the original table: Series ID Jan 2000 Feb 2000 Mar 2000 LNS11000000 142267(1) 142456 142434 LNS11300000 67.3 67.3 67.3 LNS14000000 4 4.1 4 LNS14000012 12.7 13.8 13.3 LNS14000025 3.3 3.5 3.2 to Series ID Labor force Part. rate Rate Rate – 16-19 yrs Rate – 20+ yrs (Men) Feb 2000 142456 67.3 4.1 13.8 3.5 Mar 2000 142434 67.3 4 13.3 3.2 Apr 2000 142751 67.3 3.8 12.6 3.1 May 2000 142388 67.1 4 12.8 3.3 Jun 2000 142591 67.1 4 12.3 3.2 Working with the data Now that we have the data in a nicely formatted within DataFrame, we can easily visualize it using the Pandas plot method. For example, to plot the general unemployment rate, we write df['Rate'].plot() plt.ylabel('Unemployment Rate (%)') Similarly, the following plots unemployment for each of the available different levels of education. df[['Rate', 'No High School Diploma', 'Bachelor degree and higher']].plot() plt.ylabel('Unemployment Rate (%)') Interestingly, these unemployment rates seem to evolve in a similar manner. Notice that both the green and the red curves seem to have doubled during the recent slow-down. GroupBy You can also used Pandas GroupBy functionality to do analysis on subsets of the data. For this example we GroupBy year, and then make a plot showing the mean unemployment per year. GroupBy allows one to easily split the data, apply a function to each group, and then combine the results. It is a very useful feature! df['Year']=(df.index.to_datetime()).year years = df.groupby('Year') years['Rate'].mean().plot(kind='bar') There are some functions like mean, and describe that can be run directly on a grouped object. years.get_group(2005)['Rate'].describe() >>> count 12.000000 mean 5.083333 std 0.158592 min 4.900000 25% 5.000000 50% 5.000000 75% 5.200000 max 5.400000 Name: Rate, dtype: float64 It is possible to apply any function to the grouped function using agg() . years['Rate'].agg([np.mean, np.std, max, min]) mean std max min Year 2000 3.963636 0.092442 4.1 3.8 2001 4.741667 0.528219 5.7 4.2 2002 5.783333 0.102986 6 5.7 2003 5.991667 0.178164 6.3 5.7 2004 5.541667 0.131137 5.8 5.4 2005 5.083333 0.158592 5.4 4.9 2006 4.608333 0.131137 4.8 4.4 2007 4.616667 0.164225 5 4.4 2008 5.8 0.780443 7.3 4.9 2009 9.283333 0.696528 10 7.8 2010 9.608333 0.219331 9.9 9.3 2011 8.941667 0.206522 9.2 8.5 2012 8.066667 0.214617 8.3 7.7 2013 7.366667 0.342008 8 6.7 2014 6.15 0.360555 6.7 5.6 2015 5.4125 0.180772 5.7 5.1 Boolean Indexing Another common operation is the use of boolean vectors to filter data. This allows one to easily select subsets of data. It also provides a quick method for counting — this works because True and False are represented as 1 and 0, respectively, when adding. sum(df['Rate'] > 7) >>> 59 String Methods Pandas has very useful string methods which can be access via str. This makes it easy to look for patterns in the text, do filtering, replacements, and so on. I have a couple of examples below but I highly recommend taking a look at the documentation page for many examples . s = pd.Series(['Dog', 'Bat', 'Coon', 'cAke', 'bAnk', 'CABA', 'dog', 'cat']) s[s.str.contains('B')] 1 Bat 5 CABA s.str.replace('dog|cat', 'nope ', case=False) 0 nope 1 Bat 2 Coon 3 cAke 4 bAnk 5 CABA 6 nope 7 nope Wrap Up Pandas is a very useful library that I highly recommend. Although it can have a bit of a steep learning curve, it's actually pretty easy to pick up once you get started. Give it a shot, and you won't regret it!","tags":"Programming","url":"https://efavdb.com/pandas-tips-and-tricks","loc":"https://efavdb.com/pandas-tips-and-tricks"},{"title":"Stochastic geometric series","text":"Let \\(a_1, a_2, \\ldots\\) be an infinite set of non-negative samples taken from a distribution \\(P_0(a)\\) , and write $$\\tag{1} \\label{problem} S = 1 + a_1 + a_1 a_2 + a_1 a_2 a_3 + \\ldots. $$ Notice that if the \\(a_i\\) were all the same, \\(S\\) would be a regular geometric series, with value \\(S = \\frac{1}{1-a}\\) . How will the introduction of \\(a_i\\) randomness change this sum? Will \\(S\\) necessarily converge? How is \\(S\\) distributed? In this post, we discuss some simple techniques to answer these questions. Note: This post covers work done in collaboration with my aged p, S. Landy. Introduction — a stock dividend problem To motivate the sum (\\ref{problem}), consider the problem of evaluating the total output of a stock that pays dividends each year in proportion to its present value — say \\(x %\\) . The price dynamics of a typical stock can be reasonably modeled as a geometric random walk \\(&#94;1\\) : $$\\label{prod} \\tag{2} price(t) = price(t-1) * a_t, $$ where \\(a_t\\) is a random variable, having distribution \\(P_0(a_t)\\) . Assuming this form for our hypothetical stock, its total lifetime dividends output will be $$\\tag{3} x \\times \\sum_{t = 0}&#94;{\\infty} price(t) = x \\times price(0) \\left ( 1 + a_1 + a_1 a_2 + a_1 a_2 a_3 + \\ldots \\right) $$ The inner term in parentheses here is precisely (\\ref{problem}). More generally, a series of this form will be of interest pretty much whenever geometric series are: Population growth problems, the length of a cylindrical bacterium at a series of time steps \\(&#94;2\\) , etc. Will the nature of these sums change dramatically through the introduction of growth variance? To characterize these types of stochastic geometric series, we will start below by considering their moments: This will allow us to determine the average value of (\\ref{problem}), it's variance etc. This approach will also allow us to determine a condition that is both necessary and sufficient for the sum's convergence. Following this, we will introduce an integral equation satisfied by the \\(P(S)\\) distribution. We demonstrate its application by solving the equation for a simple example. The moments of \\(S\\) To solve for the moments of \\(S\\) , we use a trick similar to that used to sum the regular geometric series: We write $$\\tag{4} \\label{trick} S = 1 + a_1 + a_1 a_2 + \\ldots \\equiv 1 + a_1 T, $$ where \\(T = 1 + a_2 + a_2 a_3 + \\ldots.\\) Now, because we assume that the \\(a_i\\) are independent, it follows that \\(a_1\\) and \\(T\\) are independent. Further, \\(S\\) and \\(T\\) are clearly distributed identically, since they take the same form. Subtracting \\(1\\) from both sides of the above equation, these observations imply $$\\tag{5} \\label{moments} \\overline{(S-1)&#94;k} = \\sum_j {k \\choose j} (-1)&#94;j \\overline{S&#94;{k-j}} = \\overline{ a&#94;k S&#94;k} = \\overline{a&#94;k} \\ \\overline{S&#94;k}. $$ This expression can be used to relate the moments of \\(S\\) to those of \\(a\\) — a useful result, whenever the distribution of \\(a\\) is known, allowing for the direct evaluation of its moments. To illustrate, let us get the first couple of moments of \\(S\\) , using (\\ref{moments}). Setting \\(k=1\\) above, we obtain $$\\tag{6} \\label{mean} \\overline{S -1} = \\overline{a} \\overline{S} \\ \\ \\to \\ \\ \\overline{S} = \\frac{1}{1 - \\overline{a}} $$ The right side here looks just like the usual geometric sum result, with \\(a\\) replaced by its average value. Similarly, setting \\(k =2\\) in (\\ref{moments}), we can solve for the second moment of \\(S\\) . Subtracting the square of the first gives the following expression for the sum's variance, $$\\tag{7} \\label{var} var(S) = \\frac{var(a)}{(1 - \\overline{a})&#94;2(1 - \\overline{a&#94;2})}. $$ As one might intuit, the variance of \\(S\\) is proportional to the variance of \\(a\\) . Expressions (\\ref{mean}) and (\\ref{var}) are the most practical results of this post: They provide formal general expressions for the mean and variance for a sum of form (\\ref{problem}). They can be used to provide a statistical estimate and error bar for a sum of form \\(S\\) in any practical context. It is interesting/nice that the mean takes such a natural looking form — one that many people likely make use of already, without putting much thought into. The expressions above are also of some theoretical interest: Note, for example, that as \\(\\overline{a} \\to 1\\) from below, the average value of \\(S\\) diverges, and then becomes negative as \\(a\\) goes above this value. This is clearly impossible, as \\(S\\) is a sum of positive terms. This indicates that \\(S\\) has no first moment whenever \\(\\overline{a} \\geq 1\\) , while (\\ref{mean}) holds whenever \\(\\overline{a} < 1\\) . Similarly, (\\ref{var}) indicates that the second moment of \\(S\\) exists and is finite whenever \\(\\overline{a&#94;2} < 1\\) . In fact, this pattern continues for all \\(k\\) : \\(\\overline{S&#94;k}\\) exists and is finite if and only if \\(\\overline{a&#94;k} < 1\\) — a result that can be obtained from (\\ref{moments}). A rigorous and elementary proof of these statements can be found in an earlier work by Szabados and Szekeley \\(&#94;3\\) . The simple moment equation (\\ref{moments}) can also be found there. Condition for the convergence of \\(S\\) A simple condition for the convergence of \\(S\\) can also be obtained using (\\ref{moments}). The trick is to consider the limit as \\(k\\) goes to zero of the \\(k\\) -th moments. This gives, for example, the average of \\(1\\) with respect to \\(P(S)\\) . If this is finite, then the distribution of \\(P\\) is normalizable. Otherwise, \\(S\\) must diverge: Setting \\(k = \\epsilon\\) in (\\ref{moments}), expanding to first order in \\(\\epsilon\\) gives $$\\tag{8} \\label{approximate_log} \\overline{ \\exp [\\epsilon \\log (S -1) ]} \\sim \\overline{ 1 + \\epsilon \\log (S -1) } \\sim \\overline{ 1 + \\epsilon \\log S } \\ \\overline{ 1 + \\epsilon \\log a}. $$ Solving for \\(\\overline{1}_S\\) , the average of \\(1\\) with respect to \\(P(S)\\) , gives $$\\tag{9} \\overline{1}_S = \\frac{\\overline{\\log( 1 - \\frac{1}{S})}}{\\log a} + O(\\epsilon). $$ Like the integer moment expressions above, the right side here is finite up to the point where its denominator diverges. That is, the series will converge, if and only if \\(\\overline{\\log a} < 0\\) , a very simple condition \\(&#94;4\\) . Integral equation for the distribution \\(P(S)\\) We have also found that one can sometimes go beyond solving for the moments of \\(S\\) , and instead solve directly for its full distribution: Integrating (\\ref{trick}) over \\(a\\) gives \\begin{eqnarray} \\tag{10} \\label{int} P(S_0) &=& \\int da P_0(a) \\int dS P(S) \\delta(1+ a S - S_0) \\\\ &=& \\int \\frac{da}{a} P_0(a) P \\left (\\frac{S_0 -1}{a} \\right). \\end{eqnarray} This is a general, linear integral equation for \\(P(S)\\) . At least in some cases, it can solved in closed-form. An example follows. Uniformly distributed \\(a_i\\) To demonstrate how one might solve the equation (\\ref{int}), we consider here the case where the \\(a_i\\) are uniform on \\([0,1]\\) . In this case, writing \\(a = \\frac{S_0 -1}{v}\\) , (\\ref{int}) goes to $$\\tag{11} \\label{int2} P(S_0) = \\int_{S_0-1}&#94;{\\infty} P\\left (v\\right) \\frac{1}{v}dv. $$ To progress, we differentiate with respect to \\(S_0\\) , which gives $$\\tag{12} \\label{delay} P&#94;{\\prime} (S_0)\\equiv - \\frac{1 }{S_0 -1}\\times P\\left (S_0 -1\\right). $$ Equation (\\ref{delay}) is a delay differential equation . It can be solved through iterated integrations: To initiate the process, we note that \\(P(S_0)\\) is equal to zero for all \\(S_0< 1\\) . Plugging this observation into (\\ref{delay}) implies that \\(P(S_0) \\equiv J\\) — some constant — for \\(S \\in (1,2)\\) . Continuing in this fashion, repeated integrations of (\\ref{delay}) gives $$\\tag{13} P (S_0) = \\begin{cases} J, \\ \\ \\ S_0 \\in (1,2) \\\\ J[1 - \\log (S_0 -1)], \\ \\ \\ S_0 \\in (2,3) \\\\ J \\left [ 1 - \\log(S_0 - 1) + Li_2(2-S_0) + \\frac{ \\log(S_0 - 2)}{\\log(S_0 - 1)} - Li_2(-1) \\right ], \\ \\ S_0 \\in (3,4) \\\\ \\ldots, \\end{cases} $$ where \\(Li_2\\) is the polylogarithm function. In practice, to find \\(J\\) one can solve (\\ref{delay}) numerically, requiring \\(P(S)\\) to be normalized. The figure below compares the result to a simulation estimate, obtained via binning the results of 250,000 random sums of form (\\ref{problem}). The two agree nicely. Discussion Consideration of this problem was motivated by a geometric series of type (\\ref{problem}) that arose in my work at Square. In this case, I was interested in understanding the bias and variance in the natural estimate (\\ref{mean}) to this problem. After some weeks of tinkering with S Landy, I was delighted to find that rigorous, simple results could be obtained to characterize these sums, the simplest being the moment and convergence results above. We now realize that these particular issues have already been well- (and better-)studied, by others \\(&#94;3\\) . As for the integral equation approach, we have not found any other works aimed at solving this problem in general. The method discussed in the example above can be used for any \\(P_0(a)\\) that is uniform over a finite segment. We have also found solutions for a few other cases. Unfortunately, we have so far been unable to obtain a formal, general solution in closed form. However, we note that standard iterative approaches can always be used to estimate the solution to (\\ref{int}). Finally, in cases where all moments exist, these can also be used to determine \\(P\\) . References and comments [1] For a discussion on the geometric random walk model for stocks, see here . [2] Elongated bacteria — eg., e. coli — grow longer at an exponential rate — see my paper on how cell shape affects growth rates . Due to randomness inherent in the growth rates, bacteria populations will have a length distribution, similar in form to \\(P(S)\\) . [3] \"An exponential functional of random walks\" by Szabados and Szekeley, Journal of Applied Probability 2003. [4] Although we have given only a hand-waving argument for this result, the authors of [3] state — and give a reference for — the fact that it can be proven using the law of large numbers: By independence of the \\(a_i\\) , the \\(k\\) -th term in the series approaches \\((\\overline{\\log a})&#94;k\\) with probability one, at large \\(k\\) . Simple convergence criteria then give the result. [5] The moment equation (\\ref{moments}) can also be obtained from the integral equation (\\ref{int}), where it arrises from the application of the convolution theorem. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Finance","url":"https://efavdb.com/stochastic-geometric-series","loc":"https://efavdb.com/stochastic-geometric-series"},{"title":"Build a web scraper for a literature search - from soup to nuts","text":"Code, references, and examples of this project are on Github . In this post, I'll describe the soup to nuts process of automating a literature search in Pubmed Central using R. It feels deeply satisfying to sit back and let the code do the dirty work. Is it as satisfying as a bowl of red-braised beef noodle soup with melt-in-your-mouth tendons from Taipei's Yong Kang Restaurant (featured image)? If you have to do a lit search like this more than once, then I have to say the answer is yes — unequivocally, yes. The three components of the project are I. Design a database to store the contents of a scraping session II . Extract information via an API and web scraper III . Generate summary reports If you want to skip the explanations and go straight to using the program, check out the quick-start HTML5 presentation or manual and an example of a report generated by this project. The task is to capture plots of tumor growth inhibition ( TGI ), i.e. tumor growth as a function of time, in animals treated with a particular cancer drug, then aggregate all the plots in a summary report. An example of a TGI plot is provided below (source: Qi 2011 ) for TGI in mice treated with the cancer drug, Docetaxel. TGI plots for several drugs (image credit: Qi 2011) Since the scraper was intended for a casual (non-exhaustive) literature search, I decided to confine the search to online articles in Pubmed Central (as opposed to Pubmed in general) since they are entirely open-access and available in a mostly uniform format. I. Set up the the database This project called for data storage beyond the scope of data frames and external flat files, e.g. Excel spreadsheets or csv files, since the following attributes were required of the data: Persistence outside the R environment => data frames unsuitable Ease of access and manipulation => writing to and reading from text/csv files would be cumbersome The data would be structured => relational database With a view towards expediency, we cover just enough to get things up and running and leave the finer details of relational database design to the experts. SQLite is well-suited for this small project; it's self-contained and doesn't require fussing with servers. To lower the barrier even further, the RSQLite package embeds SQLite in R (no separate installation needed) and allows you to very easily interface with your database within the R environment. The database itself is stored in a single file on your hard disk and easily transferred. What data will be collected? The first step is to decide what data should be collected during the web scraping process. We want to aggregate images of plots in a (html) report. However, downloading the images themselves is inefficient; instead, we'll just grab the image URLs on Pubmed Central. (The image URLs are useful because they can be referred to by markdown code to embed the images in the html report.) The image urls should be captured, along with associated information below: image data image url, figure name in the article, image caption article metadata Pubmed Central id, DOI , title, journal, year of publication, authors, abstracts, keywords search criteria met by the image topic/drug, type of plot The last point addresses the foreseeable need to be able to modify the search parameters. In the next section, we allow for the possibility that the same image might show up for more than one kind of drug or plot type. Decide on a layout for the database After pinning down the content itself, the next step is to decide how it should be arranged in the database. Namely, What tables are needed? Which fields go in which tables? How do the tables relate to one another? This particularly helpful page walks you through the concept of database normalization by providing lots of concrete examples, including \"anomalies\" that arise in poorly designed databases. Some ideas on normalization are intuitive. Let's take a look at how to restructure a table to satisfy first normal form ( 1NF ). The table below is not in 1NF because it contains sets of values within single rows. student_id name subjects grades 1234 Andrew a.i. linear algebra physics A A B+ 5678 Yaser statistics algorithms A- A Instead, we can break it up into two tables: table: student student_id name 1234 Andrew 5678 Yaser table:grades: student_id subject grade 1234 a.i. A 1234 linear algebra A 1234 physics B+ 5678 statistics A- 5678 algorithms A Column names that are primary keys are underlined. A primary key is a column, or combination of columns, whose values uniquely identify a row in a table (and accordingly guards against duplicate rows). In a first go at a schema, a table without a logical primary key reared its ugly head: pmcid topic 123456 drug A 123456 drug B 100000 drug A Note that the pmcid value is not guaranteed to be unique in the above table because the same article may show up in searches for multiple drugs. This situation hinted at the need to restructure the tables. We finally settled on the three tables below, which all had natural primary keys (underlined): table: article pmcid doi title journal year authors abstract keywords table: figure topic plot_type img_url pmc_id table: figure_text img_url fig_name caption Their tables were not in 1NF . The tables \"figure\" and \"figure_text\" are kept separate in order to minimize redundancies. For instance, the same img_url can appear in the table \"figure\" multiple times if it matches a number of different drugs or plot types, but its caption would only need to be stored once in the table \"figure_text\". The table \"figure\" is not in second normal form ( 2NF ) because of a partial key dependency; the pmcid field only depends on img_url, rather than the entire composite key {topic, plot_type, and img_url}. Although the normalization rules sound a bit intimidating, they are just guidelines—apparently, one can even get carried away with over-normalizing. The database has held up fine so far, but any suggestions on how to improve the design are very welcome! Creating a SQLite database in R With a schema in hand, creating the SQLite database in R is a matter of minutes. First, we load the packages for interfacing with the database. library ( DBI ) library ( RSQLite ) Then we create a connection to the database, which we'll call \"myDb.sqlite\". con = dbConnect ( SQLite (), dbname = \"myDb.sqlite\" ) Next, we create the three tables \"article\", \"figure\", and \"figure_text\". create figure_text table: query = 'CREATE TABLE figure_text(img_url TEXT, fig_name TEXT, caption TEXT, PRIMARY KEY(img_url))' dbGetQuery ( con , query ) create figure table: query = 'CREATE TABLE figure(topic TEXT, plot_type TEXT, img_url TEXT, pmcid INTEGER, PRIMARY KEY(topic, plot_type, img_url))' dbGetQuery ( con , query ) create article table: query = 'CREATE TABLE article(pmcid INTEGER, doi TEXT, title TEXT, journal TEXT, year INTEGER, authors TEXT, abstract TEXT, keywords TEXT, PRIMARY KEY(pmcid))' dbGetQuery ( con , query ) Last, we close the connection to the database. dbDisconnect ( con ) The SQLite database is now ready to be used! The script above is available on github as createSQLiteDatabase.R II . Scrape Pubmed Central articles The script pubmedcentral_scraper.R is where the action happens. It takes input from the user to query the Pubmed Central Database, scrape articles, and load the extracted information into the database. Input keywords for literature search and labels in database The user input section is shown below. ## <---------USER INPUT STARTS HERE---------> ## name of database where scraper results are stored database.name = \"myDb.sqlite\" ## maximum number of results to retrieve from query retmax = 10 ## topic terms to be queried via the pubmed search engine query.topic = c ( \"Docetaxel\" , \"Docetaxol\" ) ## keywords to identify plot type to be captured ## terms should be lower-case query.plottype = c ( \"tumor growth\" , \"tumor volume\" , \"tumor size\" , \"tumor inhibition\" , \"tumor growth inhibition\" , \"tgi\" , \"tumor response\" , \"tumor regression\" ) The user input for variables query.topic and query.plottype are used to construct the query to Pubmed Central via the Entrez Programming Utilities interface (details in the E-utilities guide ), made available through the National Center for Biotechnology ( NCBI ). To maximize hits to the query, the user can supply multiple terms for each variable. ## topic/drug label for database topic = \"Docetaxel\" ## plot type label for database plot_type = \"TGI\" ## <---------USER INPUT ENDS HERE-----------> The variables topic and plot_type label the data in the SQLite database (the labels should be consistent between queries in order to simplify the information retrieval process, e.g. stick to one spelling convention for a particular drug, like \"Docetaxel\", in myDb.sqlite). The first E-utility we will use is ESearch, which returns the PMC ids of articles matching a query, along with other metadata. The E-utilities API is extremely easy to use. Simply string together the set of parameters ( NCBI database name, utility name, etc.) and go to the URL . ## compose url for eSearch url.esearch = paste0 ( url.base , esearch , db , \"&\" , retmax , \"&\" , sortmethod , \"&\" , query ) ## get and parse xml data returned by eSearch data.esearch = getURL ( url.esearch ) The explicit URL constructed from the above example user input is: http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pmc&retmax=10&sort=relevance&term=(\"Docetaxel\"+ OR +\"Docetaxol\")+ AND +(\"tumor+growth\"+ OR +\"tumor+volume\"+ OR +\"tumor+size\"+ OR +\"tumor+inhibition\"+ OR +\"tumor+growth+inhibition\"+ OR +\"tgi\"+ OR +\"tumor+response\"+ OR +\"tumor+regression\") Try copying and pasting the above URL in your browser to see a sample of the xml file returned by E-utilities. Here's an excerpt of the XML document from a query to ESearch on August 25, 2015: XML output from a query to PMC via the ESearch API . We extract the PMC ids, which are sandwiched between the XML tags, using functions from the XML and rvest packages: data.xml = xmlParse ( data.esearch ) ## get pmcid's pmcids = data.xml %>% xml_nodes ( \"Id\" ) %>% xml_text () %>% is a pipe operator for chaining commands (from the magrittr package). The URLs of the html article can be simply constructed from their PMC ids. For example, the html version of the article with PMC id 3792566 is found at: http://www.ncbi.nlm.nih.gov/pmc/articles/3792566 Scrape HTML articles The scraping of the HTML article is performed by scrapeArticle.R . Note, PMC ids returned by ESearch which have already been scraped for that particular combination of search terms are skipped. The html version of the PMC articles only show excerpts of captions, so we have to extract the individual figure URLs in order to scrape their full captions (and search for keyword matches). In order to extract a data element from an html document, we need to identify the tag associated with that element. SelectorGadget is a nifty tool to help you hone in on the CSS selectors of interest. Installation is ridiculously easy: just drag the link on the SelectorGadget page to your browser bookmark bar! For example, let's identify the CSS selector for figure URLs using SelectorGadget in 3 clicks of the mouse. We'll demo SelectorGadget on a PMC article that is returned in a query on Docetaxel and TGI . In the screenshot below, I clicked on the \"Figure 3\" link as a starting point for selecting all such figure URLs. SelectorGadget identified the element as \".figpopup\" in the gray toolbar at the bottom of the screenshot, highlighted the direct click in green, and highlighted all the other elements in yellow (total: 35 elements). Notice, however, that two links to Figure 4 have been automatically highlighted in the screenshot, one of which is a reference in the body of the text. A click of the \"Figure 3\" link next to thumbnail highlights it in green. Similar elements are automatically highlighted in yellow. To reduce the number of redundant figure URL links, I then clicked on the Figure 4 link in the body of the text in order to exclude it; it is accordingly highlighted in red to signify its exclusion. The pattern-matching is momentarily worsened since the link to Figure 4 (bottom) is no longer highlighted. SelectorGadget's guess for the CSS selector becomes \"#lgnd_F3 .figpopup\", of which there is only one element, highlighted in green. Elements excluded from the pattern matching are highlighted in red. After making the pattern match more specific with an exclusion, we have to re-generalize by re-selecting the Figure 4 bottom link. This time, SelectorGadget gets the pattern right with the CSS selector \".icnblk_cntnt .figpopup\", which describes 5 elements on the page. Third time's the charm: SelectorGadget has honed in on the CSS selectors that match the desired figure URLs. Using rvest's xml_nodes function, we extract components characterized by the CSS selector .icnblk_cntnt .figpopup — namely, the URLs of tables and figures. popups.tags = article %>% xml_nodes ( \".icnblk_cntnt .figpopup\" ) With some more parsing and filtering similar to the above, the full image captions can be grepped for keywords. Caption and image metadata for keyword matches are stored in the SQLite database by pubmedcentral_scraper.R . III . Generate a report of the scraped results The results of the scraping can be examined by directly querying the SQLite database. I also put together an R script, markdown_and_plot.R , that automatically creates an html report in the simple case where only one topic and plot_type need to be included. The user only has to input the topic and plot_type, and the report is subsequently generated. markdown_and_plot.R calls on generate_markdown_code.R , which extracts the image URLs from the database: query = sprintf ( ' SELECT *\\ FROM (( figure JOIN article USING ( pmcid )) \\ JOIN figure_text USING ( img_url )) \\ WHERE ( topic = \"%s\" AND plot_type = \"%s\" ) \\ ORDER BY pmcid ASC ' ,topic, plot_type) images = dbGetQuery ( con , query ) ## construct image URLs img_links = paste0 ( \"http://www.ncbi.nlm.nih.gov\" , images $ img_url ) generate_markdown_code.R then loops through the i images per article and, line by line, writes out markdown code of the image URLs and captions. for ( i in seq_along ( img_links )) { ## ... img_md = paste0 ( \"![pmcid: \" , images $ pmcid[i] , \"](\\`r img_links[\" , i , \"]\\`)\" ) cat ( img_md , file = outfile , append = T , sep = \"\\n\" ) ## ... } markdown_and_plot.R then reads in the markdown file and renders it into the final html report, containing images embedded via href links, using the knit2html function in the knitr package. html.filename = sprintf ( \"scraper_%s_plots_for_%s.html\" , plot_type , topic ) knit2html ( md.filename , output = html.filename ) For a sample report that was generated for topic = Trastuzumab and plot_type = TGI , see here . Note, github automatically renders markdown files into html, whereas html files are displayed as source code. However, the file that is actually intended for human perusal outside Github is the html version, located in the same example subdirectory on Github. A look at the example report shows that there are a few false positives, i.e. images that don't actually correspond to plots of TGI , but the simplistic grep-keyword-method works well overall. There's plenty of room for improving the code, but as it stands, this code sure beats compiling reports by hand! We've talked about the thought process behind building the program, but to put it to use, check it out on Github .","tags":"Programming,","url":"https://efavdb.com/build-a-web-scraper-lit-search","loc":"https://efavdb.com/build-a-web-scraper-lit-search"},{"title":"Leave-one-out cross-validation","text":"This will be the first of a series of short posts relating to subject matter discussed in the text, \"An Introduction to Statistical Learning\" . This is an interesting read, but it often skips over statement proofs — that's where this series of posts comes in! Here, I consider the content of Section 5.1.2: This gives a lightning-quick \"short cut\" method for evaluating a regression's leave-one-out cross-validation error. The method is applicable to any least-squares linear fit. Introduction: Leave-one-out cross-validation When carrying out a regression analysis , one is often interested in two types of error measurement. The first is the training set error and the second is the generalization error. The former relates to how close the regression is to the data being fit. In contrast, the generalization error relates to how accurate the model will be when applied to other points. The latter is of particular interest whenever the regression will be used to make predictions on new points. Cross-validation provides one method for estimating generalization errors. The approach centers around splitting the training data available into two sets, a cross-validation training set and cross-validation test set . The first of these is used for training a regression model. Its accuracy on the test set then provides a generalization error estimate. Here, we focus on a special form of cross-validation, called leave-one-out cross-validation ( LOOCV ). In this case, we pick only one point as the test set. We then build a model on all the remaining, complementary points, and evaluate its error on the single-point held out. A generalization error estimate is obtained by repeating this procedure for each of the training points available, averaging the results. LOOCV can be computationally expensive because it generally requires one to construct many models — equal in number to the size of the training set. However, for the special case of least-squares polynomial regression we have the following \"short cut\" identity: $$ \\label{theorem} \\tag{1} \\sum_i \\left ( \\tilde{y}_i - y_i\\right)&#94;2 = \\sum_i \\left ( \\frac{\\hat{y}_i - y_i}{1 - h_i}\\right)&#94;2. $$ Here, \\(y_i\\) is the actual label value of training point \\(i\\) , \\(\\tilde{y}_i\\) is the value predicted by the cross-validation model trained on all points except \\(i\\) , \\(\\hat{y}_i\\) is the value predicted by the regression model trained on all points (including point \\(i\\) ), and \\(h_i\\) is a function of the coordinate \\(\\vec{x}_i\\) — this is defined further below. Notice that the left side of (\\ref{theorem}) is the LOOCV sum of squares error (the quantity we seek), while the right can be evaluated given only the model trained on the full data set. Fantastically, this allows us to evaluate the LOOCV error using only a single regression! Statement proof Consider the LOOCV step where we construct a model trained on all points except training example \\(k\\) . Using a linear model of form \\(\\tilde{y}(\\vec{x}) \\equiv \\vec{x}&#94;T \\cdot \\vec{\\beta}_k\\) — with \\(\\vec{\\beta}_k\\) a coefficient vector — the sum of squares that must be minimized is $$\\tag{2} \\label{error_sum} J_k \\equiv \\sum_{i \\not = k} \\left ( \\tilde{y}_i - y_i \\right)&#94;2 = \\sum_{i \\not = k} \\left (\\vec{x}&#94;T_i \\cdot \\vec{\\beta}_k - y_i \\right)&#94;2. $$ Here, we're using a subscript \\(k\\) on \\(\\vec{\\beta}_k\\) to highlight the fact that the above corresponds to the case where example \\(k\\) is held out. We minimize (\\ref{error_sum}) by taking the gradient with respect to \\(\\vec{\\beta}_k\\) . Setting this to zero gives the equation $$\\tag{3} \\left( \\sum_{i \\not = k} \\vec{x}_i \\vec{x}_i&#94;T \\right) \\cdot \\vec{\\beta}_k = \\sum_{i \\not = k} y_i \\vec{x}_i. $$ Similarly, the full model (trained on all points) coefficient vector \\(\\vec{\\beta}\\) satisfies $$\\tag{4} \\label{full_con} \\left( \\sum_{i} \\vec{x}_i \\vec{x}_i&#94;T \\right) \\cdot \\vec{\\beta} \\equiv M \\cdot \\vec{\\beta} = \\sum_{i} y_i \\vec{x}_i. $$ Combining the prior two equations gives, $$\\tag{5} \\left (M - \\vec{x}_k \\vec{x}_k&#94;T \\right) \\cdot \\vec{\\beta}_k = \\left (\\sum_{i} y_i \\vec{x}_i\\right) - y_k \\vec{x}_k = M\\cdot \\vec{\\beta} - y_k \\vec{x}_k. $$ Using the definition of \\(\\tilde{y}_k\\) , rearrangement of the above leads to the identity $$\\tag{6} M \\cdot \\left ( \\vec{\\beta}_k - \\vec{\\beta} \\right) = \\left (\\tilde{y}_k - y_k \\right) \\vec{x}_k. $$ Left multiplication by \\(\\vec{x}_k&#94;T M&#94;{-1}\\) gives, $$\\tag{7} \\tilde{y}_k - \\hat{y}_k = \\left( \\tilde{y}_k - y_k\\right) - \\left( \\hat{y}_k - y_k \\right) = \\vec{x}_k&#94;T M&#94;{-1} \\vec{x}_k \\left (\\tilde{y}_k - y_k \\right). $$ Finally, combining like-terms, squaring, and summing gives $$\\tag{8} \\sum_k \\left (\\tilde{y}_k - y_k \\right) &#94;2 = \\sum_k \\left (\\frac{\\hat{y}_k - y_k}{1 -\\vec{x}_k&#94;T M&#94;{-1} \\vec{x}_k } \\right)&#94;2. $$ This is (\\ref{theorem}), where we now see the parameter \\(h_k \\equiv \\vec{x}_k&#94;T M&#94;{-1} \\vec{x}_k\\) . This is referred to as the \"leverage\" of \\(\\vec{x}_k\\) in the text. Notice also that \\(M\\) is proportional to the correlation matrix of the \\(\\{\\vec{x}_i\\}\\) . \\(\\blacksquare\\) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods, Theory","url":"https://efavdb.com/leave-one-out-cross-validation","loc":"https://efavdb.com/leave-one-out-cross-validation"},{"title":"Machine learning to predict San Francisco crime","text":"In today's post, we document our submission to the recent Kaggle competition aimed at predicting the category of San Francisco crimes, given only their time and location of occurrence. As a reminder, Kaggle is a site where one can compete with other data scientists on various data challenges. We took this competition as an opportunity to explore the Naive Bayes algorithm. With the few steps discussed below, we were able to quickly move from the middle of the pack to the top 33% on the competition leader board, all the while continuing with this simple model! Introduction As in all cities, crime is a reality San Francisco: Everyone who lives in San Francisco seems to know someone whose car window has been smashed in, or whose bicycle was stolen within the past year or two. Even Prius' car batteries are apparently considered fair game by the city's diligent thieves. The challenge we tackle today involves attempting to guess the class of a crime committed within the city, given the time and location it took place. Such studies are representative of efforts by many police forces today: Using machine learning approaches, one can get an improved understanding of which crimes occur where and when in a city — this then allows for better, dynamic allocation of police resources . To aid in the SF challenge , Kaggle has provided about 12 years of crime reports from all over the city — a data set that is pretty interesting to comb through. Here, we outline our approach to tackling this problem, using the Naive Bayes classifier. This is one of the simplest classification algorithms, the essential ingredients of which include combining Bayes' theorem with an independence assumption on the features (this is the \"naive\" part). Although simple, it is still a popular method for text categorization. For example, using word frequencies as features, this approach can accurately classify emails as spam, or whether a particular a piece of text was written by a specific author. In fact, with careful preprocessing, the algorithm is often competitive with more advanced methods, including support vector machines. Loading package and data Below, we show the relevant commands needed to load all the packages and training/test data we will be using. As in previous posts, we will work with Pandas for quick and easy data loading and wrangling. We will be having a post dedicated to Pandas in the near future, so stay tuned! We start off with using the parse_dates method to convert the Dates column of our provided data — which can be downloaded here — from string to datetime format. import pandas as pd from sklearn.cross_validation import train_test_split from sklearn import preprocessing from sklearn.metrics import log_loss from sklearn.naive_bayes import BernoulliNB from sklearn.linear_model import LogisticRegression import numpy as np #Load Data with pandas, and parse the first column into datetime train = pd . read_csv ( 'train.csv' , parse_dates = [ 'Dates' ]) test = pd . read_csv ( 'test.csv' , parse_dates = [ 'Dates' ]) The training data provided contains the following fields: Date - date + timestamp Category - The type of crime, Larceny, etc. Descript - A more detailed description of the crime. DayOfWeek - Day of crime: Monday, Tuesday, etc. PdDistrict - Police department district. Resolution - What was the outcome, Arrest, Unfounded, None, etc. Address - Street address of crime. X and Y - GPS coordinates of crime. As we mentioned earlier, the provided data spans almost 12 years, and both the training data set and the testing data set each have about 900k records. At this point we have all the data in memory. However, the majority of this data is categorical in nature, and so will require some more preprocessing. How to handle categorical data Many machine learning algorithms — including that which we apply below — will not accept categorical, or text, features. What is the best way to convert such data into numerical values? A natural idea is to convert each unique string to a unique value. For example, in our data set we might take the crime category value to correspond to one numerical feature, with Larceny set to 1, Homicide to 2, etc. However, this scheme can cause problems for many algorithms, because they will incorrectly assume that nearby numerical values imply some sort of similarity between the underlying categorical values. To avoid the problem noted above, we will instead binarize our categorical data, using vectors of 1's and 0's. For example, we will write larceny = 1,0,0,0,... homicide = 0,1,0,0,... prostitution = 0,0,1,0,... ... There are a variety of methods to do this encoding, but Pandas has a particularly nice method called get_dummies() that can go straight from your column of text to a binarized array. Below, we also convert the crime category labels to integer values using the method LabelEncoder , and use Pandas to extract the hour from each time point. We then convert the districts, weekday, and hour into binarized arrays and combine them into a new dataframe. We then split up the train_data into a training and validation set so that we have a way of accessing the model performance while leaving the test data untouched. # Convert crime labels to numbers le_crime = preprocessing . LabelEncoder () crime = le_crime . fit_transform ( train . Category ) # Get binarized weekdays , districts , and hours . days = pd . get_dummies ( train . DayOfWeek ) district = pd . get_dummies ( train . PdDistrict ) hour = train . Dates . dt . hour hour = pd . get_dummies ( hour ) # Build new array train_data = pd . concat ([ hour , days , district ], axis = 1 ) train_data [ 'crime' ] = crime # Repeat for test data days = pd . get_dummies ( test . DayOfWeek ) district = pd . get_dummies ( test . PdDistrict ) hour = test . Dates . dt . hour hour = pd . get_dummies ( hour ) test_data = pd . concat ([ hour , days , district ], axis = 1 ) training , validation = train_test_split ( train_data , train_size = . 60 ) Model development For this competition the metric used to rate the performance of the model is the multi-class log_loss — smaller values of this loss correspond to improved performance. First pass For our first quick pass, we used just the day of the week and district for features in our classifier training. We also carried out a Logistic Regression ( LR ) on the data in order to get a feel for how the Naive Bayes ( NB ) model was performing. The results from the NB model gave us a log-loss of 2.62, while LR after tuning was able to give 2.62. However, LR took 60 seconds to run, while NB took only 1.5 seconds! As a reference, the current top score on the leader board is about 2.27, while the worst is around 35. Not bad performance! features = [ 'Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN' ] training , validation = train_test_split ( train_data , train_size = .60 ) model = BernoulliNB () model . fit ( training [ features ] , training [ 'crime' ] ) predicted = np . array ( model . predict_proba ( validation [ features ] )) log_loss ( validation [ 'crime' ] , predicted ) #Logistic Regression for comparison model = LogisticRegression ( C = .01 ) model . fit ( training [ features ] , training [ 'crime' ] ) predicted = np . array ( model . predict_proba ( validation [ features ] )) log_loss ( validation [ 'crime' ] , predicted ) Submission code model = BernoulliNB () model . fit ( train_data [ features ] , train_data [ 'crime' ] ) predicted = model . predict_proba ( test_data [ features ] ) #Write results result = pd . DataFrame ( predicted , columns = le_crime . classes_ ) result . to_csv ( 'testResult.csv' , index = True , index_label = 'Id' ) With the above model performing well, we used our code to write out our predictions on the test set to csv format, and submitted this to Kaggle. It turns out we got a score of 2.61 which is slightly better than our validation set estimate. The was a good enough score to put us in the to 50%. Pretty good for a first try! Second pass To improve the model further, we next added the time to the feature list used in training. This clearly provides some relevant information, as some types of crime happen more during the day than the night. For example, we expect public drunkenness to probably go up in the late evening. Adding this feature we were able to push our log-loss score down to 2.58 — quick and easy progress! As a side note, we also tried leaving the hours as a continuous variable, but this did not lead to any score improvements. After training on the whole data set again, we also get 2.58 on the test date. This moved us up another 32 spots, giving a final placement of 76/226! features = [ 'Friday' , 'Monday' , 'Saturday' , 'Sunday' , 'Thursday' , 'Tuesday' , 'Wednesday' , 'BAYVIEW' , 'CENTRAL' , 'INGLESIDE' , 'MISSION' , 'NORTHERN' , 'PARK' , 'RICHMOND' , 'SOUTHERN' , 'TARAVAL' , 'TENDERLOIN' ] features2 = [ x for x in range ( 0 , 24 )] features = features + features2 Discussion Although Naive Bayes is a fairly simple model, properly wielded it can give great results. In fact, in this competition our results were competitive with teams who were using much more complicated models, e.g. neural nets. We also learned a few other interesting things here: For example, Pandas' get_dummies() method looks like it will be a huge timesaver when dealing with categorical data. Till next time — keep your Prius safe!","tags":"Case studies","url":"https://efavdb.com/predicting-san-francisco-crimes","loc":"https://efavdb.com/predicting-san-francisco-crimes"},{"title":"How not to sort by average rating, revisited","text":"What is the best method for ranking items that have positive and negative reviews? Some sites, including reddit, have adopted an algorithm suggested by Evan Miller to generate their item rankings. However, this algorithm can sometimes be unfairly pessimistic about new, good items. This is especially true of items whose first few votes are negative — an issue that can be \"gamed\" by adversaries. In this post, we consider three alternative ranking methods that can enable high-quality items to more-easily bubble-up. The last is the simplest, but continues to give good results: One simply seeds each item's vote count with a suitable fixed number of hidden \"starter\" votes. Introduction — a review of Evan Miller's post In an insightful prior post , Evan Miller ( EM ) considered the problem of ranking items that had been reviewed as positive or negative (up-voted or down-voted, represented by a 1 or a 0, respectively) by a sample of users. He began by illustrating that two of the more readily-arrived at solutions to this problem are highly flawed. To review: Bad method 1: Rank item \\(i\\) by \\(n_i(1) - n_i(0)\\) , its up-vote count minus its down-vote count. Issue: If one item has garnered 60 up-votes and 40 down-votes, it will get the same score as an item with only 20 votes, all positive. Yet, the latter has a 100% up-vote rate (20 for 20), suggesting that it is of very high quality. Despite this, the algorithm ranks the two equally. Bad method 2: Rank item \\(i\\) by \\(\\hat{p} \\equiv n_i(1)/[n_i(0) + n_i(1)]\\) , its sample up-vote rate (average rating). Issue: If any one item has only one vote, an up-vote, it will be given a perfect score by this algorithm. This means that it will be ranked above all other items, despite the fact that a single vote is not particularly informative/convincing. In general, this method can work well, but only once each item has a significant number of votes. To avoid the issues of these two bad methods (BMs), EM suggests scoring and ranking each item by the lower limit of its up-vote-rate confidence interval . This is ( E.B. Wilson, 1927 ), $$\\tag{1} \\label{emsol} p_{W} = \\frac{\\hat{p} + \\frac{z_{\\alpha/2}&#94;2}{2n} - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p}) + \\frac{z_{\\alpha/2}&#94;2}{4n} }{n}}}{1 + \\frac{z_{\\alpha/2}&#94;2}{n}}, $$ where \\(\\hat{p}\\) is again the sample up-vote rate, \\(z_{\\alpha/2}\\) is a positive constant that sets the size of the confidence interval used, and \\(n\\) is the total number of votes that have so far been recorded. The score \\(p_{W}\\) approaches \\(\\hat{p}\\) once an item has a significant number of votes — it consequently avoids the pitfall of BM1 above. By construction, it also avoids the pitfall of BM2 . With both of these pitfalls avoided, the EM method can sometimes provide a reasonable, practical ranking system. Potential issue with (\\ref{emsol}) Although (\\ref{emsol}) does a good job of avoiding the pitfall associated with BM2 , it can do a poor job of handling a related pitfall: If any new item has only a few votes, and these each happen to be down-votes, its sample up-vote rate will be \\(\\hat{p} = 0\\) . In this case, (\\ref{emsol}) gives $$\\label{problem} \\tag{2} p_{W} = \\left .\\frac{\\hat{p} + \\frac{z_{\\alpha/2}&#94;2}{2n} - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p}) + \\frac{z_{\\alpha/2}&#94;2}{4n} }{n}}}{1 + \\frac{z_{\\alpha/2}&#94;2}{n}}\\right \\vert_{\\hat{p} = 0} = 0. $$ Now, \\(p_W\\) is always between \\(0\\) and \\(1\\) , so (\\ref{problem}) implies that any new, quickly-down-voted item will immediately be ranked below all others. This is extremely harsh and potentially unfair. For example, consider the case of a newly-opened restaurant: If an adversary were to quickly down-vote this restaurant on some ranking site — the day of its opening — the new restaurant would be ranked below all others, including the adversary. This would occur even if the new restaurant were of very high true quality. This could have potentially-damaging consequences, for both the restaurant and the ranking site — whose lists should provide only the best recommendations! An ideal ranking system should explicitly take into account the large uncertainty present when only a small number of votes have been recorded. The score (\\ref{emsol}) does a good job of this on the high \\(\\hat{p}\\) end, but a poor job on the low \\(\\hat{p}\\) end. This approach may be appropriate for cases where one is risk-averse on the high end only, but in general one should protect against both sorts of quick, strong judgements. Below we consider some alternative, Bayesian ranking solutions. The last is easy to understand and implement: One simply gives each item a hidden number of up- and down-votes to start with. These hidden \"starter\" votes can be chosen in various ways — they serve to simply bias new items towards an intermediate value early on, with the bias becoming less important as more votes come in. This approach avoids each of the pitfalls we have discussed. Bayesian formulation Note: This section and the next are both fairly mathematical. They can be skipped for those wishing to focus on application method only. To start our Bayesian analysis, we begin by positing a general beta distribution for the up-vote rate prior distribution, $$\\tag{3}\\label{beta} P(p) = \\tilde{\\mathcal{N}} p&#94;a (1-p)&#94;b. $$ Here, \\(\\tilde{\\mathcal{N}}\\) is a normalization factor and \\(a\\) and \\(b\\) are some constants (we suggest methods for choosing their values in the discussion section). The function \\(P(p)\\) specifies an initial guess — in the absence of any reviews for an item — for what we think the probability is that it will have up-vote rate \\(p\\) . If item \\(i\\) actually has been reviewed, we can update our guess for its distribution using Bayes' rule : $$\\begin{align} \\tag{4} \\label{bayes_rule} P(p \\vert n_i(1), n_i(0)) =\\frac{ P( n_i(1), n_i(0) \\vert p ) P(p)}{P(n_i(1), n_i(0))} = \\mathcal{N} p&#94;{n_i(1)+a}(1-p)&#94;{n_i(0)+b}. \\end{align} $$ Here, we have evaluated \\( P( n(1), n(0) \\vert p )\\) using the binomial distribution , we've plugged in (\\ref{beta}) for \\(P(p)\\) , and we've collected all \\(p\\) -independent factors into the new normalization factor \\(\\mathcal{N}\\) . The formula (\\ref{bayes_rule}) provides the basis for the three ranking methods discussed below. Three Bayesian ranking systems Let's rank! Bayesian method 1: Choose the ordering that is most likely. It is a simple matter to write down a formal expression for the probability of any ranking. For example, given two items we have $$ P(p_1 > p_2) = \\int_0&#94;1 dp_1 \\int_0&#94;{p_1} dp_2 P(p_1) P(p_2). \\tag{5} \\label{int} $$ Plugging in (\\ref{bayes_rule}) for the \\(P(p_i)\\) ‘ s, this can be evaluated numerically. Evaluating the probability for the opposite ordering, we can then choose that which is most likely to be correct. \\(\\bullet\\) Pros: Approach directly optimizes for the object we're interested in, the ranking — very appealing! \\(\\bullet\\) Cons: Given \\(N\\) items, one has \\(N!\\) integrals to carry out — untenable for large \\(N\\) . \\(\\bullet\\) Note: See posssiblywrong's post here for some related, interesting points. Bayesian method 2: Rank item \\(i\\) by its median \\(p\\) -value. Sorting by an item score provides an approach that will scale well even at large \\(N\\) . A natural score to consider is an item's median \\(p\\) -value: that which it has a \\(50/50\\) shot of being larger (or smaller) than. Using (\\ref{bayes_rule}), this satisfies $$\\tag{6}\\label{m2} \\frac{\\int_0&#94;{p_{med}} p&#94;{n_i(1)+a}(1-p)&#94;{n_i(0)+b} dp}{\\int_0&#94;{1} p&#94;{n_i(1)+a}(1-p)&#94;{n_i(0)+b} dp} = 1/2. $$ The integral at left actually has a name — it's called the incomplete beta function . Using a statistics package, it can be inverted to give \\(p_{med}\\) . For example, if we set \\(a = b = 1\\) , an item with a single up-vote and no down-votes would get a score of \\(0.614\\) . In other words, we'd guess there's a 50/50 shot that the item's up-vote rate falls above this value, so we'd rank it higher than any other item whose \\(p\\) value is known to be smaller than this. \\(\\bullet\\) Pros: Sorting is fast. Gives intuitive, meaningful score for each item. \\(\\bullet\\) Cons: Inverting (\\ref{m2}) can be somewhat slow, e.g. \\(\\sim 10&#94;{-3}\\) seconds in Mathematica. \\(\\bullet\\) Note : EM also derived this score function, in a follow-up to his original post. However, he motivated it in a slightly different way — see here . Bayesian method 3: Rank item \\(i\\) by its most likely (aka MAP ) \\(p\\) -value. The most likely \\(p\\) -value for each item provides another natural score function. To find this, we simply set the derivative of (\\ref{bayes_rule}) to zero, $$ \\begin{align} \\partial_p p&#94;{n_i(1)+a}(1-p)&#94;{n_i(0)+b} &= \\left (\\frac{n_i(1)+a}{p} + \\frac{n_i(0)+b}{1-p} \\right ) p&#94;{n_i(1)+a}(1-p)&#94;{n_i(0)+b} = 0 \\\\ \\to p = \\tilde{p} &\\equiv \\frac{n_i(1)+a}{(n_i(1)+a) + (n_i(0)+b)}. \\tag{7} \\label{final} \\end{align} $$ This form \\(\\tilde{p}\\) is interesting because it resembles the sample mean \\(\\hat{p}\\) considered above. However, the actual number of up- and down-votes, \\(n_i(1)\\) and \\(n_i(0)\\) , are supplemented in (\\ref{final}) by \\(a\\) and \\(b\\) , respectively. We can thus interpret these values as effective \"starter votes\", given to each item before any real reviews are recorded. Their effect is to bias our guess for \\(p\\) towards the prior's peak value, with the bias being most strong when \\(a\\) and \\(b\\) are chosen large and/or when we have few actual votes present. For any non-zero choices, (\\ref{final}) avoids each of the pitfalls discussed above. Further, it approaches the true up-vote rate in the limit of large review sample sizes, as required. \\(\\bullet\\) Pros: Sorting is fast. Simple method for avoiding the common pitfalls. \\(\\bullet\\) Cons: Have to pick \\(a\\) and \\(b\\) — see below for suggested methods. Discussion We consider each of the four ranking methods we've discussed here to be interesting and useful — the three Bayesian ranking systems, as well as EM 's original system , which works well when one only needs to protect against false positives (again, we note that Bayesian method 2 was also considered by EM in a follow-up to his original post). In practice, the three Bayesian approaches will each tend to return similar, but sometimes slightly different rankings. With regards to \"correctness\", the essential point is that each method is well-motivated and avoids the common pitfalls. However, the final method is the easiest to apply, so it might be the most practical. To apply the Bayesian methods, one must specify the \\(a\\) and \\(b\\) values defining the prior, (\\ref{bayes_rule}). We suggest three methods for choosing these: 1) Choose these values to provide a good approximation to your actual distribution, fitting only to items for which you have good statistics. 2) A/B test to get the ranking that optimizes some quantity you are interested in, e.g. clicks. 3) Heuristics: For example, if simplicity is key, choose \\(a= b =1\\) , which biases towards an up-vote rate of \\(0.5\\) . If a conservative estimate is desired for new items, one can set \\(b\\) larger than \\(a\\) . Finally, if you want to raise the number of actual votes required before the sample rates dominate, simply increase the values of \\(a\\) and \\(b\\) accordingly. To conclude, we present some example output in the table below. We show values for the Wilson score \\(p_W\\) , with \\(z_{\\alpha/2}\\) set to \\(1.281\\) in (\\ref{emsol}) (the value reddit uses ), and the seed score \\(\\tilde{p}\\) , with \\(a\\) and \\(b\\) set to \\(1\\) in (\\ref{final}). Notice that the two scores are in near-agreement for the last item shown, which has already accumulated a fair number of votes. However, \\(p_W\\) is significantly lower than \\(\\tilde{p}\\) for each of the first three items. For example, the third has an up-vote rate of \\(66%\\) , but is only given a Wilson score of \\(0.32\\) : This means that it would be ranked below any mature item having an up-vote rate at least this high — including fairly unpopular items liked by only one in three! This observation explains why it is nearly impossible to have new comments noticed on a reddit thread that has already hit the front page. Were reddit to move to a ranking system that were less pessimistic of new comments, its mature threads might remain dynamic. up-votes down-votes \\(p_W\\) , \\(z_{\\alpha/2}= 1.281\\) \\(\\tilde{p}\\) , \\(a=b=1\\) 1 0 0.38 0.67 1 1 0.16 0.5 2 1 0.32 0.6 40 10 0.72 0.79 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"https://efavdb.com/ranking-revisited","loc":"https://efavdb.com/ranking-revisited"},{"title":"A review of the online course \"Introduction to Big Data with Apache Spark\"","text":"This is a review of Introduction to Big Data with Apache Spark ( CS100 .1x), the first in a two-part series introducing the big data processing engine, Spark . The one month-long MOOC was offered on edX for the first time in June 2015, and its sequel, Scalable Machine Learning ( CS190 .1x) is currently under way. Still from a lecture in \"Introduction to Big Data with Apache Spark\" Learning a new syntax is an unavoidably dry exercise; despite that handicap, the course lectures are watchable and directly relevant to the labs. Of course, the fun is in applying that hard-earned knowledge…actually, the learning curve is quite shallow, since a dozen or so basic Spark commands are enough to get you started on a simple project, e.g. see an example on github . The weekly assignments, provided in the form of IPython notebooks, are well-designed, keeping the focus on learning Spark (rather than coding/debugging python) by providing a skeleton of pre-filled code chunks for each assignment. The labs are: word counting all of Shakespeare's plays NASA Apache web server log analysis entity resolution of Google and Amazon product listings collaborative filtering to make movie recommendations Students only have to fill in sections of code that relate to the application of Spark. The structure of the assignments thus results in a gratifying amount of payoff for a small amount of work. You don't need to be fluent in python—far from it—in order to complete the labs, and can likely pick up what you need along the way if you have experience in some other programming language. I did find the labs to be much more time-consuming than the course estimates, partly because: Error tracebacks in Spark tend to hit you like a great wall of angry text, and are hard to parse (to the uninitiated, at least). I was more hesitant to experiment/test freely in the IPython Notebooks out of a fear that some unsanctioned code would cause problems with the tetchy lab Autograder. However, it never did freeze on me, although it did for others who were too liberal in their use of collect() statements. The instructors were extremely responsive to student questions and nimble to implement improvements in real-time. I'm sure the course will continue to improve over the next iterations. A few lucky students, including myself, were randomly selected to have access to the Databricks cloud-computing platform for the duration of the course. Although the procedures for running Spark in the standard course software (a prepared virtual machine) and Databricks cluster are almost identical, having cloud minions doing your bidding may tickle megalomaniacal tendencies you didn't know you had.","tags":"Review","url":"https://efavdb.com/review-intro-to-big-data-with-spark","loc":"https://efavdb.com/review-intro-to-big-data-with-spark"},{"title":"Multivariate Cramer-Rao inequality","text":"The Cramer-Rao inequality addresses the question of how accurately one can estimate a set of parameters \\(\\vec{\\theta} = \\{\\theta_1, \\theta_2, \\ldots, \\theta_m \\}\\) characterizing a probability distribution \\(P(x) \\equiv P(x; \\vec{\\theta})\\) , given only some samples \\(\\{x_1, \\ldots, x_n\\}\\) taken from \\(P\\) . Specifically, the inequality provides a rigorous lower bound on the covariance matrix of any unbiased set of estimators to these \\(\\{\\theta_i\\}\\) values. In this post, we review the general, multivariate form of the inequality, including its significance and proof. Introduction and theorem statement The analysis of data very frequently requires one to attempt to characterize a probability distribution. For instance, given some random, stationary process that generates samples \\(\\{x_i\\}\\) , one might wish to estimate the mean \\(\\mu\\) of the probability distribution \\(P\\) characterizing this process. To do this, one could construct an estimator function \\(\\hat{\\mu}(\\{x_i\\})\\) — a function of some samples taken from \\(P\\) — that is intended to provide an approximation to \\(\\mu\\) . Given \\(n\\) samples, a natural choice is provided by \\begin{eqnarray} \\hat{\\mu}(\\{x_i\\}) = \\frac{1}{n}\\sum_{i = 1}&#94;n x_i, \\tag{1} \\end{eqnarray} the mean of the samples. This particular choice of estimator will always be unbiased given a stationary \\(P\\) — meaning that it will return the correct result, on average. However, each particular sample set realization will return a slightly different mean estimate. This means that \\(\\hat{\\mu}\\) is itself a random variable having its own distribution and width. More generally, one might be interested in a distribution characterized by a set of \\(m\\) parameters \\(\\{\\theta_i\\}\\) . Consistently good estimates to these values require estimators with distributions that are tightly centered around the true \\(\\{\\theta_i\\}\\) values. The Cramer-Rao inequality tells us that there is a fundamental limit to how tightly centered such estimators can be, given only \\(n\\) samples. We state the result below. Theorem: The multivariate Cramer-Rao inequality Let \\(P\\) be a distribution characterized by a set of \\(m\\) parameters \\(\\{\\theta_i\\}\\) , and let \\(\\{\\hat{\\theta_i}\\equiv \\hat{\\theta_i}(\\{x_i\\})\\}\\) be an unbiased set of estimator functions for these parameters. Then, the covariance matrix (see definition below) for the \\(\\hat{\\{\\theta_i\\}}\\) satisfies, \\begin{eqnarray} \\tag{2} \\label{cramer_rao_bound} cov(\\hat{\\theta}, \\hat{\\theta}) \\geq \\frac{1}{n} \\times \\frac{1}{ cov(\\nabla_{\\vec{\\theta}} \\log P(x),\\nabla_{\\vec{\\theta}} \\log P(x) )}. \\end{eqnarray} Here, the inequality holds in the sense that left side of the above equation, minus the right, is positive semi-definite. We discuss the meaning and significance of this equation in the next section. Interpretation of the result To understand (\\ref{cramer_rao_bound}), we must first review a couple of definitions. These follow. Definition 1 . Let \\(\\vec{u}\\) and \\(\\vec{v}\\) be two jointly-distributed vectors of stationary random variables. The covariance matrix of \\(\\vec{u}\\) and \\(\\vec{v}\\) is defined by $$ cov(\\vec{u}, \\vec{v})_{ij} = \\overline{(u_{i}- \\overline{u_i})(v_{j}- \\overline{v_j})} \\equiv \\overline{\\delta u_{i} \\delta v_{j}}\\tag{3} \\label{cov}, $$ where we use overlines for averages. In words, (\\ref{cov}) states that \\(cov(\\vec{u}, \\vec{v})_{ij}\\) is the correlation function of the fluctuations of \\(u_i\\) and \\(v_j\\) . Definition 2 . A real, square matrix \\(M\\) is said to be positive semi-definite if $$ \\vec{a}&#94;T\\cdot M \\cdot \\vec{a} \\geq 0 \\tag{4} \\label{pd} $$ for all real vectors \\(\\vec{a}\\) . It is positive definite if the \" \\(\\geq\\) \" above can be replaced by a \" \\(>\\) \" . The interesting consequences of (\\ref{cramer_rao_bound}) follow from the following observation: Observation . For any constant vectors \\(\\vec{a}\\) and \\(\\vec{b}\\) , we have $$ cov(\\vec{a}&#94;T\\cdot\\vec{u}, \\vec{b}&#94;T \\cdot \\vec{v}) = \\vec{a}&#94;T \\cdot cov(\\vec{u}, \\vec{v}) \\cdot \\vec{b}. \\tag{5} \\label{fact} $$ This follows from the definition (\\ref{cov}). Taking \\(\\vec{a}\\) and \\(\\vec{b}\\) to both be along \\(\\hat{i}\\) in (\\ref{fact}), and combining with (\\ref{pd}), we see that (\\ref{cramer_rao_bound}) implies that $$ \\sigma&#94;2(\\hat{\\theta}_i&#94;2) \\geq \\frac{1}{n} \\times \\left (\\frac{1}{ cov(\\nabla_{\\vec{\\theta}} \\log P(x),\\nabla_{\\vec{\\theta}} \\log P(x) )} \\right)_{ii},\\tag{6}\\label{CRsimple} $$ where we use \\(\\sigma&#94;2(x)\\) to represent the variance of \\(x\\) . The left side of (\\ref{CRsimple}) is the variance of the estimator function \\(\\hat{\\theta}_i\\) , whereas the right side is a function of \\(P\\) only. This tells us that there is fundamental — distribution-dependent — lower limit on the uncertainty one can achieve when attempting to estimate any parameter characterizing a distribution . In particular, (\\ref{CRsimple}) states that the best variance one can achieve scales like \\(O(1/n)\\) , where \\(n\\) is the number of samples available \\(&#94;1\\) — very interesting! Why is there a relationship between the left and right matrices in (\\ref{cramer_rao_bound})? Basically, the right side relates to the inverse rate at which the probability of a given \\(x\\) changes with \\(\\theta\\) : If \\(P(x \\vert \\theta)\\) is highly peaked, the gradient of \\(P(x \\vert \\theta)\\) will take on large values. In this case, a typical observation \\(x\\) will provide significant information relating to the true \\(\\theta\\) value, allowing for unbiased \\(\\hat{\\theta}\\) estimates that have low variance. In the opposite limit, where typical observations are not very \\(\\theta\\) -informative, unbiased \\(\\hat{\\theta}\\) estimates must have large variance \\(&#94;2\\) . We now turn to the proof of (\\ref{cramer_rao_bound}). Theorem proof Our discussion here expounds on that in the online text of Cízek, Härdle, and Weron. We start by deriving a few simple lemmas. We state and derive these sequentially below. Lemma 1 Let \\(T_j(\\{x_i\\}) \\equiv \\partial_{\\theta_j} \\log P(\\{x_i\\}; \\vec{\\theta})\\) be a function of a set of independent sample values \\(\\{x_i\\}\\) . Then, the average of \\(T_j(\\{x_i\\})\\) is zero. Proof: We obtain the average of \\(T_j(\\{x_i\\})\\) through integration over the \\(\\{x_i\\}\\) , weighted by \\(P\\) , $$ \\int P(\\{x_i\\};\\vec{\\theta}) \\partial_{\\theta_j} \\log P(\\{x_i\\}; \\vec{\\theta}) d\\vec{x} = \\int P \\frac{\\partial_{\\theta_j} P}{P} d\\vec{x} = \\partial_{\\theta_j} \\int P d\\vec{x} = \\partial_{\\theta_j} 1 = 0. \\tag{7} $$ Lemma 2 . The covariance matrix of an unbiased \\(\\hat{\\theta}\\) and \\(\\vec{T}\\) is the identity matrix. Proof: Using (\\ref{cov}), the assumed fact that \\(\\hat{\\theta}\\) is unbiased, and Lemma 1, we have $$\\begin{align} cov \\left (\\hat{\\theta}(\\{x_i\\}), \\vec{T}(\\{x_i\\}) \\right)_{jk} &= \\int P(\\{x_i\\}) (\\hat{\\theta}_j - \\theta_j ) \\partial_{\\theta_k} \\log P(\\{x_i\\}) d\\vec{x}\\\\ & = \\int (\\hat{\\theta}_j - \\theta_j ) \\partial_{\\theta_k} P d\\vec{x} \\\\ &= -\\int P \\partial_{\\theta_k} (\\hat{\\theta}_j - \\theta_j ) d \\vec{x} \\tag{8} \\end{align} $$ Here, we have integrated by parts in the last line. Now, \\(\\partial_{\\theta_k} \\theta_j = \\delta_{jk}\\) . Further, \\(\\partial_{\\theta_k} \\hat{\\theta}_j = 0\\) , since \\(\\hat{\\theta}\\) is a function of the samples \\(\\{x_i\\}\\) only. Plugging these results into the last line, we obtain $$ cov \\left (\\hat{\\theta}, \\vec{T} \\right)_{jk} = \\delta_{jk} \\int P d\\vec{x} = \\delta_{jk}. \\tag{9} $$ Lemma 3 . The covariance matrix of \\(\\vec{T}\\) is \\(n\\) times the covariance matrix of \\(\\nabla_{\\vec{\\theta}} \\log P(x_1 ; \\vec{\\theta})\\) — a single-sample version of \\(\\vec{T}\\) . Proof: From the definition of \\(\\vec{T}\\) , we have $$ T_j = \\partial_{\\theta_j} \\log P(\\{x_i\\}, \\vec{\\theta}) = \\sum_{i=1}&#94;n \\partial_{\\theta_j} \\log P(x_i, \\vec{\\theta}), \\tag{10} $$ where the last line follows from the fact that the \\(\\{x_i\\}\\) are independent, so that \\(P(\\{x_i\\}, \\vec{\\theta}) = \\prod P(x_i; \\vec{\\theta})\\) . The sum on the right side of the above equation is a sum of \\(n\\) independent, identically-distributed random variables. If follows that their covariance matrix is \\(n\\) times that for any individual. Lemma 4 . Let \\(x\\) and \\(y\\) be two scalar stationary random variables. Then, their correlation coefficient is defined to be \\(\\rho \\equiv \\frac{cov(x,y)}{\\sigma(x) \\sigma(y)}\\) . This satisfies $$ -1 \\leq \\rho \\leq 1 \\label{c_c} \\tag{11} $$ Proof: Consider the variance of \\(\\frac{x}{\\sigma(x)}+\\frac{y}{\\sigma(y)}\\) . This is $$ \\begin{align} var \\left( \\frac{x}{\\sigma(x)}+\\frac{y}{\\sigma(y)} \\right) &= \\frac{\\sigma&#94;2(x)}{\\sigma&#94;2(x)} + 2\\frac{ cov(x,y)}{\\sigma(x) \\sigma(y)} + \\frac{\\sigma&#94;2(y)}{\\sigma&#94;2(y)} \\\\ &= 2 + 2 \\frac{ cov(x,y)}{\\sigma(x) \\sigma(y)} \\geq 0. \\tag{12} \\end{align} $$ This gives the left side of (\\ref{c_c}). Similarly, considering the variance of \\(\\frac{x}{\\sigma(x)}-\\frac{y}{\\sigma(y)}\\) gives the right side. We're now ready to prove the Cramer-Rao result. Proof of Cramer-Rao inequality . Consider the correlation coefficient of the two scalars \\(\\vec{a} \\cdot \\hat{\\theta}\\) and \\( \\vec{b} \\cdot \\vec{T}\\) , with \\(\\vec{a}\\) and \\(\\vec{b}\\) some constant vectors. Using (\\ref{fact}) and Lemma 2, this can be written as $$\\begin{align} \\rho & \\equiv \\frac{cov(\\vec{a} \\cdot \\hat{\\theta} ,\\vec{b} \\cdot \\vec{T})}{\\sqrt{var(\\vec{a} \\cdot \\hat{\\theta})var(\\vec{b} \\cdot \\vec{T})}} \\\\ &= \\frac{\\vec{a}&#94;T \\cdot \\vec{b}}{\\left(\\vec{a}&#94;T \\cdot cov(\\hat{\\theta}, \\hat{\\theta}) \\cdot \\vec{a} \\right)&#94;{1/2} \\left( \\vec{b}&#94;T \\cdot cov(\\vec{T},\\vec{T}) \\cdot \\vec{b} \\right)&#94;{1/2}}\\leq 1. \\tag{13} \\end{align} $$ The last inequality here follows from Lemma 4. We can find the direction \\(\\hat{b}\\) where the bound above is most tight — at fixed \\(\\vec{a}\\) — by maximizing the numerator while holding the denominator fixed in value. Using a Lagrange multiplier to hold \\(\\left( \\vec{b}&#94;T \\cdot cov(\\vec{T},\\vec{T}) \\cdot \\vec{b} \\right) \\equiv 1\\) , the numerator's extremum occurs where $$ \\vec{a}&#94;T + 2 \\lambda \\vec{b}&#94;T \\cdot cov(\\vec{T},\\vec{T}) = 0 \\ \\ \\to \\ \\ \\vec{b}&#94;T = - \\frac{1}{2 \\lambda} \\vec{a}&#94;T \\cdot cov(\\vec{T}, \\vec{T})&#94;{-1}. \\tag{14} $$ Plugging this form into the prior line, we obtain $$ - \\frac{\\vec{a}&#94;T \\cdot cov(\\vec{T},\\vec{T})&#94;{-1} \\cdot \\vec{a}}{\\left(\\vec{a}&#94;T \\cdot cov(\\hat{\\theta}, \\hat{\\theta}) \\cdot \\vec{a} \\right)&#94;{1/2} \\left(\\vec{a}&#94;T \\cdot cov(\\vec{T},\\vec{T})&#94;{-1} \\cdot \\vec{a} \\right)&#94;{1/2}}\\leq 1. \\tag{15} $$ Squaring and rearranging terms, we obtain $$ \\vec{a}&#94;T \\cdot \\left (cov(\\hat{\\theta},\\hat{\\theta}) - cov(\\vec{T},\\vec{T})&#94;{-1} \\right ) \\cdot \\vec{a} \\geq 0. \\tag{16} $$ This holds for any \\(\\vec{a}\\), implying that \\(cov(\\hat{\\theta}, \\hat{\\theta}) - cov(\\vec{T},\\vec{T})&#94;{-1}\\) is positive semi-definite — see (\\ref{pd}). Applying Lemma 3, we obtain the result \\(&#94;3\\) . \\(\\blacksquare\\) Thank you for reading — we hope you enjoyed. [1] More generally, (\\ref{fact}) tells us that an observation similar to (\\ref{CRsimple}) holds for any linear combination of the \\(\\{\\theta_i\\}\\) . Notice also that the proof we provide here could also be applied to any individual \\(\\theta_i\\) , giving \\(\\sigma&#94;2(\\hat{\\theta}_i) \\geq 1/n \\times 1/\\langle(\\partial_{\\theta_i} \\log P)&#94;2\\rangle\\) . This is easier to apply than (\\ref{cramer_rao_bound}), but is less stringent. [2] It might be challenging to intuit the exact function that appears on the right side of \\((\\ref{cramer_rao_bound})\\) . However, the appearance of \\(\\log P\\) ‘ s does make some intuitive sense, as it allows the derivatives involved to measure rates of change relative to typical values, \\(\\nabla_{\\theta} P / P\\) . [3] The discussion here covers the \"standard proof\" of the Cramer-Rao result. Its brilliance is that it allows one to work with scalars. In contrast, when attempting to find my own proof, I began with the fact that all covariance matrices are positive definite. Applying this result to the covariance matrix of a linear combination of \\(\\hat{\\theta}\\) and \\(\\vec{T}\\) , one can quickly get to results similar in form to the Cramer-Rao bound, but not quite identical. After significant work, I was eventually able to show that \\(\\sqrt{cov(\\hat{\\theta},\\hat{\\theta})} - 1/\\sqrt{cov(\\vec{T},\\vec{T}) } \\geq 0\\) . However, I have yet to massage my way to the final result using this approach — the difficulty being that the matrices involved don't commute. By working with scalars from the start, the proof here cleanly avoids all such issues. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"https://efavdb.com/multivariate-cramer-rao-bound","loc":"https://efavdb.com/multivariate-cramer-rao-bound"},{"title":"Reshaping Data in R","text":"Today, we'll talk about reshaping data in R. At the same time, we'll see how for-loops can be avoided by using R functionals (functions of functions). Functionals are faster than for-loops and make code easier to read by clearly laying out the intent of a loop. The Task Suppose you're given the two tables below, saved as individual .csv files. Each table contains measurements of plant height on specific days. The plants are split into fertilizer treatment groups A and B, and each plant can be uniquely identified by its id. Day 1 day group id height 1 A A.1 1.1 1 A A.2 1.2 1 B B.1 1 1 B B.2 0.9 Day 2 day group id height 2 A A.1 1.5 2 A A.2 1.5 2 B B.1 2.1 2 B B.2 1.9 Your task is to split the data by treatment group rather than day. The desired output is one file per group, with day on the vertical of the new tables, plant id on the horizontal, and height as the value inside the table. For example, the new table for group A would be: Group A day A.1 A.2 1 1.1 1.2 2 1.5 1.5 This example can be manually formatted in Excel pretty quickly, but in real life, data only looks like this if you're in first grade. So, we're going to take a little time to write a script now in order to save a lot of time in the future. The code broken down First, let's load some libraries that are useful for reshaping data. library ( \"plyr\" ) library ( \"reshape2\" ) (To install, e.g. the plyr package, simply type install.packages(\"plyr\") in the R console.) Then store the full names, including directory path, of the files in a character vector. Here, the input .csv files are stored in the subdirectory \"input\". in_files = list.files ( path = \"input\" , pattern = \"*.csv\" , full.names = T ) Now, we'll use lapply(...) , one of base R's functionals, to read the input files into a list of data frames, which we call list_dfperday. list_dfperday = lapply ( in_files , read.csv ) lapply() loops over the the names of the files in in_files to each file, apply the function read.csv() , which reads the contents of the file into a data frame output the result into a list of data frames, called dfs, of the same length as the character vector containing the file names Bind together (rbind) the data frames in the list by rows into a single data frame, so we can more conveniently subset the data by group. df = ldply ( list_dfperday , rbind ) In the plyr package, the first two letters in the \"—ply\" functions indicate what type of object is being transformed into another. In this case, we are reshaping a list (‘l') of data frames into a data frame (‘d'), hence \"ldply\". Define a function to cast the data into a data frame with a shape specified by the formula day ~ id : per day (x-variable), output the corresponding height (value.var) of each plant id (y-variable). cast_short = function ( mydata ) dcast ( mydata , day ~ id , value.var = height ) So the function \"cast_short\" takes as its argument a data frame \"mydata\" (containing columns \"day\" and \"id\") and returns a reshaped data frame with \"day\" on the rows and \"id\" on the columns. Now apply the function we just defined to each treatment group. Store the outcome of each application of cast_short into a list. (Also compare to the earlier use of ldply .) list_dfpergroup = dlply ( df , .(group ), cast_short ) It's finally time to write the formatted data frames to .csv files. We'll output them in a new directory called \"output\" with the aid of another base R functional mapply . outdir = \"output\" outnames = paste0 ( names ( list_dfpergroup ), \".csv\" ) dir.create ( outdir ) mapply ( function ( x , y ) write.csv ( x , file.path ( outdir , y ), row.names = F ), list_dfpergroup , outnames ) mapply allows functions with multiple arguments to be applied in a loop. Here, we've defined an anonymous, i.e. unnamed, function within mapply that has two arguments: the value of x is given by list_dfpergroup, and y is given by the character vector outnames. In other words, mapply steps through the list and vector simultaneously, writing list_dfpergroup \\(_i\\) to a file named outnames \\(_i\\) . The code in one piece Now here's the code all in one piece. Note that the core reshaping takes place in three lines, starting at the line containing ldply . library ( \"plyr\" ) library ( \"reshape2\" ) in_files = list.files ( path = \"input\" , pattern = \"*.csv\" , full.names = T ) list_dfperday = lapply ( in_files , read.csv ) df = ldply ( list_dfperday , rbind ) cast_short = function ( mydata ) dcast ( mydata , day ~ id , value.var = height ) list_dfpergroup = dlply ( df , .(group ), cast_short ) outdir = \"output\" outnames = paste0 ( names ( list_dfpergroup ), \".csv\" ) dir.create ( outdir ) mapply ( function ( x , y ) write.csv ( x , file.path ( outdir , y ), row.names = F ), list_dfpergroup , outnames ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Programming","url":"https://efavdb.com/reshaping-data-in-r","loc":"https://efavdb.com/reshaping-data-in-r"},{"title":"MLB week 7 overview, week 8 posted","text":"52/99 this past week, or 53% accuracy overall. Breakdown by point spread follows, and new predictions are posted . Point spread # games Accuracy <= 1 32 56% <=2 17 53% <=5 34 47% >5 16 56%","tags":"MLB prediction project","url":"https://efavdb.com/mlb-week-7-overview-week-8-posted","loc":"https://efavdb.com/mlb-week-7-overview-week-8-posted"},{"title":"MLB week 6 overview, week 7 posted","text":"Super tough week this past week. We went 43/94, or 47% accuracy overall. Breakdown by point spread below. New predictions are posted . Point spread # games Accuracy <= 1 32 53% <=2 15 33% <=5 33 45% >5 14 43%","tags":"MLB prediction project","url":"https://efavdb.com/mlb-week-6-overview-week-7-posted","loc":"https://efavdb.com/mlb-week-6-overview-week-7-posted"},{"title":"MLB week 5 review, week 6 posted","text":"Back on track: 52/96, or 54% accuracy overall this past week. We've now made a number of improvements to the algorithm, and we're excited to see how our performance continues from here on out. New predictions are posted . Point spread # games Accuracy <= 1 34 53% <=2 16 56% <=5 26 50% >5 20 60%","tags":"MLB prediction project","url":"https://efavdb.com/mlb-week-5-review-week-6-posted","loc":"https://efavdb.com/mlb-week-5-review-week-6-posted"},{"title":"MLB week 4 review, week 5 posted","text":"Relatively poor results this past week, 44 of 94 games, 47% accuracy overall. Given our relatively strong performance in the NBA this past season, our early MLB results have been a bit disappointing. However, after doing a bit of learnin', we realized an interesting point: Professional baseball teams are simply much more balanced than their NBA counterparts. A statistic illustrating this point is provided by the winning rates of the top teams: This past year, the Warriors had a win rate just over 80%, while the top team in the MLB last season, the Angels, had just under a 61% win rate. This difference explains why we can't achieve 70% win prediction accuracy in the MLB . Instead, we'll be shooting for just 56% accuracy this season — this may sound modest, but we expect this rate will be a challenge to meet. To work towards that goal, we've already begun to implement a number of improvements to our algorithm. New predictions are posted . Point spread # games Accuracy <= 1 23 43% <=2 20 45% <=5 32 41% >5 19 63%","tags":"MLB prediction project","url":"https://efavdb.com/mlb-week-4-review-week-5-posted","loc":"https://efavdb.com/mlb-week-4-review-week-5-posted"},{"title":"MLB week 3 review, week 4 predictions","text":"This past week, we correctly predicted 51 of 93 games, which equates to a 54.8% accuracy. This is consistent with past weeks. It's strange that our accuracy level here is so far below what we were achieving for the NBA . Presumably, this represents some underlying increased variance in the MLB games that have happened so far. At any rate, our poor early start has motivated us to get back to work on algorithm improvements… New predictions are now posted . Point spread # games Accuracy <= 1 26 65% <=2 14 57% <=5 34 44% >5 19 58%","tags":"MLB prediction project","url":"https://efavdb.com/mlb-week-3-review-week-4-predictions","loc":"https://efavdb.com/mlb-week-3-review-week-4-predictions"},{"title":"MLB week 2 review, week 3 predictions","text":"This week, we went 52/95, giving 54.7% accuracy, a value comparable to the previous week. We believe this relatively modest early showing is largely due to the fact that we haven't yet got many games to train on for the current season. As more games come in, our accuracy should increase, perhaps approaching 70% — the value we were hitting at the end of this last NBA season. We also are working on implementing a few improvements to our algorithm, which we hope will also give us a boost. New predictions are now posted . Point spread # games Accuracy <= 1 31 52% <=2 14 50% <=5 32 56% >5 18 61%","tags":"MLB prediction project","url":"https://efavdb.com/mlb-week-2-review-week-3-predictions","loc":"https://efavdb.com/mlb-week-2-review-week-3-predictions"},{"title":"The mean shift clustering algorithm","text":"Mean shift clustering Mean shift clustering is a general non-parametric cluster finding procedure — introduced by Fukunaga and Hostetler [ 1 ], and popular within the computer vision field. Nicely, and in contrast to the more-well-known K-means clustering algorithm, the output of mean shift does not depend on any explicit assumptions on the shape of the point distribution, the number of clusters, or any form of random initialization. We describe the mean shift algorithm in some detail in the technical background section at the end of this post. However, its essence is readily explained in a few words: Essentially, mean shift treats the clustering problem by supposing that all points given represent samples from some underlying probability density function, with regions of high sample density corresponding to the local maxima of this distribution. To find these local maxima, the algorithm works by allowing the points to attract each other, via what might be considered a short-ranged \"gravitational\" force. Allowing the points to gravitate towards areas of higher density, one can show that they will eventually coalesce at a series of points, close to the local maxima of the distribution. Those data points that converge to the same local maxima are considered to be members of the same cluster. In the next couple of sections, we illustrate application of the algorithm to a couple of problems. We make use of the python package SkLearn , which contains a mean shift implementation. Following this, we provide a quick discussion and an appendix on technical details. Mean shift clustering in action In today's post we will have two examples. First, we will show how to use mean shift clustering to identify clusters of data in a 2D data set. Second, we will use the algorithm to segment a picture based on the colors in the image. To do this we need a handful of libraries from sklearn, numpy, matplotlib, and the Python Imaging Library ( PIL ) to handle reading in a jpeg image. import numpy as np from sklearn.cluster import MeanShift , estimate_bandwidth from sklearn.datasets.samples_generator import make_blobs import matplotlib.pyplot as plt from itertools import cycle from PIL import Image Finding clusters in a 2D data set This first example is based off of the sklearn tutorial for mean shift clustering: We generate data points centered at 4 locations, making use of sklearn's make_blobs library. To apply the clustering algorithm to the points generated, we must first set the attractive interaction length between examples, also know as the algorithm's bandwidth. Sklearn's implementation contains a built-in function that allows it to automatically estimate a reasonable value for this, based upon the typical distance between examples. We make use of that below, carry out the clustering, and then plot the results. # %% Generate sample data centers = [ [1, 1 ] , [ -.75, -1 ] , [ 1, -1 ] , [ -3, 2 ] ] X , _ = make_blobs ( n_samples = 10000 , centers = centers , cluster_std = 0.6 ) # %% Compute clustering with MeanShift # The bandwidth can be automatically estimated bandwidth = estimate_bandwidth ( X , quantile = .1 , n_samples = 500 ) ms = MeanShift ( bandwidth = bandwidth , bin_seeding = True ) ms . fit ( X ) labels = ms . labels_ cluster_centers = ms . cluster_centers_ n_clusters_ = labels . max () + 1 # %% Plot result plt . figure ( 1 ) plt . clf () colors = cycle ( 'bgrcmykbgrcmykbgrcmykbgrcmyk' ) for k , col in zip ( range ( n_clusters_ ), colors ) : my_members = labels == k cluster_center = cluster_centers [ k ] plt . plot ( X [ my_members, 0 ] , X [ my_members, 1 ] , col + '.' ) plt . plot ( cluster_center [ 0 ] , cluster_center [ 1 ] , 'o' , markerfacecolor = col , markeredgecolor = 'k' , markersize = 14 ) plt . title ( 'Estimated number of clusters: %d' % n_clusters_ ) plt . show () As you can see below, the algorithm has found clusters centered on each of the blobs we generated. Segmenting a color photo In the first example, we were using mean shift clustering to look for spatial clusters. In our second example, we will instead explore 3D color space, RGB , by considering pixel values taken from an image of a toy car. The procedure is similar — here, we cluster points in 3d, but instead of having data(x,y) we have data(r,g,b) taken from the image's RGB pixel values. Clustering these color values in this 3d space returns a series of clusters, where the pixels in those clusters are similar in RGB space. Recoloring pixels according to their cluster, we obtain a segmentation of the original image. #%% Part 2 : Color image segmentation using mean shift image = Image . open ( 'toy.jpg' ) image = np . array ( image ) # Need to convert image into feature array based # on rgb intensities flat_image = np . reshape ( image , [ - 1 , 3 ]) # Estimate bandwidth bandwidth2 = estimate_bandwidth ( flat_image , quantile = . 2 , n_samples = 500 ) ms = MeanShift ( bandwidth2 , bin_seeding = True ) ms . fit ( flat_image ) labels = ms . labels_ # Plot image vs segmented image plt . figure ( 2 ) plt . subplot ( 2 , 1 , 1 ) plt . imshow ( image ) plt . axis ( 'off' ) plt . subplot ( 2 , 1 , 2 ) plt . imshow ( np . reshape ( labels , [ 851 , 1280 ])) plt . axis ( 'off' ) The bottom image below illustrates that one can effectively use this approach to identify the key shapes within an image, all without doing any image processing to get rid of glare or background — pretty great! Discussion Although mean shift is a reasonably versatile algorithm, it has primarily been applied to problems in computer vision, where it has been used for image segmentation, clustering, and video tracking. Application to big data problems can be challenging due to the fact the algorithm can become relatively slow in this limit. However, research is presently underway to speed up its convergence, which should enable its application to larger data sets. Mean shift pros: No assumptions on the shape or number of data clusters. The procedure only has one parameter, the bandwidth. Output doesn't depend on initializations. Mean shift cons: Output does depend on bandwidth: too small and convergence is slow, too large and some clusters may be missed. Computationally expensive for large feature spaces. Often slower than K-Means clustering Technical details follow. Technical background Kernel density estimation A general formulation of the mean shift algorithm can be developed through consideration of density kernels. These effectively work by smearing out each point example in space over some small window. Summing up the mass from each of these smeared units gives an estimate for the probability density at every point in space (by smearing, we are able to obtain estimates at locations that do not sit exactly atop any example). This approach is often referred to as kernel density estimation — a method for density estimation that often converges more quickly than binning, or histogramming, and one that also nicely returns a continuous estimate for the density function. To illustrate, suppose we are given a data set \\(\\{\\textbf{u}_i\\}\\) of points in d-dimensional space, sampled from some larger population, and that we have chosen a kernel \\(K\\) having bandwidth parameter \\(h\\) . Together, these data and kernel function return the following kernel density estimator for the full population's density function \\begin{eqnarray} f_K(\\textbf{u}) = \\frac{1}{nh&#94;d}\\sum\\limits_{i=1}&#94;n K(\\frac{\\textbf{u}-\\textbf{u}_i}{h}) \\end{eqnarray} The kernel (smearing) function here is required to satisfy the following two conditions: \\(\\int K(\\textbf{u})d\\textbf{u} = 1\\) \\(K(\\textbf{u})=K(\\vert \\textbf{u} \\vert)\\) for all values of \\(\\textbf{u}\\) The first requirement is needed to ensure that our estimate is normalized, and the second is associated with the symmetry of our space. Two popular kernel functions that satisfy these conditions are given by Flat/Uniform \\( \\begin{align} K(\\textbf{u}) = \\frac{1}{2}\\left\\{ \\begin{array}{lr} 1 & -1 \\le \\vert \\textbf{u} \\vert \\le 1\\ 0 & else \\end{array} \\right. \\end{align} \\) Gaussian = \\(K(\\textbf{u}) = \\frac{1}{\\left(2\\pi\\right)&#94;{d/2}} e&#94;{-\\frac{1}{2} \\vert \\textbf{u} \\vert&#94;2}\\) Below we plot an example in 1-d using the gaussian kernel to estimate the density of some population along the x-axis. You can see that each sample point adds a small Gaussian to our estimate, centered about it: The equations above may look a bit intimidating, but the graphic here should clarify that the concept is pretty straightforward. Example of a kernel density estimation using a gaussian kernel for each data point: Adding up small Gaussians about each example returns our net estimate for the total density, the black curve. Mean shift algorithm Recall that the basic goal of the mean shift algorithm is to move particles in the direction of local increasing density. To obtain an estimate for this direction, a gradient is applied to the kernel density estimate discussed above. Assuming an angularly symmetric kernel function, \\(K(\\textbf{u}) = K(\\vert \\textbf{u} \\vert)\\) , one can show that this gradient takes the form \\begin{eqnarray} \\nabla f_K(\\textbf{u}) = \\frac{2}{nh&#94;{d+2}} \\left ( \\sum\\limits_{i=1}&#94;n g(\\left \\vert \\frac{\\textbf{u}-\\textbf{u}_i}{h} \\right \\vert) \\right ) \\textbf{m}(\\textbf{u}). \\end{eqnarray} where \\begin{eqnarray} \\textbf{m}(\\textbf{u}) = \\left ( \\frac{\\sum\\limits_{i=1}&#94;n \\textbf{u}_i g(\\left \\vert \\frac{\\textbf{u}-\\textbf{u}_i}{h} \\right \\vert)}{\\sum\\limits_{i=1}&#94;n g(\\left \\vert \\frac{\\textbf{u}-\\textbf{u}_i}{h} \\right \\vert)}-\\textbf{u} \\right ), \\end{eqnarray} and \\(g(\\vert \\textbf{u} \\vert ) = -K'(\\vert \\textbf{u} \\vert)\\) is the derivative of the selected kernel profile. The vector \\(\\textbf{m}(\\textbf{u})\\) here, called the mean shift vector, points in the direction of increasing density — the direction we must move our example. With this estimate, then, the mean shift algorithm protocol becomes Compute the mean shift vector \\(\\textbf{m}(\\textbf{u}_i)\\) , evaluated at the location of each training example \\(\\textbf{u}_i\\) Move each example from \\(\\textbf{u}_i \\to \\textbf{u}_i + \\textbf{m}(\\textbf{u}_i)\\) Repeat until convergence — ie, until the particles have reached equilibrium. As a final step, one determines which examples have ended up at the same points, marking them as members of the same cluster. For a proof of convergence and further mathematical details, see Comaniciu & Meer (2002) [ 2 ]. ​1. Fukunaga and Hostetler, \"The Estimation of the Gradient of a Density Function, with Applications in Pattern Recognition\", IEEE Transactions on Information Theory vol 21 , pp 32-40 ,1975 2. Dorin Comaniciu and Peter Meer, Mean Shift : A Robust approach towards feature space analysis, IEEE Transactions on Pattern Analysis and Machine Intelligence vol 24 No 5 May 2002. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods, Theory","url":"https://efavdb.com/mean-shift","loc":"https://efavdb.com/mean-shift"},{"title":"NBA week 22 results, NBA season review, MLB week 1","text":"We finished the NBA regular season strong, with correct guesses on 41 of 56 games this past week — 73% accuracy. Overall, our season average was roughly 65%. This is significantly lower than those returned by the bookies this year, whose accuracy overall sat just under 70%. The differences there likely have much to do with information relating to individual players (eg injuries) that would be difficult to take into account in our team-centric modeling approach. It also took us a number of weeks to come up with a solid way of choosing our model's hyperparameters. With these issues identified/settled, we anticipate improvements on these numbers next year. We won't be modeling the NBA playoffs, so this will be our last basketball post for the current season — we had a blast! With regards to the MLB , in our first week, we guessed correctly on 52/93 games, giving 56%. A somewhat poor result, fueled in part by the lackluster Giants. New predictions are up, with better accuracy hoped for this week!","tags":"MLB prediction project, NBA prediction project","url":"https://efavdb.com/nba-week-22-results-nba-season-review-mlb-week-1","loc":"https://efavdb.com/nba-week-22-results-nba-season-review-mlb-week-1"},{"title":"MLB predictions take off!","text":"Announcing: EFavDB's first major league baseball prediction project! Just as in our corresponding NBA project, we will be providing free-of-charge game winner predictions on a weekly basis. In addition, we've implemented a MLB dashboard (screenshot above) where you can get a quick summary of each team's prior results. You can also use the dashboard to check out our guesses for who beat whom, were they to play today, etc. The algorithm we'll be applying this season is similar to that discussed here , for the NBA . We've set its parameters to generate reasonably conservative predictions, but ones that will also lead to interesting upset predictions when appropriate. Unlike many other sites, our predictions do not take into account the over-under values published by bookies, and so are independent of their opinions. Looking forward to a great season!","tags":"MLB prediction project","url":"https://efavdb.com/mlb-predictions-take-off","loc":"https://efavdb.com/mlb-predictions-take-off"},{"title":"NBA week 21 summary, week 22 predictions","text":"Another great week for the algorithm, going 35 for 49, or 71.4% accuracy! The coming week's predictions are now up . Again, apologies for the delayed post this week. Automation should be coming soon. Point spread # games Accuracy < 6 15 60% 5-10 11 55% 11-15 9 78% >15 14 93%","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-21-summary-week-22-predictions","loc":"https://efavdb.com/nba-week-21-summary-week-22-predictions"},{"title":"NBA week 20 summary, week 21 predictions","text":"Excellent bounce-back from last week: 41 for 53, or 77.4% accuracy! The new predictions are posted . Apologies for the delay, which was due to our new-found employment. We hope to automate the posting and accuracy checks soon. Point spread # games Accuracy < 6 16 63% 5-10 9 67% 11-15 16 94% >15 12 83%","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-20-summary-week-21-predictions","loc":"https://efavdb.com/nba-week-20-summary-week-21-predictions"},{"title":"NBA week 19 summary, week 20 predictions","text":"Sadly lackluster week: 28 for 54, or 51.9% accuracy. The new predictions are now posted , which will hopefully bring us better luck. Details by point-spread for previous week below. Point spread # games Accuracy < 6 12 33% 5-10 11 27% 11-15 14 64% >15 17 71%","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-19-summary-week-20-predictions","loc":"https://efavdb.com/nba-week-19-summary-week-20-predictions"},{"title":"Forecasting Bike Sharing Demand","text":"In today's post, we document our efforts at applying a gradient boosted trees model to forecast bike sharing demand — a problem posed in a recent Kaggle competition. For those not familiar, Kaggle is a site where one can compete with other data scientists on various data challenges. Top scorers often win prize money, but the site more generally serves as a great place to grab interesting datasets to explore and play with. With the simple optimization steps discussed below, we managed to quickly move from the bottom 10% of the competition — our first-pass attempt's score — to the top 10%: no sweat! Our work here was inspired by a post by the people at Dato.com , who used the bike sharing competition as an opportunity to demonstrate their software. Here, we go through a similar, but more detailed discussion using the python package SKlearn . Introduction Bike sharing systems are gaining popularity around the world — there are over 500 different programs currently operating in various cities, and counting! These programs are generally funded through rider membership fees, or through pay-to-ride one time rental fees. Key to the convenience of these programs is the fact that riders who pick up a bicycle from one station can return the bicycle to any other in the network. These systems generate a great deal of data relating to various ride details, including travel time, departure location, arrival location, and so on. This data has the potential to be very useful for studying city mobility. The data we look at today comes from Washington D. C.'s Capital Bikeshare program. The goal of the Kaggle competition is to leverage the historical data provided in order to forecast future bike rental demand within the city. As we detailed in an earlier post , boosting provides a general method for increasing a machine learning algorithm's performance. Here, in order to model the Capital Bikeshare program's demand curves, we'll be applying a gradient boosted trees model ( GBM ). Simply put, GBM 's are constructed by iteratively fitting a series of simple trees to a training set, where each new tree attempts to fit the residuals, or errors, of the trees that came before it. With the addition of each new tree the training error is further reduced, typically asymptoting to a reasonably accurate model — but one must watch out for overfitting — see below! Loading package and data Below, we show the relevant commands needed to load all the packages and training/test data we will be using. We work with the package Pandas , whose DataFrame data structure enables quick and easy data loading and wrangling. We take advantage of this package immediately below, where in the last lines we use its parse_dates method to convert the first column of our provided data — which can be downloaded here — from string to datetime format. import numpy as np import matplotlib.pyplot as plt import pandas as pd import math from sklearn import ensemble from sklearn.cross_validation import train_test_split from sklearn.metrics import mean_absolute_error from sklearn.grid_search import GridSearchCV from datetime import datetime #Load Data with pandas, and parse the #first column into datetime train = pd . read_csv ( 'train.csv' , parse_dates = [ 0 ]) test = pd . read_csv ( 'test.csv' , parse_dates = [ 0 ]) The training data provided contains the following fields: datetime - hourly date + timestamp season - 1 = spring, 2 = summer, 3 = fall, 4 = winter holiday - whether the day is considered a holiday workingday - whether the day is neither a weekend nor holiday weather : Clear, Few clouds, Partly cloudy, Partly cloudy Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog temp - temperature in Celsius atemp - \"feels like\" temperature in Celsius humidity - relative humidity windspeed - wind speed casual - number of non-registered user rentals initiated registered - number of registered user rentals initiated count - number of total rentals The data provided spans two years. The training set contains the first 19 days of each month considered, while the test set data corresponds to the remaining days in each month. Looking ahead, we anticipate that the year, month, day of week, and hour will serve as important features for characterizing the bike demand at any given moment. These features are easily extracted from the datetime formatted-values loaded above. In the following lines, we add these features to our DataFrames. # Feature engineering temp = pd . DatetimeIndex ( train [ 'datetime' ]) train [ 'year' ] = temp . year train [ 'month' ] = temp . month train [ 'hour' ] = temp . hour train [ 'weekday' ] = temp . weekday temp = pd . DatetimeIndex ( test [ 'datetime' ]) test [ 'year' ] = temp . year test [ 'month' ] = temp . month test [ 'hour' ] = temp . hour test [ 'weekday' ] = temp . weekday # Define features vector features = [ 'season' , 'holiday' , 'workingday' , 'weather' , 'temp' , 'atemp' , 'humidity' , 'windspeed' , 'year' , 'month' , 'weekday' , 'hour' ] Evaluation metric The evaluation metric that Kaggle uses to rank competing algorithms is the Root Mean Squared Logarithmic Error ( RMSLE ). \\begin{eqnarray} J = \\sqrt{\\frac{1}{n} \\sum_{i=1}&#94;n [\\ln(p_i + 1) - \\ln(a_i+1)]&#94;2 } \\end{eqnarray} Here, \\(n\\) is the number of hours in the test set \\(p_i\\) is the predicted number of bikes rented in a given hour \\(a_i\\) is the actual rent count \\(ln(x)\\) is the natural logarithm With ranking determined as above, our aim becomes to accurately guess the natural logarithm of bike demand at different times (actually demand count plus one, in order to avoid infinities associated with times where demand is nil). To facilitate this, we add the logarithm of the casual, registered, and total counts to our training DataFrame below. #the evaluation metric is the RMSE in the log domain, #so we should transform the target columns into log domain as well. for col in [ 'casual' , 'registered' , 'count' ]: train [ 'log-' + col ] = train [ col ] . apply ( lambda x : np . log1p ( x )) Notice that in the code above we use the \\(log1p()\\) function instead of the more familiar \\(log(1+x)\\) . For large values of \\(x\\) , these two functions are actually equivalent. However, at very small values of \\(x\\) , the two can disagree. The source of the discrepancy is floating point error: For very small \\(x\\) , python will send \\(1+x \\to 1\\) , which when supplied as an argument to \\(log(1+x)\\) will return \\(log(1)=0\\) . The function \\(log1p(x) \\sim x\\) in this limit. The difference is not very important when the result is being added to other numbers, but can be very important in a multiplicative operation. We use this function instead for this reason. The inverse of \\(log(x+1)\\) is \\(e&#94;{x} -1\\) — an operation we will also need to make use of later, in order to return linear-scale demand values. We'll use an analog of the \\(log1p()\\) function, numpy's \\(expm1()\\) function, to carry out this inversion. Model development A first pass The Gradient Boosting Machine ( GBM ) we will be using has some associated hyperparameters that will eventually need to be optimized. These include: n_estimators = the number of boosting stages, or trees, to use. max_depth = maximum depth of the individual regression trees. learning_rate = shrinks the contribution of each tree by the learning rate. in_samples_leaf = the minimum number of samples required to be at a leaf node However, in order to get our feet wet, we'll begin by just picking some ad hoc values for these parameters. The code below fits a GBM to the log-demand training data, and then converts predicted log-demand into the competition's required format — in particular, the demand is output in linear scale. clf = ensemble . GradientBoostingRegressor ( n_estimators = 200 , max_depth = 3 ) clf . fit ( train [ features ] , train [ 'log-count' ] ) result = clf . predict ( test [ features ] ) result = np . expm1 ( result ) df = pd . DataFrame ( { 'datetime' : test [ 'datetime' ] , 'count' : result } ) df . to_csv ( 'results1.csv' , index = False , columns =[ 'datetime','count' ] ) In the last lines above, we have used the DataFrames to_csv() method in order to output results for competition submission. Example output is shown below. Without a hitch, we successfully submitted the results of this preliminary analysis to Kaggle. The only bad news was that our model scored in the bottom 10%. Fortunately, some simple optimizations that follow led to significant improvements in our standing. datetime count 2011-01-20 0:00:00 0 2011-01-20 0:01:00 0 2011-01-20 0:02:00 0 Hyperparameter tuning We now turn to the challenge of tuning our GBM 's hyperparameters. In order to carry this out, we segmented our training data into a training set and a validation set. The validation set allowed us to check the accuracy of our model locally, without having to submit to Kaggle. This also helped us to avoid overfitting issues. As mentioned earlier, the training data provided covers the first 19 days of each month. In segmenting this data, we opted to use days 17-19 for validation. We then used this validation set to optimize the model's hyperparameters. As a first-pass at this, we again chose an ad hoc value for n_estimators, but optimized over the remaining degrees of freedom. The code follows, where we make use of GridSearchCV() to perform our parameter sweep. #Split data into training and validation sets temp = pd . DatetimeIndex ( train [ 'datetime' ]) training = train [ temp . day <= 16 ] validation = train [ temp . day > 16 ] param_grid = { 'learning_rate' : [ 0.1 , 0.05 , 0.01 ], 'max_depth' : [ 10 , 15 , 20 ], 'min_samples_leaf' : [ 3 , 5 , 10 , 20 ], } est = ensemble . GradientBoostingRegressor ( n_estimators = 500 ) # this may take awhile gs_cv = GridSearchCV ( est , param_grid , n_jobs = 4 ) . fit ( training [ features ], training [ 'log-count' ]) # best hyperparameter setting gs_cv . best_params_ #Baseline error error_count = mean_absolute_error ( validation [ 'log-count' ], gs_cv . predict ( validation [ features ])) result = gs_cv . predict ( test [ features ]) result = np . expm1 ( result ) df = pd . DataFrame ({ 'datetime' : test [ 'datetime' ], 'count' : result }) df . to_csv ( 'results2.csv' , index = False , columns = [ 'datetime' , 'count' ]) Note: If you want to run n_jobs > 1 on a Windows machine, the script needs to be in an \"if name == ‘ main ‘:\" block. Otherwise the script will fail. day Best Parms 1 learning_rate 2 max_depth 2 min_samples_leaf The optimized parameters are shown above. Submitting the resulting model to Kaggle, we found that we had moved from the bottom 10% of models to the top 20%! An awesome improvement, but we still have one final hyperparameter to optimize. Tuning the number of estimators In boosted models, training set performance will always improve as the number of estimators is increased. However, at large estimator number, overfitting can start to become an issue. Learning curves provide a method for optimization. These are constructed by plotting the error on both the training and validation sets as a function of the number of estimators used. The code below generates such a curve for our model. error_train = [] error_validation = [] for k in range ( 10 , 501 , 10 ): clf = ensemble . GradientBoostingRegressor ( n_estimators = k , learning_rate = . 05 , max_depth = 10 , min_samples_leaf = 20 ) clf . fit ( training [ features ], training [ 'log-count' ]) result = clf . predict ( training [ features ]) error_train . append ( mean_absolute_error ( result , training [ 'log-count' ])) result = clf . predict ( validation [ features ]) error_validation . append ( mean_absolute_error ( result , validation [ 'log-count' ])) #Plot the data x = range ( 10 , 501 , 10 ) plt . style . use ( 'ggplot' ) plt . plot ( x , error_train , 'k' ) plt . plot ( x , error_validation , 'b' ) plt . xlabel ( 'Number of Estimators' , fontsize = 18 ) plt . ylabel ( 'Error' , fontsize = 18 ) plt . legend ([ 'Train' , 'Validation' ], fontsize = 18 ) plt . title ( 'Error vs. Number of Estimators' , fontsize = 20 ) Notice in the plot that by the time the number estimators in our GBM reaches about 80, the error of our model as applied to the validation set starts to slowly increase, though the error on the training set continues to decrease steadily. The diagnosis is that the model begins to overfit at this point. Moving forward, we will set n_estimators to 80, rather than 500, the value we were using above. Reducing the number of estimators reduced the calculated error and moved us to a higher position on the leaderboard. Separate models for registered and casual users Reviewing the data, we see that we have info regarding two types of riders: casual and registered riders. It is plausible that each group's behavior differs, and that we might be able to improve our performance by modeling each separately. Below, we carry this out, and then also merge the two group's predicted values to obtain a net predicted demand. We also repeat the hyperparameter sweep steps covered above — this returned similar values. Resubmitting the resulting model, we found we had increased our standing in the competition by a few percent. def merge_predict ( model1 , model2 , test_data ): # Combine the predictions of two separately trained models. # The input models are in the log domain and returns the predictions # in original domain. p1 = np . expm1 ( model1 . predict ( test_data )) p2 = np . expm1 ( model2 . predict ( test_data )) p_total = ( p1 + p2 ) return ( p_total ) est_casual = ensemble . GradientBoostingRegressor ( n_estimators = 80 , learning_rate = . 05 ) est_registered = ensemble . GradientBoostingRegressor ( n_estimators = 80 , learning_rate = . 05 ) param_grid2 = { 'max_depth' : [ 10 , 15 , 20 ], '_samples_leaf' : [ 3 , 5 , 10 , 20 ], } gs_casual = GridSearchCV ( est_casual , param_grid2 , n_jobs = 4 ) . fit ( training [ features ], training [ 'log-casual' ]) gs_registered = GridSearchCV ( est_registered , param_grid2 , n_jobs = 4 ) . fit ( training [ features ], training [ 'log-registered' ]) result3 = merge_predict ( gs_casual , gs_registered , test [ features ]) df = pd . DataFrame ({ 'datetime' : test [ 'datetime' ], 'count' : result3 }) df . to_csv ( 'results3.csv' , index = False , columns = [ 'datetime' , 'count' ]) The last step is to submit a final set of model predictions, this time training on the full labeled dataset provided. With these simple steps, we ended up in the top 11% on the competition's leaderboard with a rank of 280/2467! < pre > est_casual = ensemble . GradientBoostingRegressor ( n_estimators = 80 , learning_rate = . 05 , max_depth = 10 , min_samples_leaf = 20 ) est_registered = ensemble . GradientBoostingRegressor ( n_estimators = 80 , learning_rate = . 05 , max_depth = 10 , min_samples_leaf = 20 ) est_casual . fit ( train [ features ] . values , train [ 'log-casual' ] . values ) est_registered . fit ( train [ features ] . values , train [ 'log-registered' ] . values ) result4 = merge_predict ( est_casual , est_registered , test [ features ]) df = pd . DataFrame ({ 'datetime' : test [ 'datetime' ], 'count' : result4 }) df . to_csv ( 'results4.csv' , index = False , columns = [ 'datetime' , 'count' ]) DISCUSSION By iteratively tuning a GBM , we were able to quickly climb the leaderboard for this particular Kaggle competition. With further feature extraction work, we believe further improvements could readily be made. However, our goal here was only to practice our rapid development skills, so we won't be spending much time on further fine-tuning. At any rate, our results have convinced us that simple boosted models can often provide excellent results. Open GitHub Repo Note: With this post, we have begun to post our python scripts and data at GitHub. Clicking on the icon at left will take you to our repository. Feel free to stop by and take a look! if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Case studies","url":"https://efavdb.com/bike-share-forecasting","loc":"https://efavdb.com/bike-share-forecasting"},{"title":"NBA week 18 summary, week 19 predictions","text":"Excellent week: 39 for 53, or 73.6% accuracy! New predictions are up . This week, the algorithm continues to surprise, with lots of upset predictions. This includes a predicted OKC defeat of Atlanta tonight. Checking the dashboard for potential insight, we think this might be reasonable, given OKC 's excellent home record coupled with Atlanta's relatively poor away record. You heard it here first folks. UPDATE : OKC BEATS ATLANTA !!! Point-spread details of the past week are given below. Point spread # games Accuracy < 6 15 67% 5-10 14 71% 11-15 11 73% >15 13 85%","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-18-summary-week-19-predictions","loc":"https://efavdb.com/nba-week-18-summary-week-19-predictions"},{"title":"Machine Learning Methods: Decision trees and forests","text":"This post contains our crib notes on the basics of decision trees and forests. We first discuss the construction of individual trees, and then introduce random and boosted forests. We also discuss efficient implementations of greedy tree construction algorithms, showing that a single tree can be constructed in \\(O(k \\times n \\log n)\\) time, given \\(n\\) training examples having \\(k\\) features each. We provide exercises on interesting related points and an appendix containing relevant python/sk-learn function calls. Introduction Decision trees constitute a class of simple functions that are frequently used for carrying out regression and classification. They are constructed by hierarchically splitting a feature space into disjoint regions, where each split divides into two one of the already existing regions. In most common implementations, the splits are always taken along one of the feature axes, which causes the regions to be rectangular in shape. An example is shown in Fig. 1 below. In this example, a two-dimensional feature space is first split by a tree on \\(f_1\\) — one of the two features characterizing the space — at value \\(s_a\\) . This separates the space into two sets, that where \\(f_1 < s_a\\) and that where \\(f_1 \\geq s_a\\) . Next, the tree further splits the first of these sets on feature \\(f_2\\) at value \\(s_b\\) . With these combined splits, the tree partitions the space into three disjoint regions, labeled \\(R_1, R_2,\\) and \\(R_3\\) , where, e.g., \\(R_1 = \\{ \\textbf{f} \\vert f_1 < s_a, f_2 < s_b \\}\\) . Once a decision tree is constructed, it can be used for making predictions on unlabeled feature vectors — i.e., points in feature space not included in our training set. This is done by first deciding which of the regions a new feature vector belongs to, and then returning as its hypothesis label an average over the training example labels within that region: The mean of the region's training labels is returned for regression problems and the mode for classification problems. For instance, the tree in Fig. 1 would return an average of the five training examples in \\(R_1\\) (represented by red dots) when asked to make a hypothesis for any and all other points in that region. The art and science of tree construction is in deciding how many splits should be taken and where those splits should take place. The goal is to find a tree that provides a reasonable, piece-wise constant approximation to the underlying distribution or function that has generated the training data provided. This can be attempted through choosing a tree that breaks space up into regions such that the examples in any given region have identical — or at least similar — labels. We discuss some common approaches to finding such trees in the next section. Individual trees have the important benefit of being easy to interpret and visualize, but they are often not as accurate as other common machine learning algorithms. However, individual trees can be used as simple building blocks with which to construct more complex, competitive models. In the third section of this note, we discuss three very popular constructions of this sort: bagging, random forests (a variant on bagging), and boosting. We then discuss the runtime complexity of tree/forest construction and conclude with a summary, exercises, and an appendix containing example python code. Constructing individual decision trees Regression Regression tree construction typically proceeds by attempting to minimize a squared error cost function: Given a training set \\(T \\equiv \\{t_j = (\\textbf{f}_j, y_j) \\}\\) of feature vectors and corresponding real-valued labels, this is given by \\begin{eqnarray}\\label{treecost} \\tag{1} J = \\sum_{R_i} \\sum_{t_j \\in R_i } \\left ( \\overline{y}_{R_i} - y_j \\right)&#94;2, \\end{eqnarray} where \\(\\overline{y}_{R_i}\\) is the mean training label in region \\(R_i\\) . This mean training label is the hypothesis returned by the tree for all points in \\(R_i\\) , including its training examples. Therefore, (\\ref{treecost}) is a measure of the accuracy of the tree as applied to the training set. Unfortunately, actually minimizing (\\ref{treecost}) over any large subset of trees can be a numerically challenging task. This is true whenever you have a large number of features or training examples. Consequently, different approximate methods are generally taken to find good candidate trees. Two typical methods follow: Greedy algorithm : The tree is constructed recursively, one branching step at a time. At each step, one takes the split that will most significantly reduce the cost function \\(J\\) , relative to its current value. In this way, after \\(k-1\\) splits, a tree with \\(k\\) regions (leaves) is obtained — Fig. 2 provides an illustration of this process. The algorithm terminates whenever some specified stopping criterion is satisfied, examples of which are given below. Randomized algorithm : Randomized tree-search protocols can sometimes find global minima inaccessible to the gradient-descent-like greedy algorithm. These randomized protocols also proceed recursively. However, at each step, some randomization is introduced by hand. For example, one common approach is to select \\(r\\) candidate splits through random sampling at each branching point. The candidate split that most significantly reduces \\(J\\) is selected, and the process repeats. The benefit of this approach is that it can sometimes find paths that appear suboptimal in their first few steps, but are ultimately favorable. Classification In classification problems, the training labels take on a discrete set of values, often having no numerical significance. This means that a squared-error cost function, like that in (\\ref{treecost}) — cannot be directly applied as a useful accuracy score for guiding classification tree construction. Instead, three other cost functions are often considered, each providing a different measure of the class purity of the different regions — that is, they attempt to measure whether or not a given region consists of training examples that are mostly of the same class. These three measures are the error rate ( \\(E\\) ), the Gini index ( \\(G\\) ), and the cross-entropy ( \\(CE\\) ): If we write \\(N_i\\) for the number of training examples in region \\(R_i\\) , and \\(p_{i,j}\\) for the fraction of these that have class label \\(j\\) , then these three cost functions are given by \\begin{eqnarray}\\label{errorrate} \\tag{2} E &=& \\sum_{R_i} N_i \\times \\left ( 1 - \\max_{j} p_{i,j}\\right) \\\\ \\label{gini} \\tag{3} G &=& \\sum_{R_i, j}N_i \\times p_{i,j}\\left ( 1 - p_{i,j} \\right) \\\\ \\label{crossentropy} \\tag{4} CE &=& - \\sum_{R_i, j} N_i \\times p_{i,j} \\log p_{i,j}. \\end{eqnarray} Each of the summands here are plotted in Fig. 3 for the special case of binary classification (two labels only). Each is unfavorably maximized at the most mixed state, where \\(p_1 = 0.5\\) , and minimized in the pure states, where \\(p_1 = 0,1\\) . Although \\(E\\) is perhaps the most intuitive of the three measures above (it's simply the number of training examples misclassified by the tree — this follows from the fact that the tree returns as hypothesis the mode in each region) the latter two have the benefit of being characterized by negative curvature as a function of the \\(p_{i,j}\\) . This property tends to enhance the favorability of splits that generate region pairs where at least one is highly pure. At times, this can simultaneously result in the other region of the pair ending up relatively impure — see Exercise 1 for details. Such moves are often ultimately beneficial, since any highly impure node that results can always be broken up in later splits anyways. The plot in Fig. 3 shows that the cross-entropy has the larger curvature of the two, and so should more highly favor such splits, at least in the binary classification case. Another nice feature of the Gini and cross-entropy functions is that — in contrast to the error rate — they are both smooth functions of the \\(p_{i,j}\\) , which facilitates numerical optimization. For these reasons, one of these two functions is typically used to guide tree construction, even if \\(E\\) is the quantity one would actually like to minimize. Tree construction proceeds as in the regression case, typically by a greedy or randomized construction, each step taken so as to minimize (\\ref{gini}) or (\\ref{crossentropy}), whichever is chosen. Bias-variance trade-off and stopping conditions Decision trees that are allowed to split indefinitely will have low bias but will over-fit their training data. Placing different stopping criteria on a tree's growth can ameliorate this latter effect. Two typical conditions often used for this purpose are given by a) placing an upper bound on the number of levels permitted in the tree, or b) requiring that each region (tree leaf) retains at least some minimum number of training examples. To optimize over such constraints, one can apply cross-validation. Bagging, random forests, and boosting Another approach to alleviating the high-variance, over-fitting issue associated with decision trees is to average over many of them. This approach is motivated by the observation that the sum of \\(N\\) independent random variables — each with variance \\(\\sigma&#94;2\\) — has a relatively reduced variance, \\(\\sigma&#94;2/N\\) . Two common methods for carrying out summations of this sort are discussed below. Bagging and random forests Bootstrap aggregation , or \"bagging\", provides one common method for constructing ensemble tree models. In this approach, one samples with replacement to obtain \\(k\\) separate bootstrapped training sets from the original training data. To obtain a bootstrapped subsample of a data set of size \\(N\\) , one draws randomly from the set \\(N\\) times with replacement. Because one samples with replacement, each bootstrapped set can contain multiple copies of some examples. The average number of unique examples in a given bootstrap is simply \\(N\\) times the probability that any individual example makes it into the training set. This is \\begin{eqnarray} \\tag{5} N \\left [ 1 - \\left(\\frac{N-1}{N} \\right)&#94;N \\right ] \\approx N (1 - e&#94;{-1}) \\approx 0.63N, \\end{eqnarray} where the latter forms are accurate in the large \\(N\\) limit. Once the bootstrapped data sets are constructed, an individual decision tree is fit to each, and an average or majority rule vote over the full set is used to provide the final prediction. One nice thing about bagging methods, in general, is that one can train on the entire set of available labeled training data and still obtain an estimate of the generalization error. Such estimates are obtained by considering the error on each point in the training set, in each case averaging only over those trees that did not train on the point in question. The resulting estimate, called the out-of-bag error, typically provides a slight overestimate to the generalization error. This is because accuracy generally improves with growing ensemble size, and the full ensemble is usually about three times larger than the sub-ensemble used to vote on any particular training example in the out-of-bag error analysis. Random forests provide a popular variation on the bagging method. The individual decision trees making up a random forest are, again, each fit to an independent, bootstrapped subsample of the training data. However, at each step in their recursive construction process, these trees are restricted in that they are only allowed to split on \\(r\\) randomly selected candidate feature directions; a new set of \\(r\\) directions is chosen at random for each step in the tree construction. These restrictions serve to effect a greater degree of independence in the set of trees averaged over in a random forest, which in turn serves to reduce the ensemble's variance — see Exercise 5 for related analysis. In general, the value of \\(r\\) should be optimized through cross-validation. Boosting The final method we'll discuss is boosting , which again consists of a set of individual trees that collectively determine the ultimate prediction returned by the model. However, in the boosting scenario, one fits each of the trees to the full data set, rather than to a small sample. Because they are fit to the full data set, these trees are usually restricted to being only two or three levels deep, so as to avoid over-fitting. Further, the individual trees in a boosted forest are constructed sequentially. For instance, in regression, the process typically works as follows: In the first step, a tree is fit to the full, original training set \\(T = \\{t_i = (\\textbf{f}_i, y_i)\\}\\) . Next, a second tree is constructed on the same training feature vectors, but with the original labels replaced by residuals. These residuals are obtained by subtracting out a scaled version of the predictions \\(\\hat{y}&#94;1\\) returned by the first tree, \\begin{eqnarray} \\tag{6} y_i&#94;{(1)} \\equiv y_i - \\alpha \\hat{y}_i&#94;1. \\end{eqnarray} Here, \\(\\alpha\\) is the scaling factor, or learning rate — choosing its value small results in a gradual learning process, which often leads to very good predictions. Once the second tree is constructed, a third tree is fit to the new residuals, obtained by subtracting out the scaled hypothesis of the second tree, \\(y_i&#94;{(2)} \\equiv y_i&#94;{(1)} - \\alpha \\hat{y}_i&#94;2\\) . The process repeats until \\(m\\) trees are constructed, with their \\(\\alpha\\) -scaled hypotheses summing to a good estimate to the underlying function. Boosted classification tree ensembles are constructed in a fashion similar to that above. However, in contrast to the regression scenario, the same, original training labels are used to fit each new tree in the ensemble (as opposed to an evolving residual). To bring about a similar, gradual learning process, boosted classification ensembles instead sample from the training set with weights that are sample-dependent and that change over time: When constructing a new tree for the ensemble, one more heavily weights those examples that have been poorly fit in prior iterations. AdaBoost is a popular algorithm for carrying out boosted classification. This and other generalizations are covered in the text Elements of Statistical Learning . Implementation runtime complexity Before concluding, we take here a moment to consider the runtime complexity of tree construction. This exercise gives one a sense of how tree algorithms are constructed in practice. We begin by considering the greedy construction of a single classification tree. The extension to regression trees is straightforward. Individual decision trees Consider the problem of greedily training a single classification tree on a set of \\(n\\) training examples having \\(k\\) features. In order to construct our tree, we take as a first step the sorting of the \\(n\\) training vectors along each of the \\(k\\) directions, which will facilitate later optimal split searches. Recall that optimized algorithms, e.g. merge-sort , require \\(O(n \\log n)\\) time to sort along any one feature direction, so sorting along all \\(k\\) will require \\(O(k \\times n \\log n)\\) time. After this pre-sort step is complete, we must seek the currently optimal split, carry it out, and then iterate. We will show that — with care — the full iterative process can also be carried out in \\(O(k \\times n \\log n)\\) time. Focus on an intermediate moment in the construction process where one particular node has just been split, resulting in two new regions, \\(R_1\\) and \\(R_2\\) containing \\(n_{R_1}\\) and \\(n_{R_2}\\) training examples, respectively. We can assume that we have already calculated and stored the optimal split for every other region in the tree during prior iterations. Therefore, to determine which region contains the next optimal split, the only new searches we need to carry out are within regions \\(R_1\\) and \\(R_2\\) . Focus on \\(R_1\\) and suppose that we have been passed down the following information characterizing it: the number of training examples of each class that it contains, its total number of training examples \\(n_{R_1}\\) , its cost function value \\(J\\) (cross entropy, say), and for each of the \\(k\\) feature directions, a separate list of the region's examples, sorted along that direction. To find the optimal split, we must consider all \\(k \\times (n_{R_1}-1)\\) possible cuts of this region [ Aside : We must check all possible cuts because the cost function can have many local minima. The precludes the use of gradient-descent-like algorithms to find the optimal split.], evaluating the cost function reduction for each. The left side of Fig. 4 illustrates one method for efficiently carrying out these test cuts: For each feature direction, we proceed sequentially through that direction's ordered list, considering one cut at a time. In the first cut, we take only one example in the left sub-region induced, and all others on the right. In the second cut, we have the first two examples in the left sub-region, etc. Proceeding in this way, it turns out that the cost function of each new candidate split considered can always be evaluated in \\(O(1)\\) time. This is because we start with knowledge of the cost function \\(J\\) before any cut is taken, and the cost functions we consider here can each be updated in \\(O(1)\\) time whenever only a single example is either added to or removed from a given region — see exercises 3 and 4 for details. Using this approach, we can therefore try all possible cuts of region \\(R_1\\) in \\(O(k \\times n_{R_1})\\) time. The above analysis gives the time needed to search for the optimal split within \\(R_1\\) , and a similar form holds for \\(R_2\\) . Once these are determined, we can quickly select the current, globally-optimal split [ Aside : Using a heap data structure, the global minimum can be obtained in at most \\(O(\\log n)\\) time. Summing this effort over all nodes of the tree will lead to roughly \\(O(n \\log n)\\) evaluations.]. Carrying out this split entails partitioning the region selected into two and passing the necessary information down to each. We leave as an exercise the fact that the passing of needed information — ordered lists, etc. — can be carried out in \\(O(k \\times n_s)\\) time, with \\(n_s\\) the size of the parent region being split. The total tree construction time can now be obtained by summing up each node's search and split work, which both require \\(O(k \\times n_s\\) ) computations. Assuming a roughly balanced tree having about \\(\\log n\\) layers — see right side of Fig. 4 — we obtain \\(O(k \\times n \\log n)\\) , the runtime scaling advertised. In summary, we see that achieving \\(O(k \\times n \\log n)\\) scaling requires a) a pre-sort, b) a data structure for storing certain important facts about each region, including its optimal split, once determined, and also pointers to its parent and children, c) an efficient method for passing relevant information down to daughter regions during a split instance, d) a heap to enable quick selection of the currently optimal split, and e) a cost function that can be updated efficiently under single training example insertions or removals. Forests, parallelization If a forest of \\(N\\) trees is to be constructed, each will require \\(O(k \\times n \\log n)\\) time to construct. Recall, however, that the trees of a bagged forest can be constructed independently of one another. This allows for bagged forest constructions to take advantage of parallelization, facilitating their application in the large \\(N\\) limit. In contrast, the trees of a boosted forest are constructed in sequence and so cannot be parallelized in a similar manner. However, note that optimal split searches along different feature directions can always be run in parallel. This can speed up individual tree construction times in either case. Discussion In this note, we've quickly reviewed the basics of tree-based models and their constructions. Looking back over what we have learned, we can now consider some of the reasons why tree-based methods are so popular among practitioners. First — and very importantly — individual trees are often useful for gaining insight into the geometry of datasets in high dimensions. This is because tree structures can be visualized using simple diagrams, like that in Fig. 1. In contrast, most other machine learning algorithm outputs cannot be easily visualized — consider, e.g., support-vector machines, which return hyper-plane decision boundaries. A related point is that tree-based approaches are able to automatically fit non-linear decision boundaries. In contrast, linear algorithms can only fit such boundaries if appropriate non-linear feature combinations are constructed. This requires that one first identify these appropriate feature combinations, which can be a challenging task for feature spaces that cannot be directly visualized. Three additional positive qualities of decision trees are given by a) the fact that they are insensitive to feature scale, which reduces the need for related data preprocessing, b) the fact that they can make use of data missing certain feature values, and c) that they are relatively robust against outliers and noisy-labeling issues. Although boosted and random forests are not as easily visualized as individual decision trees, these ensemble methods are popular because they are often quite competitive. Boosted forests typically have a slightly lower generalization error than their random forest counterparts. For this reason, they are often used when accuracy is highly-valued — see last figure for an example learning curve consistent with this rule of thumb: Generalization error rate versus training set size for a hand-written digits learning problem. However, the individual trees in a bagged forest can be constructed in parallel. This benefit — not shared by boosted forests — can favor random forests as a go-to, out-of-box approach for treating large-scale machine learning problems. Exercises follow that detail some further points of interest relating to decision trees and their construction. References [1] Elements of Statistical Learning , by Hastie, Tibshirani, Friedman [2] An Introduction to Statistical Learning , by James, Witten, Hastie, and Tibshirani [3] Random Forests , by Breiman (Machine Learning, 45, 2001). [4] Sk-learn documentation on runtime complexity, see section 1.8.4. Exercises 1) Jensen's inequality and classification tree cost functions ​a) Consider a real function \\(y(x)\\) with non-positive curvature. Consider sampling \\(y\\) at values \\(\\{x_1, x_2, \\ldots, x_m\\}\\) . By considering graphically the centroid of the points \\(\\{(x_i, y(x_i))\\}\\) , prove Jensen's inequality, \\begin{eqnarray} \\tag{7} y\\left ( \\frac{1}{m} \\sum_i x_i \\right) \\geq \\frac{1}{m}\\sum_i y(x_i). \\end{eqnarray} When does equality hold? ​b) Consider binary tree classification guided by the minimization of the error rate (\\ref{errorrate}). If all possible cuts of a particular region always leave class \\(0\\) in the minority in both resulting sub-regions, will a cut here ever be made? ​c) How about if (\\ref{gini}) or (\\ref{crossentropy}) is used as the cost function? 2) Decision tree prediction runtime complexity Suppose one has constructed an approximately balanced decision tree, where each node contains one of the \\(n\\) training examples used for its construction. In general, approximately how long will it take to determine the region \\(R_i\\) to which a supplied feature vector belongs? How about for ensemble models? Any difference between typical bagged and boosted forests? 3) Classification tree construction runtime complexity ​a) Consider a region \\(R\\) within a classification tree containing \\(n_i\\) training examples of class \\(i\\) , with \\(\\sum_i n_i = N\\) . Now, suppose a cut is considered in which a single training example of class \\(1\\) is removed from the region. If the region's cross-entropy before the cut is given by \\(CE_0\\) , show that its entropy after the cut will be given by \\begin{eqnarray}\\label{DEntropy} \\tag{8} CE_f = CE_0 - N \\log\\left (\\frac{N}{N-1} \\right) + \\log \\left (\\frac{n_1}{N-1} \\right) - (n_1 -1) \\log \\left (\\frac{n_1 - 1}{n_1} \\right). \\end{eqnarray} If \\(CE_0\\) , \\(N\\) , and the \\(\\{n_i\\}\\) values are each stored in memory for a given region, this equation can be used to evaluate in \\(O(1)\\) time the change in its entropy with any single example removal. Similarly, the change in entropy of a region upon addition of a single training example can also be evaluated in \\(O(1)\\) time. Taking advantage of this is essential for obtaining an efficient tree construction algorithm. ​b) Show that a region's Gini coefficient (\\ref{gini}) can also be updated in \\(O(1)\\) time with any single training example removal. 4)Regression tree construction runtime complexity. Consider a region \\(R\\) within a regression tree containing \\(N\\) training examples, characterized by mean label value \\(\\overline{y}\\) and cost value (\\ref{treecost}) given by \\(J\\) ( \\( N\\) times the region's label variance). Suppose a cut is considered in which a single training example having label \\(y\\) is removed from the region. Show that after the cut is taken the new mean training label and cost function values within the region are given by \\begin{eqnarray} \\tag{9} \\overline{y}_f &=& \\frac{1}{N-1} \\left ( N \\overline{y} - y \\right) \\ \\label{regression_cost_change} J_f &=& J - \\frac{N}{N-1} \\left ( \\overline{y} - y\\right)&#94;2. \\end{eqnarray} These results allow for the cost function of a region to be updated in \\(O(1)\\) time as single examples are either inserted or removed from it. Their simplicity is a special virtue of the squared error cost function. Other cost function choices will generally require significant increases in tree construction runtime complexity, as most require a fresh evaluation with each new subset of examples considered. 5) Chebychev's inequality and random forest classifier accuracy Adapted from [3]. ​a) Let \\(x\\) be a random variable with well-defined mean \\(\\mu\\) and variance \\(\\sigma&#94;2\\) . Prove Chebychev's inequality, \\begin{eqnarray}\\label{Cheby} \\tag{10} P(x \\geq \\mu + t) \\leq \\frac{\\sigma&#94;2}{t&#94;2}. \\end{eqnarray} ​b) Consider a binary classification problem aimed at fitting a sampled function \\(y(\\textbf{f})\\) that takes values in \\(\\{ 0,1\\}\\) . Suppose a decision tree \\(h_{\\theta}(\\textbf{f})\\) is constructed on the samples using a greedy, randomized approach, where the randomization is characterized by the parameter \\(\\theta\\) . Define the classifier's margin \\(m\\) at \\(\\textbf{f}\\) by \\begin{eqnarray}\\label{tree_margin_def} \\tag{11} m(\\theta, \\textbf{f}) =-1 + 2 \\left [ y * h_{\\theta}+ (1- y) * (1 - h_{\\theta}) \\right ] \\end{eqnarray} This is equal to \\(1\\) if \\(h_{\\theta}\\) and \\(y\\) agree at \\(\\textbf{f}\\) , and \\(-1\\) otherwise. Now, consider a random forest, consisting of many such trees, each obtained by sampling from the same \\(\\theta\\) distribution. Argue using (\\ref{Cheby}), (\\ref{tree_margin_def}), and the law of large numbers that the generalization error \\(GE\\) of the forest is bounded by \\begin{eqnarray}\\label{rf_bound} \\tag{12} GE \\leq \\frac{var_{\\textbf{f}}\\left( \\langle m(\\theta, \\textbf{f}) \\rangle_{\\theta} \\right)}{\\langle m(\\theta, \\textbf{f}) \\rangle_{\\theta, \\textbf{f}}&#94;2 } \\end{eqnarray} ​c) Show that \\begin{eqnarray}\\label{margin_var} \\tag{13} var_{\\textbf{f}}\\left( \\langle m(\\theta, \\textbf{f}) \\rangle_{\\theta} \\right) = \\langle cov_{\\textbf{f}}(m(\\theta, \\textbf{f}), m(\\theta&#94;{\\prime},\\textbf{f})) \\rangle_{\\theta, \\theta&#94;{\\prime}} \\end{eqnarray} ​d) Writing, \\begin{eqnarray} \\tag{14} \\rho \\equiv \\frac{\\langle cov_{\\textbf{f}}(m(\\theta, \\textbf{f}), m(\\theta&#94;{\\prime},\\textbf{f})) \\rangle_{\\theta, \\theta&#94;{\\prime}}} {\\langle \\sqrt{var_{\\textbf{f}}(m(\\theta, \\textbf{f}))} \\rangle_{\\theta}&#94;2}, \\end{eqnarray} for the \\(\\theta\\) , \\(\\theta&#94;{\\prime}\\) -averaged margin-margin correlation coefficient, show that \\begin{eqnarray} var_{\\textbf{f}}\\left( \\langle m(\\theta, \\textbf{f}) \\rangle_{\\theta} \\right) \\leq \\rho \\langle var_{\\textbf{f}}(m(\\theta, \\textbf{f})) \\rangle_{\\theta} \\leq \\rho \\left ( 1 - \\langle m(\\theta, \\textbf{f}) \\rangle_{\\theta, \\textbf{f}}&#94;2\\right). \\end{eqnarray} Combining with (\\ref{rf_bound}), this gives \\begin{eqnarray}\\label{tree_bound_final} \\tag{15} GE \\leq \\rho \\times \\frac{ 1 - \\langle m(\\theta, \\textbf{f}) \\rangle_{\\theta, \\textbf{f}}&#94;2 }{ \\langle m(\\theta, \\textbf{f}) \\rangle_{\\theta, \\textbf{f}}&#94;2 }. \\end{eqnarray} The bound (\\ref{tree_bound_final}) implies that a random forest's generalization error is reduced if the individual trees making up the forest have a large average margin, and also if the trees are relatively-uncorrelated with each other. Cover image by roberts87 , creative commons license . Appendix: python/sk-learn implementations Here, we provide the python/sk-learn code used to construct the final figure in the body of this note: Learning curves on sk-learn's \"digits\" dataset for a single tree, a random forest, and a boosted forest. from sklearn.datasets import load_digits from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import GradientBoostingClassifier import numpy as np # load data: digits.data and digits.target, # array of features and labels, resp. digits = load_digits ( n_class = 10 ) n_train = [] t1_accuracy = [] t2_accuracy = [] t3_accuracy = [] # below, we average over \"trials\" num of fits for each sample # size in order to estimate the average generalization error. trials = 25 clf = DecisionTreeClassifier () clf2 = GradientBoostingClassifier ( max_depth = 3 ) clf3 = RandomForestClassifier () num_test = 500 # loop over different training set sizes for num_train in range ( 2 , len ( digits . target ) - num_test , 25 ): acc1 , acc2 , acc3 = 0 , 0 , 0 for j in range ( trials ): perm = [ 0 ] while len ( set ( digits . target [ perm [: num_train ]])) < 2 : perm = np . random . permutation ( len ( digits . data )) clf = clf . fit ( digits . data [ perm [: num_train ]], digits . target [ perm [: num_train ]]) acc1 += clf . score ( digits . data [ perm [ - num_test :]], digits . target [ perm [ - num_test :]]) clf2 = clf2 . fit ( digits . data [ perm [: num_train ]], digits . target [ perm [: num_train ]]) acc2 += clf2 . score ( digits . data [ perm [ - num_test :]], digits . target [ perm [ - num_test :]]) clf3 = clf3 . fit ( digits . data [ perm [: num_train ]], digits . target [ perm [: num_train ]]) acc3 += clf3 . score ( digits . data [ perm [ - num_test :]], digits . target [ perm [ - num_test :]]) n_train . append ( num_train ) t1_accuracy . append ( acc1 / trials ) t2_accuracy . append ( acc2 / trials ) t3_accuracy . append ( acc3 / trials ) % pylab inline plt . plot ( n_train , t1_accuracy , color = 'red' ) plt . plot ( n_train , t2_accuracy , color = 'green' ) plt . plot ( n_train , t3_accuracy , color = 'blue' ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods, Theory","url":"https://efavdb.com/notes-on-trees","loc":"https://efavdb.com/notes-on-trees"},{"title":"NBA week 17 summary, week 18 predictions","text":"We went 35 for 56 this past week, 62.5% accuracy. Just one fewer correct game than last week. New predictions are up , and point-spread details of the past week are given below. Point spread # games Accuracy < 6 9 56% 5-10 30 63% 11-15 8 50% >15 9 78%","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-17-summary-week-18-predictions","loc":"https://efavdb.com/nba-week-17-summary-week-18-predictions"},{"title":"NBA week 16 summary, week 17 predictions","text":"This week, we went 36 for 56, giving 64% accuracy — decent. New predictions are up — again, lots of predicted upsets this week! One such upset predicted is a Clippers victory over the home team Warriors on Sunday… a game that one of us will be going to. Although we root for the local team, it's always hard to feel bad when our baby guesses correctly, so we'll be happy either way. Past week details below. Point spread # games Accuracy < 6 20 75% 5-10 17 65% 11-15 7 57% >15 12 50%","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-16-summary-week-17-predictions","loc":"https://efavdb.com/nba-week-16-summary-week-17-predictions"},{"title":"NBA week 15 summary, week 16 predictions","text":"Not a terribly impressive week for the NBA algorithm. We went 31/53, equating to 58% accuracy. New predictions are up — lots of predicted upsets this week, including two tonight. As usual, I've found checking the dashboard to be a great way to gain insight into such picks. Details of past week by point spread below. Point spread # games Accuracy < 5 10 40% 5-9 13 62% 10-14 14 64% >14 16 63%","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-15-summary-week-16-predictions","loc":"https://efavdb.com/nba-week-15-summary-week-16-predictions"},{"title":"Measles vaccination rate by USA state and relation to mean outbreak size","text":"In this post, we provide a quick overview of the data and science of measles spread. Making use of python (code provided) we extract from a CDC data set the 2012 youth vaccination rate for each USA state — see figure below. To aid in the interpretation of this data, we also review and describe the results of a generalized \" SIR \" model for disease spread. The model — analyzed in our last post — predicts that measles outbreaks are supported only if the vaccination rate is below 94%. At higher rates, infection spread is suppressed, and outbreaks do not occur. As seen in the figure, the majority of the states sit just below this critical number, and so are predicted to support youth outbreaks. Measles vaccination rate by USA state for youths under 36 months in age, 2012. Dashed line at 94% is estimated critical vaccination rate needed to suppress outbreaks. The mean USA youth rate is about 91%. Introduction: History of measles in America Measles is a highly-contagious, serious illness estimated to currently be contracted by about 20 million individuals each year, globally. Just prior to the arrival of the first measles vaccine in 1963 (developed by John Enders and his colleagues) approximately 500,000 Americans contracted the disease annually, approximately 50,000 of which required hospitalization — a rate of 1/10. Among these individuals, approximately 500 would die each year, equating to a 1/1000 mortality rate. As yet, there is no treatment available to combat the measles virus, once contracted. Consequently, similar hospitalization and mortality rates continue to hold today . Sadly, the mortality rate among the malnourished can be as high as 1/10. Here, the vaccination rate shown is for youths, under 48 months in first brach (up to 1985), and under 36 months in the latter branch. Contemporary contraction rates in the USA are now extremely low, on the order of 50-500 per year . This is likely a consequence of the strong local adoption of measles vaccinations, with 91% of young children here now receiving the vaccine prior to their third birthday. The most recent large-scale USA outbreak of the disease happened between the years of 1989 and 1991, when the vaccination rates of children were significantly lower, hovering around 70%. This outbreak centered within poorer, urban areas where the vaccination rates were substantially lower than the national average. A summary plot of USA contraction counts and youth vaccination rates by year is shown above — the two curves are highly anti-correlated. Note that this plot is click and drag zoomable , which is useful for setting the scale appropriately for recent years. Data source 1 , source 2 ; no youth vaccination rate data available between 1985-1991. Full-population averages have been in the 90's for some decades , likely due to elementary school matriculation requirements. Modeling disease spread The striking USA historical data suggests a very strong relationship between the vaccination rate within a community and the frequency and size of the measles outbreaks that it supports. In fact, simple models for disease spread suggest a phase-transition-like (exhibiting abrupt changes) outbreak size dependence on vaccination rates. This is illustrated below, where we plot the predicted measles contraction rate against a population's vaccination rate — as returned by a generalized SIR model for disease spread : Notice that in the far left side of this plot, the model predicts that disease spread is completely suppressed. However, below a critical vaccination rate (the stated 94% mark — a number consistent with published estimates for measles ), outbreaks begin to be supported, growing in size with further decrease of the vaccination rate. The generalized SIR model predicts that a measles outbreak size changes in a phase-transition-like manner with vaccination rate. Further, at about 85% vaccination, the outbreak population fraction and the unvaccinated fraction curves cross. At this point, the outbreak captures nearly all unvaccinated and also a finite fraction of the vaccinated, who begin to become infected due to frequent encounters with disease. A detailed study of the SIR model's solution is not necessary to understand why disease spread exhibits a phase-transition-like form. Qualitatively, this behavior is a consequence of a simple balance of rates: If the rate at which a disease spreads is greater than the rate at which the ill recover, outbreaks grow and expand. However, if patients recover more quickly than they can spread the disease, outbreak expansion is suppressed, and the number of infected individuals will decrease with time. The balance of these competing effects is tuned by the frequency of vaccination, which directly affects the first of these rates — that at which the disease can spread. Because measles is highly contagious , its balance point occurs around the relatively-high 94% mark seen in the figure — this is the vaccination rate needed to have the average rate of disease spread just equal to the average rate of recovery. An info-graphic illustrating these points can be found here . Model implications for USA youth At the start of this post, we presented CDC estimates for the mean, by-state vaccination rates within the USA . Now that we have reviewed the results of the SIR model, we can begin to appreciate the significance of this data more deeply. First — as we noted above — we see that many states have youth vaccination rates that sit just below the critical 94% level, and thus on average should support small measles outbreaks. The graph of the previous section also allows us to estimate the mean size of a measles outbreak (once sparked) within any community below the critical vaccination rate: The further a state is from the 94% mark, the larger its mean outbreak size should be. Although all states are doing reasonably well, when considered on a global scale , it is important to realize that many sit in a region of the plot where the response to a decrease in vaccination rate is most dramatic — the curve's slope is largest just to the right of 94% vaccination. This means, e.g., that a 1% decrease in vaccination rate will result in a greater than 1% increase in the size of the average outbreak supported within that state. This observation is modestly worrisome, as the risk of contraction for all individuals — even those vaccinated — is always proportional to the number of cases present in any outbreak. Important caveats: Vaccination rates fluctuate between cities, neighborhoods, etc. This means that simple state averages may not accurately characterize your local community's vaccination rate — the quantity most relevant to your personal infection risk. See, for instance, the plot for California by county shown here . Our results are based on a descriptive, but simple model. They are intended only to provide one with a qualitative picture of the forces governing measles spread. Detailed, peer-reviewed treatments (this is just a blog…) can be found in the literature. Consult a licensed physician for qualified information on vaccines, measles etc. Discussion Interestingly, humans are the only known carriers of the measles virus. Consequently, with growing global vaccination rates , it may one day soon be possible to totally extinguish the disease. Looking back on the historical USA data helps one realize that this would be a truly remarkable accomplishment! In fact, were the USA in isolation, our current vaccination rates would likely be sufficient to bring this about. However, we are not, and sporadic outbreaks continue to occur here, ignited through international travel of infected individuals. These outbreaks can occur because our youth vaccination rates are below the critical 94% level needed to suppress them. The science of disease spread is very interesting. For those with a mathematical background, we suggest taking a look at our prior post , where we solve the SIR model analytically. Many additional insights into disease spread — not covered here — can be gleaned through the study of this model. Likewise, those with a programming background can play with the code that we provide below to sort through the CDC 's data sets in different ways. For instance, one can easily alter this code to view how vaccination rates vary by socio-economic background, rather than by state, etc. One can also use the same procedures to sort through many other interesting data sets provided by the CDC and others. Methods: Wrangling the CDC measles data sets Grabbing and loading data In this section, we outline our numerical analysis of the CDC measles vaccination rate data set. To follow along, you must first download the files. Current data can be found here , and data corresponding to years before 2009 here . Three files are needed. The Dataset file , the Codebook that explains how to read the dataset files, and the Data User's Guide which provides details about the data, including methodology and statistics descriptions. In our analysis, we will make use of a few python packages. In particular, we will use Pandas to construct high performance data structures, called DataFrames. These are easy to use, and they allow for fast, straightforward data manipulation — both helpful features for data wrangling. We will utilize the groupby DataFrame method, which enables one to easily segment data according to values along different feature directions. To illustrate, we will split our data by state name. We will then use the Plot.ly package to generate the interactive plots included above. To get a feel for the CDC data, a good first step is to look at the data in a text editor. Doing this, we quickly notice multiple rows of characters having no vernacular significance. The codebook allows one to interpret these characters. It also explains that each row corresponds to a different child surveyed, and that each row has a fixed number of entries, many corresponding to different vaccines. We will read these rows in one at a time — making use of a for loop — to save on memory. As each line is processed, we keep only what we need — here that info relevant to the MMR vaccine. import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.plotly as py from plotly.graph_objs import * py . sign_in ( 'username' , 'password' ) #Create some empty lists to store the data child_ID = [] house_ID = [] patient_data = [] measles = [] state = [] stratum = [] weights = [] #No virgin islands weights_v = [] mmr = [] #First we need to read in the data file, and parse the data #using the datasheet with open ( 'nispuf12.dat' , 'r' ) as f : for line in f : child_ID . append ( line [ 0 : 6 ]) house_ID . append ( line [ 6 : 11 ]) patient_data . append ( line [ 11 : 12 ]) measles . append ( line [ 260 : 261 ]) state . append ( line [ 181 : 183 ]) stratum . append ( line [ 88 : 92 ]) weights . append ( line [ 50 : 69 ]) weights_v . append ( line [ 31 : 50 ]) mmr . append ( line [ 261 : 262 ]) Cleaning data Now that everything is parsed and loaded into arrays, we will insert our data into a dictionary with key given by the column header. We then convert this into a DataFrame. To streamline the process, we will also replace all the missing values with a NAN , and also convert all relevant entries from string to number format. We will also replace the stateID numbers with their written names. Lastly, we will remove all the incomplete patient files. #Create a dataframe in pandas for data manipulation d = { 'child' : child_ID , 'house' : house_ID , 'state' : state , 'patient_data' : patient_data , 'measles' : measles , 'MMR' : mmr , 'stratum' : stratum , 'weights' : weights , 'weights_v' : weights_v } data = pd . DataFrame ( d ); #Clean up the data to assign . to NAN data [ 'measles' ] = data [ 'measles' ] . replace ( [ '1' , '0' , '.' ], [ 1 , 0 , 'NAN' ]) #Convert the data to numberic values data = data . convert_objects ( convert_numeric = True ) #Replace the state ID code with the state's name num , name = np . loadtxt ( 'state_ID.txt' , unpack = True , dtype = str , delimiter = ',' ) data [ 'state' ] = data [ 'state' ] . replace ( num . astype ( np . int64 ), name ) #Find all the values where there is complete provider data. #We will look at only the complete patient records, #and remove records column which is now only one value. ind = np . where ( data . patient_data == 1 )[ 0 ] data = data . iloc [ ind ] data . drop ([ 'patient_data' ], inplace = True , axis = 1 ) Below, we provide some examples of our resulting, cleaned data points. The weight columns here are described further below. The first of these is used for analyses including the Virgin Islands, the other when they are not. MMR child house measles state stratum weights weights_v 0 1 11 1 1 Texas 1054 65.155698 120.163273 1 1 21 2 1 Texas 2055 33.652064 52.360361 3 1 41 4 1 Massachusetts 1002 216.529889 271.502218 5 1 61 6 1 Georgia 1025 231.557156 562.130094 6 1 71 7 1 South Carolina 1030 150.109737 238.018808 Weighting to get averaged statistics We now have our data cleaned and ready to go. However, some additional work needs to be done before we can evaluate various statistics of interest. This is because the CDC data set is not a random sample, but instead a stratified sample — i.e. one geared towards obtaining reasonable accuracy among many minority groups, and not simply among the averaged population. The weight factors are the key to extracting averaged statistics from this data, as explained in the user guide. For example, the average is essentially just a weighted average, and the standard error can be calculated using a Taylor-Series approach. The easiest way to apply this to our data is to make use of custom functions. Once constructed, these can then be easily applied to different DataFrame groupings. # Lets define a function to caluclate the vaccination rate # for the group we are looking at, and the standard error def calculate_rate_and_error ( data ): # The rate is calucated useing a weighted average rate = ( data . MMR * data . weights_v ) . sum () / data . weights_v . sum () # The error is calculated using the formula from the data sheet data [ 'Z' ] = data . weights_v * ( data . MMR - rate ) / data . weights_v . sum () zhi = data . groupby ([ 'stratum' , 'house' ]) . agg ( np . sum ) . Z zh = zhi . groupby ( level = 'stratum' ) . sum () # Number of households per stratum nk = zhi . groupby ( level = 'stratum' ) . count () zh = zh / nk stratum_labels = zh . index . values var = np . zeros ( len ( nk )) ind = 0 for a in stratum_labels : delta2 = ( zhi . loc [ a ] - zh [ a ]) ** 2 delta2 = delta2 . sum () var [ ind ] = ( nk [ a ] / ( nk [ a ] - 1. )) * delta2 ind += 1 standard_error = np . sqrt ( sum ( var )) return rate , standard_error If we apply this function to our whole DataFrame, we will get the national MMR vaccination rate and standard error. calculate_rate_and_error(data) # output: (0.90767842904073048, 0.0043003916851249895) Data segmentation — stats by group But we can also do more! If we first apply the DateFrame's groupby method, we can split the data along any feature of interest. For example, below we split the data along the state column. This generates subgroups for each state. Next, we use the apply method to run our custom function on all the different groups of data. We then clean up the output, unzip the tuple and generate a graph showing the MMR vaccination rate by state. It should be evident that with only modest effort, one can modify this code to group the data in many varying ways — all that needs to be done is to adjust the arguments of the groupby command. # Now that we have our function it is easy to calculate values # for any group. # To examine rates by state, we will group by state grouped = data . groupby ( 'state' ) # We then apply our function to the grouped data, and save # it as a data frame result = pd . DataFrame ( grouped . apply ( calculate_rate_and_error )) # To conver the results from: # state # Alabama (0.931323319509, 0.017837146103) # Alaska (0.862202058663, 0.0257844894834) # to # Rate Standard Error # state # Alabama 0.931323 0.017837 # Alaska 0.862202 0.025784 # We use the following code new_col_list = [ 'Rate' , 'Standard Error' ] for n , col in enumerate ( new_col_list ): result [ col ] = result [ 0 ] . apply ( lambda x : x [ n ]) result . drop ( 0 , axis = 1 , inplace = True ) # Make a sorted copy of the data sorted = result . sort_index ( by = 'Rate' , ascending = True ) result [ 'Rate' ] = result [ 'Rate' ] * 100 # Save a csv file if wanted result . to_csv ( 'Rate 2012.csv' , float_format = ' %5.2f ' ) # Generate an online plot data = Data ([ Bar ( y = sorted . index . values , x = sorted [ 'Rate' ], orientation = 'h' ) ]) plot_url = py . plot ( data , filename = 'plot' )","tags":"Case studies","url":"https://efavdb.com/vaccination-rates","loc":"https://efavdb.com/vaccination-rates"},{"title":"Mathematics of measles","text":"Here, we introduce — and outline a solution to — a generalized SIR model for infectious disease. This is referenced in our following post on measles and vaccination rates. Our generalized SIR model differs from the original SIR model of Kermack and McKendrick in that we allow for two susceptible sub-populations, one vaccinated against disease and one not. We conclude by presenting some python code that integrates the equations numerically. An example solution obtained using this code is given below. The model The equations describing our generalized SIR model are \\begin{eqnarray}\\tag{1} \\label{eq1} \\dot{S}_{U} &=& - b_{U} S_{U} I \\\\ \\tag{2} \\label{eq2} \\dot{S}_{U} &=& - b_{U} S_{U} I \\\\ \\tag{3} \\label{eq3} \\dot{R} &=& k I \\\\ \\tag{4} \\label{eq4} 1 &=& I + R + S_U + S_V \\end{eqnarray} Here, \\(S_{U}\\) , \\(S_{V}\\) , \\(I\\) , and \\(R\\) are population fractions corresponding to those unvaccinated and as yet uninfected, vaccinated and as yet uninfected, currently infected and contagious, and once contagious but no longer (recovered, perhaps), respectively. The first two equations above are instances of the law of mass action . They approximate the infection rates as being proportional to the rates of susceptible-infected individual encounters. We refer to \\(b_{U}\\) and \\(b_{V}\\) here as the infection rate parameters of the two subpopulations. The third equation above approximates the dynamics of recovery: The form chosen supposes that an infected individual has a fixed probability of returning to health each day. We will refer to \\(k\\) as the recovery rate parameter . The final equation above simply states that the subpopulation fractions have to always sum to one. Parameter estimation We can estimate the values \\(b_{U}\\) and \\(b_{V}\\) by introducing a close contact number ( \\(ccn\\) ) variable, which is the average number of close contacts that individual infected, contagious people make per day. As a rough ball park, let us suppose that \\(ccn \\approx 3\\) . According to the CDC , an un-vaccinated person making close contact with someone with measles has a 90 \\(%\\) chance of contracting the illness. On the other hand, those who have been vaccinated a single time have a 95 \\(%\\) chance of being immune to the disease. Let's estimate that the combined population of individuals who have been vaccinated have a 1 \\(%\\) chance of contracting the illness upon close contact. These considerations suggest \\begin{align} b_{U} \\approx 3 \\times 0.9 &= 0.27, \\\\ b_{V} \\approx 3 \\times 0.01 &= 0.03 \\end{align} The value of \\(k\\) can be simply estimated using the fact that infected individuals are only contagious for about \\(8\\) days, only four of which occur before rash appears. Assuming those who are showing symptoms quickly stop circulating, this suggests about five \"effectively contagious\" days, or \\begin{align} k \\approx 1/5 = 0.2. \\end{align} Note that here and elsewhere, we measure time in units of days. It's important to note that, although the qualitative properties of the solutions to our model are insensitive to parameter value variations, this is not true for the numerical values that it predicts. We have chosen parameter values that seem reasonable to us. Further, with these choices, many of the model's key quantitative values line up with corresponding CDC estimates. Those interested can experiment to see what sort of flexibility is allowed through modest parameter variation. Solution by quadrature Equations (\\ref{eq1}-\\ref{eq3}) give \\begin{align} \\tag{5} \\label{Svals} S_{U} = S_{U0} e&#94;{ - \\frac{b_{U} R}{k}}, \\\\ S_{V} = S_{V0} e&#94;{- \\frac{b_{V} R}{k}}. \\end{align} Combining with (\\ref{eq4}) and integrating gives \\begin{align} \\frac{\\dot{R}}{k} =I_0 -S_{U0} \\left [ e&#94;{ - \\frac{b_{U} R}{k}}- 1 \\right ] - S_{20} \\left [e&#94;{ - \\frac{b_{V} R}{k}}- 1 \\right ] - R \\end{align} Integrating again, \\begin{align} \\tag{6} \\label{solution} kt = \\int_{0}&#94;R \\frac{d R&#94;{\\prime}}{I_0 -S_{U0} \\left [ e&#94;{ - \\frac{b_{U} R&#94;{\\prime}}{k}}- 1 \\right ] - S_{V0} \\left [e&#94;{ - \\frac{b_{V} R&#94;{\\prime}}{k}}- 1 \\right ] - R&#94;{\\prime}} . \\end{align} This implicitly defines \\(R\\) as function of time. Small time behavior At small \\(t\\) , \\(R\\) is also small, so (\\ref{solution}) can be approximated as \\begin{align} k t = \\int_{0}&#94;R \\frac{d R&#94;{\\prime}}{I_0 + \\left [ \\frac{ b_{U} S_{U0}}{k} +\\frac{ b_{V} S_{V0}}{k} - 1 \\right ]R&#94;{\\prime}}. \\end{align} This form can be integrated analytically. Doing so, and solving for \\(R\\) , we obtain \\begin{align} R = \\frac{k}{b_{U} S_{U0} + b_{V} S_{V0} - k} \\left \\{e&#94;{ (b_{U} S_{U0} + b_{V} S_{V0} - k )t } -1 \\right \\}, \\ \\ \\ I = I_0 e&#94;{ (b_{U} S_{U0} + b_{V} S_{V0} - k )t}. \\end{align} Early disease spread is characterized by either exponential growth or decay, governed by the sign of the parameter combination \\(b_{U} S_{U0} + b_{V} S_{V0} - k\\) : a phase transition! Total contractions The total number of people infected in an outbreak can be obtained by evaluating \\(R\\) at long times, where \\(I = 0\\) . In this limit, using (\\ref{eq4}) and (\\ref{Svals}), we have \\begin{align} S_{U0} e&#94;{- \\frac{b_{U} R}{k}}+ S_{V0} e&#94;{ - \\frac{b_{V} R}{k}}+ R = 1. \\end{align} This equation can be solved numerically to obtain the total contraction count as a function of the model parameters and initial conditions. A plot against \\(S_{U0}\\) of such a solution for our measles-appropriate parameter estimates is given in our next post . Numerical integration in python Below, we provide code that can be used to integrate (\\ref{eq1}-\\ref{eq4}). The plot shown in our introduction provides one example solution. It's quite interesting to see how the solutions vary with parameter values, and we suggest that those interested try it out. #Solving the SIR model for infectious disease. JSL 2/18/2015 import math ccn = 3 # \"close contact number\" = people per day #interacting closely with typical infected person k = 1. / 5 # Rate of 'recovery' [1]. b1 = ccn * 0.9 # Approximate infection rate un-vaccinated [3]. b2 = ccn * 0.01 # Approximate infection rate un-vaccinated [4]. #Initial conditions (fraction of people in each category) I0 = 0.001 # initial population fraction infected. S10 = 0.2 # population fraction unvaccinated. S20 = 1 - I0 - S10 # population fraction vacccinated. R0 = 0.0 # intial recovered fraction. dt = 0.01 # integration time step days = 100 # total days considered I = [ I0 for i in range ( int ( days / dt ))] S1 = [ S10 for i in range ( int ( days / dt ))] S2 = [ S20 for i in range ( int ( days / dt ))] R = [ R0 for i in range ( int ( days / dt ))] for i in range ( 1 , int ( days / dt )): S1 [ i ] = S1 [ i - 1 ] - b1 * S1 [ i - 1 ] * I [ i - 1 ] * dt S2 [ i ] = S2 [ i - 1 ] - b2 * S2 [ i - 1 ] * I [ i - 1 ] * dt I [ i ] = I [ i - 1 ] + ( b1 * S1 [ i - 1 ] * I [ i - 1 ] + \\ b2 * S2 [ i - 1 ] * I [ i - 1 ] - k * I [ i - 1 ] ) * dt R [ i ] = R [ i - 1 ] + k * I [ i - 1 ] * dt time = [ dt * i for i in range ( 0 , int ( days / dt ))] % pylab inline plt . plot ( time , I , color = 'red' ) plt . plot ( time , S1 , color = 'blue' ) plt . plot ( time , S2 , color = 'green' ) plt . plot ( time , R , color = 'black' ) plt . plot ( time [: 1400 ], [ I0 * math . exp (( b1 * S10 + b2 * S20 - k ) * t ) for t in time [: 1400 ]], color = 'purple' ) plt . axis ([ 0 , 100 , 10 ** ( - 4 ), 1 ]) plt . yscale ( 'log' ) plt . xlabel ( 'time [days]' ) plt . ylabel ( 'population % \\' s' ) plt . show () # [1] Measles patients are contagious for eight days # four of which are before symptoms appear. [2] # [2] http://www.cdc.gov/measles/about/transmission.html # [3] Assume infected have close contact with five people/day. # 90% of the un-vaccinated get sick in such situations. # [4] Single vaccination gives ~95% immunity rate [5]. Many # have two doses, which drops rate to very low. # [5] http://www.cdc.gov/mmwr/preview/mmwrhtml/00053391.htm if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"https://efavdb.com/math-of-measles","loc":"https://efavdb.com/math-of-measles"},{"title":"NBA week 14 summary, week 15 predictions","text":"Our first ever 100% accuracy week! We will never beat this. Breakdown by point spread below, and new predictions are up. In other news: This week, efavdb.com welcomes a new, official member to our team, Damien Ramunno-Johnson ! He previously contributed two very interesting guest posts to the site, one on facial recognition software and one on the interpretation of wearable sensor data (eg, Jawbone's up band ). We aim to eventually bring site membership up to about four or five like-minded, yet complementary contributors, each interested in contributing about one machine-learning related post a month. We're especially excited to welcome Damien on board — he's an old pal, and also one with a keen eye for finding topics of general interest. In fact, we're working on a joint post together right now that is both timely and interesting — stay tuned! Details of our perfect week: Point spread # games Accuracy < 5 1 100% 5-9 0 NA 10-14 0 NA >14 1 100%","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-14-summary-week-15-predictions","loc":"https://efavdb.com/nba-week-14-summary-week-15-predictions"},{"title":"Analyzing Analysts","text":"In this post, Dustin provides an overview of some of his work from his time in the Insight Data Science Fellowship program — work done in collaboration with Mode Analytics , an online collaborative SQL platform. Insight Demo from Dustin McIntosh Many high-value business insights have their answers rooted in data. Companies hire data analysts to extract these insights by dissecting their data, largely through querying databases with SQL (Structured Query Language). However, data analysis can often be difficult. Mode Analytics is a company that aims to streamline the process of learning and engaging in data analysis with SQL , both for experts and novices alike. As an Insight Data Science Fellow, I have been working with Mode to help them analyze their users' SQL code, mistakes and all, to identify opportunities for product improvement. Specifically, I've been trying to address the following questions: What types of SQL errors do users make? Do these vary with user ability level? Do early struggles dishearten beginners, hampering them from becoming power users? More generally, what characteristics differentiate the SQL queries of aspiring and expert analysts? Given answers to these questions, how can Mode modify their product to make analysis easier for their users? My analysis uncovered a number of actionable insights for Mode: Mostly, analysts make the same sorts of errors regardless of their ability level. The good news is that many of these errors are avoidable: Mode can help. Novice users actually do not seem to churn due to frustration with errors. It's more likely that they leave due simply to having completed the tutorials - Mode can increase user retention by proactively encouraging users to upload and interact with their own private data after finishing the tutorials. Mode can customize the user experience soon after a user joins the site in order to better serve users based on their SQL skills. To this end, I built a model to classify users as experts or novices based on the content of their queries. This model reveals that experts take extra care in the formatting of their queries, implying an advanced knowledge of the SQL structure that could be emphasized in the tutorials. WHAT IS MODE AND WHO ARE THEIR USERS ? Mode Analytics provides a one-stop shop for all the needs of a data analyst. Users can easily connect their data to Mode's web-based app, query it with SQL , and create and share visualizations of their analysis to convey the insights they've extracted. In addition, Mode offers free instruction via their SQL school , which instructs novices on the basics of SQL while querying some public, tutorial datasets. With the goal of cohorting users based on their ability level, I group users based on the number of queries they have submitted to Mode (see plot at right of tutorial reference rate and error rate by cohort): Power users (1000 + queries): this group primarily queries private data sources with no need for the tutorials, making only occasional errors. Novices (10-1000 queries): the majority of these users are heavily invested in the tutorials and are making a lot more errors than the experts as they learn. Infrequent queriers (<10 queries): these users have only visited Mode's website long enough to make a few queries - most of which are easy tutorial exercises on which they are making very few errors. The demarcations at 10 and 1000 queries are somewhat arbitrary at this point. Undoubtedly, there are many experts hidden in the 10-1000 category who already possess advanced SQL knowledge, but are yet to extensively use Mode's platform for their analysis. As we will see, we can build a model to distinguish these users based on the content of their queries rather than their query count. The primary goal, from a business perspective, is to move users up this list: get infrequent queriers invested in the product and get novice tutorial users up to speed with SQL so they can enjoy all aspects of Mode's product. For the remainder of the post, I ignore the infrequent queriers [1] in favor of analyzing novices and the differences between them and the experts. WHAT ERRORS DO SQL USERS MAKE ? SQL is a very simple language. With a simple, declarative style and only about 200 keywords in total, of which only a few dozen are in common usage [2], there is very little to memorize for SQL users. Thus, most errors should be recognizable and avoidable in product design. Examining the types of errors SQL users make informs us how we can make an analyst's experience better through the Mode platform. In the plot below, I count the number of each type of error made by the two cohorts (experts and novices). The most striking thing about this analysis is that the two most common error types are forgetting and/or misspelling table and column names. Fortunately, these errors are easily addressed through product design: simply prominently displaying the table/column names may significantly reduce errors. It may even be possible in some instances to auto-fill the names. There are a few subtle differences between expert and novice errors worth noting. Novice users tend to make more syntax errors, which is not a surprise given they are less familiar with the language. Power users, on the other hand, make a broader distribution of errors (e.g., a lot more rare errors). Further, experts much more frequently run into the limits of the system (e.g., timeouts, internal errors), an indication that they are running more complex queries. From a product perspective, correcting errors for novices is the priority as they make the most errors and are less likely to know how to correct them. Further, as novices tend to make a narrower range of errors, it is likely possible to parse out many of these errors and either auto-correct them or provide more personalized help to the user than the standard SQL error messages. DO NOVICES GET FRUSTRATED AND QUIT ? A principal concern for educating the novice users is that they may consistently run into certain types of errors, not understand how to correct them, and quit out of frustration. Should Mode customize the standard error messages to better direct novices to their problems? To see whether or not this is the case, I looked at the set of churned novice users (those that have not made a query since November) and examined their final few queries. If user frustration is causing churn, we expect the error rate to increase as the users approach churn. Surprisingly, the rate at which users commit errors before they churn is actually lower than the group's average error rate and decreases until they churn. Thus, we can infer that the average novice does not quit due to frustration with errors they are making - this is not a major concern for customer retention. However, Mode could be more proactive about transitioning customers from their SQL school to connecting their own data and using their visualization tools. 88% of users that churn have not connected their own data sources and are working exclusively on the tutorial datasets. One potential product enhancement is to periodically remind tutorial users of the possibility of connecting their own data and to add tutorial exercises introducing the visualization tools. WHAT 'S IN A SQL EXPERT 'S QUERY ? As discussed above, drawing a line at 1000 queries is not a very useful classification metric for experts versus novices. There are many experts that come to Mode with a lot of expertise in SQL , but have not made 1000 queries yet. Thus, I developed a model based on a Random Forest classifier to differentiate expert and novice SQL users based on the content of their queries [3]. This model accurately classifies users based on a single query roughly 65% of the time for both classes. Accuracy of the prediction will go up the more unique queries a user submits; for example, after ten queries accuracy may be as high as 90% [4]. One interesting aspect of the Random Forest model is that it determines the most important features that define an expert's query from a novice's. The result is clear: Of the top five most important features, three of them have to do with formatting; experts take more care in their use of white space, line breaks, and parentheses. In particular, in their queries, experts have a higher density of white space, a lower density of line breaks, and a higher density of parentheses. Thus, experts tend to have longer lines of code with more white space. The two remaining features in the top five, query length and frequency of \"select\" and \"from\", indicate that experts tend to write longer queries with more subqueries. Being able to classify a user as an expert or novice based on the content of their queries would be extremely useful for personalizing user experience. If a user connects their data early on and it becomes clear that they have limited experience with SQL , Mode would like to be able to direct those users to relevant tutorials or perhaps to work related to theirs that was performed by experts. Likewise, if a user immediately demonstrates advanced SQL knowledge, Mode would like to direct them to, for example, the data visualization tools that make Mode's platform uniquely useful to the experienced analyst. Implementation of my model would permit this. CONCLUSIONS This analysis provides a number of actionable insights for Mode in serving their user base: Everyone using the platform is affected by errors relating to misspelling or altogether forgetting the names of the tables and their contained fields in their databases. Implementing an auto-complete of these names or prominently displaying them for the user could considerably reduce the number of errors encountered on Mode. Most users that churn are not leaving out of frustration with the errors they are making. The vast majority of churning users, however, are exclusively querying tutorial datasets. Mode can try to get tutorial users more invested in their product by prompting them to connect their data periodically throughout the tutorial. In addition to tutorial-using novices and the true SQL experts, Mode has a third class of users. Their query history does not reveal much about them: they only visit Mode briefly, submitting just a few tutorial exercises without making many errors. Looking into who these visitors are via, for example, google analytics data, may provide insight into how to retain these potential users. User ability level can be determined based on the content of their first several queries to Mode. This is useful as Mode can effectively personalize response to users as they join the platform. FOOTNOTES [1] The query history of the infrequent queriers does not provide very much information as to who these users are. Gathering additional information on these users may prove useful in determining how to retain these customers. [2] Top-Heavy Nature of SQL - Of the approximately 200 keywords in the SQL vocabulary, a very small subset are even remotely common. Looking at the fraction of queries containing each keyword from the expert dataset demonstrates just how top heavy the language is. Each keyword not present on this plot appears in fewer than 4% of all unique queries in the dataset. [3] Labeling the Training Set - I define users with >1000 queries as experts and take only a subset of their most recent queries (those submitted since November 2014) - assuming that they have become better queriers with time. For the novice training set, it is not possible to strictly take the users from the 10-1000 query group as there are likely some new-comer experts in that category (experienced SQL users that have just recently joined Mode). Instead, I took only users with 100-1000 queries that had participated extensively in the tutorials. I then remove those tutorial queries from their set and label those remaining as novice queries for training. Feature Selection/Engineering - I use a bag-of-words approach with language defined by SQL 's keywords. Additional features include query length, number of unique keywords in the query (diversity), as well as fraction of the query that is white space, line breaks, and parentheses. Several keywords are strongly correlated with one another; for this reason, I combine some together (e.g., \"select\" and \"from\") and remove others from the analysis entirely (e.g., \"as\", \"by\" and \"on\"). [4] Growing confidence of classification - If a user's successive queries were independent of one another we would expect confidence in the prediction to grow in accordance with a binomial distribution: \\(P(\\) misclassify \\() = \\sum_{i = 0}&#94;{\\lfloor n/2 \\rfloor} \\binom{n}{i} p&#94;i (1-p)&#94;{n-i}\\) with \\(p \\approx 0.65\\) and \\(n\\) the number of queries. After ten queries the misclassification error reduces to roughly 10%; after 20 queries - 5%. However, a user's queries are typically not independent of one another, so this is likely a generous estimate. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Case studies","url":"https://efavdb.com/analyzinganalysts","loc":"https://efavdb.com/analyzinganalysts"},{"title":"NBA week 13 summary, week 14 predictions","text":"Correct predictions on 31/50 or 62% of the games this past week, including a number of the significant upsets. However, we didn't catch them all. Details by point spread below, and new predictions are up. Point spread # games Accuracy <= 5 11 36% 6-10 17 82% 11-15 12 58% >15 10 60%","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-13-summary-week-14-predictions","loc":"https://efavdb.com/nba-week-13-summary-week-14-predictions"},{"title":"NBA week 12 summary, week 13 predictions","text":"We correctly predicted 36/53 of the games this past week, which equates to an accuracy of 67.9% — a slight improvement over the past couple of weeks, but still short of the 70% level we strive for (off by about one game!). The breakdown by point spread is below, and the new predictions are up. This week, we also updated the design of the weekly NBA predictions page . We've improved the visual look and also opted to print out the win-loss records next to each team. This is actually pretty useful because it provides a quick check whether our predictions are unusual in some way. Often, I've found that if a prediction disagrees with the record-based favorite, a quick check at the dashboard wheel can give some insight into why (eg, checking prior matches of the pair, or perhaps checking whether the predicted winner has managed to beat some surprisingly good teams, etc.). This sort of situation is actually where I have the most fun using the wheel. We've also implemented some changes to our weekly predictions page processing code that allows for it to be viewed nicely on any screen size — a significant improvement for us cell phone viewers! We'll try to implement a similarly flexible version of the dashboard soon, too. One last point: Thanks to James Kung for inviting us to what turned out to be a very memorable game last Friday (pic above, also courtesy of James)! edit: the youtube vid of Klay's historic quarter. edit 2: What's not shown in the video, and what really got the crowd excited from the start of Klay's run, was the fact that the Warriors seemingly missed their previous 20 shots. This let the Kings come back from a large early Warriors lead. The crowd was on their feet Klay's second shot onwards. Point spread # games Accuracy < 5 10 50% 5-9 21 76% 10-14 9 67% >14 13 69%","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-12-summary-week-13-predictions","loc":"https://efavdb.com/nba-week-12-summary-week-13-predictions"},{"title":"NBA week 11 summary, week 12 predictions","text":"Again, reasonably good accuracy this past week — 36/54, or 66.6% overall. The breakdown by point spread is given below, and the new predictions are up. We made a couple of changes to the dashboard this week. First, it has a new, cleaner look, thanks to Jaireh Tecarro , who kindly donated a redesign. Secondly, we discovered that our prior processing code implementation was inadvertently defaulting to a non-smooth rendering mode as a consequence of some coordinate transformations that we were making use of. This behavior was not explained in the general processing references, and so came as a surprise to us. We've reimplemented some of the drawing commands in order to avoid the problem, and we've documented it in our processing tips and tricks post . Point spread # games Accuracy < 5 11 45% 5-9 23 65% 10-14 18 78% >14 18 72%","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-11-summary-week-12-predictions","loc":"https://efavdb.com/nba-week-11-summary-week-12-predictions"},{"title":"Machine learning for facial recognition","text":"A guest post, contributed by Damien Ramunno-Johnson ( LinkedIn , bio-sketch ) Introduction The ability to identify faces is a skill that people develop very early in life and can apply almost effortlessly. One reason for this is that our brains are very well adapted for pattern recognition. In contrast, facial recognition can be a somewhat difficult problem for computers. Today, given a full frontal image of a face, computer facial recognition software works well. However, problems can arise given large camera angles, poor lighting, or exaggerated facial expressions: Computers have a ways to go before they catch up with us in this arena. Although facial recognition algorithms remain imperfect, the methods that exist now are already quite useful and are being applied by many different companies. Two examples, first up Facebook: When you upload pictures to their website, it will now automatically suggest names for the people in your photos. This application is well-suited for machine learning for two reasons. First, every tagged photo already uploaded to the site provides labeled examples on which to train an algorithm, and second, people often post full face images in decent lighting. A second example is provided by Google's Android phone OS , which has a face unlock mode. To get this to work, you first have to train your phone by taking images of your face in different lighting conditions and from different angles. After training, the phone can attempt to recognize you. This is another cool application that also often works well. In this post, we're going to develop our own basic facial learning algorithm. We'll find that it is actually pretty straightforward to set one up that is reasonably accurate. Our post follows and expands upon the tutorial found here . Loading packages and data from __future__ import print_function from time import time import matplotlib.pyplot as plt from sklearn.cross_validation import train_test_split from sklearn.datasets import fetch_lfw_people from sklearn.grid_search import GridSearchCV from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from sklearn.decomposition import RandomizedPCA from sklearn.svm import SVC import pandas as pd % matplotlib inline The sklearn function fetch_lfw_people , imported above, will download the data that we need, if not already present in the faces folder. The dataset we are downloading consists of a set of preprocessed images from Labeled Faces in the Wild ( LFW ) , a database designed for studying unconstrained face recognition. The data set contains more than 13,000 images of faces collected from the web, each labeled with the name of the person pictured. 1680 of the people pictured have two or more distinct photos in the data set. In our analysis here, we will impose two conditions. First, we will only consider folks that have a minimum of 70 pictures in the data set. We will resize the images so that they each have a 0.4 aspect ratio. print('Loading Data') people = fetch_lfw_people( './faces', min_faces_per_person=70, resize=0.4) print('Done!') &gt;&gt; Loading Data Done! The object people contains the following data. people.data: a numpy array with the shape(n_samples, h*w), each row corresponds to a unravelled face. people.images: a numpy array with the shape(n_samples, h, w), where each row corresponds to a face. The remaining indices here contain gray-scale values for the pixels of each image. people.target: a numpy array with the shape(n_samples), where each row is the label for the face. people.target_name: a numpy array with the shape(n_labels), where each row is the name for the label. For the algorithm we will be using, we don't need the relative position data, so we will use the unraveled people.data. # Find out how many faces we have , and # the size of each picture from . n_samples , h , w = people . images . shape X = people . data n_features = X . shape [ 1 ] y = people . target target_names = people . target_names n_classes = target_names . shape [ 0 ] print (& quot ; Total dataset size :& quot ;) print (& quot ; n_images : % d & quot ; % n_samples ) print (& quot ; n_features : % d & quot ; % n_features ) print (& quot ; n_classes : % d & quot ; % n_classes ) & gt ;& gt ; Total dataset size : n_images : 1288 n_features : 1850 n_classes : 7 Looking above we see that our dataset currently has 1288 images. Each image has 1850 pixels, or features. We also have 7 classes, meaning images of 7 different people. Data segmentation and dimensional reduction At this point we need to segment our data. We are going to use train_test_split , which will take care of splitting our data into random training and testing data sets. Next, we note that we have a lot of features and that there are advantages to having fewer: First, the computational cost is reduced. Second, having fewer features reduces the data's dimension which can also reduce the complexity of the model and help avoid overfitting. Instead of dropping individual pixels outright, we will carry out a dimensional reduction via a Principle Component Analysis PCA . PCA works by attempting to represent the variance in the training data with as few dimensions as possible. So instead of dropping features, as we did in our wearable sensor example analysis, here we will compress features together, and then use only the most important feature combinations. When this is done to images, the features returned by PCA are commonly called eigenfaces (some examples are given below). The function we are going to use to carry out our PCA is RandomizedPCA . We'll keep the top 150 eigenfaces, and we'll also whiten the data — ie normalize our new, principal component feature set. The goal of whitening is to make the input less redundant. Whitening is performed by rotating into the coordinate space of the principal components, dividing each dimension by square root of variance in that direction (giving the feature unit variance), and then rotating back to pixel space. # split into a training and testing set X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0 . 25 ) # Compute the PCA ( eigenfaces ) on the face dataset n_components = 150 pca = RandomizedPCA ( n_components = n_components , whiten = True ). fit ( X_train ) eigenfaces = pca . components_ . reshape (( n_components , h , w )) X_train_pca = pca . transform ( X_train ) Visualizing the eigenfaces Let's now take a moment to examine the dataset's principal eigenfaces: the set of images that we will project each example onto to obtain independent features. To do that we will use the following helper function to make life easier — visual follows. # A helper function to make plots of the faces def plot_gallery ( images , titles , h , w , n_row = 3 , n_col = 4 ) : plt . figure ( figsize = ( 1.8 * n_col , 2.4 * n_row )) plt . subplots_adjust ( bottom = 0 , left = .01 , right = .99 , top = .90 , hspace = .35 ) for i in range ( n_row * n_col ) : plt . subplot ( n_row , n_col , i + 1 ) plt . imshow ( images [ i ] . reshape (( h , w )), cmap = plt . cm . gray ) plt . title ( titles [ i ] , size = 12 ) plt . xticks (()) plt . yticks (()) # Plot the gallery of the most significative eigenfaces eigenface_titles = [ &quot;eigenface %d&quot; % i for i in range(eigenfaces.shape[0 ] ) ] plot_gallery ( eigenfaces , eigenface_titles , h , w ) plt . show () Training a model Now that we have reduced the dimensionality of the data it is time to go ahead and train a model. I am going to use the same SVM and GridSearchCV method I explained in my previous post . However, instead of using a linear kernel, as we did last time, I'll use instead a radial basis function ( RBF ) kernel. The RBF kernel is a good choice here since we'd like to have non-linear decision boundaries — in general, it's a reasonable idea to try this out whenever the number of training examples outnumbers the number of features characterizing those examples. The parameter C here acts as a regularization term: Small C values give you smooth decision boundaries, while large C values give complicated boundaries that attempt to fit/accommodate all training data. The gamma parameter defines how far the influence of a single point example extends (the width of the RBF kernel). # Train a SVM classification model print (& quot ; Fitting the classifier to the training set & quot ;) t0 = time () param_grid = { 'C': [ 1 e3 , 5 e3 , 1 e4 , 5 e4 , 1 e5 ] , 'gamma': [ 0.0001 , 0.0005 , 0.001 , 0.005 , 0.01 , 0.1 ] , } clf = GridSearchCV ( SVC ( kernel = 'rbf' , class_weight = 'auto' ), param_grid ) clf = clf . fit ( X_train_pca , y_train ) print (& quot ; done in % 0 . 3fs & quot ; % ( time () - t0 )) print (& quot ; Best estimator found by grid search :& quot ;) print ( clf . best_estimator_ ) & gt ;& gt ; Fitting the classifier to the training set done in 16 . 056s Best estimator found by grid search : SVC ( C = 1000 . 0 , cache_size = 200 , class_weight = 'auto' , coef0 = 0 . 0 , degree = 3 , gamma = 0 . 001 , kernel = 'rbf' , max_iter = -1 , probability = False , random_state = None , shrinking = True , tol = 0 . 001 , verbose = False ) Model validation That's it for training! Next we'll validate our model on the testing data set. Below, we first use our PCA model to transform the testing data into our current feature space. Then, we apply our model to make predictions on this set. To get a feel for how well the model is doing, we print a classification_report and a confusion matrix . # Quantitative evaluation of the model quality on the test set # Validate the data X_test_pca = pca . transform ( X_test ) y_pred = clf . predict ( X_test_pca ) print ( classification_report ( y_test , y_pred , target_names = target_names )) print ( 'Confusion Matrix' ) # Make a data frame so we can have some nice labels cm = confusion_matrix ( y_test , y_pred , labels = range ( n_classes )) df = pd . DataFrame ( cm , columns = target_names , index = target_names ) print ( df ) & gt ; & gt ; precision recall f1 - score support Ariel Sharon 0 . 81 0 . 85 0 . 83 20 Colin Powell 0 . 82 0 . 78 0 . 80 54 Donald Rumsfeld 0 . 78 0 . 67 0 . 72 27 George W Bush 0 . 87 0 . 95 0 . 91 139 Gerhard Schroeder 0 . 86 0 . 73 0 . 79 26 Hugo Chavez 1 . 00 0 . 75 0 . 86 20 Tony Blair 0 . 84 0 . 89 0 . 86 36 avg / total 0 . 85 0 . 85 0 . 85 322 Confusion Matrix Ariel Sharon Colin Powell Donald Rumsfeld George W Bush \\ Ariel Sharon 17 3 0 0 Colin Powell 1 42 1 7 Donald Rumsfeld 3 1 18 3 George W Bush 0 3 3 132 Gerhard Schroeder 0 1 1 3 Hugo Chavez 0 0 0 4 Tony Blair 0 1 0 3 Gerhard Schroeder Hugo Chavez Tony Blair Ariel Sharon 0 0 0 Colin Powell 1 0 2 Donald Rumsfeld 1 0 1 George W Bush 0 0 1 Gerhard Schroeder 19 0 2 Hugo Chavez 1 15 0 Tony Blair 0 0 32 As a quick reminder, lets define what the terms above are: precision is the ratio Tp / (Tp + Fp) where Tp is the number of true positives and Fp the number of false positives. recall is the ration of Tp / (Tp + Fn) where Fn is the number of false negatives. f1-score is (precision * recall) / (precision + recall) support is the total number of occurrences of each face. In our second table here, we have printed a confusion matrix, which provides a nice summary visualization of our results: Each row is the actual class, and the columns are the predicted class. For example, in row 1 there are 17 correct identifications of Arial Sharon, and 5 wrong ones. Using our previously defined helper plotting function, we show some examples of predicted vs true names below. Our simple algorithm's accuracy is imperfect, yet satisfying! #Plot predictions on a portion of the test set def title ( y_pred , y_test , target_names , i ) : pred_name = target_names [ y_pred[i ] ] . rsplit ( ' ' , 1 ) [ -1 ] true_name = target_names [ y_test[i ] ] . rsplit ( ' ' , 1 ) [ -1 ] return 'predicted: %s\\ntrue: %s' % ( pred_name , true_name ) prediction_titles = [ title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0 ] ) ] plot_gallery ( X_test , prediction_titles , h , w , 6 , 4 ) Discussion 85% average accuracy shows that PCA (Eigenface analysis) can provide accurate face recognition results, given just a modest amount of training data. There are pros and cons to eigenfaces however: Pros Training can be automated. Once the the eigenfaces are calculated, face recognition can be performed in real time. Eigenfaces can handle large databases. Cons Sensitive to lighting conditions. Expression changes are not handled well. Has trouble when the face angle changes. Difficult to interpret eigenfaces: Eg, one can't easily read off from these eye separation distance, etc. There are more advanced facial recognition methods that take advantage of features special to faces. One example is provided by the Active Appearance Model ( AAM ) , which finds facial features (nose, mouth, etc.), and then identifies relationships between these to carry out identifications. Whatever the approach, the overall methodology is the same for all facial recognition algorithms: Take a labeled set of faces. Extract features from those faces using some method of choice (eg eigenfaces). Train a machine learning model on those features. Extract features from a new face, and predict the identity. The story doesn't end with finding faces in photos. Facial recognition is just a subset of machine vision, which is currently being applied widely in industry. For example, Intel and other semiconductor manufactures use machine vision to detect defects in the chips being produced — one application where by-hand (human) analysis is not possible and computers have the upper hand.","tags":"Case studies","url":"https://efavdb.com/machine-learning-for-facial-recognition-3","loc":"https://efavdb.com/machine-learning-for-facial-recognition-3"},{"title":"Quick tutorial on MySQL","text":"Here, we give a quick (< 30 mins) introduction to the open source database software package MySQL. The post is intended to be useful for folks totally new to the program, as well as for those who find that they often need reminders on its basic syntax (that is, people like us). Follow @efavdb Follow us on twitter for new submission alerts! Getting started MySQL is a database software package that allows users to quickly access subsets of data contained within tables, and also to carry out simple operations on this data. The software is quite powerful, but it can be surprisingly unintuitive for beginners. The best way to get the hang of it is to play around with it a bit. This post provides a set of commands that should help you get a feel for how it works. If you're a beginner reading this, we suggest installing it on your personal computer or server, and following along by trying each of the commands we go through here. Once it's installed and you have its server running, you can often access MySQL from the command line by typing mysql On a mac, you may need to use the following though /usr/local/mysql/bin/mysql -uroot Once mysql is loaded, you can see what databases are available by typing SHOW DATABASES ; Notice that a semi-colon is used to terminate commands: In general, these can extend across multiple lines and the semi-colon tells the program where the command stops. Also, MySQL is case-insensitive, but it is considered good form to have all command calls capitalized for easier reading. If no databases yet exist, you can create one as follows: CREATE DATABASE animalDB ; Here, animalDB is the name of the database. From the list of available databases, you can select one of interest with the USE command. For example, USE animalDB ; Each database can contain many tables. To see the tables contained in a database, use the SHOW command, SHOW tables ; Table creation and alteration SQL tables have a name and a set of rows and columns. The columns have types that are defined upon table creation ( INT, BIGINT, FLOAT, DOUBLE, CHAR, VARCHAR , etc.). The rows correspond to individual table entries. To illustrate, we'll now create a table called \"MyPets\", with a column for pet name, species, and age. This is done with the command CREATE TABLE MyPets ( name VARCHAR ( 10 ), species VARCHAR ( 10 ), age INT ); Here, we are using the VARCHAR type for our two string columns. The argument supplied allows us to use strings up to length 10 for these entries. We could also have used the CHAR(10) type here, but that would result in trailing spaces following names shorter than 10 characters. We now insert some entries using the INSERT command, INSERT INTO MyPets VALUES ( \"Bottles\" , \"Dog\" , 3 ); INSERT INTO MyPets ( name , species ) VALUES ( \"Mac\" , \"Dog\" ); INSERT INTO MyPets VALUES ( \"Hector\" , \"Cat\" , 1 ); Here, we've illustrated two different methods to do insertion. In the first and third lines, we have values for all columns. However, in the second, no age is supplied, so we have to specify which columns the values we are supplying correspond to. The age column for this entry will read NULL , since no value was provided for it. To view the table, we write – with SELECT and ∗ meaning \"retrieve\" and \"all\", respectively – SELECT * FROM MyPets ; >> + ---------+---------+------+ | name | species | age | + ---------+---------+------+ | Bottles | Dog | 3 | | Mac | Dog | NULL | | Hector | Cat | 1 | + ---------+---------+------+ Additional SELECT queries are given below that illustrate how one can select and operate on subsets of the columns and rows. To add an age for Mac, we use the UPDATE, SET , and WHERE commands, writing UPDATE MyPets SET age = 7 WHERE name = \"Mac\" ; To see that this and the other commands that follow work as expected, try running the SELECT command above after each application. To delete a row from the table, we use the DELETE command, DELETE from MyPets WHERE name = \"Hector\" ; It is also possible to add or subtract columns from a table. To add a column, we use the ALTER and ADD COLUMN commands, ALTER TABLE MyPets ADD COLUMN litters INT DEFAULT 0 ; The last line here is not necessary. Without it, the command would create the column and set each row's value there to NULL . To delete a column, we use the DROP command, ALTER TABLE MyPets DROP litters ; Caveat: While row addition and removal can always be carried out quickly, addition and removal of columns scales linearly with table size. The reason is that these operations are generally carried out by copying the original table into a second table having the desired new structure. For this reason, it is generally a good idea to plan ahead and make sure any new table has all the columns you foresee might be needed. SELECTION queries — learn by example Example conditional commands: What is the name and age each of my pets? SELECT name , age FROM mypets ; How many dogs have I got? SELECT COUNT ( * ) FROM mypets WHERE species = \"dog\" ; Show me just the first two pets in my table. SELECT * FROM mypets LIMIT 2 ; Show me my pets in age-descending order. SELECT * FROM MyPets ORDER BY age ; Which of my dogs are under 4 years old? SELECT * FROM MyPets WHERE age < 4 AND species = \"dog\" ; Which animals have names that start with the letter \"M\"? SELECT * FROM mypets WHERE name LIKE \"M%\" ; Which animals have the letter \"E\" somewhere in their name? SELECT * FROM mypets WHERE name LIKE \"%E%\" ; Example GROUP BY commands (see also MIN, MAX, SUM, STD , etc.): How many pets have I got of each species? SELECT species , count ( * ) FROM mypets GROUP BY species ; What is the average age of my pets, grouped by species? SELECT species , AVG ( age ) FROM mypets GROUP BY species ; Actions on multiple tables To solidify what we've learned above, try to now create a second table, called PetDetails , like that above but with different age and species values. You can add other columns to it if you like. Once that's done, apply the SHOW TABLES command to see that both tables are available. Next, learn to copy specific values from this new table into the first one, using commands like UPDATE MyPets , PetDetails SET MyPets . age = PetDetails . age WHERE MyPets . name = PetDetails . name ; Note the use of the period here to specify from which table a certain column is to be selected from. The JOIN/ON commands . The JOIN command essentially creates something like a flattened outer product of two tables: If there are \\(n\\) entries in the first table and \\(m\\) in the second, the command returns a table with \\(n \\times m\\) rows. There is one row for each possible pairing, one entry taken from the first table and one from the second. All columns from both tables are then included in the new table. The ON command can be used to specify conditions on which pairs are to be included in the combined table. To illustrate, let's define a new table of pet-trick pairs CREATE TABLE PetTricks ( name VARCHAR ( 10 ), trick VARCHAR ( 10 )); INSERT INTO PetTricks VALUES ( \"Bottles\" , \"Shake\" ); INSERT INTO PetTricks VALUES ( \"Bottles\" , \"Play dead\" ); INSERT INTO PetTricks VALUES ( \"Mac\" , \"Shake\" ); INSERT INTO PetTricks VALUES ( \"Dogbert\" , \"Consulting\" ) With the following, we get the number of tricks each of my pets can do SELECT MyPets . name AS name , count ( * ) AS num_tricks FROM ( MyPets JOIN PetTricks ON MyPets . name = PetTricks . name ) GROUP BY MyPets . name ; Here, we see for the first time that it is possible to select values from a table created \"on the fly\" (the table in parentheses, which you can print using the SELECT command). We also see for the first time the concept of aliasing, applied through use of the AS command. Our last — and most complicated — example combines many of the ideas discussed above. If you can get to the point where you can replicate commands like this one, you'll be pretty much set to construct your own complex SQL queries: Let's add a trick count to our first table, and then fill it in by querying the PetTricks table. ALTER TABLE MyPets ADD COLUMN num_tricks INT DEFAULT 0 ; UPDATE MyPets AS T1 , ( Select name , count ( * ) AS tot FROM PetTricks GROUP BY name ) AS T2 SET T1 . num_tricks = T2 . tot WHERE T1 . name = T2 . name ; SELECT * FROM MyPets ; >> + ---------+---------+------+------------+ | name | species | age | num_tricks | + ---------+---------+------+------------+ | Bottles | Dog | 3 | 2 | | Mac | Dog | 7 | 1 | | Hector | Cat | 1 | 0 | + ---------+---------+------+------------+ Other tips Lastly, a few one-off tips that can be very helpful. Creating a new table similar another. The following command can come in handy when you're dealing with tables that have many columns: CREATE TABLE TNew LIKE T1 ; Here, the command creates TNew , a new table with column names and types like those of T1 . The entries of T1 are not copied over. If you want to copy some of them over, you can do that with a command like INSERT INTO TNew ( SELECT * FROM T1 WHERE ...); Saving to a text file. Printing a table to a text file can sometimes be useful. To proceed, you first need to create a directory that MySQL can have write access to. On a mac, you can accomplish this from the terminal with the following cd /usr/local mkdir MySQLOutput sudo chmod -R 777 MySQLOutput This creates the directory /usr/local/MYSQLOutput with global read, write, and execute permissions. With this setup, we can write to a file from within MySQL with a command like SELECT * FROM MyPets INTO OUTFILE \"/usr/local/MySQLOutput/test.txt\" Scripts. For complicated queries, or queries that you would like to be able to run multiple times, it is useful to employ scripts. These can then be executed from within mysql using the SOURCE command. To illustrate, suppose we have a text file called /usr/local/MySQLOutput/test.txt within which we have written the commands CREATE table Bad_dogs ( DogID BIGINT , Barks INT ); INSERT INTO Bad_dogs VALUES ( 1234567890 , 666 ); We can run this from within MySQL using the command SOURCE / usr / local / MySQLOutput / test . txt ; This creates the table and inserts the example entry. Indexing. By creating an index, one can speed up SELECT calls on large tables. You can think of an index heuristically as a second table having two columns: The first is a sorted version of one of the original table's columns, and the second column is a pointer to the memory block where its corresponding entry sits (actually, an index usually sits in a B-tree, a structure similar to a binary-search tree). Entries can be quickly accessed via the index, generally in logarithmic time. To add a key to our first table, write ALTER TABLE MyPets ADD PRIMARY KEY ( name ); This selects the name column as our index, which will speed up all SELECT calls seeking entries with name values satisfying some condition — specified using WHERE name = ... . You can actually index as many columns of a table as you like. However, this takes up disk space, and so should be avoided when the extra indexes are not useful. It is also possible to specify that you want one or more columns to be keys upon table creation. Further study. At this point, we have covered most of the basics, but only the basics. If you get stumped by any tricky queries moving forward, we suggest visiting both stackoverflow.com — which has tons of interesting discussions on the topic — and the MySQL documentation page , which goes over most everything and includes a tutorial in chapter 3 similar to this one. Both are excellent resources. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Tools","url":"https://efavdb.com/quick-tutorial-on-mysql","loc":"https://efavdb.com/quick-tutorial-on-mysql"},{"title":"NBA week 10 summary, week 11 predictions","text":"Fairly good accuracy this past week — 33/49, or 67.8% overall. The breakdown by point spread is given below, and the new predictions are up! Point spread # games Accuracy < 5 9 44% 5-9 17 53% 10-14 15 67% >14 18 94%","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-10-summary-week-11-predictions","loc":"https://efavdb.com/nba-week-10-summary-week-11-predictions"},{"title":"Machine learning with wearable sensors","text":"A guest post, contributed by Damien Ramunno-Johnson ( LinkedIn , bio-sketch ) Introduction Wearable sensors have become increasingly popular over the last few years with the success of smartphones, fitness trackers, and smart watches. All of these devices create a large amount of data that is ideal for machine learning. Two early examples are the FitBit and Jawbone's up band, both of which analyze sensor input to determine how many steps the user has taken, a metric which is helpful for measuring physical activity. There is no reason to stop there: With all of this data available it is also possible to extract more information. For example, fitness trackers coming out now can also analyze your sleep. In that spirit, I'm going to show here that it is pretty straightforward to make an algorithm that can differentiate between 6 different states. Walking Walking Upstairs Walking Downstairs Sitting Standing Laying To do this I am going to use Python , Sklearn and Plot.ly . Plot.ly is a wonderful plotting package that makes interactive graphs you can share. The first step is to import all of the relevant packages. Load packages and source data For this example, I used one of the datasets available from the UCI Machine Learning Repository . For this data set 30 subjects were recorded performing activities of daily living ( ADL ) while carrying a waist-mounted smartphone (Samsung Galaxy II ) with embedded inertial sensors. A testing dataset and training dataset are provided. The dataset has 561 features which were created from the sensor data: XYZ acceleration, etc. from numpy import loadtxt import numpy as np from sklearn import svm , grid_search from sklearn.metrics import accuracy_score , f1_score from sklearn.feature_selection import SelectPercentile , f_classif import plotly.plotly as py from plotly.graph_objs import * Now that we have loaded all of our packages, it is time to import the data into memory. This data set is not large enough to cause any memory issues, so go ahead and load the whole thing. data_test = loadtxt(\"./Wearable/UCI_HAR_Dataset/test/X_test.txt\") label_test=loadtxt(\"./Wearable/UCI_HAR_Dataset/test/y_test.txt\") data_train = loadtxt(\"./Wearable/UCI_HAR_Dataset/train/X_train.txt\") label_train = loadtxt(\"./Wearable/UCI_HAR_Dataset/train/y_train.txt\") Feature selection Given that this data set has training and testing data with labels, it makes sense to do supervised machine learning. We have over 500 potential features to use, which is a lot. Let's see if we can get by with fewer features. To do that, we will use SK -learn'€™s SelectKBest to keep the top 20 percent of the features, and then transform the data. selector = SelectPercentile(f_classif, 20) selector.fit(data_train, label_train) data_train_transformed = selector.transform(data_train) data_test_transformed = selector.transform(data_test) Machine learning At this point you need to decide which algorithm you want to use. I tried a few of them and got the best results using a Support Vector Machine ( SVM ). SVMs attempt to determine the decision boundary between two classes that is as far away from the data of both classes as possible. In general they work pretty well. Let's try some parameters and see how good our results are. clf = svm . SVC ( kernel = \"rbf\" , C = 1 ) clf . fit ( data_train_transformed , label_train ) pred = clf . predict ( data_test_transformed ) print \"Accuracy is %.4f and the f1-score is %.4f \" % ( accuracy_score ( pred , label_test ), f1_score ( label_test , pred )) >>Accuracy is 0.8812 and the f1-score is 0.8788 Optimization That's not too bad, but I think we can still optimize our results some more. We could change the parameters manually, or we can automate the task using a grid search . This is a handy module that allows you to do a parameter sweep. Below, I set up a sweep using two different kernels and various penalty term values (C) to see if we can raise our accuracy. parameters = { 'kernel' :( 'linear' , 'rbf' ), 'C' :[ 1 , 10 , 100 , 1000 , 10000 ] } svr = svm . SVC () clf = grid_search . GridSearchCV ( svr , parameters ) clf . fit ( data_train_transformed , label_train ) pred = clf . predict ( data_test_transformed ) print \"Accuracy is %.4f and the f1-score is %.4f \" % ( accuracy_score ( pred , label_test ), f1_score ( label_test , pred )) >>Accuracy is 0.9430 and the f1-score is 0.9430 Visualization Looks like we are getting pretty good accuracy for using only 20% of the features available to us. You may have also noticed that I am outputting the F1-Score which is another measure of the accuracy which takes into account the precision and the recall. Now let's plot some of these data points to see if we can visualize why this is all working. Here, I am using Plot.ly to make the plot. You can make the plots many different ways including converting matplotlib plots into these online plots. If you click on the \"play with this data\" link at the bottom of the figure (or click here ) you can see the code used to make the plot. [iframe src=\"https://plot.ly/~Damien RJ /104\" width=\"100%\" height=\"680\"] I picked two of the features to plot, the z acceleration average, and the z acceleration standard deviation. Note, the gravity component of the acceleration was removed and placed into its own feature. Only 3/6 labels are being plotted to make it a little easier to see what is going on. For example, it is easy to see that the walking profile in the top graph differs significantly from those of standing and laying in the bottom two. Discussion From the graphs above alone, it would be difficult to differentiate between laying and standing. We might be able to comb through different combinations of features to find a set that is more easily distinguishable, but we are limited by the simple fact that it is hard to visualize data in more than 3 dimensions. If it turns out that more than a handful of features need to be considered simultaneously to separate the different classes, this approach will fail. In contrast, we have seen in our SVM analysis above that it is actually pretty easy to use machine learning to pick out, with high accuracy, a variety of motions from the sensor data. This is a neat application that is currently being applied widely in industry. It illustrates why machine learning is so interesting in general: It allows us to automate data analysis, and apply it to problems where a by-hand, visual analysis is not possible.","tags":"Case studies","url":"https://efavdb.com/machine-learning-with-wearable-sensors","loc":"https://efavdb.com/machine-learning-with-wearable-sensors"},{"title":"NBA week 9 summary, week 10 predictions","text":"A poorer than average, but not entirely out of the ordinary performance this week (given expected fluctuations ): \\(32/54 = 59.3%\\) . A slightly poorer performance might be expected on weeks like this - where a blockbuster deal completely changes part of the league: Apparently the Pistons are now unbeatable sans Josh Smith. Summary by point spread is at right. Week 10 predictions up. This past week, we also finally managed to implement the automation of our dashboard data feed: Now, if you go to the dashboard after 6am PST , records will always be up to date, as will the upcoming games listings and predictions for these games. A caveat: As new data comes in , our daily-updated predictions on the dashboard can now differ slightly from those posted in our official, weekly listings — We put some by-hand effort into the weekly listings, so they're \"official\". Nevertheless, it's the same algorithm being applied to obtain the Dashboard results. Automating our feed required using beautiful soup to do some scraping, as well as learning how to set up cron jobs — Thanks to J. Bergknoff for suggesting we look into the latter. Automation is a beautiful thing: With all the free time now opened up, we can start to consider other items on our wish list (eg, improving our predictions…) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-9-summary-week-10-predictions","loc":"https://efavdb.com/nba-week-9-summary-week-10-predictions"},{"title":"Processing and processing.js tips and tricks in WordPress","text":"We recently developed our NBA dashboard in the programming language Processing. In addition, we have Processing apps in our post on classification without negative examples as well as our weekly NBA predictions . Here, we will briefly describe (and recommend) Processing and discuss some tips and tricks we have discovered in developing and deploying our above-mentioned apps to our WordPress blog. Follow @efavdb Follow us on twitter for new submission alerts! Processing Intro Your browser does not support the canvas tag. JavaScript is required to view the contents of this page. Processing is a high-level language built on top of java designed for producing neat interactive visualizations of ideas, data, and art. A major benefit of Processing is that it is extremely accessible (see for example, this ebook by Reas and Fry ): while it can now be used to produce considerably sophisticated visualizations, it was originally developed as a tool for teaching programming to people with little to no experience with computers (e.g., visual arts students). The little guy above is an example of a processing app. You can find the (remarkably simple) code for it here (we found this example in this tutorial ). For people with technical backgrounds, you can be up and developing basic programs in a matter of hours (or perhaps even minutes). Here, our programming language of choice is typically Python and, fortunately, the people at Processing recently implemented Python-mode which made our transition all the more smooth (see Quirks below before you get too excited about this, however). In just a couple of days we were able to build a quick prototype of our tool. Porting it to the web, however, we found to be a bit more difficult. Processing in WordPress Tutorial We spent some time trying to figure out how to get everything working in WordPress. We thought it would be useful to outline the steps below for others interested in getting some Processing-based dynamic content on their own blog. In the end it is very straight forward, but getting some more advanced features of Processing (e.g., importing external images or data files) to work on the web requires some tricks which we will tackle in the next section. 1. Download the processing.js script and put it on your server. This script defines a \"canvas\" element where your Processing code is executed on your webpage. ​2. In your theme's header file (header.php), add the following line of code to the body of the file (after and before ), changing the url to point to the script from step 1: <script src=\"url_to_processing.js\" type=\"text/javascript\"></script> You can change this file using your dashboard in the Appearance - Editor menu, selecting \"Header (header.php)\" for the Template (see this screenshot - the first line is a comment). ​3. Upload your pde script(s) to your server and they can be referenced using code like the following (replace the urls to point to your pde scripts): <canvas id=\"testsketch\" class=\"alignright\" data-processing-sources= \"{static}/wp-content/uploads/sketches/testSketch2/testSketch2.pde additional_url.pde yet_another_url.pde\"> Your browser does not support the canvas tag.</canvas> <noscript>JavaScript is required to view the contents of this page. </noscript> You simply place this code in the \"text\" view of any post or page in WordPress. Note that you don't need to have multiple pde scripts, in most simple cases you will only have one. However, sometimes it is useful to call on multiple (see below quirk #3). To include multiple, simply place multiple urls (separated by spaces) between the quotes in the canvas tag. ​4. Enjoy! Wasn't that simple? Now you can mess around and create all sorts of cool, dynamic apps for your blog. Running list of processing quirks and tips ​1) It took us a while to come to the conclusion that there is, currently, no easy way to get Python-mode Processing code on the web (please let us know if we are wrong about that…). We ended up translating our Processing code from Python syntax to Javascript syntax and using processing.js (as implied in our tutorial above). ​2) In order to load images for display in your app (if applicable), you will have to use the preload directive in your pde script. This comes in the form a of a comment at the top of the script: / @pjs preload=\"url_to_myimage.jpg\"; / . The \"@pjs\" tags this comment for processing.js to read and execute before the rest of the script runs. It forces the browser to complete downloading and caching of the image before attempting to use it. ​3) We could not figure out a clean way of importing data from an external file (e.g., in csv or txt format). Instead, we found that saving the external data in Javascript format in a separate pde file and running that file in addition to your main script (see step 3 from the tutorial above) worked well. As an example, here is a pde file that we use to feed in the results of NBA games that have already been played this season into our NBA dashboard. We scrape this data off the web and run a preprocessing python script to get the data in this format and save the completed.pde file. ​4) As a security precaution, many browsers (e.g., Chrome) won't run a sketch that links to images or data stored on a separate server. As a consequence, when developing a sketch on your computer that links to data on your server, it may appear to not be working, even if coded properly. It's easy to misinterpret this issue as a bug, so keep it in mind. One way to avoid this is to try running your sketch in other browsers. Regardless, once a sketch is uploaded to your server, the sketch should work fine on any browser. 5) Interestingly, when logged in to WordPress, we have found that the Processing apps work, but function as if the mouse were several pixels lower than it actually is. This symptom is only apparent when logged in to WordPress and viewing the page (see example screenshot at right). ​6) Processing has two rendering modes, smooth and not smooth. I believe the latter requires less memory to implement, and that this is why it exists. We prefer smooth drawing, and tried to implement this from the start. However, we recently discovered that if you apply coordinate transformations in your processing sketch (translations or rotations), the program forces back to the not smooth mode. This came as a surprise to us, since it's not mentioned in any of the basic documentation. Our original NBA wheel sketch made use of a translation; we've reimplemented that without it, and see a marked aesthetic improvement. ​7) We noticed that running our first NBA wheel sketch would often cause a laptop to quickly heat up. This was due to the fact that we were having the sketch refresh with a frame rate of about 30 hz, which is totally unnecessary for this particular application. Changing the draw program so that the image refreshes only when some new information is requested totally fixed our heating issues.","tags":"Tools","url":"https://efavdb.com/processing-and-processing-js-tips-and-tricks","loc":"https://efavdb.com/processing-and-processing-js-tips-and-tricks"},{"title":"NBA week 8 results, week 9 predictions","text":"A pretty good bounce-back from last week's results: \\(35/52 = 67.3%\\) . Breakdown by point spread at right. Week 9 predictions are up now on both our NBA Dashboard and the new and improved Weekly NBA Predictions Page (this also done in Processing). if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-8-results-week-9-predictions","loc":"https://efavdb.com/nba-week-8-results-week-9-predictions"},{"title":"Quantifying the NBA Christmas week flop: one in ten thousand?","text":"There were a number of upsets in the NBA this past Christmas week. Here, we offer no explanation, but do attempt to quantify just how bad those upsets were, taken in aggregate. Short answer: real bad! To argue this point, we review and then apply a very simple predictive model for sporting event outcomes — python code given in footnotes. Quick review of x-mas week The Christmas holiday week \\(&#94;1\\) (Dec. 19 - 25) provided a steady stream of frustrating upsets. The two most perplexing, perhaps, were the Lakers win over the Warriors and the Jazz win over the Grizzlies: two of this year's greats losing to two of its most lackluster. In all, \\(24\\) of the \\(49\\) games that week were upsets (with an upset defined here to be one where the winning team started the game with a lower win percentage than the loser). That comes out to an upset ratio just under \\(49%\\) , much higher than the typical rate, about \\(34%\\) . A general sporting model A \\(49%\\) upset rate sounds significant. However, this metric does not quite capture the emotional magnitude of the debacle. To move towards obtaining such a metric, we first review here a \"standard\" \\(&#94;2\\) sporting model that will allow us to quantify the probability of observing a week as bad as this just past. For each team \\(i\\) , we introduce a variable \\(h_i\\) called its mean scoring potential: Subtracting from this the analogous value for team \\(j\\) gives the expected number of points team \\(i\\) would win by, were it to play team \\(j\\) . More formally, if we let the win-difference for any particular game be \\(y_{ij}\\) , we have \\begin{eqnarray}\\tag{1} h_i - h_j \\equiv \\langle score(i) - score(j) \\rangle \\equiv \\langle y_{ij} \\rangle, \\end{eqnarray} where we average over hypothetical outcomes on the right in order to account for the variability characterizing each individual game. By taking into account the games that have already occurred this season, one can estimate the set of \\(\\{h_i\\}\\) values. For example, summing the above equation over all past games played by team \\(1\\) , we obtain \\begin{eqnarray}\\tag{2} \\sum_{j\\text{ (past opponents of 1)}} (h_1 - h_j) = \\sum_j \\langle y_{1j} \\rangle \\approx \\sum_j y_{1j}. \\end{eqnarray} Here, in the sum on right we have approximated the averaged sum in the middle by the score differences actually observed in the games already played (note that in the sum on \\(j\\) here, each team appears exactly the number of times they have already played team \\(1\\) — this could be zero, once, twice, etc.) Writing down all equations analogous to this last one (one for each team) returns a system of \\(30\\) linear equations in the \\(30\\) \\(\\{h_i\\}\\) variables. This system can be easily solved using a computer \\(&#94;3\\) . We did this, applying the algorithm to the complete set of 2014-15 games played prior to the Christmas week, and obtained the set of \\(h\\) values shown at right \\(&#94;4\\) . The ranking looks quite reasonable, from top to bottom. A Gaussian NBA Now that we have the \\(\\{h_i\\}\\) values, we can use them to estimate the mean score difference for any game. For example, in a Warriors-76ers game, we'd expect the Warriors to win, since they have the larger \\(h\\) value. Further, on average, we'd expect them to win by about \\(h_{\\text{War's}} - h_{\\text{76's}}\\) \\( = 9.24 - (-11.96) \\approx 21\\) points. These two actually played this week, on Dec 30, and the Warriors won by \\(40\\) , a much larger margin than predicted. The distinction between our predicted and the actual Warriors-76ers outcome motivates further consideration of the variability characterizing NBA games. It turns out that if we analyze the complete set of games already played this year, something simple pops out: Plotting a histogram of our estimate errors, \\(\\epsilon_{ij} \\equiv (h_i - h_j) - y_{ij}\\) , we see that the actual score difference distribution of NBA games looks a lot like a Gaussian , or bell curve. This is centered about our predicted value and has a standard deviation of \\(\\sigma \\approx 11\\) points, as shown in the figure at right. These observations allow us to estimate various quantities of interest. For instance, we can estimate the frequency with which the Warriors should beat the 76ers by 40 or more points, as they did this week. This is simply equal to the frequency with which we underestimate the winning margin by at least \\(40 - 21 = 19\\) points. This, in turn, can be estimated by counting how often this has already occurred in past games, using our histogram. Alternatively, we can use the fact that our errors are Gaussian distributed to write this as \\begin{eqnarray}\\tag{3} P(\\epsilon \\leq -19) = \\frac{1}{\\sqrt{2 \\pi \\sigma&#94;2}} \\int_{-\\infty}&#94;{-19} e&#94;{-\\frac{\\epsilon&#94;2}{2\\sigma&#94;2}} d \\epsilon \\approx 0.042, \\end{eqnarray} where we have evaluated the integral by computer. This result says that a Warriors win by 40 or more points will only occur about \\(4.2%\\) of the time. Using a similar argument, one can show that the 76ers should beat the Warriors only about \\(2.8 %\\) of the time. Christmas week, quantified It is now a simple matter to extend our analysis method so that we can estimate the joint likelihood of a given set of outcomes all happening the same week: We need only make use of the fact that the mean estimate error \\(\\langle \\epsilon \\rangle\\) of our predictions on a set of \\(N\\) games \\((\\langle \\epsilon \\rangle = \\frac{1}{N}\\sum_{\\text{games }i = 1}&#94;N \\epsilon_i)\\) will also be Gaussian distributed, but now with standard deviation \\(\\sigma/ \\sqrt{N}\\) . The \\(1/\\sqrt{N}\\) factor here reduces the width of the mean error distribution, relative to that of the single games — it takes into account the significant cancellations that typically occur when you sum over many games, some with positive and some with negative errors. A typical week has about \\(50\\) games, so the mean error standard deviation will usually be about \\(11/\\sqrt{50} \\approx 1.6\\) . In the four figures below, we plot histograms of our prediction errors for four separate weeks: Christmas week is shown last (in red), and the other subplots correspond to the three weeks preceding it (each in green). We also show in each subplot (in gray) a histogram of all game errors preceding the week highlighted in that subplot — notice that each is quite well-fit by a Gaussian. In the first week, \\(53\\) games were played, and our average error on these games was just \\(\\langle \\epsilon \\rangle = 0.5\\) points. The probability of observing an average overestimate of \\(0.5\\) or greater in such a week is given by, \\begin{eqnarray}\\tag{4} P(\\langle \\epsilon \\rangle \\geq 0.5) = \\frac{1}{\\sqrt{2 \\pi \\sigma&#94;2/53}} \\int_{0.5}&#94;{\\infty} e&#94;{-\\frac{\\epsilon&#94;2}{2\\sigma&#94;2/53}} d \\epsilon \\approx 0.38. \\end{eqnarray} That is, a weekly average overestimate of \\(\\langle \\epsilon \\rangle \\geq 0.5\\) will happen about \\(38%\\) of the time, and so is pretty common. Similarly, in the second, third, and fourth weeks, the number of games played and average estimate errors were \\((N,\\langle \\epsilon \\rangle) = (52,0.8),\\) \\((55,2.2)\\) , and \\((49,5.7)\\) , respectively. Calculating as above, overestimates of these magnitudes or larger occur with frequency \\(30%\\) , \\(7%\\) , and \\(0.01 %\\) , respectively. The previous two are both fairly common, but — on average — it would apparently take about ten thousand trials to find a week as bad as Christmas week 2014. Discussion A week in ten thousand is equivalent to about one week in every \\(400\\) seasons! We don't really take this estimate too seriously. In fact, we suspect that one of the following might be happening here: a) there may have been something peculiar about the games held this Christmas week that caused their outcomes to not be distributed in the same manner as other games this season \\(&#94;5\\) , b) alternatively, there may be long tails in the error distribution that we can't easily observe, or c) it may be that improvements to our model (e.g., taking into account home team advantage, etc.) would result in a larger frequency estimate. Maybe all three are true, or maybe this really was a week in ten thousand. Either way, it's clear that this past Christmas week was a singular one. Footnotes [1] The NBA workweek starts on Friday. [2] We first read about this modeling method here . In the addendum, it's stated that the author thinks that nobody in particular is credited with having developed it, and that it's been around for a long time. [3] Notice that we can shift all \\(h_i \\to h_i +c\\) , with \\(c\\) some common constant. This invariance means that the solution obtained by solving the system of equations is not unique. Consequently, the matrix of coefficients is not invertible, and the system needs to be solved by Gaussian elimination, or some other irritating means. [4] Python code and data for evaluating the NBA \\(h\\) values given here . [5] Note, however, that carrying out a similar analysis over the past 9 seasons showed no similar anomalies in their respective Christmas weeks. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"NBA prediction project","url":"https://efavdb.com/an-nba-christmas","loc":"https://efavdb.com/an-nba-christmas"},{"title":"NBA week 7 results, week 8 predictions","text":"The week that defeated the NBA predictor… \\(24/49 = 49.0%\\) . That's right, worse than a coin flip. Not exactly the Christmas present we had in mind. We certainly weren't expecting things to be this bad, but Christmas week is extremely difficult to predict, historically. It's not like our algorithm went crazy or anything, everything still looks sane, we still put the 76ers at terrible and the Warriors at awesome. Of course, that didn't help us much when the former beat the Heat and the latter lost to the Lakers. Utah over Memphis and New Orleans over OKC are a couple more head-scratchers. I know, I know, more excuses, but seriously, this is well below our estimated range of plausible results (see comments here ). This deserves an explanation - we are investigating, more to come on the NBA grinch that stole our Christmas. In the meantime, a hopefully much better set of predictions for week 8 are now up. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-7-results-week-8-predictions","loc":"https://efavdb.com/nba-week-7-results-week-8-predictions"},{"title":"Machine Learning Methods: Classification without negative examples","text":"Here, we discuss some methods for carrying out classification when only positive examples are available. The latter half of our discussion borrows heavily from W.S. Lee and B. Liu, Proc. ICML -2003 (2003), which we supplement somewhat. Generic logistic regression Logistic regression is a commonly used tool for estimating the level sets of a Boolean function \\(y\\) on a set of feature vectors \\(\\textbf{F}\\) : In a sense, you can think of it as a method for playing the game \"Battleship\" on whatever data set you're interested in. Its application requires knowledge of the \\(\\{(\\textbf{f}_i,y_i)\\}\\) pairs on a training set \\(\\textbf{E} \\subseteq \\textbf{F}\\) , with label \\(y_i = 0,1\\) for negative and positive examples, respectively. Given these training examples, logistic regression estimates for arbitrary feature vector \\(\\textbf{f}\\) , \\begin{eqnarray} h(\\textbf{f}) = \\frac{1}{1 + \\exp \\left [ - \\textbf{T} \\cdot \\textbf{f} \\right]} \\approx y \\tag{1} \\end{eqnarray} where the coefficient vector \\(\\textbf{T}\\) is taken to be that vector that minimizes \\begin{eqnarray}\\tag{2} J(h) \\equiv -\\frac{1}{\\vert \\textbf{E} \\vert}\\sum_{i=1}&#94;{\\vert \\textbf{E} \\vert} y_i \\log(h_i) + (1-y_i) \\log(1- h_i) + \\frac{\\Lambda}{2}\\sum_j T_j&#94;2, \\end{eqnarray} a convex cost function that strongly penalizes poor estimates on the training set. Problem statement: no negative examples Consider now a situation where all training examples given are positive — i.e., no negative examples are available. One realistic realization of this scenario might involve a simple data set of movies already viewed by some Netflix customer. From this information, one would like to estimate the full subset of the available movies that the customer would watch, given time. We'll assign value \\(y = 1\\) to such movies and \\(y=0\\) to movies he wouldn't watch. Notice that the generic logistic regression approach outlined above would return a default-positive result if applied to this problem: Assigning \\(h = 1\\) to all of \\(\\textbf{F}\\) minimizes \\(J\\) . This means that no information contained in \\(\\textbf{E}\\) is actually utilized in the logistic learning process — a counterintuitive choice for structured \\(\\textbf{E}\\) (e.g., the case where all movies watched thus far have been in a single category — martial arts films, say). Noisy labeling Some reasonable, alternative approaches do not return the default-positive response in the situation above. To see this, we first review here noisy labeling problems. Suppose we are given a training set with noisy labeling \\(y&#94;{\\prime}\\) : Truly-positive examples \\((y = 1)\\) are stochastically mislabeled in this set with frequency \\(\\alpha\\) as negative \\((y&#94;{\\prime} = 0)\\) , and truly-negative examples \\((y=0)\\) are mislabeled with frequency \\(\\beta\\) as positive \\((y&#94;{\\prime} = 1)\\) . For hypothesis \\(h\\) , let \\begin{eqnarray}\\tag{3} C(h) = Pr[h = 0 \\vert y = 1]+ Pr[h = 1 \\vert y= 0], \\end{eqnarray} the rate at which \\(h\\) mislabels positive examples in the training set added to the rate at which it mislabels negative examples. Similarly, we define \\(C&#94;{\\prime}(h)\\) as above, but with \\(y\\) replaced by \\(y&#94;{\\prime}\\) . Because \\(y&#94;{\\prime}\\) is stochastic, we also average it in this case, giving \\begin{eqnarray}\\tag{4} C&#94;{\\prime}(h) = \\left \\langle Pr[h = 0 \\vert y&#94;{\\prime} = 1]+ Pr[h = 1 \\vert y&#94;{\\prime}= 0] \\right \\rangle_{y&#94;{\\prime}}. \\end{eqnarray} With these definitions, we have [see Blum and Michael (1998) or derive yourself] \\begin{eqnarray}\\tag{5} C(h) \\propto C&#94;{\\prime}(h), \\end{eqnarray} with \\(\\text{sign}(C) = \\text{sign}(1 - \\alpha - \\beta) \\times \\text{sign}(C&#94;{\\prime})\\) . This result is very useful whenever we take \\(C(h)\\) as our cost function \\(&#94;1\\) : Provided the total noise rate \\(\\alpha + \\beta <1\\) , it implies that we can find the \" \\(C\\) -optimizing\" \\(h\\) within any class of hypotheses by optimizing instead \\(C&#94;{\\prime}\\) — a quantity that we can estimate given any particular noisy labeling realization \\(y&#94;{\\prime}_0\\) as \\begin{eqnarray}\\tag{6} C&#94;{\\prime}(h) \\approx \\left (Pr[h = 0 \\vert y&#94;{\\prime} = 1]+ Pr[h = 1 \\vert y&#94;{\\prime}= 0] \\right ) \\vert_{y&#94;{\\prime} =y&#94;{\\prime}_0}. \\end{eqnarray} Application to no-negatives problem To make connection between the no-negatives and noisy-labeling problems, one can remodel the former as one where all unlabeled examples are considered to actually be negative examples ( \\(y&#94;{\\prime}_0 = 0\\) ). This relabeling gives a correct label to all examples in the original training set \\(\\textbf{E}\\) (where \\(y = y&#94;{\\prime}_0 = 1\\) ) as well as to all truly-negative examples (where \\(y = y&#94;{\\prime}_0 = 0\\) ). However, all positive examples not in \\(\\textbf{E}\\) are now incorrectly labeled (they are assigned \\(y&#94;{\\prime}_0 = 0\\) ): This new labeling \\(y&#94;{\\prime}_0\\) is noisy, with \\(\\alpha = Pr(y&#94;{\\prime}_0 =0 \\vert y =1)\\) and \\(\\beta = Pr(y&#94;{\\prime}_0 =1 \\vert y = 0 ) = 0\\) . We can now apply the Blum and Michael approach: We first approximate \\(C&#94;{\\prime}\\) as above, making use of the particular noisy label we have access to. Second, we minimize the approximated \\(C&#94;{\\prime}\\) over some class of hypotheses \\(\\{h\\}\\) . This will in general return a non-uniform hypothesis (i.e., one that now makes use of the information contained in \\(\\textbf{E}\\) ). Hybrid noisy-logistic approach of Lee and Liu (plus a tweak) The \\(C \\propto C&#94;{\\prime}\\) result is slick and provides a rigorous method for attacking the no-negatives problem. Unfortunately, \\(C&#94;{\\prime}\\) is not convex, and as a consequence it can be difficult to minimize for large \\(\\vert \\textbf{F} \\vert\\) — in fact, its minimization is NP -hard. To mitigate this issue, Lee and Liu combine the noisy relabeling idea — now well-motivated by the Blum and Michael analysis — with logistic regression. They also suggest a particular re-weighting of the observed samples. However, we think that their particular choice of weighting is not very well-motivated, and we suggest here that one should instead pick an optimal weighting through consideration of a cross-validation set. With this approach, the method becomes: ​1) As above, assign examples in \\(\\textbf{E}\\) label \\(y&#94;{\\prime} = 1\\) and examples in \\(\\textbf{F} - \\textbf{E}\\) label \\(y&#94;{\\prime} = 0\\) . 2) Construct the weighted logistic cost function \\begin{eqnarray}\\tag{7} J(h; \\rho) \\equiv -\\frac{1}{\\vert \\textbf{E} \\vert}\\sum_{i=1}&#94;{\\vert \\textbf{E} \\vert} \\rho y&#94;{\\prime}_i \\log(h_i) + (1-\\rho) (1-y&#94;{\\prime}_i) \\log(1- h_i) + \\frac{\\Lambda}{2}\\sum_j T_j&#94;2, \\end{eqnarray} with \\(\\rho \\in [0,1]\\) , a re-weighting factor. (Lee and Liu suggest \\(&#94;2\\) using \\(\\rho = 1-\\frac{\\vert \\textbf{E} \\vert}{\\vert \\textbf{F} \\vert}\\) ). 3) Minimize \\(J\\) . By evaluating performance on a cross-validation set using your favorite criteria, optimize \\(\\rho\\) and \\(\\Lambda\\) . Toy example Here, we provide a toy system that allows for a sense of how the latter method discussed above works in practice. Given is a set of \\(60\\) grid points in the plane, which can be added/subtracted individually to the positive training set ( \\(\\textbf{E}\\) , green fill) by mouse click (a few are selected by default). The remaining points are considered to not be in the training set, but are relabeled as negative examples — this introduces noise, as described above. Clicking compute returns the \\(h\\) values for each grid point, determined by minimizing the weighted cost function \\(J\\) above: Here, we use the features \\(\\{1,x,y,x&#94;2,xy,\\) \\(y&#94;2,x&#94;3, x&#94;2 y,\\) \\(x y&#94;2, y&#94;3\\}\\) to characterize each point. Those points with \\(h\\) values larger than \\(0.5\\) (i.e., those the hypothesis estimates as positive) are outlined in black. We have found that by carefully choosing the \\(\\rho\\) and \\(\\Lambda\\) values (often to be large and small, respectively), one can get a good fit to most training sets. By eye, the optimal weighting seems to often be close — but not necessarily equal to — the value suggested by Lee and Liu. Fig. 1: Interactive weighted noisy-no-negatives solver. Click \"compute\" to run logistic regression. [ NOTE : new site does not yet support processing - I hope to reinsert the interactive object here as soon as possible]. Discussion In this note, we have discussed methods for tackling classification sans negative examples — a problem that we found perplexing at first sight. It is interesting that standard logistic regression returns a default-positive result for such problems, while the two latter methods we discussed here are based on assuming that all points in \\(\\textbf{F} - \\textbf{E}\\) are negatives. In fact, this assumption seems to be the essence of all the other methods referenced in Lee and Liu's paper. Ultimately, these methods will only work if the training set provides a good sampling of the truly-positive space. If this is the case, then \"defocusing\" a bit, or blurring one's eyes, will give a good sense of where the positive space sits. In the noisy-logistic approach, a good choice of \\(\\rho\\) and \\(\\Lambda\\) should effect a good result. Of course, when the training set does not sample the full positive space well, one can still use this approach to get a good approximation for the outline of the subspace sampled. Footnotes \\([1]\\) : The target function \\(y\\) provides the unique minimum of \\(C\\) . Therefore, choosing \\(C\\) as our cost function and minimizing it over some class of hypotheses \\(\\{h\\}\\) should return a reasonable estimate for \\(y\\) (indeed, if \\(y\\) is in the search class, we will find it). \\([2]\\) : Lee and Liu justify their weighting suggestion on the basis that it means that a randomly selected positive example contributes with expected weight \\(>0.5\\) (see their paper). Yet, other weighting choices give even larger expected weights to the positive examples, so this is a poor justification. Nevertheless, their weighting choice does have the nice feature that the positive and negative spaces are effectively sampled with equal frequency. If optimizing over \\(\\rho\\) is too resource-costly for some application, using their weighting suggestion may be reasonable for this reason. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods, Theory","url":"https://efavdb.com/methods-regression-without-negative-examples","loc":"https://efavdb.com/methods-regression-without-negative-examples"},{"title":"NBA week 6 results, week 7 predictions, intro to dash","text":"\\(37/55 = 67.2%\\) accuracy this week. Summary by point spread is at right. This week, we have posted a beta of our NBA dashboard . A screenshot is given below — the interactive version is linked to in our page's header. We developed the tool using the programming language processing and displayed on the web using processing.js . Our thanks to Ann Hermundstad for introducing us to this tool, which we have really enjoyed! The wheel design of the dashboard was inspired by Jawbone's correlated food visual , by Emi Nomura . We think this approach does a good job of quickly conveying the state of the league: both what has happened in prior games already, and what we think will happen in upcoming games. Predictions for week 7 are now up. They can now also be viewed on the dashboard using the third \"Mode\". if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-6-predictions-week-7-predictions","loc":"https://efavdb.com/nba-week-6-predictions-week-7-predictions"},{"title":"NBA week 5 summary, week 6 predictions","text":"Another pretty good result this week: \\(36/52 = 69.2%\\) . Accuracy by spread is at right. A quick note on accuracy as an evaluation metric: We actually predict significant fluctuations in performance of our model from week to week. If we estimate our prediction accuracy at 70%, then for a week with \\(n\\approx 50\\) games we expect a distribution of accuracies from week to week with standard deviation given by the binomial distribution: \\(\\sigma = \\sqrt{n p (1-p)}/n \\approx 6.5%\\) . This is a rather broad distribution - we might expect results ranging from \\(60%\\) to \\(80%\\) to be quite common from week to week. For \\(n \\approx 15\\) games (as in each row of our tables comparing by point spread), the standard deviation increases by a factor of \\(\\approx 2\\) . We have a functioning visualization tool for our NBA content, but are still working on loading it up to our server. Expect that, and a post about our difficulties, to show up soon. Our week 6 predictions are now up. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-5-summary-week-6-predictions","loc":"https://efavdb.com/nba-week-5-summary-week-6-predictions"},{"title":"NBA week 4 summary, week 5 predictions","text":"Another good week for the NBA predictor: correct predictions for 39/53 = 73.6% of the games played. This is an excellent result considering our algorithm had a difficult time cross-validating with this week's results from last year. Also, the 76ers won a game - there goes one of our sure-thing predictions. Predictions for next week are up. We are also working on a nice dynamic visualization for our NBA content - coming soon!","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-4-week-5-predictions","loc":"https://efavdb.com/nba-week-4-week-5-predictions"},{"title":"NBA week 3 summary, week 4 predictions","text":"Our NBA predictor did considerably better this week. In fact, it did better than we even expected/hoped. We had an accuracy of 40/51 = 78.4%, making up for last week's lull. Week 4 predictions are now up. This week set a high bar and our cross-validation indicates that the coming week may be harder to predict.. but we are still optimistic.","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-3-summary-week-4-predictions","loc":"https://efavdb.com/nba-week-3-summary-week-4-predictions"},{"title":"NBA week 2 summary, week 3 predictions","text":"This was an odd week in the NBA - plenty of upsets to go around. You're right, we're making excuses - our NBA predictor had a tough week this round. We had a total accuracy of 27/50 = 54%. The summary by point spread is at right. But seriously, who would have predicted results like back-to-back Laker victories over Atlanta and Houston (aside from die-hard Kobe fans)? Well, we certainly didn't. For the coming week we have further refined our algorithm in two ways. First, we have now incorporated a team's away performance into their predicted home performance and vice-versa. And second, we now select the parameters from our model with a more thorough and systematic cross-validation method. Predictions are now up!","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-2-summary-week-3-predictions","loc":"https://efavdb.com/nba-week-2-summary-week-3-predictions"},{"title":"NBA week 1 summary, week 2 predictions","text":"The first week of our NBA game outcome prediction experiment is in the books! We had a prediction accuracy of 32/51 (= 63%). A summary of the results broken down by game point spread is given at right. The point spreads shown are from the actual games, and the accuracy values shown are the fraction of correct predictions for games within the particular point spread range specified in that row. Our accuracy this past week was significantly lower than we achieved on the 2013-2014 historical data. In training on the first 800 games of that season we achieved approximately 70% accuracy on the remaining 430 games, after incorporating momentum into the prediction. This past week, our model had only 70 games from the current season to train on, relying on last year's data to supplement the training. For this reason, we feel 63% is actually pretty reasonable for our first week. In fact, we had a few very good days late in the week — and, of course, we are particularly proud to have correctly predicted this . Our predictions for the next week are now up as well. This week we have further incorporated fatigue into the model - keeping track of the number of games each team has played in the five days preceding a game. When applied to the prior season data, we found that this feature helped boost our accuracy by about 2%.","tags":"NBA prediction project","url":"https://efavdb.com/nba-week-1-summary-week-2-predictions","loc":"https://efavdb.com/nba-week-1-summary-week-2-predictions"},{"title":"Historic daily traffic patterns and the time scale of deviations","text":"Daily traffic patterns can be decomposed into a historic average plus fluctuations from this average. Here, we examine the daily dynamics of traffic as a function of weekday to provide the first piece of this puzzle. To do this, we average the time-dependent scores \\(c_i(t)\\) for each day of the week (see plot to the right). As discussed previously , modes one and two are general indicators of overall traffic density and directional commuter density, respectively. Interestingly, we can clearly see systematic deviations in these two mode amplitudes across the days of the week: During rush hour, Mondays and Fridays have generally lower levels of traffic by both measures - most likely a consequence of people taking three-day weekends. In addition, if you look closely, you can actually see evidence of slackers taking off early on Friday afternoons. Examining higher modes, the average signal dies away, eventually being lost in the noise. The final two panels in the figure have different y-axis scales - zoomed in to show the signal-to-noise ratio. By mode 100 the signal is already quite weak, but systematic deviations from zero can still be seen above the noise. However, by mode 1000, there are no systematic or significant deviations from zero. These minor principal components likely represent rare events such as traffic accidents and thus are not reflected at all in the daily averages. Is the historic average the best we can do for prediction? To answer this we must examine the predictability of the fluctuations away from these means. Here, we examine the autocorrelation \\(&#94;1\\) of the fluctuations from the mean to find the memory time scale of each principal component's fluctuation memory. This quantity characterizes the time scale over which we can extrapolate each amplitude deviation into the future (see plot \\(&#94;2\\) below). The time scale of initial decay of the correlation decreases monotonically with the mode index - the modes that capture the most variance have the longest memory: several hours. This is excellent news: we can do better than just using the historic traffic patterns. We can, in fact, project fluctuations away from the historic average several hours into the future and expect some improvement. [1] The autocorrellation : The autocorrelation of a stochastic signal is a measure of its memory. In this particular case, \\(R_{i}(t) \\propto \\mathbb{E}[\\Delta c_i(s) \\cdot \\Delta c_i(s+t)]\\) where the expectation value is over \\(s\\) and \\(\\Delta c_i(t) = c_i(t) - \\langle c_i(t) \\rangle\\) . [2] On this plot: The plot shown is for Tuesdays — other days have exhibit similar characteristics. Note that mode 2 takes on negative autocorrelation values for a period of time (\\(t =\\) 6-9 hours). This is not surprising since mode 2, being an \"odd\" mode, tends to reverse sign between rush hours. The inset shows a few mean-subtracted signals for the first mode, ($\\Delta c_1(t) $ above). The long-time correlations of these fluctuations are apparent here. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Case studies","url":"https://efavdb.com/historic-daily-traffic-patterns-and-the-time-scale","loc":"https://efavdb.com/historic-daily-traffic-patterns-and-the-time-scale"},{"title":"NBA weekly predictions: up","text":"Every Friday, we will now be reporting our algorithm's NBA win/loss outcome predictions for the following week. These can be accessed by clicking the \"Weekly NBA predictions\" link in the header section of our blog. This week, we make predictions for a total of 51 games, training both on last year's results, as well as the 70 games that have already been played this season. As the year progresses, we will continuously tweak our program for improved accuracy (we're learning, too!). This week, we implemented one significant change: Our algorithm now takes into account the momentum of each team — in the form of its most recent 10 win/loss outcomes — when attempting to predict its subsequent win/loss result. Interestingly, we found that having a home team's algorithm track its own momentum led to no measurable improvement in its predictive power. However, when fed the momentum features of the opposing, away team, the accuracy of the algorithm improved by five percent (as tested on 2013/14 stats). Whether this is a consequence of some peculiarity of our algorithm's structure, or instead a manifestation of some meaningful psychological effect, we do not know.","tags":"NBA prediction project","url":"https://efavdb.com/nba-weekly-predictions-up","loc":"https://efavdb.com/nba-weekly-predictions-up"},{"title":"Daily traffic evolution and the Super Bowl","text":"With an eye towards predicting traffic evolution, we begin by examining the time-dependence of the contribution from the first principal components on different days of the week. Traffic throughout the day \\(\\vert x(t) \\rangle\\) can be represented in the basis of principal components; \\(\\vert x(t) \\rangle = \\sum_{i} c_i(t) \\vert \\phi_i \\rangle \\) \\(&#94;1\\) , where \\(\\vert \\phi_i \\rangle\\) is the ith principle component. The coefficients \\(c_i(t)\\) , sometimes called the \"scores\" of \\(\\vert x(t) \\rangle\\) in the basis of principal components, carry all of the dynamics. The largest deviations in the traffic patterns (and of the scores) are during weekday rush hours (around 8 am and 5 pm) - see plot of the scores for several modes throughout Jan. 15. There is an abundance of interesting information to be gleaned here. First, note that, generally, the amplitude of the lowest modes is the largest, again, as expected . In addition, there appears to be some large wavelength structure (primarily correlated with the rush hours) sitting on top of a background of higher frequency noise. Ignoring for the moment the noise, the modes generally come in two classes - something like \"even\" and \"odd\". That is, some fluctuate with the same sign in both the morning and evening, while others fluctuate with opposite signs. Let us consider the first two modes in this plot. The first mode is an even mode and we attribute this essentially to an overall shift in general speed throughout the system. Whenever there are more cars on the road, the speed generally decreases everywhere. Indeed, this is the only mode that deviates significantly from zero at night and it deviates positive - the roads are generally completely clear at night and thus the speed is somewhat higher than average. The second mode is an odd mode, which we attribute generally to directional traffic - when everyone is going to work it has one sign, when they are coming home it has the other. We have previously visualized these two modes and, indeed, these concepts are reflected in their spatial structure: the first mode is generally uniform while the second has regions with either sign and many sections of highway are positive in one direction and negative in the other. The time-dependence of these scores is remarkably consistent from week to week (See Wednesdays plot below of the scores for modes 1 and 2). On the right, we plot the same quantities for several Sundays as well. Not surprisingly, the fluctuations are smaller on Sundays than on weekdays, reflecting more homogeneous speeds in sparse traffic. However, they are still reproducible from week to week - see Sundays Jan. 5, 12, 19, and 26 - apparently there is a slight slow-down around 6 pm. Feb. 2 was Super Bowl Sunday and the traffic pattern differs qualitatively from other Sundays. Remarkably, we can identify the time of the Super Bowl kickoff from this data - before the kickoff there is slightly more traffic than the average Sunday and immediately after, less. [1] On projecting into principal components : In the original basis, our data reads \\(\\vert x(t) \\rangle = \\sum_j a_j(t) \\vert \\ell_j \\rangle\\), where the sum runs over all loop locations(there are some 2,000 loops in the Bay area) and \\(\\vert \\ell_j \\rangle\\) is a unit fluctuation in speed at the location of loop \\(j\\) . Changing bases, to the principal components \\(\\vert \\phi_i \\rangle\\), \\(\\vert x(t) \\rangle = \\sum_{i,j} a_j(t) \\vert \\phi_i \\rangle \\langle \\phi_i \\vert \\ell_j \\rangle\\) \\(= \\sum_i c_i(t) \\vert \\phi_i \\rangle\\) where \\(c_i(t) = \\sum_j a_j(t) \\langle \\phi_i \\vert \\ell_j \\rangle\\). The coefficient \\(\\langle \\phi_i \\vert \\ell_j \\rangle\\) is often called the \"loading\" of \\(\\vert \\ell_j \\rangle\\) into \\(\\vert \\phi_i \\rangle\\). if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Case studies","url":"https://efavdb.com/daily-traffic-evolution-and-the-super-bowl","loc":"https://efavdb.com/daily-traffic-evolution-and-the-super-bowl"},{"title":"NBA learner: 2013-14 warmup","text":"We've spent the last couple of evenings training some preliminary algorithms on the NBA 2013-14, regular season data, which we grabbed from basketball-reference.com . Each of the 30 NBA teams play 82 times a season, summing to 1230 games total — a sizable number that we can comfortably attempt to model. Here, we cover our first pass at the prediction problem, what we've learned so far, and challenges we're looking forward to tackling soon. The first algorithm: As mentioned in the prior post, we decided to initially train only on historical win-loss data triples of the form (home team, away team, y), where the Boolean y equals one if the home team won, zero otherwise. For prediction, we use logistic classification: We attempt to identify which teams team \\(\\alpha\\) would likely beat, were they to play them at home. In order to accomplish this task, our logistic model has at its disposal a set of variable features characterizing each team: a home feature vector \\(\\textbf{H}_{\\alpha}\\) and an away feature vector \\(\\textbf{A}_{\\alpha}\\) , each of length 10. The model predicts a home team \\(\\alpha\\) win over away team \\(\\beta\\) probability of \\(h = 1/[1 + \\exp(- \\textbf{H}_{\\alpha} \\cdot \\textbf{A}_{\\beta})]\\) \\( \\in [0,1]\\) . In training, the model is initially fed random feature vectors, which are then relaxed to minimize the logistic cost function, \\(J \\equiv $$ - \\sum_{i = 1}&#94;{m} y_i \\log h_i$$ + (1-y_i) \\log (1 - h_i) $, where the sum is over all training examples. The cost function $J\\) heavily penalizes large mismatch between the actual outcome \\(y\\) and the predicted outcome \\(h\\) for any training example — we also added to this a suppression term that prevents over-fitting. Results: We trained the above model on the first 800 games of the 2013-14 season, and then tested the accuracy of the model on the remaining 430 games it did not train on. Sample output is shown in the figure. As you can see in the last line, the algorithm correctly predicted the outcome of 64% of these games. As a first pass, this compares favorably to, for example, the accuracy of the predictions provided by teamrankings.com (about 68% for the 2013-14 season). Further, after implementing a quick improvement to the first model above, basing predictions on prior score-differentials, rather than simply win-loss results, we managed to pop our accuracy up to 69% on the same data set. Caveats & future directions: Our comparison to teamrankings.com ( TR ) above isn't really a fair one. The reason is that our analysis was only carried out on the final 2/3rds of the last season, whereas TR 's average covered its entirety. Early-season prediction is necessarily less accurate for all bettors, given the paucity of relevant data available at that time. Nevertheless, we're encouraged by our first attempts here. To improve, we aim next to incorporate the information provided by prior seasons. A closely related challenge will be to figure out how to intelligently weight data according to its age: We want to be able to capture timely effects, like current momentum and injuries, while retaining all relevant long-term trends. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"NBA prediction project","url":"https://efavdb.com/nba-learner-2013-14-warmup","loc":"https://efavdb.com/nba-learner-2013-14-warmup"},{"title":"Announcing: NBA learner v0.1","text":"Tonight is the opening night of the 2014-15 NBA season. This year, we will be running a machine learning algorithm aimed at estimating underlying features characterizing each team. With these features, we hope to identify interesting match-ups (including potential upsets), similar team-playing-style categories, and win-loss probabilities for future games. As of now, the only source data that we intend to feed our system will be win-loss results of completed games. As the season progresses, our algorithm will thus have more and more data informing it — It will be interesting to see if it can begin to provide accurate predictions by the end of the season. Stay tuned for periodic updates on this experiment!","tags":"NBA prediction project","url":"https://efavdb.com/announcing-nba-learner-v0-1","loc":"https://efavdb.com/announcing-nba-learner-v0-1"},{"title":"Data reduction by PCA","text":"Here, we characterize the data compression benefits of projection onto a subset of the eigenvectors of our traffic system's covariance matrix. We address this compression from two different perspectives: First, we consider the partial traces of the covariance matrix, and second we present visual comparisons of the actual vs. projected traffic plots. Partial traces: From the footnote to our last post, we have \\(\\text{Tr}(H&#94;{-1}) \\equiv \\sum_i e_i = \\sum_a \\left (\\delta v_a \\right)&#94;2\\). That is, the trace of the covariance matrix tells us the net variance in our traffic system's speed, summed over all loops. More generally \\(&#94;1\\) , the fraction of system variance contained within some subset of the modes is given by the eigenvalue sum over these modes, all divided by \\(\\text{Tr}(H&#94;{-1})\\). The eigenvalues thus provide us with a simple method for quantifying the significance of any particular mode. At right, is a log-log-plot of the fractional-variance-captured for each mode, ordered from largest to smallest (we also include analogous plots for the covariance matrix associated with just one week and one month \\(&#94;2\\) ) As shown, the eigenvalues decay like one over eigenvalue index at first, but eventually begin to decay much more quickly. Only 5 modes are needed to capture 50% of the variance; 25 for 65%; 793 for 95%. Visualizations: The above discussion suggests that the basic essence of a given set of traffic conditions is determined by only the first few modes, but that a large number might be needed to get correct all details. We tested this conclusion by visually inspecting plots of projected traffic conditions (again, for Jan 15, at 5:30 pm), and comparing across number of modes retained. The results are striking: upon projecting the 2,000+ original features to only 25, minimal error appears to be introduced. Further, the error that does occur tends to be highly localized to the particularly slow regions, where projected speeds are overestimates ( e.g. , the traffic jams east/south-bound out of Oakland). Increasing the mode count to 100 or greater, these problem spots are quickly ameliorated, and the error is no longer systematic in slow regions (see insets). Conclusions: The data provided by the PEMS system is highly redundant — as anticipated — in the sense that traffic conditions can be determined from far fewer measurements than it provides. If other states wanted to replicate this system, they could probably get away with reducing the number of measures by at least one order of magnitude per mile of highway. For our part, we intend to project our data onto the top 10% of the modes, or fewer: We anticipate that this will provide minimal loss, but substantial speedups. [1] Partial eigenvalue sums, physics perspective: Consider suppressing all modes that you don't want to include in a projection. This can be done by setting the energies of these modes to \\(\\infty\\) , which will result in their corresponding \\(H&#94;{-1}\\) eigenvalues going to zero. When this altered system is thermally driven, its variance will again be given its covariance trace. This altered system trace is precisely equal to the retained mode partial sum of the original matrix. [2] Sampling time dependence of covariance matrix: The first figure above shows the mode variance ratio for the first 1000 principal components over three time scales: 1 week, 1 month, and 2014 year to date (~9.5 months). Notice that the plots become more shallow given a longer sampling period. This is because the larger data sets exhibit a more diverse class of fluctuations, and more modes are needed to capture these. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Methods","url":"https://efavdb.com/data-reduction-by-pca","loc":"https://efavdb.com/data-reduction-by-pca"},{"title":"Traffic patterns of the year: 2014 edition","text":"As we mentioned in the last post, there are currently over 2000 active speed loop detectors within the Bay Area highway system. The information provided by these loops is often highly redundant because speeds at neighboring sites typically differ little from one another. This observation suggests that a higher level, \"macro\" picture of traffic conditions could provide more insight: Rather than stating the speed at each detector, we might instead offer info like \"101S is rather slow right now\". In fact, we aim to characterize traffic conditions as efficiently as possible. To move towards this goal, we have carried out a principal component analysis ( PCA ) \\(&#94;1\\) of the full 2014 (year to date) PEMS data set. As described in [1] below, PCA provides us with a slick, automated method for identifying the most common \"traffic patterns\" or \"modes\" that get excited in our system. By adding together these patterns — with appropriate time-specific amplitudes — we can reconstruct the site-by-site traffic conditions observed at any particular moment. Importantly, summing over only the most significant modes will provide us with a system-tailored, minimal-loss method of data compression that will simplify our later prediction analysis. We will discuss this compression benefit further in the next post. Here, we present the two dominant modes of the Bay Area traffic system (see figures above). Notice that the first is fairly uniform, which presumably captures some nearly-site-independent changes in mean speed associated with night vs. daytime driving. In contrast, the second mode captures some interesting structure, showing slowdowns for some highways/directions and speedups for others. Evidently, this structure is the second most highly exhibited pattern in the Bay Area system; We couldn't have intuited this pattern, but it has been captured automatically via our PCA . [1] *Statistical physics of PCA : * One way of thinking about PCA as applied here is to imagine that the traffic system is harmonic. That is, we suppose that the traffic dynamics observed can be characterized by an energy cost function that is quadratic in the speeds of the different loops, measured relative to their average values, \\(E = \\frac{\\beta&#94;{-1}}{2} \\delta \\textbf{v}&#94;{T} \\cdot H \\cdot \\delta \\textbf{v}.\\) Here, \\(\\delta v_i = v_i - \\langle v_i \\rangle $ and $H\\) is a matrix Hamiltonian. Under some effective, thermal driving , the pair correlation for two sites will be given by \\(\\langle \\delta v_a \\delta v_b \\rangle \\equiv $$ \\frac{1}{Z} \\int_{{\\delta \\textbf{v}_i }} e&#94;{- \\frac{1}{2} \\delta \\textbf{v}&#94;{T} \\cdot H \\cdot \\delta \\textbf{v}} \\delta v_a \\delta v_b =$$ H&#94;{-1}_{ab}\\) . It is this pair correlation function that is measured when one carries out a PCA analysis, and the matrix \\(H&#94;{-1}\\) is called the covariance matrix. Its eigenvectors are the modes of the system — the independent traffic patterns that we discuss above. The low lying modes are those with a larger \\(H&#94;{-1}\\) eigenvalue. These have low energy, are consequently often highly excited, and generally dominate the traffic conditions that we observe. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Case studies","url":"https://efavdb.com/traffic-patterns-of-the-year-2014-edition","loc":"https://efavdb.com/traffic-patterns-of-the-year-2014-edition"},{"title":"Obtaining and visualizing traffic data","text":"In our first set of posts here, we explore the possibility of using historical traffic data to train a machine learning algorithm capable of predicting near-term highway conditions — say, up to an hour into the future, at any given time. To try our hand at this, we will be working with publicly available data provided by the California Performance Measurement System ( PEMS ) . The data provided by PEMS takes the form of time-averaged speed measurements for a large set of points throughout the California highway system ( \\(\\gtrsim 2000\\) in the Bay Area alone). These speeds are measured using devices called \"inductive-loop detectors \\(&#94;1\\) \" that are embedded just below the pavement at each site of interest. The same sort of devices are used at traffic lights to detect waiting vehicles: If you have ever noticed what looks like a circular or rectangular cut in the concrete at a traffic light, that's what that is — informative youtube video on how bikers might more easily trigger these . As we dissect the traffic data, we will require a visualization tool. We have developed a tool to do this from scratch in Python \\(&#94;2\\) . This tool plots by color the speed of traffic along each highway. The figure above provides an example (traffic conditions for Jan 15 2014 at 5:30 pm) on top of a silhouette background of the Bay Area (courtesy of Lester Lee ). Stay tuned for updates on this and other related projects! [1] Aside on inductive loop detectors : Inductive detectors are essentially large wire loops (solenoids) that are constantly being driven by an alternating current source. When a large metallic object (car) is above the loop, the voltage needed to drive the current changes (see below). This effect allows the loops to infer vehicle proximity. In order to estimate speeds, average vehicle-loop crossing times are combined with predetermined average vehicle lengths. [To see why the voltage across a loop changes as a car passes, you can play with these equations: \\( V = L \\partial_t I\\) , \\(\\Phi = L I\\) , \\(\\nabla \\times \\textbf{E} = - \\partial_t \\textbf{B}\\) , \\(\\nabla \\times \\textbf{H} = \\textbf{J}_f\\) . See also link . [2] Aside on plotting algorithm : Traffic data on the PEMS website is provided via downloadable text files (1 file per day). Each file provides average traffic speed data, for each five minute window period in a day, for each functioning detector. The detectors themselves are identified with a 6-digit ID number. The latitude and longitude of these detectors, as well as which highway they are on, the direction of the highway, and their absolute mile marker position, are located in separate meta data files. We simply plot a line between each adjacent pair of detectors on a given highway, with color determined by the average speed of the two. North-South / East-West highway counterparts are separated by a small space by shifting their position perpendicular to the local highway tangent vector at each point. Missing data is imputed via the value of the nearest functional detector. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Case studies","url":"https://efavdb.com/obtaining-and-visualizing-traffic-data","loc":"https://efavdb.com/obtaining-and-visualizing-traffic-data"}]};