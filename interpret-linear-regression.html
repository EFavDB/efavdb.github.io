<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>Interpreting the results of linear regression</title>


  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
<link href="./interpret-linear-regression.html" rel="canonical" />
  <!-- Feed -->

  <link href="./theme/css/style.css" type="text/css" rel="stylesheet" />

  <!-- Code highlight color scheme -->
      <link href="./theme/css/code_blocks/github.css" rel="stylesheet">


  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->



    <meta name="description" content="Our last post showed how to obtain the least-squares solution for linear regression and discussed the idea of sampling variability in...">

    <meta name="author" content="Cathy Yeh">

    <meta name="tags" content="statistics">




<!-- Open Graph -->
<meta property="og:site_name" content="EFAVDB"/>
<meta property="og:title" content="Interpreting the results of linear regression"/>
<meta property="og:description" content="Our last post showed how to obtain the least-squares solution for linear regression and discussed the idea of sampling variability in..."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="./interpret-linear-regression.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2016-06-29 14:54:00-07:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="./author/cathy-yeh.html">
<meta property="article:section" content="Methods, Theory"/>
<meta property="article:tag" content="statistics"/>
<meta property="og:image" content="./theme/images/post-bg.jpg">

<!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@efavdb">
    <meta name="twitter:title" content="Interpreting the results of linear regression">
    <meta name="twitter:url" content="./interpret-linear-regression.html">

        <meta name="twitter:image:src" content="./theme/images/post-bg.jpg">

      <meta name="twitter:description" content="Our last post showed how to obtain the least-squares solution for linear regression and discussed the idea of sampling variability in...">

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "Interpreting the results of linear regression",
  "headline": "Interpreting the results of linear regression",
  "datePublished": "2016-06-29 14:54:00-07:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Cathy Yeh",
    "url": "./author/cathy-yeh.html"
  },
  "image": "./theme/images/post-bg.jpg",
  "url": "./interpret-linear-regression.html",
  "description": "Our last post showed how to obtain the least-squares solution for linear regression and discussed the idea of sampling variability in..."
}
</script>
</head>
<!-- TODO : Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>
          <li><a href="/" role="presentation">Home</a></li>
          <li><a href="/pages/about.html" role="presentation">About & Consulting</a></li>
          <li><a href="/archives.html" role="presentation">Archive</a></li>
          <li><a href="/tags.html" role="presentation">Tags</a></li>
          <li><a href="/pages/linselect.html" role="presentation">linselect - feature selection</a></li>


    </ul>
  </div>
</nav>
    <!-- Progressbar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header id="post-header" >
      <div class="inner">
        <nav id="navigation">
            <span id="home-button" class="nav-button">
                <a class="home-button" href="./" title="Home"><i class="ic ic-arrow-left"></i> Home</a>
            </span>
          <span id="menu-button" class="nav-button">
            <a class="menu-button"><i class="ic ic-menu"></i> Menu</a>
          </span>
        </nav>
        <h1 class="post-title">Interpreting the results of linear regression</h1>
        <!-- TODO : Proper class for headline -->
        <span class="post-meta">
                <a href="./author/cathy-yeh.html">Cathy Yeh</a>
            | <time datetime="Wed 29 June 2016">Wed 29 June 2016</time>
        </span>
        <!-- TODO : Modified check -->
      </div>
    </header>

  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
            <section class="post-content">
                <p>Our <a href="http://efavdb.com/linear-regression/">last post</a> showed how to obtain the least-squares solution for linear regression and discussed the idea of sampling variability in the best estimates for the coefficients. In this post, we continue the discussion about uncertainty in linear regression -- both in the estimates of individual linear regression coefficients and the quality of the overall fit.</p>
<p>Specifically, we'll discuss how to calculate the 95% confidence intervals and p-values from hypothesis tests that are output by many statistical packages like python's statsmodels or R. An example with code is provided at the end.</p>
<h2>Review</h2>
<p>We wish to predict a scalar response variable <span class="math">\(y_i\)</span> given a vector of predictors <span class="math">\(\vec{x}_i\)</span> of dimension <span class="math">\(K\)</span>. In linear regression, we assume that <span class="math">\(y_i\)</span> is a linear function of <span class="math">\(\vec{x}_i\)</span>, parameterized by a set of coefficients <span class="math">\(\vec{\beta}\)</span> and an error term <span class="math">\(\epsilon_i\)</span>. The linear model (in matrix format and dropping the arrows over the vectors) for predicting <span class="math">\(N\)</span> response variables is<br>
</p>
<div class="math">\begin{eqnarray}\tag{1}  
y = X\beta + \epsilon.  
\end{eqnarray}</div>
<p>The dimensions of each component are: dim(<span class="math">\(X\)</span>) = (<span class="math">\(N\)</span>,<span class="math">\(K\)</span>), dim(<span class="math">\(\beta\)</span>) = (<span class="math">\(K\)</span>,1), dim(<span class="math">\(y\)</span>) = dim(<span class="math">\(\epsilon\)</span>) = (<span class="math">\(N\)</span>,1), where <span class="math">\(N\)</span> = # of examples, <span class="math">\(K\)</span> = # of regressors / predictors, counting an optional intercept/constant term.</p>
<p>The ordinary least-squares best estimator of the coefficients, <span class="math">\(\hat{\beta}\)</span>, was <a href="http://efavdb.com/linear-regression/#mjx-eqn-optimal">derived last time</a>:<br>
</p>
<div class="math">\begin{eqnarray}\tag{2}\label{optimal}  
\hat{\beta} = (X'X)^{-1}X'y,  
\end{eqnarray}</div>
<p>where the hat "^" denotes an estimator, not a true population parameter.</p>
<p>(\ref{optimal}) is a point estimate, but fitting different samples of data from the population will cause the best estimators to shift around. The amount of shifting can be explained by the variance-covariance matrix of <span class="math">\(\hat{\beta}\)</span>, <a href="http://efavdb.com/linear-regression/#mjx-eqn-cov">also derived</a> last time (independent of assumptions of normality):<br>
</p>
<div class="math">\begin{eqnarray}\tag{3}\label{cov}  
cov(\hat{\beta}, \hat{\beta}) = \sigma^2 (X'X)^{-1}.  
\end{eqnarray}</div>
<h2>Goodness of fit - <span class="math">\(R^2\)</span></h2>
<p>To get a better feel for (\ref{cov}), it's helpful to rewrite it in terms of the coefficient of determination <span class="math">\(R^2\)</span>. <span class="math">\(R^2\)</span> measures how much of the variation in the response variable <span class="math">\(y\)</span> is explained by variation in the regressors <span class="math">\(X\)</span> (as opposed to the unexplained variation from <span class="math">\(\epsilon\)</span>).</p>
<p>The variation in <span class="math">\(y\)</span>, i.e. the "total sum of squares" <span class="math">\(SST\)</span>, can be partitioned into the sum of two terms, "regression sum of squares" and "error sum of squares": <span class="math">\(SST = SSR + SSE\)</span>.</p>
<p>For convenience, let's center <span class="math">\(y\)</span> and <span class="math">\(X\)</span> around their means, e.g. <span class="math">\(y \rightarrow y - \bar{y}\)</span> so that the mean <span class="math">\(\bar{y}=0\)</span> for the centered variables. Then,<br>
</p>
<div class="math">\begin{eqnarray}\tag{4}\label{SS}  
SST &amp;=&amp; \sum_i^N (y - \bar{y})^2 = y'y \  
SSR &amp;=&amp; \sum_i^N (X\hat{\beta} - \bar{y})^2 = \hat{y}'\hat{y} \  
SSE &amp;=&amp; \sum_i^N (y - \hat{y})^2 = e'e,  
\end{eqnarray}</div>
<p>where <span class="math">\(\hat{y} \equiv X\hat{\beta}\)</span>. Then <span class="math">\(R^2\)</span> is defined as the ratio of the regression sum of squares to the total sum of squares:<br>
</p>
<div class="math">\begin{eqnarray}\tag{5}\label{R2}  
R^2 \equiv \frac{SSR}{SST} = 1 - \frac{SSE}{SST}  
\end{eqnarray}</div>
<p><span class="math">\(R^2\)</span> ranges between 0 and 1, with 1 being a perfect fit. According to (\ref{cov}), the variance of a single coefficient <span class="math">\(\hat{\beta}_k\)</span> is proportional to the quantity <span class="math">\((X'X)_{kk}^{-1}\)</span>, where <span class="math">\(k\)</span> denotes the kth diagonal element of <span class="math">\((X'X)^{-1}\)</span>, and can be rewritten as<br>
</p>
<div class="math">\begin{eqnarray}\tag{6}\label{cov2}  
var(\hat{\beta}_k) = \sigma^2 (X'X)_{kk}^{-1} = \frac{\sigma^2}{(1 - R_k^2)\sum_i^N (x_{ik} - \bar{x}_k)^2},  
\end{eqnarray}</div>
<p>where <span class="math">\(R_k^2\)</span> is the <span class="math">\(R^2\)</span> in the regression of the kth variable, <span class="math">\(x_k\)</span>, against the other predictors <a href="#A1">[A1]</a>.</p>
<p>The key observation from (\ref{cov2}) is that the precision in the estimator decreases if the fit is made over highly correlated regressors, for which <span class="math">\(R_k^2\)</span> approaches 1. This problem of multicollinearity in linear regression will be manifested in our simulated example.</p>
<p>(\ref{cov2}) is also consistent with the observation from our previous post that, all things being equal, the precision in the estimator increases if the fit is made over a direction of greater variance in the data.</p>
<p>In the next section, <span class="math">\(R^2\)</span> will again be useful for interpreting the behavior of one of our test statistics.</p>
<h2>Calculating test statistics</h2>
<p>If we assume that the vector of residuals has a multivariate normal distribution, <span class="math">\(\epsilon \sim N(0, \sigma^2I)\)</span>, then we can construct test statistics to characterize the uncertainty in the regression. In this section, we'll calculate</p>
<p>​(a) <strong>confidence intervals</strong> - random intervals around individual estimators <span class="math">\(\hat{\beta}_k\)</span> that, if constructed for regressions over multiple samples, would contain the true population parameter, <span class="math">\(\beta_k\)</span>, a certain fraction, e.g. 95%, of the time.<br>
(b) <strong>p-value</strong> - the probability of events as extreme or more extreme than an observed value (a test statistic) occurring under the null hypothesis. If the p-value is less than a given significance level <span class="math">\(\alpha\)</span> (a common choice is <span class="math">\(\alpha = 0.05\)</span>), then the null hypothesis is rejected, e.g. a regression coefficient is said to be significant.</p>
<p>From the assumption of the distribution of <span class="math">\(\epsilon\)</span>, it follows that <span class="math">\(\hat{\beta}\)</span> has a multivariate normal distribution <a href="#A2">[A2]</a>:<br>
</p>
<div class="math">\begin{eqnarray}\tag{7}  
\hat{\beta} \sim N(\beta, \sigma^2 (X'X)^{-1}).  
\end{eqnarray}</div>
<p> To be explicit, a single coefficient, <span class="math">\(\hat{\beta}_k\)</span>, is distributed as<br>
</p>
<div class="math">\begin{eqnarray}\tag{8}  
\hat{\beta}_k \sim N(\beta_k, \sigma^2 (X'X)_{kk}^{-1}).  
\end{eqnarray}</div>
<p>This variable can be standardized as a z-score:<br>
</p>
<div class="math">\begin{eqnarray}\tag{9}  
z_k = \frac{\hat{\beta}_k - \beta_k}{\sigma^2 (X'X)_{kk}^{-1}} \sim N(0,1)  
\end{eqnarray}</div>
<p>In practice, we don't know the population parameter, <span class="math">\(\sigma^2\)</span>, so we can't use the z-score. Instead, we can construct a pivotal quantity, a t-statistic. The t-statistic for <span class="math">\(\hat{\beta}_k\)</span> follows a t-distribution with n-K degrees of freedom <a href="#ref1">[1]</a>,<br>
</p>
<div class="math">\begin{eqnarray}\tag{10}\label{tstat}  
t_{\hat{\beta}_k} = \frac{\hat{\beta}_k - \beta_k}{s(\hat{\beta}_k)} \sim t_{n-K},  
\end{eqnarray}</div>
<p> where <span class="math">\(s(\hat{\beta}_k)\)</span> is the standard error of <span class="math">\(\hat{\beta}_k\)</span><br>
</p>
<div class="math">\begin{eqnarray}\tag{11}  
s(\hat{\beta}_k)^2 = \hat{\sigma}^2 (X'X)_{kk}^{-1},  
\end{eqnarray}</div>
<p> and <span class="math">\(\hat{\sigma}^2\)</span> is the unbiased estimator of <span class="math">\(\sigma^2\)</span><br>
</p>
<div class="math">\begin{eqnarray}\tag{12}  
\hat{\sigma}^2 = \frac{\epsilon'\epsilon}{n - K}.  
\end{eqnarray}</div>
<h3>Confidence intervals around regression coefficients</h3>
<p>The <span class="math">\((1-\alpha)\)</span> confidence interval around an estimator, <span class="math">\(\hat{\beta}_k \pm \Delta\)</span>, is defined such that the probability of a random interval containing the true population parameter is <span class="math">\((1-\alpha)\)</span>:<br>
</p>
<div class="math">\begin{eqnarray}\tag{13}  
P[\hat{\beta}_k - \Delta &lt; \beta_k &lt; \hat{\beta}_k + \Delta ] = 1 - \alpha,  
\end{eqnarray}</div>
<p> where <span class="math">\(\Delta = t_{1-\alpha/2, n-K} s(\hat{\beta}_k)\)</span>, and <span class="math">\(t_{1-\alpha/2, n-K}\)</span> is the <span class="math">\(\alpha/2\)</span>-level critical value for the t-distribution with <span class="math">\(n-K\)</span> degrees of freedom.</p>
<h3>t-test for the significance of a predictor</h3>
<p>Directly related to the calculation of confidence intervals is testing whether a regressor, <span class="math">\(\hat{\beta}_k\)</span>, is statistically significant. The t-statistic for the kth regression coefficient under the null hypothesis that <span class="math">\(x_k\)</span> and <span class="math">\(y\)</span> are independent follows a t-distribution with n-K degrees of freedom, c.f. (\ref{tstat}) with <span class="math">\(\beta_k = 0\)</span>:<br>
</p>
<div class="math">\begin{eqnarray}\tag{14}  
t = \frac{\hat{\beta}_k - 0}{s(\hat{\beta}_k)} \sim t_{n-K}.  
\end{eqnarray}</div>
<p>We reject the null-hypothesis if <span class="math">\(P[t] &lt; \alpha\)</span>.</p>
<p>According to (\ref{cov2}), <span class="math">\(s(\hat{\beta}_k)\)</span> increases with multicollinearity. Hence, the estimator must be more "extreme" in order to be statistically significant in the presence of multicollinearity.</p>
<h3>F-test for the significance of the regression</h3>
<p>Whereas the t-test considers the significance of a single regressor, the F-test evaluates the significance of the entire regression, where the null hypothesis is that <em>all</em> the regressors except the constant are equal to zero: <span class="math">\(\hat{\beta}_1 = \hat{\beta}_2 = ... = \hat{\beta}_{K-1} = 0\)</span>.</p>
<p>The F-statistic under the null hypothesis follows an F-distribution with {K-1, N-K} degrees of freedom <a href="#ref1">[1]</a>:<br>
</p>
<div class="math">\begin{eqnarray}\tag{15}\label{F}  
F = \frac{SSR/(K-1)}{SSE/(N-K)} \sim F_{K-1, N-K}.  
\end{eqnarray}</div>
<p>It is useful to rewrite the F-statistic in terms of <span class="math">\(R^2\)</span> by substituting the expressions from (\ref{SS}) and (\ref{R2}):<br>
</p>
<div class="math">\begin{eqnarray}\tag{16}\label{F2}  
F = \frac{(N-K) R^2}{(K-1) (1-R^2)}  
\end{eqnarray}</div>
<p>Notice how, for fixed <span class="math">\(R^2\)</span>, the F-statistic decreases with an increasing number of predictors <span class="math">\(K\)</span>. Adding uninformative predictors to the model will decrease the significance of the regression, which motivates parsimony in constructing linear models.</p>
<h2>Example</h2>
<p>With these formulas in hand, let's consider the problem of predicting the weight of adult women using some simulated data (loosely based on reality). We'll look at two models:<br>
(1) <strong>weight ~ height</strong>.<br>
As expected, height will be a strong predictor of weight, corroborated by a significant p-value for the coefficient of height in the model.<br>
(2) <strong>weight ~ height + shoe size</strong>.<br>
Height and shoe size are strongly correlated in the simulated data, while height is still a strong predictor of weight. We'll find that neither of the predictors has a significant individual p-value, a consequence of collinearity.</p>
<p>First, import some libraries. We use <code>statsmodels.api.OLS</code> for the linear regression since it contains a much more detailed report on the results of the fit than <code>sklearn.linear_model.LinearRegression</code>.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>  
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>  
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">t</span>  
<span class="kn">import</span> <span class="nn">random</span>  
</pre></div>


<p>Next, set the population parameters for the simulated data.  </p>
<div class="highlight"><pre><span></span><span class="o">#</span> <span class="n">height</span> <span class="p">(</span><span class="n">inches</span><span class="p">)</span>  
<span class="n">mean_height</span> <span class="o">=</span> <span class="mi">65</span>  
<span class="n">std_height</span> <span class="o">=</span> <span class="mi">2</span><span class="p">.</span><span class="mi">25</span>

<span class="o">#</span> <span class="n">shoe</span> <span class="k">size</span> <span class="p">(</span><span class="n">inches</span><span class="p">)</span>  
<span class="n">mean_shoe_size</span> <span class="o">=</span> <span class="mi">7</span><span class="p">.</span><span class="mi">5</span>  
<span class="n">std_shoe_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">.</span><span class="mi">25</span>

<span class="o">#</span> <span class="n">correlation</span> <span class="k">between</span> <span class="n">height</span> <span class="k">and</span> <span class="n">shoe</span> <span class="k">size</span>  
<span class="n">r_height_shoe</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">98</span> <span class="o">#</span> <span class="n">height</span> <span class="k">and</span> <span class="n">shoe</span> <span class="k">size</span> <span class="k">are</span> <span class="n">highly</span> <span class="n">correlated</span>  
<span class="o">#</span> <span class="n">covariance</span> <span class="n">b</span><span class="o">/</span><span class="n">w</span> <span class="n">height</span> <span class="k">and</span> <span class="n">shoe</span> <span class="k">size</span>  
<span class="n">var_height_shoe</span> <span class="o">=</span> <span class="n">r_height_shoe</span><span class="o">*</span><span class="n">std_height</span><span class="o">*</span><span class="n">std_shoe_size</span>

<span class="o">#</span> <span class="n">matrix</span> <span class="k">of</span> <span class="n">means</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="k">and</span> <span class="n">covariance</span><span class="p">,</span> <span class="n">cov</span>  
<span class="n">mu</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_height</span><span class="p">,</span> <span class="n">mean_shoe_size</span><span class="p">)</span>  
<span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">std_height</span><span class="p">),</span> <span class="n">var_height_shoe</span><span class="p">],</span>  
<span class="p">[</span><span class="n">var_height_shoe</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">std_shoe_size</span><span class="p">)]]</span>  
</pre></div>


<p>Generate the simulated data:  </p>
<div class="highlight"><pre><span></span><span class="o">#</span> <span class="nb">number</span> <span class="k">of</span> <span class="k">data</span> <span class="n">points</span>  
<span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">85</span><span class="p">)</span>  
<span class="o">#</span> <span class="n">height</span> <span class="k">and</span> <span class="n">shoe</span> <span class="k">size</span>  
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>  
<span class="o">#</span> <span class="n">height</span><span class="p">,</span> <span class="n">alone</span>  
<span class="n">X0</span> <span class="o">=</span> <span class="n">X1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

<span class="n">weight</span> <span class="o">=</span> <span class="o">-</span><span class="mi">220</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">X0</span><span class="o">*</span><span class="mi">5</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>  
</pre></div>


<p>Below is the simulated data plotted against each other.<br>
<a href="./wp-content/uploads/2016/06/scatter_height_weight_shoesize_cropped.png"><img alt="scatterplots" src="./wp-content/uploads/2016/06/scatter_height_weight_shoesize_cropped.png"></a></p>
<p>Fit the linear models:  </p>
<div class="highlight"><pre><span></span><span class="o">#</span> <span class="k">add</span> <span class="k">column</span> <span class="k">of</span> <span class="n">ones</span> <span class="k">for</span> <span class="n">intercept</span>  
<span class="n">X0</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X0</span><span class="p">)</span>  
<span class="n">X1</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span>

<span class="o">#</span> <span class="ss">&quot;OLS&quot;</span> <span class="n">stands</span> <span class="k">for</span> <span class="n">Ordinary</span> <span class="n">Least</span> <span class="n">Squares</span>  
<span class="n">sm0</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">X0</span><span class="p">).</span><span class="n">fit</span><span class="p">()</span>  
<span class="n">sm1</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">X1</span><span class="p">).</span><span class="n">fit</span><span class="p">()</span>  
</pre></div>


<p>Look at the summary report, <code>sm0.summary()</code>, for the weight ~ height model.  </p>
<div class="highlight"><pre><span></span><span class="err">OLS Regression Results  </span>
<span class="err">==============================================================================  </span>
<span class="err">Dep. Variable: y R-squared: 0.788  </span>
<span class="c">Model: OLS Adj. R-squared: 0.776  </span>
<span class="c">Method: Least Squares F-statistic: 66.87  </span>
<span class="c">Date: Wed, 29 Jun 2016 Prob (F-statistic): 1.79e-07  </span>
<span class="c">Time: 14:28:08 Log-Likelihood: -70.020  </span>
<span class="err">No. Observations: 20 AIC: 144.0  </span>
<span class="err">Df Residuals: 18 BIC: 146.0  </span>
<span class="err">Df Model: 1  </span>
<span class="err">Covariance Type: nonrobust  </span>
<span class="err">==============================================================================  </span>
<span class="err">coef std err t P&gt;|t| [95.0% Conf. Int.]  </span>
<span class="err">------------------------------------------------------------------------------  </span>
<span class="err">const -265.2764 49.801 -5.327 0.000 -369.905 -160.648  </span>
<span class="err">x1 6.1857 0.756 8.178 0.000 4.596 7.775  </span>
<span class="err">==============================================================================  </span>
<span class="c">Omnibus: 0.006 Durbin-Watson: 2.351  </span>
<span class="err">Prob(Omnibus): 0.997 Jarque-Bera (JB): 0.126  </span>
<span class="c">Skew: 0.002 Prob(JB): 0.939  </span>
<span class="c">Kurtosis: 2.610 Cond. No. 1.73e+03  </span>
<span class="err">==============================================================================  </span>
</pre></div>


<p>The height variable, <code>x1</code>, is significant according to the t-test, as is the intercept, denoted <code>const</code> in the report. Also, notice the coefficient used to simulate the dependence of weight on height (<span class="math">\(\beta_1\)</span> = 5.5), is contained in the 95% confidence interval of <code>x1</code>.</p>
<p>Next, let's look at the summary report, <code>sm1.summary()</code>, for the weight ~ height + shoe_size model.  </p>
<div class="highlight"><pre><span></span><span class="err">OLS Regression Results  </span>
<span class="err">==============================================================================  </span>
<span class="err">Dep. Variable: y R-squared: 0.789  </span>
<span class="c">Model: OLS Adj. R-squared: 0.765  </span>
<span class="c">Method: Least Squares F-statistic: 31.86  </span>
<span class="c">Date: Wed, 29 Jun 2016 Prob (F-statistic): 1.78e-06  </span>
<span class="c">Time: 14:28:08 Log-Likelihood: -69.951  </span>
<span class="err">No. Observations: 20 AIC: 145.9  </span>
<span class="err">Df Residuals: 17 BIC: 148.9  </span>
<span class="err">Df Model: 2  </span>
<span class="err">Covariance Type: nonrobust  </span>
<span class="err">==============================================================================  </span>
<span class="err">coef std err t P&gt;|t| [95.0% Conf. Int.]  </span>
<span class="err">------------------------------------------------------------------------------  </span>
<span class="err">const -333.1599 204.601 -1.628 0.122 -764.829 98.510  </span>
<span class="err">x1 7.4944 3.898 1.923 0.071 -0.729 15.718  </span>
<span class="err">x2 -2.3090 6.739 -0.343 0.736 -16.527 11.909  </span>
<span class="err">==============================================================================  </span>
<span class="c">Omnibus: 0.015 Durbin-Watson: 2.342  </span>
<span class="err">Prob(Omnibus): 0.993 Jarque-Bera (JB): 0.147  </span>
<span class="c">Skew: 0.049 Prob(JB): 0.929  </span>
<span class="c">Kurtosis: 2.592 Cond. No. 7.00e+03  </span>
<span class="err">==============================================================================  </span>
</pre></div>


<p>Neither of the regressors <code>x1</code> and <code>x2</code> is significant at a significance level of <span class="math">\(\alpha=0.05\)</span>. In the simulated data, adult female weight has a positive linear correlation with height and shoe size, but the strong collinearity of the predictors (simulated with a correlation coefficient of 0.98) causes each variable to fail a t-test in the model -- and even results in the wrong sign for the dependence on shoe size.</p>
<p>Although the predictors fail individual t-tests, the overall regression <em>is</em> significant, i.e. the predictors are jointly informative, according to the F-test.</p>
<p>Notice, however, that the p-value of the F-test has decreased compared to the simple linear model, as expected from (\ref{F2}), since including the extra variable, shoe size, did not improve <span class="math">\(R^2\)</span> but did increase <span class="math">\(K\)</span>.</p>
<p>Let's manually calculate the standard error, t-statistics, F-statistic, corresponding p-values, and confidence intervals using the equations from above.  </p>
<div class="highlight"><pre><span></span><span class="o">#</span> <span class="n">OLS</span> <span class="n">solution</span><span class="p">,</span> <span class="n">eqn</span> <span class="k">of</span> <span class="n">form</span> <span class="n">ax</span><span class="o">=</span><span class="n">b</span> <span class="o">=&gt;</span> <span class="p">(</span><span class="n">X</span><span class="s1">&#39;X)*beta_hat = X&#39;</span><span class="o">*</span><span class="n">y</span>  
<span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">solve</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X1</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X1</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X1</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">weight</span><span class="p">))</span>

<span class="o">#</span> <span class="n">residuals</span>  
<span class="n">epsilon</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">beta_hat</span><span class="p">)</span>

<span class="o">#</span> <span class="n">degrees</span> <span class="k">of</span> <span class="n">freedom</span> <span class="k">of</span> <span class="n">residuals</span>  
<span class="n">dof</span> <span class="o">=</span> <span class="n">X1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">X1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="o">#</span> <span class="n">best</span> <span class="n">estimator</span> <span class="k">of</span> <span class="n">sigma</span>  
<span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">/</span> <span class="n">dof</span><span class="p">)</span>

<span class="o">#</span> <span class="n">standard</span> <span class="n">error</span> <span class="k">of</span> <span class="n">beta_hat</span>  
<span class="n">s</span> <span class="o">=</span> <span class="n">sigma_hat</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X1</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X1</span><span class="p">)),</span> <span class="mi">0</span><span class="p">))</span>

<span class="o">#</span> <span class="mi">95</span><span class="o">%</span> <span class="n">confidence</span> <span class="n">intervals</span>  
<span class="o">#</span> <span class="o">+/-</span><span class="n">t_</span><span class="err">{</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="o">-</span><span class="n">K</span><span class="err">}</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nb">interval</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">,</span> <span class="n">dof</span><span class="p">)</span>  
<span class="n">conf_intervals</span> <span class="o">=</span> <span class="n">beta_hat</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">s</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">array</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="nb">interval</span><span class="p">(</span><span class="mi">0</span><span class="p">.</span><span class="mi">95</span><span class="p">,</span> <span class="n">dof</span><span class="p">))</span>

<span class="o">#</span> <span class="n">t</span><span class="o">-</span><span class="k">statistics</span> <span class="k">under</span> <span class="k">null</span> <span class="n">hypothesis</span>  
<span class="n">t_stat</span> <span class="o">=</span> <span class="n">beta_hat</span> <span class="o">/</span> <span class="n">s</span>

<span class="o">#</span> <span class="n">p</span><span class="o">-</span><span class="k">values</span>  
<span class="o">#</span> <span class="n">survival</span> <span class="k">function</span> <span class="n">sf</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">CDF</span>  
<span class="n">p_values</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">sf</span><span class="p">(</span><span class="k">abs</span><span class="p">(</span><span class="n">t_stat</span><span class="p">),</span> <span class="n">dof</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span>

<span class="o">#</span> <span class="n">SSR</span> <span class="p">(</span><span class="n">regression</span> <span class="k">sum</span> <span class="k">of</span> <span class="n">squares</span><span class="p">)</span>  
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">beta_hat</span><span class="p">)</span>  
<span class="n">y_mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>  
<span class="n">mean_SSR</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">((</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">y_mu</span><span class="p">).</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">y_mu</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">beta_hat</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="o">#</span> <span class="n">f</span><span class="o">-</span><span class="n">statistic</span>  
<span class="n">f_stat</span> <span class="o">=</span> <span class="n">mean_SSR</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">sigma_hat</span><span class="p">)</span>  
<span class="n">print</span><span class="p">(</span><span class="s1">&#39;f-statistic:&#39;</span><span class="p">,</span> <span class="n">f_stat</span><span class="p">,</span> <span class="s1">&#39;\n&#39;</span><span class="p">)</span>

<span class="o">#</span> <span class="n">p</span><span class="o">-</span><span class="n">value</span> <span class="k">of</span> <span class="n">f</span><span class="o">-</span><span class="n">statistic</span>  
<span class="n">p_values_f_stat</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">sf</span><span class="p">(</span><span class="k">abs</span><span class="p">(</span><span class="n">f_stat</span><span class="p">),</span> <span class="n">dfn</span><span class="o">=</span><span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">beta_hat</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dfd</span><span class="o">=</span><span class="n">dof</span><span class="p">)</span>  
<span class="n">print</span><span class="p">(</span><span class="s1">&#39;p-value of f-statistic:&#39;</span><span class="p">,</span> <span class="n">p_values_f_stat</span><span class="p">,</span> <span class="s1">&#39;\n&#39;</span><span class="p">)</span>  
</pre></div>


<p>The output values, below, from printing the manual calculations are consistent with the summary report:  </p>
<div class="highlight"><pre><span></span><span class="n">beta_hat</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">333</span><span class="p">.</span><span class="mi">15990097</span> <span class="mi">7</span><span class="p">.</span><span class="mi">49444671</span> <span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">30898743</span><span class="p">]</span>

<span class="n">degrees</span> <span class="k">of</span> <span class="n">freedom</span> <span class="k">of</span> <span class="n">residuals</span><span class="p">:</span> <span class="mi">17</span>

<span class="n">sigma_hat</span><span class="p">:</span> <span class="mi">8</span><span class="p">.</span><span class="mi">66991550428</span>

<span class="n">standard</span> <span class="n">error</span> <span class="k">of</span> <span class="n">beta_hat</span><span class="p">:</span> <span class="p">[</span> <span class="mi">204</span><span class="p">.</span><span class="mi">60056111</span> <span class="mi">3</span><span class="p">.</span><span class="mi">89776076</span> <span class="mi">6</span><span class="p">.</span><span class="mi">73900599</span><span class="p">]</span>

<span class="n">confidence</span> <span class="n">intervals</span><span class="p">:</span>  
<span class="p">[[</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">64829352</span><span class="n">e</span><span class="o">+</span><span class="mi">02</span> <span class="mi">9</span><span class="p">.</span><span class="mi">85095501</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">]</span>  
<span class="p">[</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">29109662</span><span class="n">e</span><span class="o">-</span><span class="mi">01</span> <span class="mi">1</span><span class="p">.</span><span class="mi">57180031</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">]</span>  
<span class="p">[</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">65270473</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span> <span class="mi">1</span><span class="p">.</span><span class="mi">19090724</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">]]</span>

<span class="n">t</span><span class="o">-</span><span class="k">statistics</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">62834305</span> <span class="mi">1</span><span class="p">.</span><span class="mi">92275698</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">34263027</span><span class="p">]</span>

<span class="n">p</span><span class="o">-</span><span class="k">values</span> <span class="k">of</span> <span class="n">t</span><span class="o">-</span><span class="k">statistics</span><span class="p">:</span> <span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">1218417</span> <span class="mi">0</span><span class="p">.</span><span class="mi">07142839</span> <span class="mi">0</span><span class="p">.</span><span class="mi">73607656</span><span class="p">]</span>  
<span class="n">f</span><span class="o">-</span><span class="n">statistic</span><span class="p">:</span> <span class="mi">31</span><span class="p">.</span><span class="mi">8556171105</span>

<span class="n">p</span><span class="o">-</span><span class="n">value</span> <span class="k">of</span> <span class="n">f</span><span class="o">-</span><span class="n">statistic</span><span class="p">:</span> <span class="mi">1</span><span class="p">.</span><span class="mi">77777555162</span><span class="n">e</span><span class="o">-</span><span class="mi">06</span>  
</pre></div>


<p>The full code is available as an <a href="https://github.com/EFavDB/linear-regression">IPython notebook on github</a>.</p>
<h2>Summary</h2>
<p>Assuming a multivariate normal distribution for the residuals in linear regression allows us to construct test statistics and therefore specify uncertainty in our fits.</p>
<p>A t-test judges the explanatory power of a predictor in isolation, although the standard error that appears in the calculation of the t-statistic is a function of the other predictors in the model. On the other hand, an F-test is a global test that judges the explanatory power of all the predictors together, and we've seen that parsimony in choosing predictors can improve the quality of the overall regression.</p>
<p>We've also seen that multicollinearity can throw off the results of individual t-tests as well as obscure the interpretation of the signs of the fitted coefficients. A symptom of multicollinearity is when none of the individual coefficients are significant but the overall F-test is significant.</p>
<h3>Reference</h3>
<p>[1] Greene, W., Econometric Analysis, Seventh edition, Prentice Hall, 2011 - <a href="http://people.stern.nyu.edu/wgreene/MathStat/Outline.htm">chapters available online</a></p>
<h3>Appendix</h3>
<p>[A1]<br>
We specifically want the kth diagonal element from the inverse moment matrix, <span class="math">\((X'X)^{-1}\)</span>. The matrix <span class="math">\(X\)</span> can be <a href="https://en.wikipedia.org/wiki/Block_matrix">partitioned</a> as </p>
<div class="math">$$[X_{(k)} \vec{x}_k],$$</div>
<p> where <span class="math">\(\vec{x}_k\)</span> is an N x 1 column vector containing the kth variable of each of the N samples, and <span class="math">\(X_{(k)}\)</span> is the N x (K-1) matrix containing the rest of the variables and constant intercept. For convenience, let <span class="math">\(X_{(k)}\)</span> and <span class="math">\(\vec{x}_k\)</span> be centered about their (column-wise) means.</p>
<p>Matrix multiplication of the block-partitioned form of <span class="math">\(X\)</span> with its transpose results in the following block matrix:<br>
</p>
<div class="math">\begin{eqnarray}  
(X'X) =  
\begin{bmatrix}  
X_{(k)}'X_{(k)} &amp; X_{(k)}'\vec{x}_k \  
\vec{x}_k'X_{(k)} &amp; \vec{x}_k'\vec{x}_k  
\end{bmatrix}  
\end{eqnarray}</div>
<p>The above matrix has four blocks, and <a href="https://en.wikipedia.org/wiki/Block_matrix#Block_matrix_inversion">can be inverted blockwise</a> to obtain another matrix with four blocks. The lower right block corresponding to the kth diagonal element of the inverted matrix is a scalar:<br>
</p>
<div class="math">\begin{eqnarray}  
(X'X)^{-1}_{kk} &amp;=&amp; [\vec{x}_k'\vec{x}_k - \vec{x}_k'X_{(k)}(X_{(k)}'X_{(k)})^{-1}X_{(k)}'\vec{x}_k]^{-1} \  
&amp;=&amp; \left[\vec{x}_k'\vec{x}_k \left( 1 - \frac{\vec{x}_k'X_{(k)}(X_{(k)}'X_{(k)})^{-1}X_{(k)}'\vec{x}_k}{\vec{x}_k'\vec{x}_k} \right)\right]^{-1}  
\end{eqnarray}</div>
<p>Then the numerator of the fraction in the parentheses above can be simplified:<br>
</p>
<div class="math">\begin{eqnarray}  
\vec{x}_k'X_{(k)} ((X_{(k)}'X_{(k)})^{-1}X_{(k)}'\vec{x}_k) &amp;=&amp; \vec{x}_k' X_{(k)} \hat{\beta}_{(k)} \  
&amp;=&amp; (X_{(k)}\hat{\beta}_{(k)} + \epsilon_k)'X_{(k)}\hat{\beta}_{(k)} \  
&amp;=&amp; \hat{x}_k'\hat{x}_k,  
\end{eqnarray}</div>
<p>where <span class="math">\(\hat{\beta}_{(k)}\)</span> is the OLS solution for the coefficients in the regression on the <span class="math">\(\vec{x}_k\)</span> by the remaining variables <span class="math">\(X_{(k)}\)</span>: <span class="math">\(\vec{x}_k = X_{(k)} \beta_{(k)} + \epsilon_k\)</span>. In the last line, we used one of the constraints on the residuals -- that the residuals and predictors are uncorrelated, <span class="math">\(\epsilon_k'X_{(k)} = 0\)</span>. Plugging in this simplification for the numerator and using the definition of <span class="math">\(R^2\)</span> from (\ref{R2}), we obtain our final result:<br>
</p>
<div class="math">\begin{eqnarray}  
(X'X)^{-1}_{kk} &amp;=&amp; \left[\vec{x}_k'\vec{x}_k \left( 1 - \frac{\hat{x}_k'\hat{x}_k}{\vec{x}_k'\vec{x}_k} \right)\right]^{-1} \  
&amp;=&amp; \left[\vec{x}_k'\vec{x}_k ( 1 - R_k^2 )\right]^{-1}  
\end{eqnarray}</div>
<p>[A2]<br>
</p>
<div class="math">\begin{eqnarray}  
\hat{\beta} &amp;=&amp; (X'X)^{-1}X'y \  
&amp;=&amp; (X'X)^{-1}X'(X\beta + \epsilon) \  
&amp;=&amp; \beta + (X'X)^{-1}X'N(0, \sigma^2I) \  
&amp; \sim &amp; N(\beta, \sigma^2 (X'X)^{-1})\  
\end{eqnarray}</div>
<p> The last line is by properties of <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Affine_transformation">affine transformations on multivariate normal distributions</a>.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </section>

            <section class="post-info">
                <div class="post-share">
                    <a class="twitter" href="https://twitter.com/share?text=Interpreting the results of linear regression&amp;url=./interpret-linear-regression.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="ic ic-twitter"></i><span class="hidden">Twitter</span>
                    </a>
                    <a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=./interpret-linear-regression.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="ic ic-facebook"></i><span class="hidden">Facebook</span>
                    </a>
                    <a class="googleplus" href="https://plus.google.com/share?url=./interpret-linear-regression.html" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <i class="ic ic-googleplus"></i><span class="hidden">Google+</span>
                    </a>
                    <div class="clear"></div>
                </div>

                <aside class="post-tags">
<a href="./tag/statistics.html">statistics</a>                </aside>

                <div class="clear"></div>

                <aside class="post-author">


                        <figure class="post-author-avatar">
                            <img src=".//wp-content/uploads/2014/12/cathy_photo.jpg" alt="Cathy Yeh" />
                        </figure>
                    <div class="post-author-bio">
                        <h4 class="post-author-name"><a href="./author/cathy-yeh.html">Cathy Yeh</a></h4>
                            <p class="post-author-about">Cathy Yeh got a PhD at UC Santa Barbara studying soft-matter/polymer physics. She is currently looking to transition to a career in data science after wrapping up work with the translational modeling group in a pharmaceutical company in San Diego. She enjoys mining big data and big ice cream and seeing how both can help make the world a better place.</p>
                        <!-- Social linkes in alphabet order. -->
                    </div>
                    <div class="clear"></div>
                </aside>

                </section>


                <aside class="post-nav">
                    <a class="post-nav-next" href="./metropolis.html">
                        <section class="post-nav-teaser">
                            <i class="ic ic-arrow-left"></i>
                                <h2 class="post-nav-title">Bayesian Statistics: MCMC</h2>
                            <p class="post-nav-excerpt">We review the Metropolis algorithm -- a simple Markov Chain Monte Carlo (MCMC)...</p>
                        </section>
                    </a>
                    <a class="post-nav-prev" href="./linear-regression.html">
                        <section class="post-nav-teaser">
                            <i class="ic ic-arrow-right"></i>
                                <h2 class="post-nav-title">Linear Regression</h2>
                            <p class="post-nav-excerpt">We review classical linear regression using vector-matrix notation. In particular, we...</p>
                        </section>
                    </a>
                    <div class="clear"></div>
                </aside>

            </div>
        </article>
    </main>
      <!-- TODO : Body class -->
    <div id="body-class" style="display: none;" class=""></div>

    <footer id="footer">
      <div class="inner">
        <section class="credits">
        </section>
      </div>
    </footer>
  </section>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script type="text/javascript" src="./theme/js/script.js"></script>

</body>
</html>