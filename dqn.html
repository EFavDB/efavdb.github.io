<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Cathy Yeh" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="machine learning, reinforcement learning, OpenAI, Machine learning, " />

<meta property="og:title" content="Q-learning and DQN "/>
<meta property="og:url" content="https://efavdb.com/dqn" />
<meta property="og:description" content="Q-learning is a reinforcement learning (RL) algorithm that is the basis for deep Q networks (DQN), the algorithm by Google DeepMind that achieved human-level performance for a range of Atari games and kicked off the deep RL revolution starting in 2013-2015. We begin with some historical context, then provide an …" />
<meta property="og:site_name" content="EFAVDB" />
<meta property="og:article:author" content="Cathy Yeh" />
<meta property="og:article:published_time" content="2020-04-06T00:00:00-07:00" />
<meta name="twitter:title" content="Q-learning and DQN ">
<meta name="twitter:description" content="Q-learning is a reinforcement learning (RL) algorithm that is the basis for deep Q networks (DQN), the algorithm by Google DeepMind that achieved human-level performance for a range of Atari games and kicked off the deep RL revolution starting in 2013-2015. We begin with some historical context, then provide an …">

        <title>Q-learning and DQN  · EFAVDB
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Fira+Sans:300,400,700" rel="stylesheet" type='text/css' />
        <link href="https://fonts.googleapis.com/css?family=Cardo:400,700" rel="stylesheet" type='text/css' />        
        <link rel="stylesheet" type="text/css" href="https://efavdb.com/theme/css/elegant.prod.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://efavdb.com/theme/css/custom.css" media="screen">

        <link href="https://efavdb.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="EFAVDB - Full Atom Feed" />
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-57405119-1', 'auto');
    ga('send', 'pageview');
</script>


    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://efavdb.com/"><span class=site-name>EFAVDB</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://efavdb.com
                                    >Home</a>
                                </li>
                                <li ><a href="https://efavdb.com/pages/authors.html">Authors</a></li>
                                <li ><a href="https://efavdb.com/categories">Categories</a></li>
                                <li ><a href="https://efavdb.com/tags">Tags</a></li>
                                <li ><a href="https://efavdb.com/archives">Archives</a></li>
                                <li><form class="navbar-search" action="https://efavdb.com/search" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span8 offset2">
        <h1>
            <a href="https://efavdb.com/dqn">
                Q-learning and <span class="caps">DQN</span>
            </a>
        </h1>
    </header>
    <div class="span2"></div>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <p>Q-learning is a reinforcement learning (<span class="caps">RL</span>) algorithm that is the basis for deep Q networks (<span class="caps">DQN</span>), the algorithm by Google DeepMind that achieved human-level performance for a range of Atari games and kicked off the deep <span class="caps">RL</span> revolution starting in&nbsp;2013-2015.</p>
<p>We begin with some historical context, then provide an overview of value function methods / Q-learning, and conclude with a discussion of <span class="caps">DQN</span>.</p>
<p>If you want to skip straight to code, the implementation of <span class="caps">DQN</span> that we used to train the agent playing Atari Breakout below is available <a href="https://github.com/frangipane/reinforcement-learning/tree/master/DQN">here</a>.</p>
<p align="center">
<img src="images/atari_breakout.gif" alt="Atari Breakout" style="width:250px;">
</p>

<p>If you watch the video long enough, you&#8217;ll see the agent has learned a strategy that favors breaking bricks at the edges so the ball &#8220;breaks out&#8221; to the upper side, resulting in a cascade of&nbsp;points.</p>
<h2>Historical&nbsp;context</h2>
<p>The theories that underpin today’s reinforcement learning algorithms were developed decades ago.  For example, Watkins developed Q-learning, a value function method, in <a href="http://www.cs.rhul.ac.uk/~chrisw/thesis.html">1989</a>, and Williams proposed the <span class="caps">REINFORCE</span> policy gradient method in <a href="https://link.springer.com/content/pdf/10.1007%2FBF00992696.pdf">1987</a>. So why the recent surge of interest in deep <span class="caps">RL</span>?</p>
<h3>Representational power from Neural&nbsp;Networks</h3>
<p>Until 2013, most applications of <span class="caps">RL</span> relied on hand engineered inputs for value function and policy representations, which drastically limited the scope of applicability to the real world.  Mnih et. al [1] made use of advances in computational power and neural network (<span class="caps">NN</span>) architectures to use a deep <span class="caps">NN</span> for <em>value function approximation</em>, showing that NNs can learn a useful representation from raw pixel inputs in Atari&nbsp;games.</p>
<h3>Variations on a theme: vanilla <span class="caps">RL</span> algorithms don’t work well&nbsp;out-of-the-box</h3>
<p>The basic <span class="caps">RL</span> algorithms that were developed decades ago do not work well in practice without modifications.  For example, <span class="caps">REINFORCE</span> relies on Monte Carlo estimates of the performance gradient; such estimates of the performance gradient are high variance, resulting in unstable or impractically slow learning (poor sample efficiency).  The original Q-learning algorithm also suffers from instability due to correlated sequential training data and parameter updates affecting both the estimator and target, creating a “moving target” and hence&nbsp;divergences.</p>
<p>We can think of these original <span class="caps">RL</span> algorithms as the Wright Brothers plane.
<p align="center">
<img src="images/wright_brothers_plane.png" alt="Wright brothers plane" style="width:500px;">
</p></p>
<p>The foundational shape is there and recognizable in newer models.  However, the enhancements of newer algorithms aren&#8217;t just bells and whistles &#8212; they have enabled the move from toy problems into more functional&nbsp;territory.</p>
<h2>Q-learning</h2>
<h3>Background</h3>
<p><span class="caps">RL</span> models the sequential decision-making problem as a Markov Decision Process (<span class="caps">MDP</span>): transitions from state to state involve both environment dynamics and an agent whose actions affect both the probability of transitioning to the next state and the reward&nbsp;received.</p>
<p>The goal is to find a policy, a mapping from state to actions, that will maximize the agent’s expected returns, i.e. their cumulative future&nbsp;rewards.</p>
<p>Q-learning is an algorithm for learning the eponymous <span class="math">\(Q(s,a)\)</span> action-value function, defined as the expected returns for each state-action <span class="math">\((s,a)\)</span> pair, corresponding to following the optimal&nbsp;policy.</p>
<h3>Goal: solve the Bellman optimality&nbsp;equation</h3>
<p>Recall that <span class="math">\(q_*\)</span> is described by a self-consistent, recursive relation, the Bellman optimality equation, that falls out from the Markov property [6, 7] of&nbsp;MDPs</p>
<div class="math">\begin{eqnarray}\label{action-value-bellman-optimality} \tag{1}
q_*(s, a) &amp;=&amp; \mathbb{E}_{\pi*} [R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}', a') | S_t = s, A_t = a] \\
          &amp;=&amp; \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a') ]
\end{eqnarray}</div>
<p>where <span class="math">\(0 \leq \gamma \leq 1\)</span> is the <em>discount rate</em> which characterizes how much we weight rewards now vs. later, <span class="math">\(R_{t+1}\)</span> is the reward at timestep <span class="math">\(t+1\)</span>, and <span class="math">\(p(s', r | s, a)\)</span> is the environment transition&nbsp;dynamics.</p>
<p>Our <a href="https://efavdb.com/intro-rl-toy-example.html">introduction to <span class="caps">RL</span></a> provides more background on the Bellman equations in case (\ref{action-value-bellman-optimality}) looks&nbsp;unfamiliar.</p>
<h3>The Q-learning approach to solving the Bellman&nbsp;equation</h3>
<p>We use capitalized <span class="math">\(Q\)</span> to denote an estimate and lowercase <span class="math">\(q\)</span> to denote the real action-value function.  The Q-learning algorithm makes the following&nbsp;update:</p>
<div class="math">\begin{eqnarray}\label{q-learning} \tag{2}
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)]
\end{eqnarray}</div>
<p>The quantity in square brackets in (\ref{q-learning}) is exactly 0 for the optimal action-value, <span class="math">\(q*\)</span>, based on (\ref{action-value-bellman-optimality}).  We can think of it as an error term, “the Bellman error”, that describes how far off the target quantity <span class="math">\(R_{t+1} + \gamma \max_a Q(S_{t+1}, a)\)</span> is from our current estimate <span class="math">\(Q(S_t, A_t)\)</span>.</p>
<p>The goal with Q-learning is to iteratively calculate (\ref{q-learning}), updating our estimate of <span class="math">\(Q\)</span> to reduce the Bellman error, until we have converged on a&nbsp;solution.</p>
<p><strong>Q-learning makes two&nbsp;approximations:</strong></p>
<p>I. It replaces the expectation value in (\ref{action-value-bellman-optimality}) with sampled estimates, similar to Monte Carlo estimates.  Unlike the dynamic programming approach we described in an earlier <a href="https://efavdb.com/dp-in-rl.html">post</a>, sampling is necessary since we don&#8217;t have access to the model of the environment, i.e. the environment transition&nbsp;dynamics.</p>
<p><span class="caps">II</span>. It replaces the target <span class="math">\(R_{t+1} + \max_a \gamma q_*(s’,a’)\)</span> in (\ref{action-value-bellman-optimality}), which contains the true action-value function <span class="math">\(q_*\)</span>, with the one-step temporal difference, <span class="caps">TD</span>(0), target <span class="math">\(R_{t+1} + \gamma \max_a Q(S_{t+1}, a)\)</span>.  The <span class="caps">TD</span>(0) target is an example of <em>bootstrapping</em> because it makes use of the current estimate of the action-value function, instead of, say the cumulative rewards from an entire episode, which would be a Monte Carlo target.  Temporal difference methods reduce variance that comes from sampling a single trajectory like Monte Carlo at the cost of introducing bias from using an approximate function in the target for&nbsp;updates.</p>
<p>Figure 8.11 of [7] nicely summarizes the types of approximations and their limits in the following&nbsp;diagram:</p>
<p><img alt="backup approximations" src="https://efavdb.com/images/backup_limits_diagram_sutton_barto.png"></p>
<h2>Deep Q-Networks (<span class="caps">DQN</span>)</h2>
<h3>Key contributions to&nbsp;Q-learning</h3>
<p>The <span class="caps">DQN</span> authors made two key enhancements to the original Q-learning algorithm to actually make it&nbsp;work:</p>
<ol>
<li>
<p><strong>Experience replay buffer</strong>: to reduce the instability caused by training on highly correlated sequential data, store samples (transition tuples <span class="math">\((s, a, s’, r)\)</span>) in an “experience replay buffer”.  Cut down correlations by randomly sampling the buffer for minibatches of training data.  The idea of experience replay was introduced by <a href="http://www.incompleteideas.net/lin-92.pdf">Lin in 1992</a>.</p>
</li>
<li>
<p><strong>Freeze the target network</strong>: to address the instability caused by chasing a moving target, freeze the target network and only update it periodically with the latest parameters from the trained&nbsp;estimator.</p>
</li>
</ol>
<p>These modifications enabled [1] to successfully train a deep Q-network, an action-value function approximated by a convolutional neural net, on the high dimensional visual inputs of a variety of Atari&nbsp;games.</p>
<p>The authors also employed a number of tweaks / data preprocessing on top of the aforementioned key enhancements.  One preprocessing trick of note was the concatenation of the four most recent frames as input into the Q-network in order to provide some sense of velocity or trajectory, e.g. the trajectory of a ball in games such as Pong or Breakout.  This preprocessing decision helps uphold the assumption that the problem is a Markov Decision Process, which underlies the Bellman optimality equations and Q-learning algorithms; otherwise, the assumption is violated if the agent only observes some fraction of the state of the environment, turning the problem into a <a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">partially observable <span class="caps">MDP</span></a>.</p>
<h3><span class="caps">DQN</span> implementation in&nbsp;code</h3>
<p>We’ve implemented <span class="caps">DQN</span> <a href="https://github.com/frangipane/reinforcement-learning/blob/master/DQN/dqn.py">here</a>, tested for (1) the <a href="https://gym.openai.com/envs/CartPole-v1/">Cartpole</a> toy problem, which uses a multilayer perceptron <code>MLPCritic</code> as the Q-function approximator for non-visual input data, and (2) Atari Breakout, which uses a convolutional neural network <code>CNNCritic</code> as the Q-function approximator for the (visual) Atari pixel&nbsp;data.</p>
<p>The Cartpole problem is trainable on the average modern laptop <span class="caps">CPU</span>, but we recommend using a beefier setup with GPUs and lots of memory to do Q-learning on Atari.  Thanks to the OpenAI Scholars program and Microsoft, we were able to train <span class="caps">DQN</span> on Breakout using an Azure <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/nc-series">Standard_NC24</a> consisting of 224 GiB <span class="caps">RAM</span> and 2 K80&nbsp;GPUs.</p>
<p>The values from the <span class="math">\(Q\)</span> estimator and frozen target network are fed into the Huber loss that is used to update the parameters of the Q-function in this code&nbsp;snippet:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_loss_q</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
        <span class="n">o</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">o2</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;obs&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;act&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;rew&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;obs2&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;done&#39;</span><span class="p">]</span>

        <span class="c1"># Pick out q-values associated with / indexed by the action that was taken</span>
        <span class="c1"># for that observation</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">ac</span><span class="o">.</span><span class="n">q</span><span class="p">(</span><span class="n">o</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>

        <span class="c1"># Bellman backup for Q function</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="c1"># Targets come from frozen target Q-network</span>
                <span class="n">q_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">target_q_network</span><span class="p">(</span><span class="n">o2</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
                <span class="n">backup</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">d</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">q_target</span>

        <span class="n">loss_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="n">q</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">backup</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">loss_q</span>
</pre></div>


<p>The experience replay buffer was taken from OpenAI’s Spinning Up in <span class="caps">RL</span> [6] code tutorials for the&nbsp;problem:</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A simple FIFO experience replay buffer for DDPG agents.</span>

<span class="sd">    Copied from: https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py#L11,</span>
<span class="sd">    modified action buffer for discrete action space.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs_dim</span><span class="p">,</span> <span class="n">act_dim</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">store</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">act</span><span class="p">,</span> <span class="n">rew</span><span class="p">,</span> <span class="n">next_obs</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obs_buf</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">obs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obs2_buf</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_obs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act_buf</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">act</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rew_buf</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">rew</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">done_buf</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">done</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ptr</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">obs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">obs_buf</span><span class="p">[</span><span class="n">idxs</span><span class="p">],</span>
                     <span class="n">obs2</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">obs2_buf</span><span class="p">[</span><span class="n">idxs</span><span class="p">],</span>
                     <span class="n">act</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">act_buf</span><span class="p">[</span><span class="n">idxs</span><span class="p">],</span>
                     <span class="n">rew</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rew_buf</span><span class="p">[</span><span class="n">idxs</span><span class="p">],</span>
                     <span class="n">done</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">done_buf</span><span class="p">[</span><span class="n">idxs</span><span class="p">])</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s1">&#39;act&#39;</span>
                <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> 
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</pre></div>


<p>Finally, we used OpenAI’s baselines <a href="https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py">Atari wrappers</a> to handle the rather involved data preprocessing&nbsp;steps.</p>
<p>You can see logs and plots like this plot of the mean raw returns per step in the environment for the Atari <span class="caps">DQN</span> training run in our <a href="https://app.wandb.ai/frangipane/dqn/runs/30fhfv6y?workspace=user-frangipane">wandb dashboard</a>.</p>
<p><img alt="training curve" src="https://efavdb.com/images/atari_training_returns.png"></p>
<h2>Conclusion</h2>
<p>From a pedagogical point of view, Q-learning is a good study for someone getting off the ground with <span class="caps">RL</span> since it pulls together many core <span class="caps">RL</span> concepts,&nbsp;namely:</p>
<ol>
<li>Model the sequential decision making process as an <strong><span class="caps">MDP</span></strong> where environment dynamics are&nbsp;unknown.</li>
<li>Frame the problem as finding <strong>action-value functions</strong> that satisfy the Bellman&nbsp;equations.</li>
<li>Iteratively solve the Bellman equations using <strong>bootstrapped  estimates</strong> from samples of an agent’s interactions with an&nbsp;environment.</li>
<li>Use neural networks to <strong>approximate value functions</strong> to handle the more realistic situation of an observation space being too high-dimensional to be stored in&nbsp;table.</li>
</ol>
<p><span class="caps">DQN</span> on top of vanilla Q-learning itself is noteworthy because the modifications &#8212; experience replay and frozen target networks &#8212; are what make Q-learning actually work, demonstrating that the devil is in the&nbsp;details.</p>
<p>Furthermore, the <span class="caps">DQN</span> tricks have been incorporated in many other <span class="caps">RL</span> algorithms, e.g. see [6] for more examples.  The tricks aren’t necessarily “pretty”, but they come from understanding/intuition about shortcomings of the basic&nbsp;algorithms.</p>
<h2>References</h2>
<p><strong>Papers</strong></p>
<ul>
<li>[1] Mnih et al 2015 - <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#the-optimal-q-function-and-the-optimal-action">Human-level control through deep reinforcement&nbsp;learning</a></li>
</ul>
<p><strong>Video&nbsp;lectures</strong></p>
<ul>
<li>[2] David Silver - <span class="caps">RL</span> lecture 6 Value Function Approximation (<a href="https://www.youtube.com/watch?v=UoPei5o4fps">video</a>, <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/FA.pdf">slides</a>)</li>
<li>[3] Sergey Levine’s lecture (<span class="caps">CS285</span>) on value function methods (video,&nbsp;slides)</li>
<li>[4] Sergey Levine’s lecture (<span class="caps">CS285</span>) on deep <span class="caps">RL</span> with Q-functions (video,&nbsp;slides)</li>
<li>[5] Vlad Mnih - Berkeley Deep <span class="caps">RL</span> Bootcamp 2017 - Core Lecture 3 <span class="caps">DQN</span> + Variants (<a href="https://www.youtube.com/watch?v=fevMOp5TDQs">video</a>, <a href="https://drive.google.com/open?id=0BxXI_RttTZAhVUhpbDhiSUFFNjg">slides</a>)</li>
</ul>
<p><strong>Books /&nbsp;tutorials</strong></p>
<ul>
<li>[6] OpenAI - Spinning Up: <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#the-optimal-q-function-and-the-optimal-action">The Optimal Q-Function and the Optimal&nbsp;Action</a></li>
<li>[7] Sutton and Barto - <a href="http://incompleteideas.net/book/RLbook2018.pdf">Reinforcement Learning: An Introduction (2nd Edition)</a>, section 6.5 “Q-learning: Off-policy <span class="caps">TD</span> Control”, section 16.5 “Human-level Video Game&nbsp;Play”</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


             
 
                <p id="post-share-links">
    Like this post?  Share on:
    <a href="https://twitter.com/intent/tweet?text=Q-learning%20and%20DQN&url=https%3A//efavdb.com/dqn&hashtags=machine-learning,reinforcement-learning,openai" target="_blank" rel="nofollow noopener noreferrer" title="Share on Twitter">Twitter</a>
    ❄
    <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//efavdb.com/dqn" target="_blank" rel="nofollow noopener noreferrer" title="Share on Facebook">Facebook</a>
    ❄
    <a href="mailto:?subject=Q-learning%20and%20DQN&amp;body=https%3A//efavdb.com/dqn" target="_blank" rel="nofollow noopener noreferrer" title="Share via Email">Email</a>
    </p>

                <hr />
    <div class="author_blurb">
        <a href="" target="_blank" rel="nofollow noopener noreferrer">
            <img src=/images/cy_efavdb_headshot.jpg alt="Cathy Yeh Avatar" title="Cathy Yeh">
            <span class="author_name">Cathy Yeh</span>
        </a>
        Cathy Yeh got a PhD at UC Santa Barbara studying soft-matter/polymer physics. After stints in a translational modeling group at Pfizer in San Diego, Driver (a no longer extant startup trying to match cancer patients to clinical trials) in San Francisco, and Square, she is currently participating in the OpenAI Scholars program.
    </div>

            






<section>
    <h6 style="display:none;">Comments</h6>
    <p id="comment-message"> </p>

    <div class="accordion" id="accordion2">
        <div class="accordion-group">
            <div class="accordion-heading">
                <a class="accordion-toggle disqus-comment-count comment-count"
                   data-toggle="collapse"
                   data-parent="#accordion2"
                   data-disqus-identifier="https://efavdb.com/dqn"
                   href="https://efavdb.com/dqn#comment_thread"
                   id="comment-accordion-toggle">
                    Comments
                </a>
            </div>
            <div id="comment_thread" class="accordion-body collapse">
                <div class="accordion-inner">
                    <div class="comments">
                        <div id="disqus_thread"></div>
                        <script>
    var disqus_shortname = 'efavdb2';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());

    var disqus_identifier = 'https://efavdb.com/dqn';
    var disqus_url = 'https://efavdb.com/dqn';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>

<noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

            <hr/>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="https://efavdb.com/pooling" title="Previous: Sample pooling to reduce needed disease screening test counts">Sample pooling to reduce needed disease screening test counts</a></li>
            </ul>
            </nav>
            </aside>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2020-04-06T00:00:00-07:00">Apr 6, 2020</time>
            <h4>Category</h4>
            <a class="category-link" href="https://efavdb.com/categories#machine-learning-ref">Machine learning</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://efavdb.com/tags#machine-learning-ref">machine learning
                    <span>9</span>
</a></li>
                <li><a href="https://efavdb.com/tags#openai-ref">OpenAI
                    <span>3</span>
</a></li>
                <li><a href="https://efavdb.com/tags#reinforcement-learning-ref">reinforcement learning
                    <span>5</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="https://twitter.com/efavdb" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Twitter" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1da1f3"/><path fill="#fff" d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
    </a>
    <a href="https://github.com/efavdb" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
    <a href="https://www.youtube.com/channel/UClfvjoSiu0VvWOh5OpnuusA" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="YouTube" role="img" viewBox="0 0 512 512" fill="#ed1d24"><rect width="512" height="512" rx="15%"/><path d="m427 169c-4-15-17-27-32-31-34-9-239-10-278 0-15 4-28 16-32 31-9 38-10 135 0 174 4 15 17 27 32 31 36 10 241 10 278 0 15-4 28-16 32-31 9-36 9-137 0-174" fill="#fff"/><path d="m220 203v106l93-53"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
                        <button onclick="topFunction()" id="myBtn" title="Go to top">&#x25B2;</button>
                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>

    <div>
        <span class="site-name">EFAVDB</span> - Everybody's Favorite Data Blog
    </div>



    <!-- <div id="fpowered"> -->
    <!--     Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a> -->
    <!--     Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a> -->
    <!--      -->
    <!-- </div> -->
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script>
            //** Scroll to top button **
            //Get the button:
            mybutton = document.getElementById("myBtn");

            // When the user scrolls down 30px from the top of the document, show the button
            window.onscroll = function() {scrollFunction()};

            function scrollFunction() {
              if (document.body.scrollTop > 30 || document.documentElement.scrollTop > 30) {
                mybutton.style.display = "block";
              } else {
                mybutton.style.display = "none";
              }
            }

            // When the user clicks on the button, scroll to the top of the document
            function topFunction() {
              document.body.scrollTop = 0; // For Safari
              document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>