<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Cathy Yeh" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="reinforcement learning, machine learning, Machine learning, " />

<meta property="og:title" content="Dynamic programming in reinforcement learning "/>
<meta property="og:url" content="https://efavdb.com/reinforcement-learning-dynamic-programming" />
<meta property="og:description" content="Background We discuss how to use dynamic programming (DP) to solve reinforcement learning (RL) problems where we have a perfect model of the environment. DP is a general approach to solving problems by breaking them into subproblems that can be solved separately, cached, then combined to solve the overall problem …" />
<meta property="og:site_name" content="EFAVDB" />
<meta property="og:article:author" content="Cathy Yeh" />
<meta property="og:article:published_time" content="2020-03-28T00:00:00-07:00" />
<meta name="twitter:title" content="Dynamic programming in reinforcement learning ">
<meta name="twitter:description" content="Background We discuss how to use dynamic programming (DP) to solve reinforcement learning (RL) problems where we have a perfect model of the environment. DP is a general approach to solving problems by breaking them into subproblems that can be solved separately, cached, then combined to solve the overall problem …">

        <title>Dynamic programming in reinforcement learning  · EFAVDB
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Fira+Sans:300,400,700" rel="stylesheet" type='text/css' />
        <link href="https://fonts.googleapis.com/css?family=Cardo:400,700" rel="stylesheet" type='text/css' />        
        <link rel="stylesheet" type="text/css" href="https://efavdb.com/theme/css/elegant.prod.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://efavdb.com/theme/css/custom.css" media="screen">

        <link href="https://efavdb.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="EFAVDB - Full Atom Feed" />
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-57405119-1', 'auto');
    ga('send', 'pageview');
</script>


    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://efavdb.com/"><span class=site-name>EFAVDB</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://efavdb.com
                                    >Home</a>
                                </li>
                                <li ><a href="https://efavdb.com/pages/authors.html">Authors</a></li>
                                <li ><a href="https://efavdb.com/categories">Categories</a></li>
                                <li ><a href="https://efavdb.com/tags">Tags</a></li>
                                <li ><a href="https://efavdb.com/archives">Archives</a></li>
                                <li><form class="navbar-search" action="https://efavdb.com/search" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span8 offset2">
        <h1>
            <a href="https://efavdb.com/reinforcement-learning-dynamic-programming">
                Dynamic programming in reinforcement&nbsp;learning
            </a>
        </h1>
    </header>
    <div class="span2"></div>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <h2>Background</h2>
<p>We discuss how to use dynamic programming (<span class="caps">DP</span>) to solve reinforcement learning (<span class="caps">RL</span>) problems where we have a perfect model of the environment.  <span class="caps">DP</span> is a general approach to solving problems by breaking them into subproblems that can be solved separately, cached, then combined to solve the overall&nbsp;problem.</p>
<p>We’ll use a toy model, taken from [1], of a student transitioning between five states in college, which we also used in our <a href="https://efavdb.com/intro-rl-toy-example.html">introduction</a> to <span class="caps">RL</span>:</p>
<p><img alt="student MDP" src="https://efavdb.com/images/student_mdp.png"></p>
<p>The model (dynamics) of the environment describe the probabilities of receiving a reward <span class="math">\(r\)</span> in the next state <span class="math">\(s'\)</span> given the current state <span class="math">\(s\)</span> and action <span class="math">\(a\)</span> taken, <span class="math">\(p(s’, r | s, a)\)</span>.  We can read these dynamics off the diagram of the student Markov Decision Process (<span class="caps">MDP</span>), for&nbsp;example:</p>
<p><span class="math">\(p(s'=\text{CLASS2}, r=-2 | s=\text{CLASS1}, a=\text{study}) =&nbsp;1.0\)</span></p>
<p><span class="math">\(p(s'=\text{CLASS2}, r=1 | s=\text{CLASS3}, a=\text{pub}) =&nbsp;0.4\)</span></p>
<p>If you&#8217;d like to jump straight to code, see this <a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb">jupyter notebook</a>.</p>
<h3>The role of value functions in <span class="caps">RL</span></h3>
<p>The agent’s (student’s) policy maps states to actions, <span class="math">\(\pi(a|s) := p(a|s)\)</span>. 
The goal is to find the optimal policy <span class="math">\(\pi_*\)</span> that will maximize the expected cumulative rewards, the discounted return <span class="math">\(G_t\)</span>, in each state <span class="math">\(s\)</span>.</p>
<p>The value functions, <span class="math">\(v_{\pi}(s)\)</span> and <span class="math">\(q_{\pi}(s, a)\)</span>, in MDPs formalize this&nbsp;goal.</p>
<div class="math">\begin{eqnarray}
v_{\pi}(s) &amp;=&amp; \mathbb{E}_{\pi}[G_t | S_t = s] \\
q_{\pi}(s, a) &amp;=&amp; \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]
\end{eqnarray}</div>
<p>We want to be able to calculate the value function for an arbitrary policy, i.e. <em>prediction</em>, as well as use the value functions to find an optimal policy, i.e. the <em>control</em>&nbsp;problem.</p>
<h2>Policy&nbsp;evaluation</h2>
<p>Policy evaluation deals with the problem of calculating the value function for some arbitrary policy.  In our introduction to <span class="caps">RL</span> <a href="https://efavdb.com/intro-rl-toy-example.html">post</a>, we showed that the value functions obey self-consistent, recursive relations, that make them amenable to <span class="caps">DP</span> approaches given a model of the&nbsp;environment.</p>
<p>These recursive relations are the Bellman expectation equations, which write the value of each state in terms of an average over the values of its successor / neighboring states, along with the expected reward along the&nbsp;way.</p>
<p>The Bellman expectation equation for <span class="math">\(v_{\pi}(s)\)</span>&nbsp;is</p>
<div class="math">\begin{eqnarray}\label{state-value-bellman} \tag{1}
v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_{\pi}(s’) ],
\end{eqnarray}</div>
<p>where <span class="math">\(\gamma\)</span> is the discount factor <span class="math">\(0 \leq \gamma \leq 1\)</span> that weights the importance of future vs. current returns. <strong><span class="caps">DP</span> turns (\ref{state-value-bellman}) into an update rule</strong> (\ref{policy-evaluation}), <span class="math">\(\{v_k(s’)\} \rightarrow v_{k+1}(s)\)</span>, which iteratively converges towards the solution, <span class="math">\(v_\pi(s)\)</span>, for&nbsp;(\ref{state-value-bellman}):</p>
<div class="math">\begin{eqnarray}\label{policy-evaluation} \tag{2}
v_{k+1}(s) = \sum_{a} \pi(a|s) \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_k(s’) ]
\end{eqnarray}</div>
<p>Applying policy evaluation to our student model for an agent with a random policy, we arrive at the following state value function (see <a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb">jupyter notebook</a> for&nbsp;implementation):</p>
<p><img alt="student MDP value function random policy" src="https://efavdb.com/images/student_mdp_values_random_policy.png"></p>
<h2>Finding the optimal value functions and&nbsp;policy</h2>
<h3>Policy&nbsp;iteration</h3>
<p>We can evaluate the value functions for a given policy by turning the Bellman expectation equation (\ref{state-value-bellman}) into an update equation with the iterative policy evaluation&nbsp;algorithm.</p>
<p>But how do we use value functions to achieve our end goal of finding an optimal policy that corresponds to the optimal value&nbsp;functions?</p>
<p>Imagine we know the value function for a policy.  If taking the greedy action, corresponding to taking <span class="math">\(\text{arg} \max_a q_{\pi}(s,a)\)</span>, from any state in that policy is not consistent with that policy, or, equivalently, <span class="math">\(\max_a q_{\pi}(s,a) &gt; v_\pi(s)\)</span>, then the policy is not optimal since we can improve the policy by taking the greedy action in that state and then onwards following the original&nbsp;policy.</p>
<p>The <em>policy iteration</em> algorithm involves taking turns calculating the value function for a policy (policy evaluation) and improving on the policy (policy improvement) by taking the greedy action in each state for that value function until converging to <span class="math">\(\pi_*\)</span> and <span class="math">\(v_*\)</span> (see [2] for pseudocode for policy&nbsp;iteration).</p>
<h3>Value&nbsp;iteration</h3>
<p>Unlike policy iteration, the value iteration algorithm does not require complete convergence of policy evaluation before policy improvement, and, in fact, makes use of just a single iteration of policy evaluation.  Just as policy evaluation could be viewed as turning the Bellman expectation equation into an update, value iteration turns the Bellman optimality equation into an&nbsp;update.</p>
<p>In our previous <a href="https://efavdb.com/intro-rl-toy-example.html">post</a> introducing <span class="caps">RL</span> using the student example, we saw that the optimal value functions are the solutions to the Bellman optimality equation, e.g. for the optimal state-value&nbsp;function:</p>
<div class="math">\begin{eqnarray}\label{state-value-bellman-optimality} \tag{3}
v_*(s) &amp;=&amp; \max_a q_{\pi*}(s, a) \\
    &amp;=&amp; \max_a \mathbb{E} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] \\
    &amp;=&amp; \max_a \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_*(s’) ]
\end{eqnarray}</div>
<p>As a <span class="caps">DP</span> update equation, (\ref{state-value-bellman-optimality})&nbsp;becomes:
</p>
<div class="math">\begin{eqnarray}\label{value-iteration} \tag{4}
v_{k+1}(s) = \max_a \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_k(s’) ]
\end{eqnarray}</div>
<p>Value iteration combines (truncated) policy evaluation with policy improvement in a single step; the state-value functions are updated with the averages of the value functions of the neighbor states that can occur from a greedy action, i.e. the action that maximizes the right hand side of&nbsp;(\ref{value-iteration}).</p>
<p>Applying value iteration to our student model, we arrive at the following optimal state value function, with the optimal policy delineated by red arrows (see <a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb">jupyter notebook</a>):</p>
<p><img alt="student MDP optimal policy and value function" src="https://efavdb.com/images/student_mdp_optimal_policy.png"></p>
<h2>Summary</h2>
<p>We’ve discussed how to solve for (a) the value functions of an arbitrary policy, (b) the optimal value functions and optimal policy.  Solving for (a) involves turning the Bellman expectation equations into an update, whereas (b) involves turning the Bellman optimality equations into an update.  These algorithms are guaranteed to converge (see [1] for notes on how the contraction mapping theorem guarantees&nbsp;convergence).</p>
<p>You can see the application of both policy evaluation and value iteration to the student model problem in this <a href="https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb">jupyter notebook</a>.</p>
<h2><a name="References">References</a></h2>
<p>[1] David Silver&#8217;s <span class="caps">RL</span> Course Lecture 3 - Planning by Dynamic Programming (<a href="https://www.youtube.com/watch?v=Nd1-UUMVfz4">video</a>,
  <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf">slides</a>)</p>
<p>[2] Sutton and Barto -
  <a href="http://incompleteideas.net/book/RLbook2018.pdf">Reinforcement Learning: An Introduction</a> - Chapter 4: Dynamic&nbsp;Programming</p>
<p>[3] Denny Britz’s <a href="https://github.com/dennybritz/reinforcement-learning/tree/master/DP">notes</a> on <span class="caps">RL</span> and <span class="caps">DP</span>, including crisp implementations in code of policy evaluation, policy iteration, and value iteration for the gridworld example discussed in&nbsp;[2].</p>
<p>[4] Deep <span class="caps">RL</span> Bootcamp Lecture 1: Motivation + Overview + Exact Solution Methods, by Pieter Abbeel (<a href="https://www.youtube.com/watch?v=qaMdN6LS9rA">video</a>, <a href="https://drive.google.com/open?id=0BxXI_RttTZAhVXBlMUVkQ1BVVDQ">slides</a>) - a very compressed&nbsp;intro.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


             
 
                <p id="post-share-links">
    Like this post?  Share on:
    <a href="https://twitter.com/intent/tweet?text=Dynamic%20programming%20in%20reinforcement%C2%A0learning&url=https%3A//efavdb.com/reinforcement-learning-dynamic-programming&hashtags=reinforcement-learning,machine-learning" target="_blank" rel="nofollow noopener noreferrer" title="Share on Twitter">Twitter</a>
    ❄
    <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//efavdb.com/reinforcement-learning-dynamic-programming" target="_blank" rel="nofollow noopener noreferrer" title="Share on Facebook">Facebook</a>
    ❄
    <a href="mailto:?subject=Dynamic%20programming%20in%20reinforcement%C2%A0learning&amp;body=https%3A//efavdb.com/reinforcement-learning-dynamic-programming" target="_blank" rel="nofollow noopener noreferrer" title="Share via Email">Email</a>
    </p>

                <hr />
    <div class="author_blurb">
        <a href="" target="_blank" rel="nofollow noopener noreferrer">
            <img src=/images/cy_efavdb_headshot.jpg alt="Cathy Yeh Avatar" title="Cathy Yeh">
            <span class="author_name">Cathy Yeh</span>
        </a>
        Cathy Yeh got a PhD at UC Santa Barbara studying soft-matter/polymer physics. After stints in a translational modeling group at Pfizer in San Diego, Driver (a no longer extant startup trying to match cancer patients to clinical trials) in San Francisco, and Square, she is currently participating in the OpenAI Scholars program.
    </div>

            






<section>
    <h6 style="display:none;">Comments</h6>
    <p id="comment-message"> </p>

    <div class="accordion" id="accordion2">
        <div class="accordion-group">
            <div class="accordion-heading">
                <a class="accordion-toggle disqus-comment-count comment-count"
                   data-toggle="collapse"
                   data-parent="#accordion2"
                   data-disqus-identifier="https://efavdb.com/reinforcement-learning-dynamic-programming"
                   href="https://efavdb.com/reinforcement-learning-dynamic-programming#comment_thread"
                   id="comment-accordion-toggle">
                    Comments
                </a>
            </div>
            <div id="comment_thread" class="accordion-body collapse">
                <div class="accordion-inner">
                    <div class="comments">
                        <div id="disqus_thread"></div>
                        <script>
    var disqus_shortname = 'efavdb2';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());

    var disqus_identifier = 'https://efavdb.com/reinforcement-learning-dynamic-programming';
    var disqus_url = 'https://efavdb.com/reinforcement-learning-dynamic-programming';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>

<noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

            <hr/>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="https://efavdb.com/intro-rl-toy-example" title="Previous: Introduction to reinforcement learning by example">Introduction to reinforcement learning by example</a></li>
            </ul>
            </nav>
            </aside>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2020-03-28T00:00:00-07:00">Mar 28, 2020</time>
            <h4>Category</h4>
            <a class="category-link" href="https://efavdb.com/categories#machine-learning-ref">Machine learning</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://efavdb.com/tags#machine-learning-ref">machine learning
                    <span>8</span>
</a></li>
                <li><a href="https://efavdb.com/tags#reinforcement-learning-ref">reinforcement learning
                    <span>4</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="https://twitter.com/efavdb" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Twitter" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1da1f3"/><path fill="#fff" d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
    </a>
    <a href="https://github.com/efavdb" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
    <a href="https://www.youtube.com/channel/UClfvjoSiu0VvWOh5OpnuusA" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="YouTube" role="img" viewBox="0 0 512 512" fill="#ed1d24"><rect width="512" height="512" rx="15%"/><path d="m427 169c-4-15-17-27-32-31-34-9-239-10-278 0-15 4-28 16-32 31-9 38-10 135 0 174 4 15 17 27 32 31 36 10 241 10 278 0 15-4 28-16 32-31 9-36 9-137 0-174" fill="#fff"/><path d="m220 203v106l93-53"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
                        <button onclick="topFunction()" id="myBtn" title="Go to top">&#x25B2;</button>
                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>

    <div>
        <span class="site-name">EFAVDB</span> - Everybody's Favorite Data Blog
    </div>



    <!-- <div id="fpowered"> -->
    <!--     Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a> -->
    <!--     Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a> -->
    <!--      -->
    <!-- </div> -->
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script>
            //** Scroll to top button **
            //Get the button:
            mybutton = document.getElementById("myBtn");

            // When the user scrolls down 30px from the top of the document, show the button
            window.onscroll = function() {scrollFunction()};

            function scrollFunction() {
              if (document.body.scrollTop > 30 || document.documentElement.scrollTop > 30) {
                mybutton.style.display = "block";
              } else {
                mybutton.style.display = "none";
              }
            }

            // When the user clicks on the button, scroll to the top of the document
            function topFunction() {
              document.body.scrollTop = 0; // For Safari
              document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>