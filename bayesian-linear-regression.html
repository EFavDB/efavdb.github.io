<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>A review of parameter regularization and Bayesian regression</title>


  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
<link href="./bayesian-linear-regression.html" rel="canonical" />
  <!-- Feed -->

  <link href="./theme/css/style.css" type="text/css" rel="stylesheet" />

  <!-- Code highlight color scheme -->
      <link href="./theme/css/code_blocks/github.css" rel="stylesheet">


  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->



    <meta name="description" content="Here, we review parameter regularization, which is a method for improving regression models through the penalization of non-zero...">

    <meta name="author" content="jslandy">





<!-- Open Graph -->
<meta property="og:site_name" content="EFAVDB"/>
<meta property="og:title" content="A review of parameter regularization and Bayesian regression"/>
<meta property="og:description" content="Here, we review parameter regularization, which is a method for improving regression models through the penalization of non-zero..."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="./bayesian-linear-regression.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2015-10-11 00:01:00-07:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="./author/jslandy.html">
<meta property="article:section" content="Statistics"/>
<meta property="og:image" content="./theme/images/post-bg.jpg">

<!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@efavdb">
    <meta name="twitter:title" content="A review of parameter regularization and Bayesian regression">
    <meta name="twitter:url" content="./bayesian-linear-regression.html">

        <meta name="twitter:image:src" content="./theme/images/post-bg.jpg">

      <meta name="twitter:description" content="Here, we review parameter regularization, which is a method for improving regression models through the penalization of non-zero...">

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "A review of parameter regularization and Bayesian regression",
  "headline": "A review of parameter regularization and Bayesian regression",
  "datePublished": "2015-10-11 00:01:00-07:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "jslandy",
    "url": "./author/jslandy.html"
  },
  "image": "./theme/images/post-bg.jpg",
  "url": "./bayesian-linear-regression.html",
  "description": "Here, we review parameter regularization, which is a method for improving regression models through the penalization of non-zero..."
}
</script>
</head>
<!-- TODO : Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>
          <li><a href="/" role="presentation">Home</a></li>
          <li><a href="/pages/about.html" role="presentation">About & Consulting</a></li>
          <li><a href="/archives.html" role="presentation">Archive</a></li>
          <li><a href="/tags.html" role="presentation">Tags</a></li>
          <li><a href="/pages/linselect.html" role="presentation">linselect - feature selection</a></li>


    </ul>
  </div>
</nav>
    <!-- Progressbar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header id="post-header" >
      <div class="inner">
        <nav id="navigation">
            <span id="home-button" class="nav-button">
                <a class="home-button" href="./" title="Home"><i class="ic ic-arrow-left"></i> Home</a>
            </span>
          <span id="menu-button" class="nav-button">
            <a class="menu-button"><i class="ic ic-menu"></i> Menu</a>
          </span>
        </nav>
        <h1 class="post-title">A review of parameter regularization and Bayesian regression</h1>
        <!-- TODO : Proper class for headline -->
        <span class="post-meta">
                <a href="./author/jslandy.html">Jslandy</a>
            | <time datetime="Sun 11 October 2015">Sun 11 October 2015</time>
        </span>
        <!-- TODO : Modified check -->
      </div>
    </header>

  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
            <section class="post-content">
                <p>Here, we review parameter regularization, which is a method for improving regression models through the penalization of non-zero parameter estimates. Why is this effective? Biasing parameters towards zero will (of course!) unfavorably bias a model, but it will also reduce its variance. At times the latter effect can win out, resulting in a net reduction in generalization error. We also review Bayesian regressions -- in effect, these generalize the regularization approach, biasing model parameters to any specified prior estimates, not necessarily zero.</p>
<p>This is the second of a series of posts expounding on topics discussed in the text, <a href="http://www-bcf.usc.edu/~gareth/ISL/">"An Introduction to Statistical Learning"</a>. Here, we cover material from its Chapters 2 and 6. See prior post <a href="http://efavdb.com/leave-one-out-cross-validation/">here</a>.</p>
<p><a href="http://twitter.com/efavdb">Follow @efavdb</a><br>
Follow us on twitter for new submission alerts!</p>
<h3>Introduction and overview</h3>
<p>In this post, we will be concerned with the problem of fitting a function of the form<br>
</p>
<div class="math">$$\label{function}  
y(\vec{x}_i) = f(\vec{x}_i) + \epsilon_i \tag{1},  
$$</div>
<p><br>
where <span class="math">\(f\)</span> is the function's systematic part and <span class="math">\(\epsilon_i\)</span> is a random error. These errors have mean zero and are iid -- their presence is meant to take into account dependences in <span class="math">\(y\)</span> on features that we don't have access to. To "fit" such a function, we will suppose that one has chosen some appropriate regression algorithm (perhaps a linear model, a random forest, etc.) that can be used to generate an approximation <span class="math">\(\hat{f}\)</span> to <span class="math">\(y\)</span>, given a training set of example <span class="math">\((\vec{x}_i, y_i)\)</span> pairs.</p>
<p>The primary concern when carrying out a regression is often to find a fit that will be accurate when applied to points not included in the training set. There are two sources of error that one has to grapple with: Bias in the algorithm -- sometimes the result of using an algorithm that has insufficient flexibility to capture the nature of the function being fit, and variance -- this relates to how sensitive the resulting fit is to the samples chosen for the training set. The latter issue is closely related to the concept of <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>.</p>
<p>To mitigate overfitting, <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">parameter regularization</a> is often applied. As we detail below, this entails penalizing non-zero parameter estimates. Although this can favorably reduce the variance of the resulting model, it will also introduce bias. The optimal amount of regularization is therefore determined by appropriately balancing these two effects.</p>
<p>In the following, we carefully review the mathematical definitions of model bias and variance, as well as how these effects contribute to the error of an algorithm. We then show that regularization is equivalent to assuming a particular form of Bayesian prior that causes the parameters to be somewhat "sticky" around zero -- this stickiness is what results in model variance reduction. Because standard regularization techniques bias towards zero, they work best when the underlying true feature dependences are sparse. When this is not true, one should attempt an analogous variance reduction through application of the more general Bayesian regression framework.</p>
<h3>Squared error decomposition</h3>
<p>The first step to understanding regression error is the following identity: Given any fixed <span class="math">\(\vec{x}\)</span>, we have<br>
</p>
<div class="math">$$  
\begin{align}  
\overline{\left (\hat{f}(\vec{x}) - y(\vec{x}) \right)^2} &amp;= \overline{\left (\hat{f}(\vec{x}) - \overline{\hat{f}(\vec{x})} \right)^2} + \left (\overline{\hat{f}(\vec{x})} - f(\vec{x}) \right)^2 + \overline{ \epsilon^2} \\  
&amp; \equiv var\left(\hat{f}(\vec{x})\right) + bias\left(\hat{f}(\vec{x})\right)^2 + \overline{\epsilon^2}. \tag{2}\label{error_decomp}  
\end{align}  
$$</div>
<p><br>
Here, overlines represent averages over two things: The first is the random error <span class="math">\(\epsilon\)</span> values, and the second is the training set used to construct <span class="math">\(\hat{f}\)</span>. The left side of (\ref{error_decomp}) gives the average squared error of our algorithm, at point <span class="math">\(\vec{x}\)</span> -- i.e., the average squared error we can expect to get, given a typical training set and <span class="math">\(\epsilon\)</span> value. The right side of the equation decomposes this error into separate, independent components. The first term at right -- the variance of <span class="math">\(\hat{f}(\vec{x})\)</span> -- relates to how widely the estimate at <span class="math">\(\vec{x}\)</span> changes as one randomly samples from the space of possible training sets. Similarly, the second term -- the algorithm's squared bias -- relates to the systematic error of the algorithm at <span class="math">\(\vec{x}\)</span>. The third and final term above gives the average squared random error -- this provides a fundamental lower bound on the accuracy of any estimator of <span class="math">\(y\)</span>.</p>
<p>We turn now to the proof of (\ref{error_decomp}). We write the left side of this equation as<br>
</p>
<div class="math">$$\label{detail}  
\begin{align} \tag{3}  
\overline{\left (\hat{f}(\vec{x}) - y(\vec{x}) \right)^2} &amp;= \overline{\left ( \left \{\hat{f}(\vec{x}) - f(\vec{x}) \right \} - \left \{ y(\vec{x}) - f(\vec{x}) \right \} \right)^2}\\  
&amp;=  
\overline{\left ( \hat{f}(\vec{x}) - f(\vec{x}) \right)^2}  
- 2 \overline{ \left (\hat{f}(\vec{x}) - f(\vec{x}) \right ) \left (y(\vec{x}) - f(\vec{x}) \right ) }  
+ \overline{ \left (y(\vec{x}) - f(\vec{x}) \right)^2}.  
\end{align}  
$$</div>
<p><br>
The middle term here is zero. To see this, note that it is the average of the product of two independent quantities: The first factor, <span class="math">\(\hat{f}(\vec{x}) - f(\vec{x})\)</span>, varies only with the training set, while the second factor, <span class="math">\(y(\vec{x}) - f(\vec{x})\)</span>, varies only with <span class="math">\(\epsilon\)</span>. Because these two factors are independent, their average product is the product of their individual averages, the second of which is zero, by definition. Now, the third term in (\ref{detail}) is simply <span class="math">\(\overline{\epsilon^2}\)</span>. To complete the proof, we need only evaluate the first term above. To do that, we write<br>
</p>
<div class="math">$$\begin{align} \tag{4} \label{detail2}  
\overline{\left ( \hat{f}(\vec{x}) - f(\vec{x}) \right)^2} &amp;=  
\overline{\left ( \left \{ \hat{f}(\vec{x}) - \overline{\hat{f}(\vec{x})} \right \}- \left \{f(\vec{x}) -\overline{\hat{f}(\vec{x})} \right \}\right)^2} \\  
&amp;=  
\overline{\left ( \hat{f}(\vec{x}) - \overline{\hat{f}(\vec{x})} \right)^2}  
-2  
\overline{ \left \{ \hat{f}(\vec{x}) - \overline{\hat{f}(\vec{x})} \right \} \left \{f(\vec{x}) -\overline{\hat{f}(\vec{x})} \right \} }  
+  
\left ( f(\vec{x}) -\overline{\hat{f}(\vec{x})} \right)^2.  
\end{align}  
$$</div>
<p><br>
The middle term here is again zero. This is because its second factor is a constant, while the first averages to zero, by definition. The first and third terms above are the algorithm's variance and squared bias, respectively. Combining these observations with (\ref{detail}), we obtain (\ref{error_decomp}).</p>
<h3>Bayesian regression</h3>
<p>In order to introduce Bayesian regression, we focus on the special case of least-squares regressions. In this context, one posits that the samples generated take the form (\ref{function}), with the error <span class="math">\(\epsilon_i\)</span> terms now iid, Gaussian distributed with mean zero and standard deviation <span class="math">\(\sigma\)</span>. Under this assumption, the probability of observing values <span class="math">\((y_1, y_2,\ldots, y_N)\)</span> at <span class="math">\((\vec{x}_1, \vec{x}_2,\ldots,\vec{x}_N)\)</span> is given by<br>
</p>
<div class="math">$$  
\begin{align}  
\tag{5} \label{5}  
P(\vec{y} \vert f) &amp;= \prod_{i=1}^N \frac{1}{(2 \pi \sigma)^{1/2}} \exp \left [-\frac{1}{2 \sigma^2} (y_i - f(\vec{x}_i))^2 \right]\\  
&amp;= \frac{1}{(2 \pi \sigma)^{N/2}} \exp \left [-\frac{1}{2 \sigma^2} (\vec{y} - \vec{f})^2 \right],  
\end{align}  
$$</div>
<p><br>
where <span class="math">\(\vec{y} \equiv (y_1, y_2,\ldots, y_N)\)</span> and <span class="math">\(\vec{f} \equiv (f_1, f_2,\ldots, f_N)\)</span>. In order to carry out a maximum-likelihood analysis, one posits a parameterization for <span class="math">\(f(\vec{x})\)</span>. For example, one could posit the linear form,<br>
</p>
<div class="math">$$\tag{6}  
f(\vec{x}) = \vec{\theta} \cdot \vec{x}.  
$$</div>
<p><br>
Once a parameterization is selected, its optimal <span class="math">\(\vec{\theta}\)</span> values are selected by maximizing (\ref{5}), which gives the least-squares fit.</p>
<p>One sometimes would like to nudge (or bias) the parameters away from those that maximize (\ref{5}), towards some values considered reasonable ahead of time. A simple way to do this is to introduce a Bayesian prior for the parameters <span class="math">\(\vec{\theta}\)</span>. For example, one might posit a prior of the form<br>
</p>
<div class="math">$$ \tag{7} \label{7}  
P(f) \equiv P(\vec{\theta}) \propto \exp \left [- \frac{1}{2\sigma^2} (\vec{\theta} - \vec{\theta}_0)  
\Lambda (\vec{\theta} - \vec{\theta}_0)\right].  
$$</div>
<p><br>
Here, <span class="math">\(\vec{\theta}_0\)</span> represents a best guess for what <span class="math">\(\theta\)</span> should be before any data is taken, and the matrix <span class="math">\(\Lambda\)</span> determines how strongly we wish to bias <span class="math">\(\theta\)</span> to this value: If the components of <span class="math">\(\Lambda\)</span> are large (small), then we strongly (weakly) constrain <span class="math">\(\vec{\theta}\)</span> to sit near <span class="math">\(\vec{\theta}_0\)</span>. To carry out the regression, we combine (\ref{5}-\ref{7}) with Bayes' rule, giving<br>
</p>
<div class="math">$$  
\tag{8}  
P(\vec{\theta} \vert \vec{y}) = \frac{P(\vec{y}\vert \vec{\theta}) P(\vec{\theta})}{P(\vec{y})}  
\propto \exp \left [-\frac{1}{2 \sigma^2} (\vec{y} - \vec{\theta} \cdot \vec{x})^2 - \frac{1}{2\sigma^2} (\vec{\theta} - \vec{\theta}_0)  
\Lambda (\vec{\theta} - \vec{\theta}_0)\right].  
$$</div>
<p><br>
The most likely <span class="math">\(\vec{\theta}\)</span> now minimizes the quadratic "cost function",<br>
</p>
<div class="math">$$\tag{9} \label{9}  
F(\theta) \equiv (\vec{y} - \vec{\theta} \cdot \vec{x})^2 +(\vec{\theta} - \vec{\theta}_0)  
\Lambda (\vec{\theta} - \vec{\theta}_0),  
$$</div>
<p><br>
a Bayesian generalization of the usual squared error. With this, our heavy-lifting is at an end. We now move to a quick review of regularization, which will appear as a simple application of the Bayesian method.</p>
<h3>Parameter regularization as special cases</h3>
<p>The most common forms of regularization are the so-called "ridge" and "lasso". In the context of least-squares fits, the former involves minimization of the quadratic form<br>
</p>
<div class="math">$$  
\tag{10} \label{ridge}  
F_{ridge}(\theta) \equiv (\vec{y} - \hat{f}(\vec{x}; \vec{\theta}))^2 + \Lambda \sum_i \theta_i^2,  
$$</div>
<p><br>
while in the latter, one minimizes<br>
</p>
<div class="math">$$  
\tag{11} \label{lasso}  
F_{lasso}(\theta) \equiv (\vec{y} - \hat{f}(\vec{x}; \vec{\theta}))^2 + \Lambda \sum_i \vert\theta_i \vert.  
$$</div>
<p><br>
The terms proportional to <span class="math">\(\Lambda\)</span> above are the so-called regularization terms. In elementary courses, these are generally introduced to least-squares fits in an ad-hoc manner: Conceptually, it is suggested that these terms serve to penalize the inclusion of too many parameters in the model, with individual parameters now taking on large values only if they are really essential to the fit.</p>
<p>While the conceptual argument above may be correct, the framework we've reviewed here allows for a more sophisticated understanding of regularization: (\ref{ridge}) is a special case of (\ref{9}), with <span class="math">\(\vec{\theta}_0\)</span> set to <span class="math">\((0,0,\ldots, 0)\)</span>. Further, the lasso form (\ref{lasso}) is also a special-case form of Bayesian regression, with the prior set to <span class="math">\(P(\vec{\theta}) \propto \exp \left (- \frac{\Lambda}{2 \sigma^2} \sum_i \vert \theta_i \vert \right)\)</span>. As advertised, regularization is a form of Bayesian regression.</p>
<p>Why then does regularization "work"? For the same reason any other Bayesian approach does: Introduction of a prior will bias a model (if chosen well, hopefully not by much), but will also effect a reduction in its variance. The appropriate amount of regularization balances these two effects. Sometimes -- but not always -- a non-zero amount of bias is required.</p>
<h3>Discussion</h3>
<p>In summary, our main points here were three-fold: (i) We carefully reviewed the mathematical definitions of model bias and variance, deriving (\ref{error_decomp}). (ii) We reviewed how one can inject Bayesian priors to regressions: The key is to use the random error terms to write down the probability of seeing a particular observational data point. (iii) We reviewed the fact that the ridge and lasso -- (\ref{ridge}) and (\ref{lasso}) -- can be considered Bayesian priors.</p>
<p>Intuitively, one might think introduction of a prior serves to reduce the bias in a model: Outside information is injected into a model, nudging its parameters towards values considered reasonable ahead of time. In fact, this nudging introduces bias! Bayesian methods work through reduction in variance, not bias -- A good prior is one that does not introduce too much bias.</p>
<p>When, then, should one use regularization? Only when one expects the optimal model to be largely sparse. This is often the case when working on machine learning algorithms, as one has the freedom there to throw a great many feature variables into a model, expecting only a small (a prior, unknown) minority of them to really prove informative. However, when not working in high-dimensional feature spaces, sparseness should not be expected. In this scenario, one should reason some other form of prior, and attempt a variance reduction through the more general Bayesian framework.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </section>

            <section class="post-info">
                <div class="post-share">
                    <a class="twitter" href="https://twitter.com/share?text=A review of parameter regularization and Bayesian regression&amp;url=./bayesian-linear-regression.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="ic ic-twitter"></i><span class="hidden">Twitter</span>
                    </a>
                    <a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=./bayesian-linear-regression.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="ic ic-facebook"></i><span class="hidden">Facebook</span>
                    </a>
                    <a class="googleplus" href="https://plus.google.com/share?url=./bayesian-linear-regression.html" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <i class="ic ic-googleplus"></i><span class="hidden">Google+</span>
                    </a>
                    <div class="clear"></div>
                </div>


                <div class="clear"></div>


                </section>


                <aside class="post-nav">
                    <a class="post-nav-next" href="./svm-classification.html">
                        <section class="post-nav-teaser">
                            <i class="ic ic-arrow-left"></i>
                                <h2 class="post-nav-title">Support Vector Machines for classification</h2>
                            <p class="post-nav-excerpt">To whet your appetite for support vector machines, here's a quote from machine...</p>
                        </section>
                    </a>
                    <a class="post-nav-prev" href="./pandas-tips-and-tricks.html">
                        <section class="post-nav-teaser">
                            <i class="ic ic-arrow-right"></i>
                                <h2 class="post-nav-title">Getting started with Pandas</h2>
                            <p class="post-nav-excerpt">We have made use of Python's Pandas package in a variety of posts on the site. These...</p>
                        </section>
                    </a>
                    <div class="clear"></div>
                </aside>

            </div>
        </article>
    </main>
      <!-- TODO : Body class -->
    <div id="body-class" style="display: none;" class=""></div>

    <footer id="footer">
      <div class="inner">
        <section class="credits">
        </section>
      </div>
    </footer>
  </section>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script type="text/javascript" src="./theme/js/script.js"></script>

</body>
</html>